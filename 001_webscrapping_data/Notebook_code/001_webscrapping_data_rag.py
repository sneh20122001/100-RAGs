# -*- coding: utf-8 -*-
"""001_WebScrapping_Data_RAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cdQsJ8B0YMlxLMVJXBN8u377Fm784GfG
"""

!pip install langchain langchain_community langchainhub chromadb langchain-google-genai

from google.colab import userdata
import os
os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')

# Commented out IPython magic to ensure Python compatibility.
# %pip install -qU langchain_community beautifulsoup4

from langchain_community.document_loaders import WebBaseLoader

loader = WebBaseLoader(web_path=["https://www.educosys.com/course/genai"])

docs = loader.load()
print(docs)

# Commented out IPython magic to ensure Python compatibility.
# %pip install -qU langchain-text-splitters

from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000,
                                               chunk_overlap = 200)
splits = text_splitter.split_documents(docs)

print(splits[0])
print(splits[1])

print(len(splits))

# Commented out IPython magic to ensure Python compatibility.
# %pip install chromadb

from langchain.vectorstores import Chroma
from langchain_google_genai import GoogleGenerativeAIEmbeddings

embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")
vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)

print(vectorstore._collection.get())

retriever = vectorstore.as_retriever()

from langchain import hub
prompt = hub.pull("rlm/rag-prompt")

from langchain_google_genai import ChatGoogleGenerativeAI

llm = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash",
    temperature=0,
)

from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

def format_docs(docs):
  return "\n".join(doc.page_content for doc in docs)

rag_chain = ({"context":retriever | format_docs, "question":RunnablePassthrough()}
             | prompt
             | llm
             | StrOutputParser())

rag_chain.invoke("Are the recordings of the course available? For how long?")

rag_chain.invoke("Testimonials available ? ")

from langchain_core.runnables import RunnableLambda

def print_prompt(prompt_text):
  print("Prompt - ", prompt_text)
  return prompt_text

rag_chain_with_print = ({"context":retriever | format_docs, "question":RunnablePassthrough()}
             | prompt
             | RunnableLambda(print_prompt)
             | llm
             | StrOutputParser())

rag_chain_with_print.invoke("Is there doubt support ?")

