{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "23a1d4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adrta-011\\AppData\\Local\\Temp\\ipykernel_8376\\3154634806.py:11: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  for result in search.results():\n"
     ]
    }
   ],
   "source": [
    "import arxiv\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "search = arxiv.Search(\n",
    "    query=\"cat:cs.AI\",\n",
    "    max_results=500,\n",
    "    sort_by=arxiv.SortCriterion.SubmittedDate\n",
    ")\n",
    "\n",
    "docs = []\n",
    "for result in search.results():\n",
    "    metadata = {\n",
    "        \"title\": result.title,\n",
    "        \"authors\": \", \".join([author.name for author in result.authors]),\n",
    "        \"published\": result.published.strftime(\"%Y-%m-%d\"),\n",
    "        \"url\": result.entry_id\n",
    "    }\n",
    "    content = result.summary.strip()\n",
    "    doc = Document(page_content=content, metadata=metadata)\n",
    "    docs.append(doc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9bf8a181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'title': 'Evaluating Memory in LLM Agents via Incremental Multi-Turn Interactions', 'authors': 'Yuanzhe Hu, Yu Wang, Julian McAuley', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.05257v1'}, page_content='Recent benchmarks for Large Language Model (LLM) agents primarily focus on\\nevaluating reasoning, planning, and execution capabilities, while another\\ncritical component-memory, encompassing how agents memorize, update, and\\nretrieve long-term information-is under-evaluated due to the lack of\\nbenchmarks. We term agents with memory mechanisms as memory agents. In this\\npaper, we identify four core competencies essential for memory agents: accurate\\nretrieval, test-time learning, long-range understanding, and conflict\\nresolution. Existing datasets either rely on limited context lengths or are\\ntailored for static, long-context settings like book-based QA, which do not\\nreflect the interactive, multi-turn nature of memory agents that incrementally\\naccumulate information. Furthermore, no existing benchmarks cover all four\\ncompetencies. Therefore, we introduce MemoryAgentBench, a new benchmark\\nspecifically designed for memory agents. Our benchmark combines reformulated\\nexisting datasets with newly constructed ones, covering the above four memory\\ncompetencies, providing a systematic and challenging testbed for assessing\\nmemory quality. We evaluate a diverse set of memory agents, ranging from simple\\ncontext-based and retrieval-augmented generation (RAG) systems to advanced\\nagents with external memory modules and tool integration. Empirical results\\nreveal that current methods fall short of mastering all four competencies,\\nunderscoring the need for further research into comprehensive memory mechanisms\\nfor LLM agents.'),\n",
       " Document(metadata={'title': 'From Marginal to Joint Predictions: Evaluating Scene-Consistent Trajectory Prediction Approaches for Automated Driving', 'authors': 'Fabian Konstantinidis, Ariel Dallari Guerreiro, Raphael Trumpp, Moritz Sackmann, Ulrich Hofmann, Marco Caccamo, Christoph Stiller', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.05254v1'}, page_content=\"Accurate motion prediction of surrounding traffic participants is crucial for\\nthe safe and efficient operation of automated vehicles in dynamic environments.\\nMarginal prediction models commonly forecast each agent's future trajectories\\nindependently, often leading to sub-optimal planning decisions for an automated\\nvehicle. In contrast, joint prediction models explicitly account for the\\ninteractions between agents, yielding socially and physically consistent\\npredictions on a scene level. However, existing approaches differ not only in\\ntheir problem formulation but also in the model architectures and\\nimplementation details used, making it difficult to compare them. In this work,\\nwe systematically investigate different approaches to joint motion prediction,\\nincluding post-processing of the marginal predictions, explicitly training the\\nmodel for joint predictions, and framing the problem as a generative task. We\\nevaluate each approach in terms of prediction accuracy, multi-modality, and\\ninference efficiency, offering a comprehensive analysis of the strengths and\\nlimitations of each approach. Several prediction examples are available at\\nhttps://frommarginaltojointpred.github.io/.\"),\n",
       " Document(metadata={'title': 'Action Space Reduction Strategies for Reinforcement Learning in Autonomous Driving', 'authors': 'Elahe Delavari, Feeza Khan Khanzada, Jaerock Kwon', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.05251v1'}, page_content='Reinforcement Learning (RL) offers a promising framework for autonomous\\ndriving by enabling agents to learn control policies through interaction with\\nenvironments. However, large and high-dimensional action spaces often used to\\nsupport fine-grained control can impede training efficiency and increase\\nexploration costs. In this study, we introduce and evaluate two novel\\nstructured action space modification strategies for RL in autonomous driving:\\ndynamic masking and relative action space reduction. These approaches are\\nsystematically compared against fixed reduction schemes and full action space\\nbaselines to assess their impact on policy learning and performance. Our\\nframework leverages a multimodal Proximal Policy Optimization agent that\\nprocesses both semantic image sequences and scalar vehicle states. The proposed\\ndynamic and relative strategies incorporate real-time action masking based on\\ncontext and state transitions, preserving action consistency while eliminating\\ninvalid or suboptimal choices. Through comprehensive experiments across diverse\\ndriving routes, we show that action space reduction significantly improves\\ntraining stability and policy performance. The dynamic and relative schemes, in\\nparticular, achieve a favorable balance between learning speed, control\\nprecision, and generalization. These findings highlight the importance of\\ncontext-aware action space design for scalable and reliable RL in autonomous\\ndriving tasks.'),\n",
       " Document(metadata={'title': 'When Chain of Thought is Necessary, Language Models Struggle to Evade Monitors', 'authors': 'Scott Emmons, Erik Jenner, David K. Elson, Rif A. Saurous, Senthooran Rajamanoharan, Heng Chen, Irhum Shafkat, Rohin Shah', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.05246v1'}, page_content='While chain-of-thought (CoT) monitoring is an appealing AI safety defense,\\nrecent work on \"unfaithfulness\" has cast doubt on its reliability. These\\nfindings highlight an important failure mode, particularly when CoT acts as a\\npost-hoc rationalization in applications like auditing for bias. However, for\\nthe distinct problem of runtime monitoring to prevent severe harm, we argue the\\nkey property is not faithfulness but monitorability. To this end, we introduce\\na conceptual framework distinguishing CoT-as-rationalization from\\nCoT-as-computation. We expect that certain classes of severe harm will require\\ncomplex, multi-step reasoning that necessitates CoT-as-computation. Replicating\\nthe experimental setups of prior work, we increase the difficulty of the bad\\nbehavior to enforce this necessity condition; this forces the model to expose\\nits reasoning, making it monitorable. We then present methodology guidelines to\\nstress-test CoT monitoring against deliberate evasion. Applying these\\nguidelines, we find that models can learn to obscure their intentions, but only\\nwhen given significant help, such as detailed human-written strategies or\\niterative optimization against the monitor. We conclude that, while not\\ninfallible, CoT monitoring offers a substantial layer of defense that requires\\nactive protection and continued stress-testing.'),\n",
       " Document(metadata={'title': 'Modeling Latent Partner Strategies for Adaptive Zero-Shot Human-Agent Collaboration', 'authors': 'Benjamin Li, Shuyang Shi, Lucia Romero, Huao Li, Yaqi Xie, Woojun Kim, Stefanos Nikolaidis, Michael Lewis, Katia Sycara, Simon Stepputtis', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.05244v1'}, page_content='In collaborative tasks, being able to adapt to your teammates is a necessary\\nrequirement for success. When teammates are heterogeneous, such as in\\nhuman-agent teams, agents need to be able to observe, recognize, and adapt to\\ntheir human partners in real time. This becomes particularly challenging in\\ntasks with time pressure and complex strategic spaces where the dynamics can\\nchange rapidly. In this work, we introduce TALENTS, a strategy-conditioned\\ncooperator framework that learns to represent, categorize, and adapt to a range\\nof partner strategies, enabling ad-hoc teamwork. Our approach utilizes a\\nvariational autoencoder to learn a latent strategy space from trajectory data.\\nThis latent space represents the underlying strategies that agents employ.\\nSubsequently, the system identifies different types of strategy by clustering\\nthe data. Finally, a cooperator agent is trained to generate partners for each\\ntype of strategy, conditioned on these clusters. In order to adapt to\\npreviously unseen partners, we leverage a fixed-share regret minimization\\nalgorithm that infers and adjusts the estimated partner strategy dynamically.\\nWe assess our approach in a customized version of the Overcooked environment,\\nposing a challenging cooperative cooking task that demands strong coordination\\nacross a wide range of possible strategies. Using an online user study, we show\\nthat our agent outperforms current baselines when working with unfamiliar human\\npartners.'),\n",
       " Document(metadata={'title': \"SciMaster: Towards General-Purpose Scientific AI Agents, Part I. X-Master as Foundation: Can We Lead on Humanity's Last Exam?\", 'authors': 'Jingyi Chai, Shuo Tang, Rui Ye, Yuwen Du, Xinyu Zhu, Mengcheng Zhou, Yanfeng Wang, Weinan E, Siheng Chen', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.05241v1'}, page_content=\"The rapid advancements of AI agents have ignited the long-held ambition of\\nleveraging them to accelerate scientific discovery. Achieving this goal\\nrequires a deep understanding of the frontiers of human knowledge. As such,\\nHumanity's Last Exam (HLE) provides an exceptionally challenging touchstone for\\nevaluating scientific AI agents. In this work, we aim to construct the\\nfoundational architecture for general-purpose agents and validate the\\ncapabilities through leading performance on HLE. To achieve this, we introduce\\nX-Master, a tool-augmented reasoning agent designed to emulate human\\nresearchers by interacting flexibly with external tools during its reasoning\\nprocess. This agent, guided by the conceptualization of code as an interaction\\nlanguage, can flexibly leverage built-in Python libraries and our customized\\ntools to augment the reasoning. We further scale its capabilities through\\nX-Masters, a scattered-and-stacked agentic workflow that systematically\\nenhances breadth and depth of reasoning. Our open-source solution, X-Masters,\\nsets a new state-of-the-art record on HLE with a score of 32.1%, surpassing\\nOpenAI's and Google's Deep Research (26.6% and 26.9%) and becoming the first to\\nexceed the 30% threshold. This work allows us to gain a deeper understanding of\\ncomplex task-solving and accumulates valuable experience that can inform future\\nadvancements, guiding subsequent model training.\"),\n",
       " Document(metadata={'title': 'CTA: Cross-Task Alignment for Better Test Time Training', 'authors': 'Samuel Barbeau, Pedram Fekri, David Osowiechi, Ali Bahri, Moslem YazdanpanahMasih Aminbeidokhti, Christian Desrosiers', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.05221v1'}, page_content='Deep learning models have demonstrated exceptional performance across a wide\\nrange of computer vision tasks. However, their performance often degrades\\nsignificantly when faced with distribution shifts, such as domain or dataset\\nchanges. Test-Time Training (TTT) has emerged as an effective method to enhance\\nmodel robustness by incorporating an auxiliary unsupervised task during\\ntraining and leveraging it for model updates at test time. In this work, we\\nintroduce CTA (Cross-Task Alignment), a novel approach for improving TTT.\\nUnlike existing TTT methods, CTA does not require a specialized model\\narchitecture and instead takes inspiration from the success of multi-modal\\ncontrastive learning to align a supervised encoder with a self-supervised one.\\nThis process enforces alignment between the learned representations of both\\nmodels, thereby mitigating the risk of gradient interference, preserving the\\nintrinsic robustness of self-supervised learning and enabling more semantically\\nmeaningful updates at test-time. Experimental results demonstrate substantial\\nimprovements in robustness and generalization over the state-of-the-art on\\nseveral benchmark datasets.'),\n",
       " Document(metadata={'title': 'All in One: Visual-Description-Guided Unified Point Cloud Segmentation', 'authors': 'Zongyan Han, Mohamed El Amine Boudjoghra, Jiahua Dong, Jinhong Wang, Rao Muhammad Anwer', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.05211v1'}, page_content='Unified segmentation of 3D point clouds is crucial for scene understanding,\\nbut is hindered by its sparse structure, limited annotations, and the challenge\\nof distinguishing fine-grained object classes in complex environments. Existing\\nmethods often struggle to capture rich semantic and contextual information due\\nto limited supervision and a lack of diverse multimodal cues, leading to\\nsuboptimal differentiation of classes and instances. To address these\\nchallenges, we propose VDG-Uni3DSeg, a novel framework that integrates\\npre-trained vision-language models (e.g., CLIP) and large language models\\n(LLMs) to enhance 3D segmentation. By leveraging LLM-generated textual\\ndescriptions and reference images from the internet, our method incorporates\\nrich multimodal cues, facilitating fine-grained class and instance separation.\\nWe further design a Semantic-Visual Contrastive Loss to align point features\\nwith multimodal queries and a Spatial Enhanced Module to model scene-wide\\nrelationships efficiently. Operating within a closed-set paradigm that utilizes\\nmultimodal knowledge generated offline, VDG-Uni3DSeg achieves state-of-the-art\\nresults in semantic, instance, and panoptic segmentation, offering a scalable\\nand practical solution for 3D understanding. Our code is available at\\nhttps://github.com/Hanzy1996/VDG-Uni3DSeg.'),\n",
       " Document(metadata={'title': 'MedGemma Technical Report', 'authors': 'Andrew Sellergren, Sahar Kazemzadeh, Tiam Jaroensri, Atilla Kiraly, Madeleine Traverse, Timo Kohlberger, Shawn Xu, Fayaz Jamil, Cían Hughes, Charles Lau, Justin Chen, Fereshteh Mahvar, Liron Yatziv, Tiffany Chen, Bram Sterling, Stefanie Anna Baby, Susanna Maria Baby, Jeremy Lai, Samuel Schmidgall, Lu Yang, Kejia Chen, Per Bjornsson, Shashir Reddy, Ryan Brush, Kenneth Philbrick, Howard Hu, Howard Yang, Richa Tiwari, Sunny Jansen, Preeti Singh, Yun Liu, Shekoofeh Azizi, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Riviere, Louis Rouillard, Thomas Mesnard, Geoffrey Cideron, Jean-bastien Grill, Sabela Ramos, Edouard Yvinec, Michelle Casbon, Elena Buchatskaya, Jean-Baptiste Alayrac, Dmitry, Lepikhin, Vlad Feinberg, Sebastian Borgeaud, Alek Andreev, Cassidy Hardin, Robert Dadashi, Léonard Hussenot, Armand Joulin, Olivier Bachem, Yossi Matias, Katherine Chou, Avinatan Hassidim, Kavi Goel, Clement Farabet, Joelle Barral, Tris Warkentin, Jonathon Shlens, David Fleet, Victor Cotruta, Omar Sanseviero, Gus Martins, Phoebe Kirk, Anand Rao, Shravya Shetty, David F. Steiner, Can Kirmizibayrak, Rory Pilgrim, Daniel Golden, Lin Yang', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.05201v1'}, page_content=\"Artificial intelligence (AI) has significant potential in healthcare\\napplications, but its training and deployment faces challenges due to\\nhealthcare's diverse data, complex tasks, and the need to preserve privacy.\\nFoundation models that perform well on medical tasks and require less\\ntask-specific tuning data are critical to accelerate the development of\\nhealthcare AI applications. We introduce MedGemma, a collection of medical\\nvision-language foundation models based on Gemma 3 4B and 27B. MedGemma\\ndemonstrates advanced medical understanding and reasoning on images and text,\\nsignificantly exceeding the performance of similar-sized generative models and\\napproaching the performance of task-specific models, while maintaining the\\ngeneral capabilities of the Gemma 3 base models. For out-of-distribution tasks,\\nMedGemma achieves 2.6-10% improvement on medical multimodal question answering,\\n15.5-18.1% improvement on chest X-ray finding classification, and 10.8%\\nimprovement on agentic evaluations compared to the base models. Fine-tuning\\nMedGemma further improves performance in subdomains, reducing errors in\\nelectronic health record information retrieval by 50% and reaching comparable\\nperformance to existing specialized state-of-the-art methods for pneumothorax\\nclassification and histopathology patch classification. We additionally\\nintroduce MedSigLIP, a medically-tuned vision encoder derived from SigLIP.\\nMedSigLIP powers the visual understanding capabilities of MedGemma and as an\\nencoder achieves comparable or better performance than specialized medical\\nimage encoders. Taken together, the MedGemma collection provides a strong\\nfoundation of medical image and text capabilities, with potential to\\nsignificantly accelerate medical research and development of downstream\\napplications. The MedGemma collection, including tutorials and model weights,\\ncan be found at https://goo.gle/medgemma.\"),\n",
       " Document(metadata={'title': 'EmbodieDreamer: Advancing Real2Sim2Real Transfer for Policy Training via Embodied World Modeling', 'authors': 'Boyuan Wang, Xinpan Meng, Xiaofeng Wang, Zheng Zhu, Angen Ye, Yang Wang, Zhiqin Yang, Chaojun Ni, Guan Huang, Xingang Wang', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.05198v1'}, page_content='The rapid advancement of Embodied AI has led to an increasing demand for\\nlarge-scale, high-quality real-world data. However, collecting such embodied\\ndata remains costly and inefficient. As a result, simulation environments have\\nbecome a crucial surrogate for training robot policies. Yet, the significant\\nReal2Sim2Real gap remains a critical bottleneck, particularly in terms of\\nphysical dynamics and visual appearance. To address this challenge, we propose\\nEmbodieDreamer, a novel framework that reduces the Real2Sim2Real gap from both\\nthe physics and appearance perspectives. Specifically, we propose PhysAligner,\\na differentiable physics module designed to reduce the Real2Sim physical gap.\\nIt jointly optimizes robot-specific parameters such as control gains and\\nfriction coefficients to better align simulated dynamics with real-world\\nobservations. In addition, we introduce VisAligner, which incorporates a\\nconditional video diffusion model to bridge the Sim2Real appearance gap by\\ntranslating low-fidelity simulated renderings into photorealistic videos\\nconditioned on simulation states, enabling high-fidelity visual transfer.\\nExtensive experiments validate the effectiveness of EmbodieDreamer. The\\nproposed PhysAligner reduces physical parameter estimation error by 3.74%\\ncompared to simulated annealing methods while improving optimization speed by\\n89.91\\\\%. Moreover, training robot policies in the generated photorealistic\\nenvironment leads to a 29.17% improvement in the average task success rate\\nacross real-world tasks after reinforcement learning. Code, model and data will\\nbe publicly available.'),\n",
       " Document(metadata={'title': 'Train-before-Test Harmonizes Language Model Rankings', 'authors': 'Guanhua Zhang, Ricardo Dominguez-Olmedo, Moritz Hardt', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.05195v1'}, page_content='Existing language model benchmarks provide contradictory model rankings, even\\nfor benchmarks that aim to capture similar skills. This dilemma of conflicting\\nrankings hampers model selection, clouds model comparisons, and adds confusion\\nto a growing ecosystem of competing models. Recent work attributed ranking\\ndisagreement to the phenomenon of training on the test task: As released,\\ndifferent models exhibit a different level of preparation for any given test\\ntask. A candidate solution to the problem is train-before-test: Give each model\\nthe same benchmark-specific finetuning before evaluation. Our primary\\ncontribution is a broad empirical evaluation of train-before-test across 24\\nbenchmarks and 61 models. We show that train-before-test significantly improves\\nranking agreement consistently across all benchmarks. Whereas rankings have\\nlittle external validity to start with, they enjoy a significant degree of\\nexternal validity when applying train-before-test: Model rankings transfer\\ngracefully from one benchmark to the other. Even within the same model family,\\ntrain-before-test reduces strong ranking disagreement to near-perfect\\nagreement. In addition, train-before-test reduces the model-score matrix to\\nessentially rank one, revealing new insights into the latent factors of\\nbenchmark performance. Our work supports the recommendation to make\\ntrain-before-test a default component of LLM benchmarking.'),\n",
       " Document(metadata={'title': 'Infrastructuring Contestability: A Framework for Community-Defined AI Value Pluralism', 'authors': 'Andreas Mayer', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.05187v1'}, page_content=\"The proliferation of AI-driven systems presents a fundamental challenge to\\nHuman-Computer Interaction (HCI) and Computer-Supported Cooperative Work\\n(CSCW), often diminishing user agency and failing to account for value\\npluralism. Current approaches to value alignment, which rely on centralized,\\ntop-down definitions, lack the mechanisms for meaningful contestability. This\\nleaves users and communities unable to challenge or shape the values embedded\\nin the systems that govern their digital lives, creating a crisis of legitimacy\\nand trust. This paper introduces Community-Defined AI Value Pluralism (CDAVP),\\na socio-technical framework that addresses this gap. It reframes the design\\nproblem from achieving a single aligned state to infrastructuring a dynamic\\necosystem for value deliberation and application. At its core, CDAVP enables\\ndiverse, self-organizing communities to define and maintain explicit value\\nprofiles - rich, machine-readable representations that can encompass not only\\npreferences but also community-specific rights and duties. These profiles are\\nthen contextually activated by the end-user, who retains ultimate control\\n(agency) over which values guide the AI's behavior. AI applications, in turn,\\nare designed to transparently interpret these profiles and moderate conflicts,\\nadhering to a set of non-negotiable, democratically-legitimated meta-rules. The\\ndesigner's role shifts from crafting static interfaces to becoming an architect\\nof participatory ecosystems. We argue that infrastructuring for pluralism is a\\nnecessary pathway toward achieving robust algorithmic accountability and\\ngenuinely contestable, human-centric AI.\"),\n",
       " Document(metadata={'title': 'CREW-WILDFIRE: Benchmarking Agentic Multi-Agent Collaborations at Scale', 'authors': 'Jonathan Hyun, Nicholas R Waytowich, Boyuan Chen', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.05178v1'}, page_content='Despite rapid progress in large language model (LLM)-based multi-agent\\nsystems, current benchmarks fall short in evaluating their scalability,\\nrobustness, and coordination capabilities in complex, dynamic, real-world\\ntasks. Existing environments typically focus on small-scale, fully observable,\\nor low-complexity domains, limiting their utility for developing and assessing\\nnext-generation multi-agent Agentic AI frameworks. We introduce CREW-Wildfire,\\nan open-source benchmark designed to close this gap. Built atop the human-AI\\nteaming CREW simulation platform, CREW-Wildfire offers procedurally generated\\nwildfire response scenarios featuring large maps, heterogeneous agents, partial\\nobservability, stochastic dynamics, and long-horizon planning objectives. The\\nenvironment supports both low-level control and high-level natural language\\ninteractions through modular Perception and Execution modules. We implement and\\nevaluate several state-of-the-art LLM-based multi-agent Agentic AI frameworks,\\nuncovering significant performance gaps that highlight the unsolved challenges\\nin large-scale coordination, communication, spatial reasoning, and long-horizon\\nplanning under uncertainty. By providing more realistic complexity, scalable\\narchitecture, and behavioral evaluation metrics, CREW-Wildfire establishes a\\ncritical foundation for advancing research in scalable multi-agent Agentic\\nintelligence. All code, environments, data, and baselines will be released to\\nsupport future research in this emerging domain.'),\n",
       " Document(metadata={'title': 'OpenS2S: Advancing Open-Source End-to-End Empathetic Large Speech Language Model', 'authors': 'Chen Wang, Tianyu Peng, Wen Yang, Yinan Bai, Guangfu Wang, Jun Lin, Lanpeng Jia, Lingxiang Wu, Jinqiao Wang, Chengqing Zong, Jiajun Zhang', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.05177v1'}, page_content='Empathetic interaction is a cornerstone of human-machine communication, due\\nto the need for understanding speech enriched with paralinguistic cues and\\ngenerating emotional and expressive responses. However, the most powerful\\nempathetic LSLMs are increasingly closed off, leaving the crucial details about\\nthe architecture, data and development opaque to researchers. Given the\\ncritical need for transparent research into the LSLMs and empathetic behavior,\\nwe present OpenS2S, a fully open-source, transparent and end-to-end LSLM\\ndesigned to enable empathetic speech interactions. Based on our empathetic\\nspeech-to-text model BLSP-Emo, OpenS2S further employs a streaming interleaved\\ndecoding architecture to achieve low-latency speech generation. To facilitate\\nend-to-end training, OpenS2S incorporates an automated data construction\\npipeline that synthesizes diverse, high-quality empathetic speech dialogues at\\nlow cost. By leveraging large language models to generate empathetic content\\nand controllable text-to-speech systems to introduce speaker and emotional\\nvariation, we construct a scalable training corpus with rich paralinguistic\\ndiversity and minimal human supervision. We release the fully open-source\\nOpenS2S model, including the dataset, model weights, pre-training and\\nfine-tuning codes, to empower the broader research community and accelerate\\ninnovation in empathetic speech systems. The project webpage can be accessed at\\nhttps://casia-lm.github.io/OpenS2S'),\n",
       " Document(metadata={'title': 'Critiques of World Models', 'authors': 'Eric Xing, Mingkai Deng, Jinyu Hou, Zhiting Hu', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.05169v1'}, page_content='World Model, the supposed algorithmic surrogate of the real-world environment\\nwhich biological agents experience with and act upon, has been an emerging\\ntopic in recent years because of the rising needs to develop virtual agents\\nwith artificial (general) intelligence. There has been much debate on what a\\nworld model really is, how to build it, how to use it, and how to evaluate it.\\nIn this essay, starting from the imagination in the famed Sci-Fi classic Dune,\\nand drawing inspiration from the concept of \"hypothetical thinking\" in\\npsychology literature, we offer critiques of several schools of thoughts on\\nworld modeling, and argue the primary goal of a world model to be simulating\\nall actionable possibilities of the real world for purposeful reasoning and\\nacting. Building on the critiques, we propose a new architecture for a\\ngeneral-purpose world model, based on hierarchical, multi-level, and mixed\\ncontinuous/discrete representations, and a generative and self-supervision\\nlearning framework, with an outlook of a Physical, Agentic, and Nested (PAN)\\nAGI system enabled by such a model.'),\n",
       " Document(metadata={'title': 'LAID: Lightweight AI-Generated Image Detection in Spatial and Spectral Domains', 'authors': 'Nicholas Chivaran, Jianbing Ni', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.05162v1'}, page_content='The recent proliferation of photorealistic AI-generated images (AIGI) has\\nraised urgent concerns about their potential misuse, particularly on social\\nmedia platforms. Current state-of-the-art AIGI detection methods typically rely\\non large, deep neural architectures, creating significant computational\\nbarriers to real-time, large-scale deployment on platforms like social media.\\nTo challenge this reliance on computationally intensive models, we introduce\\nLAID, the first framework -- to our knowledge -- that benchmarks and evaluates\\nthe detection performance and efficiency of off-the-shelf lightweight neural\\nnetworks. In this framework, we comprehensively train and evaluate selected\\nmodels on a representative subset of the GenImage dataset across spatial,\\nspectral, and fusion image domains. Our results demonstrate that lightweight\\nmodels can achieve competitive accuracy, even under adversarial conditions,\\nwhile incurring substantially lower memory and computation costs compared to\\ncurrent state-of-the-art methods. This study offers valuable insight into the\\ntrade-off between efficiency and performance in AIGI detection and lays a\\nfoundation for the development of practical, scalable, and trustworthy\\ndetection systems. The source code of LAID can be found at:\\nhttps://github.com/nchivar/LAID.'),\n",
       " Document(metadata={'title': 'AI Generated Text Detection Using Instruction Fine-tuned Large Language and Transformer-Based Models', 'authors': 'Chinnappa Guggilla, Budhaditya Roy, Trupti Ramdas Chavan, Abdul Rahman, Edward Bowen', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.05157v1'}, page_content='Large Language Models (LLMs) possess an extraordinary capability to produce\\ntext that is not only coherent and contextually relevant but also strikingly\\nsimilar to human writing. They adapt to various styles and genres, producing\\ncontent that is both grammatically correct and semantically meaningful.\\nRecently, LLMs have been misused to create highly realistic phishing emails,\\nspread fake news, generate code to automate cyber crime, and write fraudulent\\nscientific articles. Additionally, in many real-world applications, the\\ngenerated content including style and topic and the generator model are not\\nknown beforehand. The increasing prevalence and sophistication of artificial\\nintelligence (AI)-generated texts have made their detection progressively more\\nchallenging. Various attempts have been made to distinguish machine-generated\\ntext from human-authored content using linguistic, statistical, machine\\nlearning, and ensemble-based approaches. This work focuses on two primary\\nobjectives Task-A, which involves distinguishing human-written text from\\nmachine-generated text, and Task-B, which attempts to identify the specific LLM\\nmodel responsible for the generation. Both of these tasks are based on fine\\ntuning of Generative Pre-trained Transformer (GPT_4o-mini), Large Language\\nModel Meta AI (LLaMA) 3 8B, and Bidirectional Encoder Representations from\\nTransformers (BERT). The fine-tuned version of GPT_4o-mini and the BERT model\\nhas achieved accuracies of 0.9547 for Task-A and 0.4698 for Task-B.'),\n",
       " Document(metadata={'title': 'Effects of Unplanned Incoming Flights on Airport Relief Processes after a Major Natural Disaster', 'authors': 'Luka Van de Sype, Matthieu Vert, Alexei Sharpanskykh, Seyed Sahand Mohammadi Ziabari', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.05150v1'}, page_content=\"The severity of natural disasters is increasing every year, impacting many\\npeople's lives. During the response phase of disasters, airports are important\\nhubs where relief aid arrives and people need to be evacuated. However, the\\nairport often forms a bottleneck in these relief operations due to the sudden\\nneed for increased capacity. Limited research has been done on the operational\\nside of airport disaster management. Experts identify the main problems as,\\nfirst, the asymmetry of information between the airport and incoming flights,\\nand second, the lack of resources. The goal of this research is to understand\\nthe effects of incomplete knowledge of incoming flights with different resource\\nallocation strategies on the performance of cargo handling operations at an\\nairport after a natural disaster. An agent-based model is created, implementing\\nrealistic offloading strategies with different degrees of information\\nuncertainty. Model calibration and verification are performed with experts in\\nthe field. The model performance is measured by the average turnaround time,\\nwhich is divided into offloading time, boarding time, and cumulative waiting\\ntimes. The results show that the effects of one unplanned aircraft are\\nnegligible. However, all waiting times increase with more arriving unplanned\\naircraft.\"),\n",
       " Document(metadata={'title': 'OGF: An Online Gradient Flow Method for Optimizing the Statistical Steady-State Time Averages of Unsteady Turbulent Flows', 'authors': 'Tom Hickling, Jonathan F. MacArt, Justin Sirignano, Den Waidmann', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.05149v1'}, page_content='Turbulent flows are chaotic and unsteady, but their statistical distribution\\nconverges to a statistical steady state. Engineering quantities of interest\\ntypically take the form of time-average statistics such as $ \\\\frac{1}{t}\\n\\\\int_0^t f ( u(x,\\\\tau; \\\\theta) ) d\\\\tau \\\\overset{t \\\\rightarrow\\n\\\\infty}{\\\\rightarrow} F(x; \\\\theta)$, where $u(x,t; \\\\theta)$ are solutions of the\\nNavier--Stokes equations with parameters $\\\\theta$. Optimizing over $F(x;\\n\\\\theta)$ has many engineering applications including geometric optimization,\\nflow control, and closure modeling. However, this remains an open challenge, as\\nexisting computational approaches are incapable of scaling to physically\\nrepresentative numbers of grid points. The fundamental obstacle is the\\nchaoticity of turbulent flows: gradients calculated with the adjoint method\\ndiverge exponentially as $t \\\\rightarrow \\\\infty$.\\n  We develop a new online gradient-flow (OGF) method that is scalable to large\\ndegree-of-freedom systems and enables optimizing for the steady-state\\nstatistics of chaotic, unsteady, turbulence-resolving simulations. The method\\nforward-propagates an online estimate for the gradient of $F(x; \\\\theta)$ while\\nsimultaneously performing online updates of the parameters $\\\\theta$. A key\\nfeature is the fully online nature of the algorithm to facilitate faster\\noptimization progress and its combination with a finite-difference estimator to\\navoid the divergence of gradients due to chaoticity. The proposed OGF method is\\ndemonstrated for optimizations over three chaotic ordinary and partial\\ndifferential equations: the Lorenz-63 equation, the Kuramoto--Sivashinsky\\nequation, and Navier--Stokes solutions of compressible, forced, homogeneous\\nisotropic turbulence. In each case, the OGF method successfully reduces the\\nloss based on $F(x; \\\\theta)$ by several orders of magnitude and accurately\\nrecovers the optimal parameters.'),\n",
       " Document(metadata={'title': 'GIST: Cross-Domain Click-Through Rate Prediction via Guided Content-Behavior Distillation', 'authors': 'Wei Xu, Haoran Li, Baoyuan Ou, Lai Xu, Yingjie Qin, Ruilong Su, Ruiwen Xu', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.05142v1'}, page_content='Cross-domain Click-Through Rate prediction aims to tackle the data sparsity\\nand the cold start problems in online advertising systems by transferring\\nknowledge from source domains to a target domain. Most existing methods rely on\\noverlapping users to facilitate this transfer, often focusing on joint training\\nor pre-training with fine-tuning approach to connect the source and target\\ndomains. However, in real-world industrial settings, joint training struggles\\nto learn optimal representations with different distributions, and pre-training\\nwith fine-tuning is not well-suited for continuously integrating new data. To\\naddress these issues, we propose GIST, a cross-domain lifelong sequence model\\nthat decouples the training processes of the source and target domains. Unlike\\nprevious methods that search lifelong sequences in the source domains using\\nonly content or behavior signals or their simple combinations, we innovatively\\nintroduce a Content-Behavior Joint Training Module (CBJT), which aligns\\ncontent-behavior distributions and combines them with guided information to\\nfacilitate a more stable representation. Furthermore, we develop an Asymmetric\\nSimilarity Integration strategy (ASI) to augment knowledge transfer through\\nsimilarity computation. Extensive experiments demonstrate the effectiveness of\\nGIST, surpassing SOTA methods on offline evaluations and an online A/B test.\\nDeployed on the Xiaohongshu (RedNote) platform, GIST effectively enhances\\nonline ads system performance at scale, serving hundreds of millions of daily\\nactive users.'),\n",
       " Document(metadata={'title': 'Interpretable Mnemonic Generation for Kanji Learning via Expectation-Maximization', 'authors': 'Jaewook Lee, Alexander Scarlatos, Andrew Lan', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.05137v1'}, page_content='Learning Japanese vocabulary is a challenge for learners from Roman alphabet\\nbackgrounds due to script differences. Japanese combines syllabaries like\\nhiragana with kanji, which are logographic characters of Chinese origin. Kanji\\nare also complicated due to their complexity and volume. Keyword mnemonics are\\na common strategy to aid memorization, often using the compositional structure\\nof kanji to form vivid associations. Despite recent efforts to use large\\nlanguage models (LLMs) to assist learners, existing methods for LLM-based\\nkeyword mnemonic generation function as a black box, offering limited\\ninterpretability. We propose a generative framework that explicitly models the\\nmnemonic construction process as driven by a set of common rules, and learn\\nthem using a novel Expectation-Maximization-type algorithm. Trained on\\nlearner-authored mnemonics from an online platform, our method learns latent\\nstructures and compositional rules, enabling interpretable and systematic\\nmnemonics generation. Experiments show that our method performs well in the\\ncold-start setting for new learners while providing insight into the mechanisms\\nbehind effective mnemonic creation.'),\n",
       " Document(metadata={'title': 'An Evaluation of Large Language Models on Text Summarization Tasks Using Prompt Engineering Techniques', 'authors': 'Walid Mohamed Aly, Taysir Hassan A. Soliman, Amr Mohamed AbdelAziz', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.05123v1'}, page_content='Large Language Models (LLMs) continue to advance natural language processing\\nwith their ability to generate human-like text across a range of tasks. Despite\\nthe remarkable success of LLMs in Natural Language Processing (NLP), their\\nperformance in text summarization across various domains and datasets has not\\nbeen comprehensively evaluated. At the same time, the ability to summarize text\\neffectively without relying on extensive training data has become a crucial\\nbottleneck. To address these issues, we present a systematic evaluation of six\\nLLMs across four datasets: CNN/Daily Mail and NewsRoom (news), SAMSum (dialog),\\nand ArXiv (scientific). By leveraging prompt engineering techniques including\\nzero-shot and in-context learning, our study evaluates the performance using\\nthe ROUGE and BERTScore metrics. In addition, a detailed analysis of inference\\ntimes is conducted to better understand the trade-off between summarization\\nquality and computational efficiency. For Long documents, introduce a\\nsentence-based chunking strategy that enables LLMs with shorter context windows\\nto summarize extended inputs in multiple stages. The findings reveal that while\\nLLMs perform competitively on news and dialog tasks, their performance on long\\nscientific documents improves significantly when aided by chunking strategies.\\nIn addition, notable performance variations were observed based on model\\nparameters, dataset properties, and prompt design. These results offer\\nactionable insights into how different LLMs behave across task types,\\ncontributing to ongoing research in efficient, instruction-based NLP systems.'),\n",
       " Document(metadata={'title': 'LVM4CSI: Enabling Direct Application of Pre-Trained Large Vision Models for Wireless Channel Tasks', 'authors': 'Jiajia Guo, Peiwen Jiang, Chao-Kai Wen, Shi Jin, Jun Zhang', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.05121v1'}, page_content='Accurate channel state information (CSI) is critical to the performance of\\nwireless communication systems, especially with the increasing scale and\\ncomplexity introduced by 5G and future 6G technologies. While artificial\\nintelligence (AI) offers a promising approach to CSI acquisition and\\nutilization, existing methods largely depend on task-specific neural networks\\n(NNs) that require expert-driven design and large training datasets, limiting\\ntheir generalizability and practicality. To address these challenges, we\\npropose LVM4CSI, a general and efficient framework that leverages the\\nstructural similarity between CSI and computer vision (CV) data to directly\\napply large vision models (LVMs) pre-trained on extensive CV datasets to\\nwireless tasks without any fine-tuning, in contrast to large language\\nmodel-based methods that generally necessitate fine-tuning. LVM4CSI maps CSI\\ntasks to analogous CV tasks, transforms complex-valued CSI into visual formats\\ncompatible with LVMs, and integrates lightweight trainable layers to adapt\\nextracted features to specific communication objectives. We validate LVM4CSI\\nthrough three representative case studies, including channel estimation, human\\nactivity recognition, and user localization. Results demonstrate that LVM4CSI\\nachieves comparable or superior performance to task-specific NNs, including an\\nimprovement exceeding 9.61 dB in channel estimation and approximately 40%\\nreduction in localization error. Furthermore, it significantly reduces the\\nnumber of trainable parameters and eliminates the need for task-specific NN\\ndesign.'),\n",
       " Document(metadata={'title': 'VerifyLLM: LLM-Based Pre-Execution Task Plan Verification for Robots', 'authors': 'Danil S. Grigorev, Alexey K. Kovalev, Aleksandr I. Panov', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.05118v1'}, page_content='In the field of robotics, researchers face a critical challenge in ensuring\\nreliable and efficient task planning. Verifying high-level task plans before\\nexecution significantly reduces errors and enhance the overall performance of\\nthese systems. In this paper, we propose an architecture for automatically\\nverifying high-level task plans before their execution in simulator or\\nreal-world environments. Leveraging Large Language Models (LLMs), our approach\\nconsists of two key steps: first, the conversion of natural language\\ninstructions into Linear Temporal Logic (LTL), followed by a comprehensive\\nanalysis of action sequences. The module uses the reasoning capabilities of the\\nLLM to evaluate logical coherence and identify potential gaps in the plan.\\nRigorous testing on datasets of varying complexity demonstrates the broad\\napplicability of the module to household tasks. We contribute to improving the\\nreliability and efficiency of task planning and addresses the critical need for\\nrobust pre-execution verification in autonomous systems. The code is available\\nat https://verifyllm.github.io.'),\n",
       " Document(metadata={'title': 'Rule Learning for Knowledge Graph Reasoning under Agnostic Distribution Shift', 'authors': 'Shixuan Liu, Yue He, Yunfei Wang, Hao Zou, Haoxiang Cheng, Wenjing Yang, Peng Cui, Zhong Liu', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.05110v1'}, page_content=\"Knowledge graph (KG) reasoning remains a critical research area focused on\\ninferring missing knowledge by analyzing relationships among observed facts.\\nDespite its success, a key limitation of existing KG reasoning methods is their\\ndependence on the I.I.D assumption. This assumption can easily be violated due\\nto unknown sample selection bias during training or agnostic distribution\\nshifts during testing, significantly compromising model performance and\\nreliability. To facilitate the deployment of KG reasoning in wild environments,\\nthis study investigates learning logical rules from KGs affected by unknown\\nselection bias. Additionally, we address test sets with agnostic distribution\\nshifts, formally defining this challenge as out-of-distribution (OOD) KG\\nreasoning-a previously underexplored problem. To solve the issue, we propose\\nthe Stable Rule Learning (StableRule) framework, an end-to-end methodology that\\nintegrates feature decorrelation with rule learning network, to enhance OOD\\ngeneralization performance. By leveraging feature decorrelation, the StableRule\\nframework mitigates the adverse effects of covariate shifts arising in OOD\\nscenarios, thereby improving the robustness of the rule learning component in\\neffectively deriving logical rules. Extensive experiments on seven benchmark\\nKGs demonstrate the framework's superior effectiveness and stability across\\ndiverse heterogeneous environments, underscoring its practical significance for\\nreal-world applications.\"),\n",
       " Document(metadata={'title': 'Reviving Cultural Heritage: A Novel Approach for Comprehensive Historical Document Restoration', 'authors': 'Yuyi Zhang, Peirong Zhang, Zhenhua Yang, Pengyu Yan, Yongxin Shi, Pengwei Liu, Fengjun Guo, Lianwen Jin', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.05108v1'}, page_content=\"Historical documents represent an invaluable cultural heritage, yet have\\nundergone significant degradation over time through tears, water erosion, and\\noxidation. Existing Historical Document Restoration (HDR) methods primarily\\nfocus on single modality or limited-size restoration, failing to meet practical\\nneeds. To fill this gap, we present a full-page HDR dataset (FPHDR) and a novel\\nautomated HDR solution (AutoHDR). Specifically, FPHDR comprises 1,633 real and\\n6,543 synthetic images with character-level and line-level locations, as well\\nas character annotations in different damage grades. AutoHDR mimics historians'\\nrestoration workflows through a three-stage approach: OCR-assisted damage\\nlocalization, vision-language context text prediction, and patch autoregressive\\nappearance restoration. The modular architecture of AutoHDR enables seamless\\nhuman-machine collaboration, allowing for flexible intervention and\\noptimization at each restoration stage. Experiments demonstrate AutoHDR's\\nremarkable performance in HDR. When processing severely damaged documents, our\\nmethod improves OCR accuracy from 46.83\\\\% to 84.05\\\\%, with further enhancement\\nto 94.25\\\\% through human-machine collaboration. We believe this work represents\\na significant advancement in automated historical document restoration and\\ncontributes substantially to cultural heritage preservation. The model and\\ndataset are available at https://github.com/SCUT-DLVCLab/AutoHDR.\"),\n",
       " Document(metadata={'title': 'PRING: Rethinking Protein-Protein Interaction Prediction from Pairs to Graphs', 'authors': 'Xinzhe Zheng, Hao Du, Fanding Xu, Jinzhe Li, Zhiyuan Liu, Wenkang Wang, Tao Chen, Wanli Ouyang, Stan Z. Li, Yan Lu, Nanqing Dong, Yang Zhang', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.05101v1'}, page_content=\"Deep learning-based computational methods have achieved promising results in\\npredicting protein-protein interactions (PPIs). However, existing benchmarks\\npredominantly focus on isolated pairwise evaluations, overlooking a model's\\ncapability to reconstruct biologically meaningful PPI networks, which is\\ncrucial for biology research. To address this gap, we introduce PRING, the\\nfirst comprehensive benchmark that evaluates protein-protein interaction\\nprediction from a graph-level perspective. PRING curates a high-quality,\\nmulti-species PPI network dataset comprising 21,484 proteins and 186,818\\ninteractions, with well-designed strategies to address both data redundancy and\\nleakage. Building on this golden-standard dataset, we establish two\\ncomplementary evaluation paradigms: (1) topology-oriented tasks, which assess\\nintra and cross-species PPI network construction, and (2) function-oriented\\ntasks, including protein complex pathway prediction, GO module analysis, and\\nessential protein justification. These evaluations not only reflect the model's\\ncapability to understand the network topology but also facilitate protein\\nfunction annotation, biological module detection, and even disease mechanism\\nanalysis. Extensive experiments on four representative model categories,\\nconsisting of sequence similarity-based, naive sequence-based, protein language\\nmodel-based, and structure-based approaches, demonstrate that current PPI\\nmodels have potential limitations in recovering both structural and functional\\nproperties of PPI networks, highlighting the gap in supporting real-world\\nbiological applications. We believe PRING provides a reliable platform to guide\\nthe development of more effective PPI prediction models for the community. The\\ndataset and source code of PRING are available at\\nhttps://github.com/SophieSarceau/PRING.\"),\n",
       " Document(metadata={'title': 'Beyond Features: How Dataset Design Influences Multi-Agent Trajectory Prediction Performance', 'authors': 'Tobias Demmler, Jakob Häringer, Andreas Tamke, Thao Dang, Alexander Hegai, Lars Mikelsons', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.05098v1'}, page_content='Accurate trajectory prediction is critical for safe autonomous navigation,\\nyet the impact of dataset design on model performance remains understudied.\\nThis work systematically examines how feature selection, cross-dataset\\ntransfer, and geographic diversity influence trajectory prediction accuracy in\\nmulti-agent settings. We evaluate a state-of-the-art model using our novel L4\\nMotion Forecasting dataset based on our own data recordings in Germany and the\\nUS. This includes enhanced map and agent features. We compare our dataset to\\nthe US-centric Argoverse 2 benchmark. First, we find that incorporating\\nsupplementary map and agent features unique to our dataset, yields no\\nmeasurable improvement over baseline features, demonstrating that modern\\narchitectures do not need extensive feature sets for optimal performance. The\\nlimited features of public datasets are sufficient to capture convoluted\\ninteractions without added complexity. Second, we perform cross-dataset\\nexperiments to evaluate how effective domain knowledge can be transferred\\nbetween datasets. Third, we group our dataset by country and check the\\nknowledge transfer between different driving cultures.'),\n",
       " Document(metadata={'title': 'The Hidden Threat in Plain Text: Attacking RAG Data Loaders', 'authors': 'Alberto Castagnaro, Umberto Salviati, Mauro Conti, Luca Pajola, Simeone Pizzi', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.05093v1'}, page_content=\"Large Language Models (LLMs) have transformed human-machine interaction since\\nChatGPT's 2022 debut, with Retrieval-Augmented Generation (RAG) emerging as a\\nkey framework that enhances LLM outputs by integrating external knowledge.\\nHowever, RAG's reliance on ingesting external documents introduces new\\nvulnerabilities. This paper exposes a critical security gap at the data loading\\nstage, where malicious actors can stealthily corrupt RAG pipelines by\\nexploiting document ingestion.\\n  We propose a taxonomy of 9 knowledge-based poisoning attacks and introduce\\ntwo novel threat vectors -- Content Obfuscation and Content Injection --\\ntargeting common formats (DOCX, HTML, PDF). Using an automated toolkit\\nimplementing 19 stealthy injection techniques, we test five popular data\\nloaders, finding a 74.4% attack success rate across 357 scenarios. We further\\nvalidate these threats on six end-to-end RAG systems -- including white-box\\npipelines and black-box services like NotebookLM and OpenAI Assistants --\\ndemonstrating high success rates and critical vulnerabilities that bypass\\nfilters and silently compromise output integrity. Our results emphasize the\\nurgent need to secure the document ingestion process in RAG systems against\\ncovert content manipulations.\"),\n",
       " Document(metadata={'title': 'How Rules Represent Causal Knowledge: Causal Modeling with Abductive Logic Programs', 'authors': 'Kilian Rückschloß, Felix Weitkämper', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.05088v1'}, page_content=\"Pearl observes that causal knowledge enables predicting the effects of\\ninterventions, such as actions, whereas descriptive knowledge only permits\\ndrawing conclusions from observation. This paper extends Pearl's approach to\\ncausality and interventions to the setting of stratified abductive logic\\nprograms. It shows how stable models of such programs can be given a causal\\ninterpretation by building on philosophical foundations and recent work by\\nBochman and Eelink et al. In particular, it provides a translation of abductive\\nlogic programs into causal systems, thereby clarifying the informal causal\\nreading of logic program rules and supporting principled reasoning about\\nexternal actions. The main result establishes that the stable model semantics\\nfor stratified programs conforms to key philosophical principles of causation,\\nsuch as causal sufficiency, natural necessity, and irrelevance of unobserved\\neffects. This justifies the use of stratified abductive logic programs as a\\nframework for causal modeling and for predicting the effects of interventions\"),\n",
       " Document(metadata={'title': 'Sequential Attention-based Sampling for Histopathological Analysis', 'authors': 'Tarun G, Naman Malpani, Gugan Thoppe, Sridharan Devarajan', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.05077v1'}, page_content='Deep neural networks are increasingly applied for automated histopathology.\\nYet, whole-slide images (WSIs) are often acquired at gigapixel sizes, rendering\\nit computationally infeasible to analyze them entirely at high resolution.\\nDiagnostic labels are largely available only at the slide-level, because expert\\nannotation of images at a finer (patch) level is both laborious and expensive.\\nMoreover, regions with diagnostic information typically occupy only a small\\nfraction of the WSI, making it inefficient to examine the entire slide at full\\nresolution. Here, we propose SASHA -- {\\\\it S}equential {\\\\it A}ttention-based\\n{\\\\it S}ampling for {\\\\it H}istopathological {\\\\it A}nalysis -- a deep\\nreinforcement learning approach for efficient analysis of histopathological\\nimages. First, SASHA learns informative features with a lightweight\\nhierarchical, attention-based multiple instance learning (MIL) model. Second,\\nSASHA samples intelligently and zooms selectively into a small fraction\\n(10-20\\\\%) of high-resolution patches, to achieve reliable diagnosis. We show\\nthat SASHA matches state-of-the-art methods that analyze the WSI fully at\\nhigh-resolution, albeit at a fraction of their computational and memory costs.\\nIn addition, it significantly outperforms competing, sparse sampling methods.\\nWe propose SASHA as an intelligent sampling model for medical imaging\\nchallenges that involve automated diagnosis with exceptionally large images\\ncontaining sparsely informative features.'),\n",
       " Document(metadata={'title': 'ICAS: Detecting Training Data from Autoregressive Image Generative Models', 'authors': 'Hongyao Yu, Yixiang Qiu, Yiheng Yang, Hao Fang, Tianqu Zhuang, Jiaxin Hong, Bin Chen, Hao Wu, Shu-Tao Xia', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.05068v1'}, page_content='Autoregressive image generation has witnessed rapid advancements, with\\nprominent models such as scale-wise visual auto-regression pushing the\\nboundaries of visual synthesis. However, these developments also raise\\nsignificant concerns regarding data privacy and copyright. In response,\\ntraining data detection has emerged as a critical task for identifying\\nunauthorized data usage in model training. To better understand the\\nvulnerability of autoregressive image generative models to such detection, we\\nconduct the first study applying membership inference to this domain. Our\\napproach comprises two key components: implicit classification and an adaptive\\nscore aggregation strategy. First, we compute the implicit token-wise\\nclassification score within the query image. Then we propose an adaptive score\\naggregation strategy to acquire a final score, which places greater emphasis on\\nthe tokens with lower scores. A higher final score indicates that the sample is\\nmore likely to be involved in the training set. To validate the effectiveness\\nof our method, we adapt existing detection algorithms originally designed for\\nLLMs to visual autoregressive models. Extensive experiments demonstrate the\\nsuperiority of our method in both class-conditional and text-to-image\\nscenarios. Moreover, our approach exhibits strong robustness and generalization\\nunder various data transformations. Furthermore, sufficient experiments suggest\\ntwo novel key findings: (1) A linear scaling law on membership inference,\\nexposing the vulnerability of large foundation models. (2) Training data from\\nscale-wise visual autoregressive models is easier to detect than other\\nautoregressive paradigms.Our code is available at\\nhttps://github.com/Chrisqcwx/ImageAR-MIA.'),\n",
       " Document(metadata={'title': 'Replacing thinking with tool usage enables reasoning in small language models', 'authors': 'Corrado Rainone, Tim Bakker, Roland Memisevic', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.05065v1'}, page_content='Recent advances have established a new machine learning paradigm based on\\nscaling up compute at inference time as well as at training time. In that line\\nof work, a combination of Supervised Fine-Tuning (SFT) on synthetic\\ndemonstrations and Reinforcement Learning with Verifiable Rewards (RLVR) is\\nused for training Large Language Models to expend extra compute during\\ninference in the form of \"thoughts\" expressed in natural language. In this\\npaper, we propose to instead format these tokens as a multi-turn interaction\\ntrace with a stateful tool. At each turn, the new state of the tool is appended\\nto the context of the model, whose job is to generate the tokens necessary to\\ncontrol the tool via a custom DSL. We benchmark this approach on the problem of\\nrepairing malfunctioning Python code, and show that this constrained setup\\nallows for faster sampling of experience and a denser reward signal, allowing\\neven models of size up to 3B parameters to learn how to proficiently expend\\nadditional compute on the task.'),\n",
       " Document(metadata={'title': 'INTER: Mitigating Hallucination in Large Vision-Language Models by Interaction Guidance Sampling', 'authors': 'Xin Dong, Shichao Dong, Jin Wang, Jing Huang, Li Zhou, Zenghui Sun, Lihua Jing, Jingsong Lan, Xiaoyong Zhu, Bo Zheng', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.05056v1'}, page_content=\"Hallucinations in large vision-language models (LVLMs) pose significant\\nchallenges for real-world applications, as LVLMs may generate responses that\\nappear plausible yet remain inconsistent with the associated visual content.\\nThis issue rarely occurs in human cognition. We argue that this discrepancy\\narises from humans' ability to effectively leverage multimodal interaction\\ninformation in data samples. Specifically, humans typically first gather\\nmultimodal information, analyze the interactions across modalities for\\nunderstanding, and then express their understanding through language. Motivated\\nby this observation, we conduct extensive experiments on popular LVLMs and\\nobtained insights that surprisingly reveal human-like, though less pronounced,\\ncognitive behavior of LVLMs on multimodal samples. Building on these findings,\\nwe further propose \\\\textbf{INTER}: \\\\textbf{Inter}action Guidance Sampling, a\\nnovel training-free algorithm that mitigate hallucinations without requiring\\nadditional data. Specifically, INTER explicitly guides LVLMs to effectively\\nreapply their understanding of multimodal interaction information when\\ngenerating responses, thereby reducing potential hallucinations. On six\\nbenchmarks including VQA and image captioning tasks, INTER achieves an average\\nimprovement of up to 3.4\\\\% on five LVLMs compared to the state-of-the-art\\ndecoding strategy. The code will be released when the paper is accepted.\"),\n",
       " Document(metadata={'title': 'Perspectives on How Sociology Can Advance Theorizing about Human-Chatbot Interaction and Developing Chatbots for Social Good', 'authors': 'Celeste Campos-Castillo, Xuan Kang, Linnea I. Laestadius', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.05030v1'}, page_content='Recently, research into chatbots (also known as conversational agents, AI\\nagents, voice assistants), which are computer applications using artificial\\nintelligence to mimic human-like conversation, has grown sharply. Despite this\\ngrowth, sociology lags other disciplines (including computer science, medicine,\\npsychology, and communication) in publishing about chatbots. We suggest\\nsociology can advance understanding of human-chatbot interaction and offer four\\nsociological theories to enhance extant work in this field. The first two\\ntheories (resource substitution theory, power-dependence theory) add new\\ninsights to existing models of the drivers of chatbot use, which overlook\\nsociological concerns about how social structure (e.g., systemic\\ndiscrimination, the uneven distribution of resources within networks) inclines\\nindividuals to use chatbots, including problematic levels of emotional\\ndependency on chatbots. The second two theories (affect control theory,\\nfundamental cause of disease theory) help inform the development of\\nchatbot-driven interventions that minimize safety risks and enhance equity by\\nleveraging sociological insights into how chatbot outputs could attend to\\ncultural contexts (e.g., affective norms) to promote wellbeing and enhance\\ncommunities (e.g., opportunities for civic participation). We discuss the value\\nof applying sociological theories for advancing theorizing about human-chatbot\\ninteraction and developing chatbots for social good.'),\n",
       " Document(metadata={'title': 'Adaptation of Multi-modal Representation Models for Multi-task Surgical Computer Vision', 'authors': 'Soham Walimbe, Britty Baby, Vinkle Srivastav, Nicolas Padoy', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.05020v1'}, page_content='Surgical AI often involves multiple tasks within a single procedure, like\\nphase recognition or assessing the Critical View of Safety in laparoscopic\\ncholecystectomy. Traditional models, built for one task at a time, lack\\nflexibility, requiring a separate model for each. To address this, we introduce\\nMML-SurgAdapt, a unified multi-task framework with Vision-Language Models\\n(VLMs), specifically CLIP, to handle diverse surgical tasks through natural\\nlanguage supervision. A key challenge in multi-task learning is the presence of\\npartial annotations when integrating different tasks. To overcome this, we\\nemploy Single Positive Multi-Label (SPML) learning, which traditionally reduces\\nannotation burden by training models with only one positive label per instance.\\nOur framework extends this approach to integrate data from multiple surgical\\ntasks within a single procedure, enabling effective learning despite incomplete\\nor noisy annotations. We demonstrate the effectiveness of our model on a\\ncombined dataset consisting of Cholec80, Endoscapes2023, and CholecT50,\\nutilizing custom prompts. Extensive evaluation shows that MML-SurgAdapt\\nperforms comparably to task-specific benchmarks, with the added advantage of\\nhandling noisy annotations. It also outperforms the existing SPML frameworks\\nfor the task. By reducing the required labels by 23%, our approach proposes a\\nmore scalable and efficient labeling process, significantly easing the\\nannotation burden on clinicians. To our knowledge, this is the first\\napplication of SPML to integrate data from multiple surgical tasks, presenting\\na novel and generalizable solution for multi-task learning in surgical computer\\nvision. Implementation is available at:\\nhttps://github.com/CAMMA-public/MML-SurgAdapt'),\n",
       " Document(metadata={'title': 'Meta-Learning Transformers to Improve In-Context Generalization', 'authors': 'Lorenzo Braccaioli, Anna Vettoruzzo, Prabhant Singh, Joaquin Vanschoren, Mohamed-Rafik Bouguelia, Nicola Conci', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.05019v1'}, page_content='In-context learning enables transformer models to generalize to new tasks\\nbased solely on input prompts, without any need for weight updates. However,\\nexisting training paradigms typically rely on large, unstructured datasets that\\nare costly to store, difficult to evaluate for quality and balance, and pose\\nprivacy and ethical concerns due to the inclusion of sensitive information.\\nMotivated by these limitations and risks, we propose an alternative training\\nstrategy where we leverage a collection of multiple, small-scale, and\\ndomain-specific datasets. We empirically demonstrate that the increased quality\\nand diversity of such data improve the generalization abilities of in-context\\nlearners beyond their training domain, while achieving comparable performance\\nwith models trained on a single large-scale dataset. We investigate this\\nparadigm by leveraging meta-learning to train an in-context learner on the\\nMeta-Album collection under several settings. Firstly, we show the performance\\nin a controlled environment, where the test domain is completely excluded from\\nthe training knowledge. Secondly, we explore the robustness of these models to\\nforgetting in a continual scenario where the information is accessible for a\\nlimited time. Finally, we explore the more challenging unsupervised scenario.\\nOur findings demonstrate that transformers still generalize for in-context\\nprediction when trained on a curated dataset collection while offering\\nadvantages in modularity and replaceability.'),\n",
       " Document(metadata={'title': 'When Imitation Learning Outperforms Reinforcement Learning in Surgical Action Planning', 'authors': 'Maxence Boels, Harry Robertshaw, Alejandro Granados, Prokar Dasgupta, Sebastien Ourselin', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.05011v1'}, page_content='Surgical action planning requires predicting future instrument-verb-target\\ntriplets for real-time assistance. While teleoperated robotic surgery provides\\nnatural expert demonstrations for imitation learning (IL), reinforcement\\nlearning (RL) could potentially discover superior strategies through\\nexploration. We present the first comprehensive comparison of IL versus RL for\\nsurgical action planning on CholecT50. Our Dual-task Autoregressive Imitation\\nLearning (DARIL) baseline achieves 34.6% action triplet recognition mAP and\\n33.6% next frame prediction mAP with smooth planning degradation to 29.2% at\\n10-second horizons. We evaluated three RL variants: world model-based RL,\\ndirect video RL, and inverse RL enhancement. Surprisingly, all RL approaches\\nunderperformed DARIL i.e. world model RL dropped to 3.1% mAP at 10s while\\ndirect video RL achieved only 15.9%. Our analysis reveals that distribution\\nmatching on expert-annotated test sets systematically favors IL over\\npotentially valid RL policies that differ from training demonstrations. This\\nchallenges assumptions about RL superiority in sequential decision making and\\nprovides crucial insights for surgical AI development.'),\n",
       " Document(metadata={'title': 'Multi-modal Representations for Fine-grained Multi-label Critical View of Safety Recognition', 'authors': 'Britty Baby, Vinkle Srivastav, Pooja P. Jain, Kun Yuan, Pietro Mascagni, Nicolas Padoy', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.05007v1'}, page_content=\"The Critical View of Safety (CVS) is crucial for safe laparoscopic\\ncholecystectomy, yet assessing CVS criteria remains a complex and challenging\\ntask, even for experts. Traditional models for CVS recognition depend on\\nvision-only models learning with costly, labor-intensive spatial annotations.\\nThis study investigates how text can be harnessed as a powerful tool for both\\ntraining and inference in multi-modal surgical foundation models to automate\\nCVS recognition. Unlike many existing multi-modal models, which are primarily\\nadapted for multi-class classification, CVS recognition requires a multi-label\\nframework. Zero-shot evaluation of existing multi-modal surgical models shows a\\nsignificant performance gap for this task. To address this, we propose\\nCVS-AdaptNet, a multi-label adaptation strategy that enhances fine-grained,\\nbinary classification across multiple labels by aligning image embeddings with\\ntextual descriptions of each CVS criterion using positive and negative prompts.\\nBy adapting PeskaVLP, a state-of-the-art surgical foundation model, on the\\nEndoscapes-CVS201 dataset, CVS-AdaptNet achieves 57.6 mAP, improving over the\\nResNet50 image-only baseline (51.5 mAP) by 6 points. Our results show that\\nCVS-AdaptNet's multi-label, multi-modal framework, enhanced by textual prompts,\\nboosts CVS recognition over image-only methods. We also propose text-specific\\ninference methods, that helps in analysing the image-text alignment. While\\nfurther work is needed to match state-of-the-art spatial annotation-based\\nmethods, this approach highlights the potential of adapting generalist models\\nto specialized surgical tasks. Code:\\nhttps://github.com/CAMMA-public/CVS-AdaptNet\"),\n",
       " Document(metadata={'title': 'Supported Abstract Argumentation for Case-Based Reasoning', 'authors': 'Adam Gould, Gabriel de Olim Gaul, Francesca Toni', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04994v1'}, page_content='We introduce Supported Abstract Argumentation for Case-Based Reasoning\\n(sAA-CBR), a binary classification model in which past cases engage in debates\\nby arguing in favour of their labelling and attacking or supporting those with\\nopposing or agreeing labels. With supports, sAA-CBR overcomes the limitation of\\nits precursor AA-CBR, which can contain extraneous cases (or spikes) that are\\nnot included in the debates. We prove that sAA-CBR contains no spikes, without\\ntrading off key model properties'),\n",
       " Document(metadata={'title': 'Classification of autoimmune diseases from Peripheral blood TCR repertoires by multimodal multi-instance learning', 'authors': 'Ruihao Zhang, Fei Ye, Dandan Meng, Yixuan Huang, Maochen, Xiao Liu', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04981v1'}, page_content='T cell receptor (TCR) repertoires encode critical immunological signatures\\nfor autoimmune diseases, yet their clinical application remains limited by\\nsequence sparsity and low witness rates. We developed EAMil, a multi-instance\\ndeep learning framework that leverages TCR sequencing data to diagnose systemic\\nlupus erythematosus (SLE) and rheumatoid arthritis (RA) with exceptional\\naccuracy. By integrating PrimeSeq feature extraction with ESMonehot encoding\\nand enhanced gate attention mechanisms, our model achieved state-of-the-art\\nperformance with AUCs of 98.95% for SLE and 97.76% for RA. EAMil successfully\\nidentified disease-associated genes with over 90% concordance with established\\ndifferential analyses and effectively distinguished disease-specific TCR genes.\\nThe model demonstrated robustness in classifying multiple disease categories,\\nutilizing the SLEDAI score to stratify SLE patients by disease severity as well\\nas to diagnose the site of damage in SLE patients, and effectively controlling\\nfor confounding factors such as age and gender. This interpretable framework\\nfor immune receptor analysis provides new insights for autoimmune disease\\ndetection and classification with broad potential clinical applications across\\nimmune-mediated conditions.'),\n",
       " Document(metadata={'title': 'LAPS-Diff: A Diffusion-Based Framework for Singing Voice Synthesis With Language Aware Prosody-Style Guided Learning', 'authors': 'Sandipan Dhar, Mayank Gupta, Preeti Rao', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04966v1'}, page_content='The field of Singing Voice Synthesis (SVS) has seen significant advancements\\nin recent years due to the rapid progress of diffusion-based approaches.\\nHowever, capturing vocal style, genre-specific pitch inflections, and\\nlanguage-dependent characteristics remains challenging, particularly in\\nlow-resource scenarios. To address this, we propose LAPS-Diff, a diffusion\\nmodel integrated with language-aware embeddings and a vocal-style guided\\nlearning mechanism, specifically designed for Bollywood Hindi singing style. We\\ncurate a Hindi SVS dataset and leverage pre-trained language models to extract\\nword and phone-level embeddings for an enriched lyrics representation.\\nAdditionally, we incorporated a style encoder and a pitch extraction model to\\ncompute style and pitch losses, capturing features essential to the naturalness\\nand expressiveness of the synthesized singing, particularly in terms of vocal\\nstyle and pitch variations. Furthermore, we utilize MERT and IndicWav2Vec\\nmodels to extract musical and contextual embeddings, serving as conditional\\npriors to refine the acoustic feature generation process further. Based on\\nobjective and subjective evaluations, we demonstrate that LAPS-Diff\\nsignificantly improves the quality of the generated samples compared to the\\nconsidered state-of-the-art (SOTA) model for our constrained dataset that is\\ntypical of the low resource scenario.'),\n",
       " Document(metadata={'title': 'Hear-Your-Click: Interactive Video-to-Audio Generation via Object-aware Contrastive Audio-Visual Fine-tuning', 'authors': 'Yingshan Liang, Keyu Fan, Zhicheng Du, Yiran Wang, Qingyang Shi, Xinyu Zhang, Jiasheng Lu, Peiwu Qin', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04959v1'}, page_content=\"Video-to-audio (V2A) generation shows great potential in fields such as film\\nproduction. Despite significant advances, current V2A methods, which rely on\\nglobal video information, struggle with complex scenes and often fail to\\ngenerate audio tailored to specific objects or regions in the videos. To\\naddress these limitations, we introduce Hear-Your-Click, an interactive V2A\\nframework that enables users to generate sounds for specific objects in the\\nvideos by simply clicking on the frame. To achieve this, we propose\\nObject-aware Contrastive Audio-Visual Fine-tuning (OCAV) with a Mask-guided\\nVisual Encoder (MVE) to obtain object-level visual features aligned with\\ncorresponding audio segments. Furthermore, we tailor two data augmentation\\nstrategies: Random Video Stitching (RVS) and Mask-guided Loudness Modulation\\n(MLM), aimed at enhancing the model's sensitivity to the segmented objects. To\\neffectively measure the audio-visual correspondence, we design a new evaluation\\nmetric, the CAV score, for evaluation. Extensive experiments demonstrate that\\nour framework offers more precise control and improved generation performance\\nacross various metrics. Project Page:\\nhttps://github.com/SynapGrid/Hear-Your-Click\"),\n",
       " Document(metadata={'title': 'EXPOTION: Facial Expression and Motion Control for Multimodal Music Generation', 'authors': 'Fathinah Izzati, Xinyue Li, Gus Xia', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04955v1'}, page_content='We propose Expotion (Facial Expression and Motion Control for Multimodal\\nMusic Generation), a generative model leveraging multimodal visual controls -\\nspecifically, human facial expressions and upper-body motion - as well as text\\nprompts to produce expressive and temporally accurate music. We adopt\\nparameter-efficient fine-tuning (PEFT) on the pretrained text-to-music\\ngeneration model, enabling fine-grained adaptation to the multimodal controls\\nusing a small dataset. To ensure precise synchronization between video and\\nmusic, we introduce a temporal smoothing strategy to align multiple modalities.\\nExperiments demonstrate that integrating visual features alongside textual\\ndescriptions enhances the overall quality of generated music in terms of\\nmusicality, creativity, beat-tempo consistency, temporal alignment with the\\nvideo, and text adherence, surpassing both proposed baselines and existing\\nstate-of-the-art video-to-music generation models. Additionally, we introduce a\\nnovel dataset consisting of 7 hours of synchronized video recordings capturing\\nexpressive facial and upper-body gestures aligned with corresponding music,\\nproviding significant potential for future research in multimodal and\\ninteractive music generation.'),\n",
       " Document(metadata={'title': 'DC-AR: Efficient Masked Autoregressive Image Generation with Deep Compression Hybrid Tokenizer', 'authors': 'Yecheng Wu, Junyu Chen, Zhuoyang Zhang, Enze Xie, Jincheng Yu, Junsong Chen, Jinyi Hu, Yao Lu, Song Han, Han Cai', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04947v1'}, page_content=\"We introduce DC-AR, a novel masked autoregressive (AR) text-to-image\\ngeneration framework that delivers superior image generation quality with\\nexceptional computational efficiency. Due to the tokenizers' limitations, prior\\nmasked AR models have lagged behind diffusion models in terms of quality or\\nefficiency. We overcome this limitation by introducing DC-HT - a deep\\ncompression hybrid tokenizer for AR models that achieves a 32x spatial\\ncompression ratio while maintaining high reconstruction fidelity and\\ncross-resolution generalization ability. Building upon DC-HT, we extend MaskGIT\\nand create a new hybrid masked autoregressive image generation framework that\\nfirst produces the structural elements through discrete tokens and then applies\\nrefinements via residual tokens. DC-AR achieves state-of-the-art results with a\\ngFID of 5.49 on MJHQ-30K and an overall score of 0.69 on GenEval, while\\noffering 1.5-7.9x higher throughput and 2.0-3.5x lower latency compared to\\nprior leading diffusion and autoregressive models.\"),\n",
       " Document(metadata={'title': 'Object-centric Denoising Diffusion Models for Physical Reasoning', 'authors': 'Moritz Lange, Raphael C. Engelhardt, Wolfgang Konen, Andrew Melnik, Laurenz Wiskott', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04920v1'}, page_content='Reasoning about the trajectories of multiple, interacting objects is integral\\nto physical reasoning tasks in machine learning. This involves conditions\\nimposed on the objects at different time steps, for instance initial states or\\ndesired goal states. Existing approaches in physical reasoning generally rely\\non autoregressive modeling, which can only be conditioned on initial states,\\nbut not on later states. In fields such as planning for reinforcement learning,\\nsimilar challenges are being addressed with denoising diffusion models. In this\\nwork, we propose an object-centric denoising diffusion model architecture for\\nphysical reasoning that is translation equivariant over time, permutation\\nequivariant over objects, and can be conditioned on arbitrary time steps for\\narbitrary objects. We demonstrate how this model can solve tasks with multiple\\nconditions and examine its performance when changing object numbers and\\ntrajectory lengths during inference.'),\n",
       " Document(metadata={'title': 'Leadership Detection via Time-Lagged Correlation-Based Network Inference', 'authors': 'Thayanne França da Silva, José Everardo Bessa Maia', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04917v1'}, page_content='Understanding leadership dynamics in collective behavior is a key challenge\\nin animal ecology, swarm robotics, and intelligent transportation. Traditional\\ninformation-theoretic approaches, including Transfer Entropy (TE) and\\nTime-Lagged Mutual Information (TLMI), have been widely used to infer\\nleader-follower relationships but face critical limitations in noisy or\\nshort-duration datasets due to their reliance on robust probability\\nestimations. This study proposes a method based on dynamic network inference\\nusing time-lagged correlations across multiple kinematic variables: velocity,\\nacceleration, and direction. Our approach constructs directed influence graphs\\nover time, enabling the identification of leadership patterns without the need\\nfor large volumes of data or parameter-sensitive discretization. We validate\\nour method through two multi-agent simulations in NetLogo: a modified Vicsek\\nmodel with informed leaders and a predator-prey model featuring coordinated and\\nindependent wolf groups. Experimental results demonstrate that the\\nnetwork-based method outperforms TE and TLMI in scenarios with limited\\nspatiotemporal observations, ranking true leaders at the top of influence\\nmetrics more consistently than TE and TLMI.'),\n",
       " Document(metadata={'title': 'HV-MMBench: Benchmarking MLLMs for Human-Centric Video Understanding', 'authors': 'Yuxuan Cai, Jiangning Zhang, Zhenye Gan, Qingdong He, Xiaobin Hu, Junwei Zhu, Yabiao Wang, Chengjie Wang, Zhucun Xue, Xinwei He, Xiang Bai', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04909v1'}, page_content='Multimodal Large Language Models (MLLMs) have demonstrated significant\\nadvances in visual understanding tasks involving both images and videos.\\nHowever, their capacity to comprehend human-centric video data remains\\nunderexplored, primarily due to the absence of comprehensive and high-quality\\nevaluation benchmarks. Existing human-centric benchmarks predominantly\\nemphasize video generation quality and action recognition, while overlooking\\nessential perceptual and cognitive abilities required in human-centered\\nscenarios. Furthermore, they are often limited by single-question paradigms and\\noverly simplistic evaluation metrics. To address above limitations, we propose\\na modern HV-MMBench, a rigorously curated benchmark designed to provide a more\\nholistic evaluation of MLLMs in human-centric video understanding. Compared to\\nexisting human-centric video benchmarks, our work offers the following key\\nfeatures: (1) Diverse evaluation dimensions: HV-MMBench encompasses 15 tasks,\\nranging from basic attribute perception (e.g., age estimation, emotion\\nrecognition) to advanced cognitive reasoning (e.g., social relationship\\nprediction, intention prediction), enabling comprehensive assessment of model\\ncapabilities; (2) Varied data types: The benchmark includes multiple-choice,\\nfill-in-blank, true/false, and open-ended question formats, combined with\\ndiverse evaluation metrics, to more accurately and robustly reflect model\\nperformance; (3) Multi-domain video coverage: The benchmark spans 50 distinct\\nvisual scenarios, enabling comprehensive evaluation across fine-grained scene\\nvariations; (4) Temporal coverage: The benchmark covers videos from short-term\\n(10 seconds) to long-term (up to 30min) durations, supporting systematic\\nanalysis of models temporal reasoning abilities across diverse contextual\\nlengths.'),\n",
       " Document(metadata={'title': 'BackFed: An Efficient & Standardized Benchmark Suite for Backdoor Attacks in Federated Learning', 'authors': 'Thinh Dao, Dung Thuy Nguyen, Khoa D Doan, Kok-Seng Wong', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04903v1'}, page_content='Federated Learning (FL) systems are vulnerable to backdoor attacks, where\\nadversaries train their local models on poisoned data and submit poisoned model\\nupdates to compromise the global model. Despite numerous proposed attacks and\\ndefenses, divergent experimental settings, implementation errors, and\\nunrealistic assumptions hinder fair comparisons and valid conclusions about\\ntheir effectiveness in real-world scenarios. To address this, we introduce\\nBackFed - a comprehensive benchmark suite designed to standardize, streamline,\\nand reliably evaluate backdoor attacks and defenses in FL, with a focus on\\npractical constraints. Our benchmark offers key advantages through its\\nmulti-processing implementation that significantly accelerates experimentation\\nand the modular design that enables seamless integration of new methods via\\nwell-defined APIs. With a standardized evaluation pipeline, we envision BackFed\\nas a plug-and-play environment for researchers to comprehensively and reliably\\nevaluate new attacks and defenses. Using BackFed, we conduct large-scale\\nstudies of representative backdoor attacks and defenses across both Computer\\nVision and Natural Language Processing tasks with diverse model architectures\\nand experimental settings. Our experiments critically assess the performance of\\nproposed attacks and defenses, revealing unknown limitations and modes of\\nfailures under practical conditions. These empirical insights provide valuable\\nguidance for the development of new methods and for enhancing the security of\\nFL systems. Our framework is openly available at\\nhttps://github.com/thinh-dao/BackFed.'),\n",
       " Document(metadata={'title': 'MARBLE: A Multi-Agent Rule-Based LLM Reasoning Engine for Accident Severity Prediction', 'authors': 'Kaleem Ullah Qasim, Jiashu Zhang', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04893v1'}, page_content='Accident severity prediction plays a critical role in transportation safety\\nsystems but is a persistently difficult task due to incomplete data, strong\\nfeature dependencies, and severe class imbalance in which rare but\\nhigh-severity cases are underrepresented and hard to detect. Existing methods\\noften rely on monolithic models or black box prompting, which struggle to scale\\nin noisy, real-world settings and offer limited interpretability. To address\\nthese challenges, we propose MARBLE a multiagent rule based LLM engine that\\ndecomposes the severity prediction task across a team of specialized reasoning\\nagents, including an interchangeable ML-backed agent. Each agent focuses on a\\nsemantic subset of features (e.g., spatial, environmental, temporal), enabling\\nscoped reasoning and modular prompting without the risk of prompt saturation.\\nPredictions are coordinated through either rule-based or LLM-guided consensus\\nmechanisms that account for class rarity and confidence dynamics. The system\\nretains structured traces of agent-level reasoning and coordination outcomes,\\nsupporting in-depth interpretability and post-hoc performance diagnostics.\\nAcross both UK and US datasets, MARBLE consistently outperforms traditional\\nmachine learning classifiers and state-of-the-art (SOTA) prompt-based reasoning\\nmethods including Chain-of-Thought (CoT), Least-to-Most (L2M), and\\nTree-of-Thought (ToT) achieving nearly 90% accuracy where others plateau below\\n48%. This performance redefines the practical ceiling for accident severity\\nclassification under real world noise and extreme class imbalance. Our results\\nposition MARBLE as a generalizable and interpretable framework for reasoning\\nunder uncertainty in safety-critical applications.'),\n",
       " Document(metadata={'title': 'Emergent Semantics Beyond Token Embeddings: Transformer LMs with Frozen Visual Unicode Representations', 'authors': 'A. Bochkov', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04886v1'}, page_content='Understanding the locus of semantic representation in large language models\\n(LLMs) is crucial for interpretability and architectural innovation. The\\ndominant paradigm posits that trainable input embeddings serve as foundational\\n\"meaning vectors.\" This paper challenges that view. We construct Transformer\\nmodels where the embedding layer is entirely frozen, with vectors derived not\\nfrom data, but from the visual structure of Unicode glyphs. These non-semantic,\\nprecomputed visual embeddings are fixed throughout training. Our method is\\ncompatible with any tokenizer, including a novel Unicode-centric tokenizer we\\nintroduce to ensure universal text coverage. Despite the absence of trainable,\\nsemantically initialized embeddings, our models converge, generate coherent\\ntext, and, critically, outperform architecturally identical models with\\ntrainable embeddings on the MMLU reasoning benchmark. We attribute this to\\n\"representational interference\" in conventional models, where the embedding\\nlayer is burdened with learning both structural and semantic features. Our\\nresults indicate that high-level semantics are not inherent to input embeddings\\nbut are an emergent property of the Transformer\\'s compositional architecture\\nand data scale. This reframes the role of embeddings from meaning containers to\\nstructural primitives. We release all code and models to foster further\\nresearch.'),\n",
       " Document(metadata={'title': 'Beyond Training-time Poisoning: Component-level and Post-training Backdoors in Deep Reinforcement Learning', 'authors': 'Sanyam Vyas, Alberto Caron, Chris Hicks, Pete Burnap, Vasilios Mavroudis', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04883v1'}, page_content='Deep Reinforcement Learning (DRL) systems are increasingly used in\\nsafety-critical applications, yet their security remains severely\\nunderexplored. This work investigates backdoor attacks, which implant hidden\\ntriggers that cause malicious actions only when specific inputs appear in the\\nobservation space. Existing DRL backdoor research focuses solely on\\ntraining-time attacks requiring unrealistic access to the training pipeline. In\\ncontrast, we reveal critical vulnerabilities across the DRL supply chain where\\nbackdoors can be embedded with significantly reduced adversarial privileges. We\\nintroduce two novel attacks: (1) TrojanentRL, which exploits component-level\\nflaws to implant a persistent backdoor that survives full model retraining; and\\n(2) InfrectroRL, a post-training backdoor attack which requires no access to\\ntraining, validation, nor test data. Empirical and analytical evaluations\\nacross six Atari environments show our attacks rival state-of-the-art\\ntraining-time backdoor attacks while operating under much stricter adversarial\\nconstraints. We also demonstrate that InfrectroRL further evades two leading\\nDRL backdoor defenses. These findings challenge the current research focus and\\nhighlight the urgent need for robust defenses.'),\n",
       " Document(metadata={'title': 'HGNet: High-Order Spatial Awareness Hypergraph and Multi-Scale Context Attention Network for Colorectal Polyp Detection', 'authors': 'Xiaofang Liu, Lingling Sun, Xuqing Zhang, Yuannong Ye, Bin zhao', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04880v1'}, page_content='Colorectal cancer (CRC) is closely linked to the malignant transformation of\\ncolorectal polyps, making early detection essential. However, current models\\nstruggle with detecting small lesions, accurately localizing boundaries, and\\nproviding interpretable decisions. To address these issues, we propose HGNet,\\nwhich integrates High-Order Spatial Awareness Hypergraph and Multi-Scale\\nContext Attention. Key innovations include: (1) an Efficient Multi-Scale\\nContext Attention (EMCA) module to enhance lesion feature representation and\\nboundary modeling; (2) the deployment of a spatial hypergraph convolution\\nmodule before the detection head to capture higher-order spatial relationships\\nbetween nodes; (3) the application of transfer learning to address the scarcity\\nof medical image data; and (4) Eigen Class Activation Map (Eigen-CAM) for\\ndecision visualization. Experimental results show that HGNet achieves 94%\\naccuracy, 90.6% recall, and 90% mAP@0.5, significantly improving small lesion\\ndifferentiation and clinical interpretability. The source code will be made\\npublicly available upon publication of this paper.'),\n",
       " Document(metadata={'title': 'DoPI: Doctor-like Proactive Interrogation LLM for Traditional Chinese Medicine', 'authors': 'Zewen Sun, Ruoxiang Huang, Jiahe Feng, Rundong Kong, Yuqian Wang, Hengyu Liu, Ziqi Gong, Yuyuan Qin, Yingxue Wang, Yu Wang', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04877v1'}, page_content=\"Enhancing interrogation capabilities in Traditional Chinese Medicine (TCM)\\ndiagnosis through multi-turn dialogues and knowledge graphs presents a\\nsignificant challenge for modern AI systems. Current large language models\\n(LLMs), despite their advancements, exhibit notable limitations in medical\\napplications, particularly in conducting effective multi-turn dialogues and\\nproactive questioning. These shortcomings hinder their practical application\\nand effectiveness in simulating real-world diagnostic scenarios. To address\\nthese limitations, we propose DoPI, a novel LLM system specifically designed\\nfor the TCM domain. The DoPI system introduces a collaborative architecture\\ncomprising a guidance model and an expert model. The guidance model conducts\\nmulti-turn dialogues with patients and dynamically generates questions based on\\na knowledge graph to efficiently extract critical symptom information.\\nSimultaneously, the expert model leverages deep TCM expertise to provide final\\ndiagnoses and treatment plans. Furthermore, this study constructs a multi-turn\\ndoctor-patient dialogue dataset to simulate realistic consultation scenarios\\nand proposes a novel evaluation methodology that does not rely on manually\\ncollected real-world consultation data. Experimental results show that the DoPI\\nsystem achieves an accuracy rate of 84.68 percent in interrogation outcomes,\\nsignificantly enhancing the model's communication ability during diagnosis\\nwhile maintaining professional expertise.\"),\n",
       " Document(metadata={'title': 'A Novel Approach for Estimating Positive Lyapunov Exponents in One-Dimensional Chaotic Time Series Using Machine Learning', 'authors': 'A. Velichko, M. Belyaev, P. Boriskov', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04868v1'}, page_content='Understanding and quantifying chaos in nonlinear dynamical systems remains a\\nfundamental challenge in science and engineering. The Lyapunov exponent is a\\nkey measure of chaotic behavior, but its accurate estimation from experimental\\ndata is often hindered by methodological and computational limitations. In this\\nwork, we present a novel machine-learning-based approach for estimating the\\npositive Lyapunov exponent (MLE) from one-dimensional time series, using the\\ngrowth of out-of-sample prediction errors as a proxy for trajectory divergence.\\nOur method demonstrates high scientific relevance, offering a robust,\\ndata-driven alternative to traditional analytic techniques. Through\\ncomprehensive testing on several canonical chaotic maps - including the\\nlogistic, sine, cubic, and Chebyshev maps - we achieved a coefficient of\\ndetermination R2pos > 0.9 between predicted and theoretical MLE values for time\\nseries as short as M = 200 points. The best accuracy was observed for the\\nChebyshev map (R2pos = 0.999). Notably, the proposed method maintains high\\ncomputational efficiency and generalizes well across various machine learning\\nalgorithms. These results highlight the significance of our approach for\\npractical chaos analysis in both synthetic and experimental settings, opening\\nnew possibilities for robust nonlinear dynamics assessment when only time\\nseries data are available.'),\n",
       " Document(metadata={'title': 'Towards Human-in-the-Loop Onset Detection: A Transfer Learning Approach for Maracatu', 'authors': 'António Sá Pinto', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04858v1'}, page_content='We explore transfer learning strategies for musical onset detection in the\\nAfro-Brazilian Maracatu tradition, which features complex rhythmic patterns\\nthat challenge conventional models. We adapt two Temporal Convolutional Network\\narchitectures: one pre-trained for onset detection (intra-task) and another for\\nbeat tracking (inter-task). Using only 5-second annotated snippets per\\ninstrument, we fine-tune these models through layer-wise retraining strategies\\nfor five traditional percussion instruments. Our results demonstrate\\nsignificant improvements over baseline performance, with F1 scores reaching up\\nto 0.998 in the intra-task setting and improvements of over 50 percentage\\npoints in best-case scenarios. The cross-task adaptation proves particularly\\neffective for time-keeping instruments, where onsets naturally align with beat\\npositions. The optimal fine-tuning configuration varies by instrument,\\nhighlighting the importance of instrument-specific adaptation strategies. This\\napproach addresses the challenges of underrepresented musical traditions,\\noffering an efficient human-in-the-loop methodology that minimizes annotation\\neffort while maximizing performance. Our findings contribute to more inclusive\\nmusic information retrieval tools applicable beyond Western musical contexts.'),\n",
       " Document(metadata={'title': 'Fast-VGAN: Lightweight Voice Conversion with Explicit Control of F0 and Duration Parameters', 'authors': 'Mathilde Abrassart, Nicolas Obin, Axel Roebel', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04817v1'}, page_content='Precise control over speech characteristics, such as pitch, duration, and\\nspeech rate, remains a significant challenge in the field of voice conversion.\\nThe ability to manipulate parameters like pitch and syllable rate is an\\nimportant element for effective identity conversion, but can also be used\\nindependently for voice transformation, achieving goals that were historically\\naddressed by vocoder-based methods.\\n  In this work, we explore a convolutional neural network-based approach that\\naims to provide means for modifying fundamental frequency (F0), phoneme\\nsequences, intensity, and speaker identity. Rather than relying on\\ndisentanglement techniques, our model is explicitly conditioned on these\\nfactors to generate mel spectrograms, which are then converted into waveforms\\nusing a universal neural vocoder. Accordingly, during inference, F0 contours,\\nphoneme sequences, and speaker embeddings can be freely adjusted, allowing for\\nintuitively controlled voice transformations.\\n  We evaluate our approach on speaker conversion and expressive speech tasks\\nusing both perceptual and objective metrics. The results suggest that the\\nproposed method offers substantial flexibility, while maintaining high\\nintelligibility and speaker similarity.'),\n",
       " Document(metadata={'title': 'From Vision To Language through Graph of Events in Space and Time: An Explainable Self-supervised Approach', 'authors': 'Mihai Masala, Marius Leordeanu', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04815v1'}, page_content='The task of describing video content in natural language is commonly referred\\nto as video captioning. Unlike conventional video captions, which are typically\\nbrief and widely available, long-form paragraph descriptions in natural\\nlanguage are scarce. This limitation of current datasets is due to the\\nexpensive human manual annotation required and to the highly challenging task\\nof explaining the language formation process from the perspective of the\\nunderlying story, as a complex system of interconnected events in space and\\ntime. Through a thorough analysis of recently published methods and available\\ndatasets, we identify a general lack of published resources dedicated to the\\nproblem of describing videos in complex language, beyond the level of\\ndescriptions in the form of enumerations of simple captions. Furthermore, while\\nstate-of-the-art methods produce impressive results on the task of generating\\nshorter captions from videos by direct end-to-end learning between the videos\\nand text, the problem of explaining the relationship between vision and\\nlanguage is still beyond our reach. In this work, we propose a shared\\nrepresentation between vision and language, based on graphs of events in space\\nand time, which can be obtained in an explainable and analytical way, to\\nintegrate and connect multiple vision tasks to produce the final natural\\nlanguage description. Moreover, we also demonstrate how our automated and\\nexplainable video description generation process can function as a fully\\nautomatic teacher to effectively train direct, end-to-end neural student\\npathways, within a self-supervised neuro-analytical system. We validate that\\nour explainable neuro-analytical approach generates coherent, rich and relevant\\ntextual descriptions on videos collected from multiple varied datasets, using\\nboth standard evaluation metrics, human annotations and consensus from\\nensembles of state-of-the-art VLMs.'),\n",
       " Document(metadata={'title': 'Application and Evaluation of Large Language Models for Forecasting the Impact of Traffic Incidents', 'authors': 'George Jagadeesh, Srikrishna Iyer, Michal Polanowski, Kai Xin Thia', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04803v1'}, page_content=\"This study examines the feasibility of applying large language models (LLMs)\\nfor forecasting the impact of traffic incidents on the traffic flow. The use of\\nLLMs for this task has several advantages over existing machine learning-based\\nsolutions such as not requiring a large training dataset and the ability to\\nutilize free-text incident logs. We propose a fully LLM-based solution that\\npredicts the incident impact using a combination of traffic features and\\nLLM-extracted incident features. A key ingredient of this solution is an\\neffective method of selecting examples for the LLM's in-context learning. We\\nevaluate the performance of three advanced LLMs and two state-of-the-art\\nmachine learning models on a real traffic incident dataset. The results show\\nthat the best-performing LLM matches the accuracy of the most accurate machine\\nlearning model, despite the former not having been trained on this prediction\\ntask. The findings indicate that LLMs are a practically viable option for\\ntraffic incident impact prediction.\"),\n",
       " Document(metadata={'title': 'A Survey of Pun Generation: Datasets, Evaluations and Methodologies', 'authors': 'Yuchen Su, Yonghua Zhu, Ruofan Wang, Zijian Huang, Diana Benavides-Prado, Michael Witbrock', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04793v1'}, page_content='Pun generation seeks to creatively modify linguistic elements in text to\\nproduce humour or evoke double meanings. It also aims to preserve coherence and\\ncontextual appropriateness, making it useful in creative writing and\\nentertainment across various media and contexts. Although pun generation has\\nreceived considerable attention in computational linguistics, there is\\ncurrently no dedicated survey that systematically reviews this specific area.\\nTo bridge this gap, this paper provides a comprehensive review of pun\\ngeneration datasets and methods across different stages, including conventional\\napproaches, deep learning techniques, and pre-trained language models.\\nAdditionally, we summarise both automated and human evaluation metrics used to\\nassess the quality of pun generation. Finally, we discuss the research\\nchallenges and propose promising directions for future work.'),\n",
       " Document(metadata={'title': 'Model Compression using Progressive Channel Pruning', 'authors': 'Jinyang Guo, Weichen Zhang, Wanli Ouyang, Dong Xu', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04792v1'}, page_content='In this work, we propose a simple but effective channel pruning framework\\ncalled Progressive Channel Pruning (PCP) to accelerate Convolutional Neural\\nNetworks (CNNs). In contrast to the existing channel pruning methods that prune\\nchannels only once per layer in a layer-by-layer fashion, our new progressive\\nframework iteratively prunes a small number of channels from several selected\\nlayers, which consists of a three-step attempting-selecting-pruning pipeline in\\neach iteration. In the attempting step, we attempt to prune a pre-defined\\nnumber of channels from one layer by using any existing channel pruning methods\\nand estimate the accuracy drop for this layer based on the labelled samples in\\nthe validation set. In the selecting step, based on the estimated accuracy\\ndrops for all layers, we propose a greedy strategy to automatically select a\\nset of layers that will lead to less overall accuracy drop after pruning these\\nlayers. In the pruning step, we prune a small number of channels from these\\nselected layers. We further extend our PCP framework to prune channels for the\\ndeep transfer learning methods like Domain Adversarial Neural Network (DANN),\\nin which we effectively reduce the data distribution mismatch in the channel\\npruning process by using both labelled samples from the source domain and\\npseudo-labelled samples from the target domain. Our comprehensive experiments\\non two benchmark datasets demonstrate that our PCP framework outperforms the\\nexisting channel pruning approaches under both supervised learning and transfer\\nlearning settings.'),\n",
       " Document(metadata={'title': 'Interaction-Merged Motion Planning: Effectively Leveraging Diverse Motion Datasets for Robust Planning', 'authors': 'Giwon Lee, Wooseong Jeong, Daehee Park, Jaewoo Jeong, Kuk-Jin Yoon', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04790v1'}, page_content='Motion planning is a crucial component of autonomous robot driving. While\\nvarious trajectory datasets exist, effectively utilizing them for a target\\ndomain remains challenging due to differences in agent interactions and\\nenvironmental characteristics. Conventional approaches, such as domain\\nadaptation or ensemble learning, leverage multiple source datasets but suffer\\nfrom domain imbalance, catastrophic forgetting, and high computational costs.\\nTo address these challenges, we propose Interaction-Merged Motion Planning\\n(IMMP), a novel approach that leverages parameter checkpoints trained on\\ndifferent domains during adaptation to the target domain. IMMP follows a\\ntwo-step process: pre-merging to capture agent behaviors and interactions,\\nsufficiently extracting diverse information from the source domain, followed by\\nmerging to construct an adaptable model that efficiently transfers diverse\\ninteractions to the target domain. Our method is evaluated on various planning\\nbenchmarks and models, demonstrating superior performance compared to\\nconventional approaches.'),\n",
       " Document(metadata={'title': 'From Imitation to Innovation: The Emergence of AI Unique Artistic Styles and the Challenge of Copyright Protection', 'authors': 'Zexi Jia, Chuanwei Huang, Yeshuang Zhu, Hongyan Fei, Ying Deng, Zhiqiang Yuan, Jiapei Zhang, Jinchao Zhang, Jie Zhou', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04769v1'}, page_content='Current legal frameworks consider AI-generated works eligible for copyright\\nprotection when they meet originality requirements and involve substantial\\nhuman intellectual input. However, systematic legal standards and reliable\\nevaluation methods for AI art copyrights are lacking. Through comprehensive\\nanalysis of legal precedents, we establish three essential criteria for\\ndetermining distinctive artistic style: stylistic consistency, creative\\nuniqueness, and expressive accuracy. To address these challenges, we introduce\\nArtBulb, an interpretable and quantifiable framework for AI art copyright\\njudgment that combines a novel style description-based multimodal clustering\\nmethod with multimodal large language models (MLLMs). We also present AICD, the\\nfirst benchmark dataset for AI art copyright annotated by artists and legal\\nexperts. Experimental results demonstrate that ArtBulb outperforms existing\\nmodels in both quantitative and qualitative evaluations. Our work aims to\\nbridge the gap between the legal and technological communities and bring\\ngreater attention to the societal issue of AI art copyrights.'),\n",
       " Document(metadata={'title': 'FurniMAS: Language-Guided Furniture Decoration using Multi-Agent System', 'authors': 'Toan Nguyen, Tri Le, Quang Nguyen, Anh Nguyen', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04770v1'}, page_content='Furniture decoration is an important task in various industrial applications.\\nHowever, achieving a high-quality decorative result is often time-consuming and\\nrequires specialized artistic expertise. To tackle these challenges, we explore\\nhow multi-agent systems can assist in automating the decoration process. We\\npropose FurniMAS, a multi-agent system for automatic furniture decoration.\\nSpecifically, given a human prompt and a household furniture item such as a\\nworking desk or a TV stand, our system suggests relevant assets with\\nappropriate styles and materials, and arranges them on the item, ensuring the\\ndecorative result meets functionality, aesthetic, and ambiance preferences.\\nFurniMAS assembles a hybrid team of LLM-based and non-LLM agents, each\\nfulfilling distinct roles in a typical decoration project. These agents\\ncollaborate through communication, logical reasoning, and validation to\\ntransform the requirements into the final outcome. Extensive experiments\\ndemonstrate that our FurniMAS significantly outperforms other baselines in\\ngenerating high-quality 3D decor.'),\n",
       " Document(metadata={'title': 'CoSteer: Collaborative Decoding-Time Personalization via Local Delta Steering', 'authors': 'Hang Lv, Sheng Liang, Hao Wang, Hongchao Gu, Yaxiong Wu, Wei Guo, Defu Lian, Yong Liu, Enhong Chen', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04756v1'}, page_content=\"Personalized text generation has become crucial for adapting language models\\nto diverse and evolving users' personal context across cultural, temporal, and\\ncontextual dimensions. While existing methods often rely on centralized\\nfine-tuning or static preference alignment, they struggle to achieve real-time\\nadaptation under resource constraints inherent to personal devices. This\\nlimitation creates a dilemma: large cloud-based models lack access to localized\\nuser-specific information, while small on-device models cannot match the\\ngeneration quality of their cloud counterparts. To address this dichotomy, we\\npresent CoSteer, a novel collaborative framework that enables decoding-time\\npersonalization through localized delta steering. Our key insight lies in\\nleveraging the logits difference between personal context-aware and -agnostic\\noutputs from local small models as steering signals for cloud-based LLMs.\\nSpecifically, we formulate token-level optimization as an online learning\\nproblem, where local delta vectors dynamically adjust the remote LLM's logits\\nwithin the on-device environment. This approach preserves privacy by\\ntransmitting only the final steered tokens rather than raw data or intermediate\\nvectors, while maintaining cloud-based LLMs' general capabilities without\\nfine-tuning. Through comprehensive experiments on various personalized\\ngeneration tasks, we demonstrate that CoSteer effectively assists LLMs in\\ngenerating personalized content by leveraging locally stored user profiles and\\nhistories, ensuring privacy preservation through on-device data processing\\nwhile maintaining acceptable computational overhead.\"),\n",
       " Document(metadata={'title': 'Large Language Models for Network Intrusion Detection Systems: Foundations, Implementations, and Future Directions', 'authors': 'Shuo Yang, Xinran Zheng, Xinchen Zhang, Jinfeng Xu, Jinze Li, Donglin Xie, Weicai Long, Edith C. H. Ngai', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04752v1'}, page_content='Large Language Models (LLMs) have revolutionized various fields with their\\nexceptional capabilities in understanding, processing, and generating\\nhuman-like text. This paper investigates the potential of LLMs in advancing\\nNetwork Intrusion Detection Systems (NIDS), analyzing current challenges,\\nmethodologies, and future opportunities. It begins by establishing a\\nfoundational understanding of NIDS and LLMs, exploring the enabling\\ntechnologies that bridge the gap between intelligent and cognitive systems in\\nAI-driven NIDS. While Intelligent NIDS leverage machine learning and deep\\nlearning to detect threats based on learned patterns, they often lack\\ncontextual awareness and explainability. In contrast, Cognitive NIDS integrate\\nLLMs to process both structured and unstructured security data, enabling deeper\\ncontextual reasoning, explainable decision-making, and automated response for\\nintrusion behaviors. Practical implementations are then detailed, highlighting\\nLLMs as processors, detectors, and explainers within a comprehensive AI-driven\\nNIDS pipeline. Furthermore, the concept of an LLM-centered Controller is\\nproposed, emphasizing its potential to coordinate intrusion detection\\nworkflows, optimizing tool collaboration and system performance. Finally, this\\npaper identifies critical challenges and opportunities, aiming to foster\\ninnovation in developing reliable, adaptive, and explainable NIDS. By\\npresenting the transformative potential of LLMs, this paper seeks to inspire\\nadvancement in next-generation network security systems.'),\n",
       " Document(metadata={'title': 'MCFormer: A Multi-Cost-Volume Network and Comprehensive Benchmark for Particle Image Velocimetry', 'authors': 'Zicheng Lin, Xiaoqiang Li, Yichao Wang, Chuan Zhu', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04750v1'}, page_content=\"Particle Image Velocimetry (PIV) is fundamental to fluid dynamics, yet deep\\nlearning applications face significant hurdles. A critical gap exists: the lack\\nof comprehensive evaluation of how diverse optical flow models perform\\nspecifically on PIV data, largely due to limitations in available datasets and\\nthe absence of a standardized benchmark. This prevents fair comparison and\\nhinders progress. To address this, our primary contribution is a novel,\\nlarge-scale synthetic PIV benchmark dataset generated from diverse CFD\\nsimulations (JHTDB and Blasius). It features unprecedented variety in particle\\ndensities, flow velocities, and continuous motion, enabling, for the first\\ntime, a standardized and rigorous evaluation of various optical flow and PIV\\nalgorithms. Complementing this, we propose Multi Cost Volume PIV (MCFormer), a\\nnew deep network architecture leveraging multi-frame temporal information and\\nmultiple cost volumes, specifically designed for PIV's sparse nature. Our\\ncomprehensive benchmark evaluation, the first of its kind, reveals significant\\nperformance variations among adapted optical flow models and demonstrates that\\nMCFormer significantly outperforms existing methods, achieving the lowest\\noverall normalized endpoint error (NEPE). This work provides both a\\nfoundational benchmark resource essential for future PIV research and a\\nstate-of-the-art method tailored for PIV challenges. We make our benchmark\\ndataset and code publicly available to foster future research in this area.\"),\n",
       " Document(metadata={'title': 'LLM-based Question-Answer Framework for Sensor-driven HVAC System Interaction', 'authors': 'Sungmin Lee, Minju Kang, Joonhee Lee, Seungyong Lee, Dongju Kim, Jingi Hong, Jun Shin, Pei Zhang, JeongGil Ko', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04748v1'}, page_content='Question-answering (QA) interfaces powered by large language models (LLMs)\\npresent a promising direction for improving interactivity with HVAC system\\ninsights, particularly for non-expert users. However, enabling accurate,\\nreal-time, and context-aware interactions with HVAC systems introduces unique\\nchallenges, including the integration of frequently updated sensor data,\\ndomain-specific knowledge grounding, and coherent multi-stage reasoning. In\\nthis paper, we present JARVIS, a two-stage LLM-based QA framework tailored for\\nsensor data-driven HVAC system interaction. JARVIS employs an Expert-LLM to\\ntranslate high-level user queries into structured execution instructions, and\\nan Agent that performs SQL-based data retrieval, statistical processing, and\\nfinal response generation. To address HVAC-specific challenges, JARVIS\\nintegrates (1) an adaptive context injection strategy for efficient HVAC and\\ndeployment-specific information integration, (2) a parameterized SQL builder\\nand executor to improve data access reliability, and (3) a bottom-up planning\\nscheme to ensure consistency across multi-stage response generation. We\\nevaluate JARVIS using real-world data collected from a commercial HVAC system\\nand a ground truth QA dataset curated by HVAC experts to demonstrate its\\neffectiveness in delivering accurate and interpretable responses across diverse\\nqueries. Results show that JARVIS consistently outperforms baseline and\\nablation variants in both automated and user-centered assessments, achieving\\nhigh response quality and accuracy.'),\n",
       " Document(metadata={'title': 'Activation Steering for Chain-of-Thought Compression', 'authors': 'Seyedarmin Azizi, Erfan Baghaei Potraghloo, Massoud Pedram', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04742v1'}, page_content='Large language models (LLMs) excel at complex reasoning when they include\\nintermediate steps, known as \"chains of thought\" (CoTs). However, these\\nrationales are often overly verbose, even for simple problems, leading to\\nwasted context, increased latency, and higher energy consumption. We observe\\nthat verbose, English-heavy CoTs and concise, math-centric CoTs occupy distinct\\nregions in the model\\'s residual-stream activation space. By extracting and\\ninjecting a \"steering vector\" to transition between these modes, we can\\nreliably shift generation toward more concise reasoning, effectively\\ncompressing CoTs without retraining. We formalize this approach as\\nActivation-Steered Compression (ASC), an inference-time technique that shortens\\nreasoning traces by directly modifying hidden representations. In addition, we\\nprovide a theoretical analysis of the impact of ASC on the output distribution,\\nderived from a closed-form KL-divergence-bounded constraint to regulate\\nsteering strength. Using only 100 paired verbose and concise examples, ASC\\nachieves up to 67.43% reduction in CoT length on MATH500 and GSM8K datasets,\\nwhile maintaining accuracy across 7B, 8B, and 32B parameter models. As a\\ntraining-free method, ASC introduces negligible runtime overhead and, on\\nMATH500, delivers an average 2.73x speedup in end-to-end reasoning wall-clock\\ntime on an 8B model. This makes ASC a practical and efficient tool for\\nstreamlining the deployment of reasoning-capable LLMs in latency- or\\ncost-sensitive settings. The code is available at:\\nhttps://github.com/ArminAzizi98/ASC'),\n",
       " Document(metadata={'title': 'Word stress in self-supervised speech models: A cross-linguistic comparison', 'authors': 'Martijn Bentum, Louis ten Bosch, Tomas O. Lentz', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04738v1'}, page_content='In this paper we study word stress representations learned by self-supervised\\nspeech models (S3M), specifically the Wav2vec 2.0 model. We investigate the S3M\\nrepresentations of word stress for five different languages: Three languages\\nwith variable or lexical stress (Dutch, English and German) and two languages\\nwith fixed or demarcative stress (Hungarian and Polish). We train diagnostic\\nstress classifiers on S3M embeddings and show that they can distinguish between\\nstressed and unstressed syllables in read-aloud short sentences with high\\naccuracy. We also tested language-specificity effects of S3M word stress. The\\nresults indicate that the word stress representations are language-specific,\\nwith a greater difference between the set of variable versus the set of fixed\\nstressed languages.'),\n",
       " Document(metadata={'title': 'ChipSeek-R1: Generating Human-Surpassing RTL with LLM via Hierarchical Reward-Driven Reinforcement Learning', 'authors': 'Zhirong Chen, Kaiyan Chang, Zhuolin Li, Xinyang He, Chujie Chen, Cangyuan Li, Mengdi Wang, Haobo Xu, Yinhe Han, Ying Wang', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04736v1'}, page_content=\"Large Language Models (LLMs) show significant potential for automating\\nRegister-Transfer Level (RTL) code generation. However, current approaches face\\na critical challenge: they can not simultaneously optimize for functional\\ncorrectness and hardware quality (Power, Performance, Area - PPA). Methods\\nbased on supervised fine-tuning often generate functionally correct but\\nPPA-suboptimal code, lacking mechanisms to learn optimization principles. In\\ncontrast, post-processing techniques that attempt to improve PPA metrics after\\ngeneration are often inefficient because they operate externally without\\nupdating the LLM's parameters, thus failing to enhance the model's intrinsic\\ndesign capabilities.\\n  To bridge this gap, we introduce ChipSeek-R1, a hierarchical reward-driven\\nreinforcement learning framework to train LLMs to generate RTL code that\\nachieves both functional correctness and optimized PPA metrics. ChipSeek-R1\\nemploys a hierarchical reward system, which incorporates direct feedback on\\nsyntax, functional correctness (from simulators) and PPA metrics (from\\nsynthesis tools) during reinforcement learning. This enables the model to learn\\ncomplex hardware design trade-offs via trial-and-error, generating RTL code\\nthat is both functionally correct and PPA-optimized. Evaluating ChipSeek-R1 on\\nstandard benchmarks (VerilogEval, RTLLM), we achieve state-of-the-art results\\nin functional correctness. Notably, on the RTLLM benchmark, ChipSeek-R1\\ngenerated 27 RTL designs surpassing the PPA metrics of the original\\nhuman-written code. Our findings demonstrate the effectiveness of integrating\\ntoolchain feedback into LLM training and highlight the potential for\\nreinforcement learning to enable automated generation of human-surpassing RTL\\ncode. We open-source our code in anonymous github.\"),\n",
       " Document(metadata={'title': 'Losing Control: Data Poisoning Attack on Guided Diffusion via ControlNet', 'authors': 'Raz Lapid, Almog Dubin', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04726v1'}, page_content='Text-to-image diffusion models have achieved remarkable success in\\ntranslating textual prompts into high-fidelity images. ControlNets further\\nextend these models by allowing precise, image-based conditioning (e.g., edge\\nmaps, depth, pose), enabling fine-grained control over structure and style.\\nHowever, their dependence on large, publicly scraped datasets -- and the\\nincreasing use of community-shared data for fine-tuning -- exposes them to\\nstealthy data poisoning attacks. In this work, we introduce a novel data\\npoisoning method that manipulates ControlNets to generate images containing\\nspecific content without any text triggers. By injecting poisoned samples --\\neach pairing a subtly triggered input with an NSFW target -- the model retains\\nclean-prompt fidelity yet reliably produces NSFW outputs when the trigger is\\npresent. On large-scale, high-quality datasets, our backdoor achieves high\\nattack success rate while remaining imperceptible in raw inputs. These results\\nreveal a critical vulnerability in open-source ControlNets pipelines and\\nunderscore the need for robust data sanitization and defense mechanisms.'),\n",
       " Document(metadata={'title': \"Who's the Mole? Modeling and Detecting Intention-Hiding Malicious Agents in LLM-Based Multi-Agent Systems\", 'authors': 'Yizhe Xie, Congcong Zhu, Xinyue Zhang, Minghao Wang, Chi Liu, Minglu Zhu, Tianqing Zhu', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04724v1'}, page_content='Multi-agent systems powered by Large Language Models (LLM-MAS) demonstrate\\nremarkable capabilities in collaborative problem-solving. While LLM-MAS exhibit\\nstrong collaborative abilities, the security risks in their communication and\\ncoordination remain underexplored. We bridge this gap by systematically\\ninvestigating intention-hiding threats in LLM-MAS, and design four\\nrepresentative attack paradigms that subtly disrupt task completion while\\nmaintaining high concealment. These attacks are evaluated in centralized,\\ndecentralized, and layered communication structures. Experiments conducted on\\nsix benchmark datasets, including MMLU, MMLU-Pro, HumanEval, GSM8K, arithmetic,\\nand biographies, demonstrate that they exhibit strong disruptive capabilities.\\nTo identify these threats, we propose a psychology-based detection framework\\nAgentXposed, which combines the HEXACO personality model with the Reid\\nTechnique, using progressive questionnaire inquiries and behavior-based\\nmonitoring. Experiments conducted on six types of attacks show that our\\ndetection framework effectively identifies all types of malicious behaviors.\\nThe detection rate for our intention-hiding attacks is slightly lower than that\\nof the two baselines, Incorrect Fact Injection and Dark Traits Injection,\\ndemonstrating the effectiveness of intention concealment. Our findings reveal\\nthe structural and behavioral risks posed by intention-hiding attacks and offer\\nvaluable insights into securing LLM-based multi-agent systems through\\npsychological perspectives, which contributes to a deeper understanding of\\nmulti-agent safety. The code and data are available at\\nhttps://anonymous.4open.science/r/AgentXposed-F814.'),\n",
       " Document(metadata={'title': 'LumiCRS: Asymmetric Contrastive Prototype Learning for Long-Tail Conversational Movie Recommendation', 'authors': 'Jinzhi Wang, Bin Li, Qingke Peng, Haozhou Li, Zeyuan Zeng, Ruimeng Li, Biyi Zhou', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04722v1'}, page_content='Conversational recommender systems (CRSs) often suffer from an extreme\\nlong-tail distribution of dialogue data, causing a strong bias toward\\nhead-frequency blockbusters that sacrifices diversity and exacerbates the\\ncold-start problem. An empirical analysis of DCRS and statistics on the REDIAL\\ncorpus show that only 10% of head movies account for nearly half of all\\nmentions, whereas about 70% of tail movies receive merely 26% of the attention.\\nThis imbalance gives rise to three critical challenges: head over-fitting, body\\nrepresentation drift, and tail sparsity. To address these issues, we propose\\nLumiCRS, an end-to-end framework that mitigates long-tail imbalance through\\nthree mutually reinforcing layers: (i) an Adaptive Comprehensive Focal Loss\\n(ACFL) that dynamically adjusts class weights and focusing factors to curb head\\nover-fitting and reduce popularity bias; (ii) Prototype Learning for Long-Tail\\nRecommendation, which selects semantic, affective, and contextual prototypes to\\nguide clustering and stabilize body and tail representations; and (iii) a\\nGPT-4o-driven prototype-guided dialogue augmentation module that automatically\\ngenerates diverse long-tail conversational snippets to alleviate tail sparsity\\nand distribution shift. Together, these strategies enable LumiCRS to markedly\\nimprove recommendation accuracy, diversity, and fairness: on the REDIAL and\\nINSPIRED benchmarks, LumiCRS boosts Recall@10 and Tail-Recall@10 by 7-15% over\\nfifteen strong baselines, while human evaluations confirm superior fluency,\\ninformativeness, and long-tail relevance. These results demonstrate the\\neffectiveness of multi-layer collaboration in building an efficient and fair\\nlong-tail conversational recommender.'),\n",
       " Document(metadata={'title': 'Advocate for Complete Benchmarks for Formal Reasoning with Formal/Informal Statements and Formal/Informal Proofs', 'authors': 'Roozbeh Yousefzadeh, Xuenan Cao', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04719v1'}, page_content='This position paper provides a critical but constructive discussion of\\ncurrent practices in benchmarking and evaluative practices in the field of\\nformal reasoning and automated theorem proving. We take the position that open\\ncode, open data, and benchmarks that are complete and error-free will\\naccelerate progress in this field. We identify practices that create barriers\\nto contributing to this field and suggest ways to remove them. We also discuss\\nsome of the practices that might produce misleading evaluative information. We\\naim to create discussions that bring together people from various groups\\ncontributing to automated theorem proving, autoformalization, and informal\\nreasoning.'),\n",
       " Document(metadata={'title': 'Geometric-Guided Few-Shot Dental Landmark Detection with Human-Centric Foundation Model', 'authors': 'Anbang Wang, Marawan Elbatel, Keyuan Liu, Lizhuo Lin, Meng Lan, Yanqi Yang, Xiaomeng Li', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04710v1'}, page_content=\"Accurate detection of anatomic landmarks is essential for assessing alveolar\\nbone and root conditions, thereby optimizing clinical outcomes in orthodontics,\\nperiodontics, and implant dentistry. Manual annotation of landmarks on\\ncone-beam computed tomography (CBCT) by dentists is time-consuming,\\nlabor-intensive, and subject to inter-observer variability. Deep learning-based\\nautomated methods present a promising approach to streamline this process\\nefficiently. However, the scarcity of training data and the high cost of expert\\nannotations hinder the adoption of conventional deep learning techniques. To\\novercome these challenges, we introduce GeoSapiens, a novel few-shot learning\\nframework designed for robust dental landmark detection using limited annotated\\nCBCT of anterior teeth. Our GeoSapiens framework comprises two key components:\\n(1) a robust baseline adapted from Sapiens, a foundational model that has\\nachieved state-of-the-art performance in human-centric vision tasks, and (2) a\\nnovel geometric loss function that improves the model's capacity to capture\\ncritical geometric relationships among anatomical structures. Experiments\\nconducted on our collected dataset of anterior teeth landmarks revealed that\\nGeoSapiens surpassed existing landmark detection methods, outperforming the\\nleading approach by an 8.18% higher success detection rate at a strict 0.5 mm\\nthreshold-a standard widely recognized in dental diagnostics. Code is available\\nat: https://github.com/xmed-lab/GeoSapiens.\"),\n",
       " Document(metadata={'title': 'UrbanMind: Towards Urban General Intelligence via Tool-Enhanced Retrieval-Augmented Generation and Multilevel Optimization', 'authors': 'Kai Yang, Zelin Zhu, Chengtao Jian, Hui Ma, Shengjie Zhao, Xiaozhou Ye, Ye Ouyang', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04706v1'}, page_content='Urban general intelligence (UGI) refers to the capacity of AI systems to\\nautonomously perceive, reason, and act within dynamic and complex urban\\nenvironments. In this paper, we introduce UrbanMind, a tool-enhanced\\nretrieval-augmented generation (RAG) framework designed to facilitate UGI.\\nCentral to UrbanMind is a novel architecture based on Continual\\nRetrieval-Augmented MoE-based LLM (C-RAG-LLM), which dynamically incorporates\\ndomain-specific knowledge and evolving urban data to support long-term\\nadaptability. The architecture of C-RAG-LLM aligns naturally with a multilevel\\noptimization framework, where different layers are treated as interdependent\\nsub-problems. Each layer has distinct objectives and can be optimized either\\nindependently or jointly through a hierarchical learning process. The framework\\nis highly flexible, supporting both end-to-end training and partial layer-wise\\noptimization based on resource or deployment constraints. To remain adaptive\\nunder data drift, it is further integrated with an incremental corpus updating\\nmechanism. Evaluations on real-world urban tasks of a variety of complexity\\nverify the effectiveness of the proposed framework. This work presents a\\npromising step toward the realization of general-purpose LLM agents in future\\nurban environments.'),\n",
       " Document(metadata={'title': 'SPATIA: Multimodal Model for Prediction and Generation of Spatial Cell Phenotypes', 'authors': 'Zhenglun Kong, Mufan Qiu, John Boesen, Xiang Lin, Sukwon Yun, Tianlong Chen, Manolis Kellis, Marinka Zitnik', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04704v1'}, page_content='Understanding how cellular morphology, gene expression, and spatial\\norganization jointly shape tissue function is a central challenge in biology.\\nImage-based spatial transcriptomics technologies now provide high-resolution\\nmeasurements of cell images and gene expression profiles, but machine learning\\nmethods typically analyze these modalities in isolation or at limited\\nresolution. We address the problem of learning unified, spatially aware\\nrepresentations that integrate cell morphology, gene expression, and spatial\\ncontext across biological scales. This requires models that can operate at\\nsingle-cell resolution, reason across spatial neighborhoods, and generalize to\\nwhole-slide tissue organization. Here, we introduce SPATIA, a multi-scale\\ngenerative and predictive model for spatial transcriptomics. SPATIA learns\\ncell-level embeddings by fusing image-derived morphological tokens and\\ntranscriptomic vector tokens using cross-attention and then aggregates them at\\nniche and tissue levels using transformer modules to capture spatial\\ndependencies. SPATIA incorporates token merging in its generative diffusion\\ndecoder to synthesize high-resolution cell images conditioned on gene\\nexpression. We assembled a multi-scale dataset consisting of 17 million\\ncell-gene pairs, 1 million niche-gene pairs, and 10,000 tissue-gene pairs\\nacross 49 donors, 17 tissue types, and 12 disease states. We benchmark SPATIA\\nagainst 13 existing models across 12 individual tasks, which span several\\ncategories including cell annotation, cell clustering, gene imputation,\\ncross-modal prediction, and image generation. SPATIA achieves improved\\nperformance over all baselines and generates realistic cell morphologies that\\nreflect transcriptomic perturbations.'),\n",
       " Document(metadata={'title': 'Tempo-R0: A Video-MLLM for Temporal Video Grounding through Efficient Temporal Sensing Reinforcement Learning', 'authors': 'Feng Yue, Zhaoxing Zhang, Junming Jiao, Zhengyu Liang, Shiwen Cao, Feifei Zhang, Rong Shen', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04702v1'}, page_content=\"Temporal Video Grounding (TVG), which requires pinpointing relevant temporal\\nsegments from video based on language query, has always been a highly\\nchallenging task in the field of video understanding. Videos often have a\\nlarger volume of information and redundancy than texts or images. Models should\\npresent comprehensive understanding of the whole video to accurately retrieve\\nquery-relevant clips. We thus propose Tempo-R0: a Video Multimodal Large\\nLanguage Model (Video-MLLM) for the temporal video grounding task via\\nmultimodal temporal sensing reinforcement. Specifically, during the\\npreprocessing stage of our pipeline, we employ Self-adaptive Attention\\nAllocation (SAA) method based on frame content variation to efficiently use the\\nMLLM's limited attention. The Explicit Timestamp-modal Aligned (ETA) method is\\nalso utilized to strengthen our model's capability to perceive the boundaries\\nof events in the video. In the fine-tuning part of our pipeline, we creatively\\napply Partial Irrelevance Refusing-based Group Relative Policy Optimization\\n(PIR-GRPO) in TVG area to foster model's temporal reasoning from not only\\naccepting relevant video-query pairs but also refusing irrelevant ones.\\nExperiments demonstrate that our method accomplishes a notable advantage over\\nSOTA solutions by around 3.5% on both the original QVHighlights testbench and\\nits corrected version with more reasonable ground truth annotations.\"),\n",
       " Document(metadata={'title': 'Bridging KAN and MLP: MJKAN, a Hybrid Architecture with Both Efficiency and Expressiveness', 'authors': 'Hanseon Joo, Hayoung Choi, Ook Lee, Minjong Cheon', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04690v1'}, page_content=\"Kolmogorov-Arnold Networks (KANs) have garnered attention for replacing fixed\\nactivation functions with learnable univariate functions, but they exhibit\\npractical limitations, including high computational costs and performance\\ndeficits in general classification tasks. In this paper, we propose the\\nModulation Joint KAN (MJKAN), a novel neural network layer designed to overcome\\nthese challenges. MJKAN integrates a FiLM (Feature-wise Linear Modulation)-like\\nmechanism with Radial Basis Function (RBF) activations, creating a hybrid\\narchitecture that combines the non-linear expressive power of KANs with the\\nefficiency of Multilayer Perceptrons (MLPs). We empirically validated MJKAN's\\nperformance across a diverse set of benchmarks, including function regression,\\nimage classification (MNIST, CIFAR-10/100), and natural language processing (AG\\nNews, SMS Spam). The results demonstrate that MJKAN achieves superior\\napproximation capabilities in function regression tasks, significantly\\noutperforming MLPs, with performance improving as the number of basis functions\\nincreases. Conversely, in image and text classification, its performance was\\ncompetitive with MLPs but revealed a critical dependency on the number of basis\\nfunctions. We found that a smaller basis size was crucial for better\\ngeneralization, highlighting that the model's capacity must be carefully tuned\\nto the complexity of the data to prevent overfitting. In conclusion, MJKAN\\noffers a flexible architecture that inherits the theoretical advantages of KANs\\nwhile improving computational efficiency and practical viability.\"),\n",
       " Document(metadata={'title': 'Identify, Isolate, and Purge: Mitigating Hallucinations in LVLMs via Self-Evolving Distillation', 'authors': 'Wenhao Li, Xiu Su, Jingyi Wu, Feng Yang, Yang Liu, Yi Chen, Shan You, Chang Xu', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04680v1'}, page_content='Large Vision-Language Models (LVLMs) have demonstrated remarkable\\nadvancements in numerous areas such as multimedia. However, hallucination\\nissues significantly limit their credibility and application potential.\\nExisting mitigation methods typically rely on external tools or the comparison\\nof multi-round inference, which significantly increase inference time. In this\\npaper, we propose \\\\textbf{SE}lf-\\\\textbf{E}volving \\\\textbf{D}istillation\\n(\\\\textbf{SEED}), which identifies hallucinations within the inner knowledge of\\nLVLMs, isolates and purges them, and then distills the purified knowledge back\\ninto the model, enabling self-evolution. Furthermore, we identified that\\ntraditional distillation methods are prone to inducing void spaces in the\\noutput space of LVLMs. To address this issue, we propose a Mode-Seeking\\nEvolving approach, which performs distillation to capture the dominant modes of\\nthe purified knowledge distribution, thereby avoiding the chaotic results that\\ncould emerge from void spaces. Moreover, we introduce a Hallucination\\nElimination Adapter, which corrects the dark knowledge of the original model by\\nlearning purified knowledge. Extensive experiments on multiple benchmarks\\nvalidate the superiority of our SEED, demonstrating substantial improvements in\\nmitigating hallucinations for representative LVLM models such as LLaVA-1.5 and\\nInternVL2. Remarkably, the F1 score of LLaVA-1.5 on the hallucination\\nevaluation metric POPE-Random improved from 81.3 to 88.3.'),\n",
       " Document(metadata={'title': 'Trojan Horse Prompting: Jailbreaking Conversational Multimodal Models by Forging Assistant Message', 'authors': 'Wei Duan, Li Qian', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04673v1'}, page_content='The rise of conversational interfaces has greatly enhanced LLM usability by\\nleveraging dialogue history for sophisticated reasoning. However, this reliance\\nintroduces an unexplored attack surface. This paper introduces Trojan Horse\\nPrompting, a novel jailbreak technique. Adversaries bypass safety mechanisms by\\nforging the model\\'s own past utterances within the conversational history\\nprovided to its API. A malicious payload is injected into a model-attributed\\nmessage, followed by a benign user prompt to trigger harmful content\\ngeneration. This vulnerability stems from Asymmetric Safety Alignment: models\\nare extensively trained to refuse harmful user requests but lack comparable\\nskepticism towards their own purported conversational history. This implicit\\ntrust in its \"past\" creates a high-impact vulnerability. Experimental\\nvalidation on Google\\'s Gemini-2.0-flash-preview-image-generation shows Trojan\\nHorse Prompting achieves a significantly higher Attack Success Rate (ASR) than\\nestablished user-turn jailbreaking methods. These findings reveal a fundamental\\nflaw in modern conversational AI security, necessitating a paradigm shift from\\ninput-level filtering to robust, protocol-level validation of conversational\\ncontext integrity.'),\n",
       " Document(metadata={'title': \"What's Making That Sound Right Now? Video-centric Audio-Visual Localization\", 'authors': 'Hahyeon Choi, Junhoo Lee, Nojun Kwak', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04667v1'}, page_content='Audio-Visual Localization (AVL) aims to identify sound-emitting sources\\nwithin a visual scene. However, existing studies focus on image-level\\naudio-visual associations, failing to capture temporal dynamics. Moreover, they\\nassume simplified scenarios where sound sources are always visible and involve\\nonly a single object. To address these limitations, we propose AVATAR, a\\nvideo-centric AVL benchmark that incorporates high-resolution temporal\\ninformation. AVATAR introduces four distinct scenarios -- Single-sound,\\nMixed-sound, Multi-entity, and Off-screen -- enabling a more comprehensive\\nevaluation of AVL models. Additionally, we present TAVLO, a novel video-centric\\nAVL model that explicitly integrates temporal information. Experimental results\\nshow that conventional methods struggle to track temporal variations due to\\ntheir reliance on global audio features and frame-level mappings. In contrast,\\nTAVLO achieves robust and precise audio-visual alignment by leveraging\\nhigh-resolution temporal modeling. Our work empirically demonstrates the\\nimportance of temporal dynamics in AVL and establishes a new standard for\\nvideo-centric audio-visual localization.'),\n",
       " Document(metadata={'title': 'LTMSformer: A Local Trend-Aware Attention and Motion State Encoding Transformer for Multi-Agent Trajectory Prediction', 'authors': 'Yixin Yan, Yang Li, Yuanfan Wang, Xiaozhou Zhou, Beihao Xia, Manjiang Hu, Hongmao Qin', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04634v1'}, page_content='It has been challenging to model the complex temporal-spatial dependencies\\nbetween agents for trajectory prediction. As each state of an agent is closely\\nrelated to the states of adjacent time steps, capturing the local temporal\\ndependency is beneficial for prediction, while most studies often overlook it.\\nBesides, learning the high-order motion state attributes is expected to enhance\\nspatial interaction modeling, but it is rarely seen in previous works. To\\naddress this, we propose a lightweight framework, LTMSformer, to extract\\ntemporal-spatial interaction features for multi-modal trajectory prediction.\\nSpecifically, we introduce a Local Trend-Aware Attention mechanism to capture\\nthe local temporal dependency by leveraging a convolutional attention mechanism\\nwith hierarchical local time boxes. Next, to model the spatial interaction\\ndependency, we build a Motion State Encoder to incorporate high-order motion\\nstate attributes, such as acceleration, jerk, heading, etc. To further refine\\nthe trajectory prediction, we propose a Lightweight Proposal Refinement Module\\nthat leverages Multi-Layer Perceptrons for trajectory embedding and generates\\nthe refined trajectories with fewer model parameters. Experiment results on the\\nArgoverse 1 dataset demonstrate that our method outperforms the baseline\\nHiVT-64, reducing the minADE by approximately 4.35%, the minFDE by 8.74%, and\\nthe MR by 20%. We also achieve higher accuracy than HiVT-128 with a 68%\\nreduction in model size.'),\n",
       " Document(metadata={'title': 'Can Prompt Difficulty be Online Predicted for Accelerating RL Finetuning of Reasoning Models?', 'authors': 'Yun Qu, Qi Cheems Wang, Yixiu Mao, Vincent Tao Hu, Xiangyang Ji', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04632v1'}, page_content=\"Recent advances have witnessed the effectiveness of reinforcement learning\\n(RL) finetuning in enhancing the reasoning capabilities of large language\\nmodels (LLMs). The optimization process often requires numerous iterations to\\nachieve satisfactory performance, resulting in high computational costs due to\\nthe need for frequent prompt evaluations under intensive LLM interactions and\\nrepeated policy updates. Appropriate online prompt selection methods reduce\\niteration steps by prioritizing informative prompts during training, while the\\npipeline's reliance on exhaustive prompt evaluation and subset selection for\\noptimization still incurs substantial computational overhead due to frequent\\nLLM inference calls. Distinguished from these direct evaluate-then-select\\nschemes, this work investigates iterative approximate evaluation for arbitrary\\nprompts and introduces Model Predictive Prompt Selection (MoPPS), a Bayesian\\nrisk-predictive framework that online estimates prompt difficulty without\\nrequiring costly LLM interactions. Technically, MoPPS models each prompt's\\nsuccess rate as a latent variable, performs streaming Bayesian inference, and\\nemploys posterior sampling in a constructed multi-armed bandit machine,\\nenabling sample efficient and adaptive prompt selection. Extensive experiments\\nacross mathematics, planning, and vision-based geometry tasks show that MoPPS\\nreliably predicts prompt difficulty and accelerates training with significantly\\nreduced LLM rollouts.\"),\n",
       " Document(metadata={'title': 'Learning Robust Stereo Matching in the Wild with Selective Mixture-of-Experts', 'authors': 'Yun Wang, Longguang Wang, Chenghao Zhang, Yongjian Zhang, Zhanjie Zhang, Ao Ma, Chenyou Fan, Tin Lun Lam, Junjie Hu', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04631v1'}, page_content=\"Recently, learning-based stereo matching networks have advanced\\nsignificantly. However, they often lack robustness and struggle to achieve\\nimpressive cross-domain performance due to domain shifts and imbalanced\\ndisparity distributions among diverse datasets. Leveraging Vision Foundation\\nModels (VFMs) can intuitively enhance the model's robustness, but integrating\\nsuch a model into stereo matching cost-effectively to fully realize their\\nrobustness remains a key challenge. To address this, we propose SMoEStereo, a\\nnovel framework that adapts VFMs for stereo matching through a tailored,\\nscene-specific fusion of Low-Rank Adaptation (LoRA) and Mixture-of-Experts\\n(MoE) modules. SMoEStereo introduces MoE-LoRA with adaptive ranks and\\nMoE-Adapter with adaptive kernel sizes. The former dynamically selects optimal\\nexperts within MoE to adapt varying scenes across domains, while the latter\\ninjects inductive bias into frozen VFMs to improve geometric feature\\nextraction. Importantly, to mitigate computational overhead, we further propose\\na lightweight decision network that selectively activates MoE modules based on\\ninput complexity, balancing efficiency with accuracy. Extensive experiments\\ndemonstrate that our method exhibits state-of-the-art cross-domain and joint\\ngeneralization across multiple benchmarks without dataset-specific adaptation.\\nThe code is available at\\n\\\\textcolor{red}{https://github.com/cocowy1/SMoE-Stereo}.\"),\n",
       " Document(metadata={'title': 'Knowledge-Aware Self-Correction in Language Models via Structured Memory Graphs', 'authors': 'Swayamjit Saha', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04625v1'}, page_content='Large Language Models (LLMs) are powerful yet prone to generating factual\\nerrors, commonly referred to as hallucinations. We present a lightweight,\\ninterpretable framework for knowledge-aware self-correction of LLM outputs\\nusing structured memory graphs based on RDF triples. Without retraining or\\nfine-tuning, our method post-processes model outputs and corrects factual\\ninconsistencies via external semantic memory. We demonstrate the approach using\\nDistilGPT-2 and show promising results on simple factual prompts.'),\n",
       " Document(metadata={'title': 'Hierarchical Intent-guided Optimization with Pluggable LLM-Driven Semantics for Session-based Recommendation', 'authors': 'Jinpeng Chen, Jianxiang He, Huan Li, Senzhang Wang, Yuan Cao, Kaimin Wei, Zhenye Yang, Ye Ji', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04623v1'}, page_content=\"Session-based Recommendation (SBR) aims to predict the next item a user will\\nlikely engage with, using their interaction sequence within an anonymous\\nsession. Existing SBR models often focus only on single-session information,\\nignoring inter-session relationships and valuable cross-session insights. Some\\nmethods try to include inter-session data but struggle with noise and\\nirrelevant information, reducing performance. Additionally, most models rely on\\nitem ID co-occurrence and overlook rich semantic details, limiting their\\nability to capture fine-grained item features. To address these challenges, we\\npropose a novel hierarchical intent-guided optimization approach with pluggable\\nLLM-driven semantic learning for session-based recommendations, called HIPHOP.\\nFirst, we introduce a pluggable embedding module based on large language models\\n(LLMs) to generate high-quality semantic representations, enhancing item\\nembeddings. Second, HIPHOP utilizes graph neural networks (GNNs) to model item\\ntransition relationships and incorporates a dynamic multi-intent capturing\\nmodule to address users' diverse interests within a session. Additionally, we\\ndesign a hierarchical inter-session similarity learning module, guided by user\\nintent, to capture global and local session relationships, effectively\\nexploring users' long-term and short-term interests. To mitigate noise, an\\nintent-guided denoising strategy is applied during inter-session learning.\\nFinally, we enhance the model's discriminative capability by using contrastive\\nlearning to optimize session representations. Experiments on multiple datasets\\nshow that HIPHOP significantly outperforms existing methods, demonstrating its\\neffectiveness in improving recommendation quality. Our code is available:\\nhttps://github.com/hjx159/HIPHOP.\"),\n",
       " Document(metadata={'title': 'Multimodal LLM Integrated Semantic Communications for 6G Immersive Experiences', 'authors': 'Yusong Zhang, Yuxuan Sun, Lei Guo, Wei Chen, Bo Ai, Deniz Gunduz', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04621v1'}, page_content='6G networks promise revolutionary immersive communication experiences\\nincluding augmented reality (AR), virtual reality (VR), and holographic\\ncommunications. These applications demand high-dimensional multimodal data\\ntransmission and intelligent data processing in real-time, which is extremely\\nchallenging over resource-limited wireless communication systems. Moreover, a\\njoint understanding of the environment, context, and user intent is essential\\nto deliver task-relevant content effectively. This article presents a novel\\nmultimodal large language model (MLLM) integrated semantic communications\\nframework, termed MLLM-SC, which fully leverages reasoning and generative\\ncapabilities of pre-trained foundation models for context-aware and\\ntask-oriented wireless communication. The MLLM-SC framework adopts a\\ndevice-edge collaborative architecture. At the edge, MLLM-empowered semantic\\nguidance module analyzes multimodal inputs, user intents, and channel\\nconditions to generate importance-aware attention maps prioritizing\\nsemantically critical information. An importance-aware semantic encoder and a\\nresource-adaptive semantic decoder are jointly designed and optimized, which\\ncan utilize the semantic guidance for adaptive bandwidth allocation and\\nhigh-quality content reconstruction or generation. Extensive case studies on\\nvisual question answering for AR/VR applications and diffusion-driven image\\ngeneration validate the effectiveness of MLLM-SC.'),\n",
       " Document(metadata={'title': 'Information-Guided Diffusion Sampling for Dataset Distillation', 'authors': 'Linfeng Ye, Shayan Mohajer Hamidi, Guang Li, Takahiro Ogawa, Miki Haseyama, Konstantinos N. Plataniotis', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04619v1'}, page_content='Dataset distillation aims to create a compact dataset that retains essential\\ninformation while maintaining model performance. Diffusion models (DMs) have\\nshown promise for this task but struggle in low images-per-class (IPC)\\nsettings, where generated samples lack diversity. In this paper, we address\\nthis issue from an information-theoretic perspective by identifying two key\\ntypes of information that a distilled dataset must preserve: ($i$) prototype\\ninformation $\\\\mathrm{I}(X;Y)$, which captures label-relevant features; and\\n($ii$) contextual information $\\\\mathrm{H}(X | Y)$, which preserves intra-class\\nvariability. Here, $(X,Y)$ represents the pair of random variables\\ncorresponding to the input data and its ground truth label, respectively.\\nObserving that the required contextual information scales with IPC, we propose\\nmaximizing $\\\\mathrm{I}(X;Y) + \\\\beta \\\\mathrm{H}(X | Y)$ during the DM sampling\\nprocess, where $\\\\beta$ is IPC-dependent. Since directly computing\\n$\\\\mathrm{I}(X;Y)$ and $\\\\mathrm{H}(X | Y)$ is intractable, we develop\\nvariational estimations to tightly lower-bound these quantities via a\\ndata-driven approach. Our approach, information-guided diffusion sampling\\n(IGDS), seamlessly integrates with diffusion models and improves dataset\\ndistillation across all IPC settings. Experiments on Tiny ImageNet and ImageNet\\nsubsets show that IGDS significantly outperforms existing methods, particularly\\nin low-IPC regimes. The code will be released upon acceptance.'),\n",
       " Document(metadata={'title': 'HiLa: Hierarchical Vision-Language Collaboration for Cancer Survival Prediction', 'authors': 'Jiaqi Cui, Lu Wen, Yuchen Fei, Bo Liu, Luping Zhou, Dinggang Shen, Yan Wang', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04613v1'}, page_content='Survival prediction using whole-slide images (WSIs) is crucial in cancer\\nre-search. Despite notable success, existing approaches are limited by their\\nreliance on sparse slide-level labels, which hinders the learning of\\ndiscriminative repre-sentations from gigapixel WSIs. Recently, vision language\\n(VL) models, which incorporate additional language supervision, have emerged as\\na promising solu-tion. However, VL-based survival prediction remains largely\\nunexplored due to two key challenges. First, current methods often rely on only\\none simple lan-guage prompt and basic cosine similarity, which fails to learn\\nfine-grained associ-ations between multi-faceted linguistic information and\\nvisual features within WSI, resulting in inadequate vision-language alignment.\\nSecond, these methods primarily exploit patch-level information, overlooking\\nthe intrinsic hierarchy of WSIs and their interactions, causing ineffective\\nmodeling of hierarchical interac-tions. To tackle these problems, we propose a\\nnovel Hierarchical vision-Language collaboration (HiLa) framework for improved\\nsurvival prediction. Specifically, HiLa employs pretrained feature extractors\\nto generate hierarchical visual features from WSIs at both patch and region\\nlevels. At each level, a series of language prompts describing various\\nsurvival-related attributes are constructed and aligned with visual features\\nvia Optimal Prompt Learning (OPL). This ap-proach enables the comprehensive\\nlearning of discriminative visual features cor-responding to different\\nsurvival-related attributes from prompts, thereby improv-ing vision-language\\nalignment. Furthermore, we introduce two modules, i.e., Cross-Level Propagation\\n(CLP) and Mutual Contrastive Learning (MCL) to maximize hierarchical\\ncooperation by promoting interactions and consistency be-tween patch and region\\nlevels. Experiments on three TCGA datasets demonstrate our SOTA performance.'),\n",
       " Document(metadata={'title': 'any4: Learned 4-bit Numeric Representation for LLMs', 'authors': 'Mostafa Elhoushi, Jeff Johnson', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04610v1'}, page_content='We present any4, a learned 4-bit weight quantization solution for large\\nlanguage models (LLMs) providing arbitrary numeric representations without\\nrequiring pre-processing of weights or activations. any4 yields higher accuracy\\ncompared to other related 4-bit numeric representation types: int4, fp4 and\\nnf4, as evaluated on a range of model sizes, generations and families (Llama 2,\\nLlama 3, Mistral and Mixtral). While any4 does not require preprocessing of\\nweights or activations, it is also competitive with orthogonal techniques that\\nrequire such preprocessing (e.g., AWQ and GPTQ). We also experiment with any3\\nand any2 and show competitiveness at lower bits. Additionally, we show that we\\ncan calibrate using a single curated diverse sample rather than hundreds of\\nsamples from a dataset as done in most quantization approaches. We also open\\nsource tinygemm, a latency optimized GPU matrix multiplication library for\\nLLMs, that implements any4 using a GPU-efficient lookup table strategy along\\nwith other common quantization methods. We open source our code at\\nhttps://github.com/facebookresearch/any4 .'),\n",
       " Document(metadata={'title': 'PRIME: Large Language Model Personalization with Cognitive Memory and Thought Processes', 'authors': 'Xinliang Frederick Zhang, Nick Beauchamp, Lu Wang', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04607v1'}, page_content=\"Large language model (LLM) personalization aims to align model outputs with\\nindividuals' unique preferences and opinions. While recent efforts have\\nimplemented various personalization methods, a unified theoretical framework\\nthat can systematically understand the drivers of effective personalization is\\nstill lacking. In this work, we integrate the well-established cognitive\\ndual-memory model into LLM personalization, by mirroring episodic memory to\\nhistorical user engagements and semantic memory to long-term, evolving user\\nbeliefs. Specifically, we systematically investigate memory instantiations and\\nintroduce a unified framework, PRIME, using episodic and semantic memory\\nmechanisms. We further augment PRIME with a novel personalized thinking\\ncapability inspired by the slow thinking strategy. Moreover, recognizing the\\nabsence of suitable benchmarks, we introduce a dataset using Change My View\\n(CMV) from Reddit, specifically designed to evaluate long-context\\npersonalization. Extensive experiments validate PRIME's effectiveness across\\nboth long- and short-context scenarios. Further analysis confirms that PRIME\\neffectively captures dynamic personalization beyond mere popularity biases.\"),\n",
       " Document(metadata={'title': 'Accelerated Online Reinforcement Learning using Auxiliary Start State Distributions', 'authors': 'Aman Mehra, Alexandre Capone, Jeff Schneider', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04606v1'}, page_content='A long-standing problem in online reinforcement learning (RL) is of ensuring\\nsample efficiency, which stems from an inability to explore environments\\nefficiently. Most attempts at efficient exploration tackle this problem in a\\nsetting where learning begins from scratch, without prior information available\\nto bootstrap learning. However, such approaches fail to leverage expert\\ndemonstrations and simulators that can reset to arbitrary states. These\\naffordances are valuable resources that offer enormous potential to guide\\nexploration and speed up learning. In this paper, we explore how a small number\\nof expert demonstrations and a simulator allowing arbitrary resets can\\naccelerate learning during online RL. We find that training with a suitable\\nchoice of an auxiliary start state distribution that may differ from the true\\nstart state distribution of the underlying Markov Decision Process can\\nsignificantly improve sample efficiency. We find that using a notion of safety\\nto inform the choice of this auxiliary distribution significantly accelerates\\nlearning. By using episode length information as a way to operationalize this\\nnotion, we demonstrate state-of-the-art sample efficiency on a sparse-reward\\nhard-exploration environment.'),\n",
       " Document(metadata={'title': 'DisMS-TS: Eliminating Redundant Multi-Scale Features for Time Series Classification', 'authors': 'Zhipeng Liu, Peibo Duan, Binwu Wang, Xuan Tang, Qi Chu, Changsheng Zhang, Yongsheng Huang, Bin Zhang', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04600v1'}, page_content='Real-world time series typically exhibit complex temporal variations, making\\nthe time series classification task notably challenging. Recent advancements\\nhave demonstrated the potential of multi-scale analysis approaches, which\\nprovide an effective solution for capturing these complex temporal patterns.\\nHowever, existing multi-scale analysis-based time series prediction methods\\nfail to eliminate redundant scale-shared features across multi-scale time\\nseries, resulting in the model over- or under-focusing on scale-shared\\nfeatures. To address this issue, we propose a novel end-to-end Disentangled\\nMulti-Scale framework for Time Series classification (DisMS-TS). The core idea\\nof DisMS-TS is to eliminate redundant shared features in multi-scale time\\nseries, thereby improving prediction performance. Specifically, we propose a\\ntemporal disentanglement module to capture scale-shared and scale-specific\\ntemporal representations, respectively. Subsequently, to effectively learn both\\nscale-shared and scale-specific temporal representations, we introduce two\\nregularization terms that ensure the consistency of scale-shared\\nrepresentations and the disparity of scale-specific representations across all\\ntemporal scales. Extensive experiments conducted on multiple datasets validate\\nthe superiority of DisMS-TS over its competitive baselines, with the accuracy\\nimprovement up to 9.71%.'),\n",
       " Document(metadata={'title': 'Exploring Core and Periphery Precepts in Biological and Artificial Intelligence: An Outcome-Based Perspective', 'authors': 'Niloofar Shadab, Tyler Cody, Alejandro Salado, Taylan G. Topcu, Mohammad Shadab, Peter Beling', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04594v1'}, page_content='Engineering methodologies predominantly revolve around established principles\\nof decomposition and recomposition. These principles involve partitioning\\ninputs and outputs at the component level, ensuring that the properties of\\nindividual components are preserved upon composition. However, this view does\\nnot transfer well to intelligent systems, particularly when addressing the\\nscaling of intelligence as a system property. Our prior research contends that\\nthe engineering of general intelligence necessitates a fresh set of overarching\\nsystems principles. As a result, we introduced the \"core and periphery\"\\nprinciples, a novel conceptual framework rooted in abstract systems theory and\\nthe Law of Requisite Variety. In this paper, we assert that these abstract\\nconcepts hold practical significance. Through empirical evidence, we illustrate\\ntheir applicability to both biological and artificial intelligence systems,\\nbridging abstract theory with real-world implementations. Then, we expand on\\nour previous theoretical framework by mathematically defining core-dominant vs\\nperiphery-dominant systems.'),\n",
       " Document(metadata={'title': 'Lilith: Developmental Modular LLMs with Chemical Signaling', 'authors': 'Mohid Farooqi, Alejandro Comas-Leon', 'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04575v1'}, page_content='Current paradigms in Artificial Intelligence rely on layers of feedforward\\nnetworks which model brain activity at the neuronal level. We conjecture that\\nexpanding to the level of multiple brain regions with chemical signaling may be\\na productive step toward understanding the emergence of consciousness. We\\npropose LILITH, a novel architecture that combines developmental training of\\nmodular language models with brain-inspired token-based communication\\nprotocols, mirroring chemical signaling in the brain. Our approach models\\ndistinct brain regions as specialized LLM modules including thinking, memory,\\nsensory, and regulatory components that communicate through emergent\\ntoken-based signaling protocols analogous to neurotransmitter networks. Unlike\\ntraditional pre-trained systems, LILITH would employ developmental training\\nwhere untrained LLM architectures learn through simulated life experiences,\\ndeveloping communication pathways and cognitive abilities through environmental\\ninteraction and evolutionary optimization. This framework would enable direct\\nempirical investigation of consciousness emergence using Integrated Information\\nTheory metrics while providing unprecedented insight into inter-module\\nsignaling patterns during development. By optimizing for consciousness\\nemergence rather than task performance, LILITH could provide insight into\\ndifferent emergent phenomena at multiple levels of neural correlates,\\ncontrasting neuronal-level processing with multi-region coordination dynamics.\\nThe goal of this paper is to put the idea forward while recognizing the\\nsubstantial challenges in implementing such a system.'),\n",
       " Document(metadata={'title': 'Nile-Chat: Egyptian Language Models for Arabic and Latin Scripts', 'authors': 'Guokan Shang, Hadi Abdine, Ahmad Chamma, Amr Mohamed, Mohamed Anwar, Abdelaziz Bounhar, Omar El Herraoui, Preslav Nakov, Michalis Vazirgiannis, Eric Xing', 'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04569v1'}, page_content='We introduce Nile-Chat-4B, 3x4B-A6B, and 12B, a collection of LLMs for\\nEgyptian dialect, uniquely designed to understand and generate texts written in\\nboth Arabic and Latin scripts. Specifically, with Nile-Chat-3x4B-A6B, we\\nintroduce a novel language adaptation approach by leveraging the\\nBranch-Train-MiX strategy to merge script-specialized experts, into a single\\nMoE model. Our Nile-Chat models significantly outperform leading multilingual\\nand Arabic LLMs, such as LLaMa, Jais, and ALLaM, on our newly introduced\\nEgyptian evaluation benchmarks, which span both understanding and generative\\ntasks. Notably, our 12B model yields a 14.4% performance gain over\\nQwen2.5-14B-Instruct on Latin-script benchmarks. All our resources are publicly\\navailable. We believe this work presents a comprehensive methodology for\\nadapting LLMs to dual-script languages, addressing an often overlooked aspect\\nin modern LLM development.'),\n",
       " Document(metadata={'title': 'Evaluating LLMs on Real-World Forecasting Against Human Superforecasters', 'authors': 'Janna Lu', 'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04562v1'}, page_content='Large language models (LLMs) have demonstrated remarkable capabilities across\\ndiverse tasks, but their ability to forecast future events remains\\nunderstudied. A year ago, large language models struggle to come close to the\\naccuracy of a human crowd. I evaluate state-of-the-art LLMs on 464 forecasting\\nquestions from Metaculus, comparing their performance against human\\nsuperforecasters. Frontier models achieve Brier scores that ostensibly surpass\\nthe human crowd but still significantly underperform a group of\\nsuperforecasters.'),\n",
       " Document(metadata={'title': 'SPIRA: Building an Intelligent System for Respiratory Insufficiency Detection', 'authors': 'Renato Cordeiro Ferreira, Dayanne Gomes, Vitor Tamae, Francisco Wernke, Alfredo Goldman', 'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04548v1'}, page_content='Respiratory insufficiency is a medic symptom in which a person gets a reduced\\namount of oxygen in the blood. This paper reports the experience of building\\nSPIRA: an intelligent system for detecting respiratory insufficiency from\\nvoice. It compiles challenges faced in two succeeding implementations of the\\nsame architecture, summarizing lessons learned on data collection, training,\\nand inference for future projects in similar systems.'),\n",
       " Document(metadata={'title': 'DP-Fusion: Token-Level Differentially Private Inference for Large Language Models', 'authors': 'Rushil Thareja, Preslav Nakov, Praneeth Vepakomma, Nils Lukas', 'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04531v1'}, page_content=\"Large language models (LLMs) can leak sensitive information from their\\ncontext through generated outputs, either accidentally or when prompted\\nadversarially. Existing defenses that aim to preserve context privacy during\\ninference either lack formal guarantees or suffer from a poor utility/privacy\\ntrade-off. We propose DP-Fusion, a token-level Differentially Private Inference\\n(DPI) mechanism that provably bounds how much an LLM's outputs reveal about\\nsensitive tokens in its context. We demonstrate DPI through the task of\\ndocument privatization, where the goal is to paraphrase documents so that\\nsensitive content (e.g., Personally Identifiable Information, PII) cannot be\\nreliably inferred, while still preserving the overall utility of the text. This\\nis controlled by a parameter $\\\\epsilon$: $\\\\epsilon=0$ hides PII entirely, while\\nhigher values trade off privacy for improved paraphrase quality. DP-Fusion\\nworks as follows: (i) partition sensitive tokens into disjoint privacy groups,\\n(ii) run the LLM once per group, and (iii) blend the output distributions so\\nthat the final output remains within a fixed statistical distance of the\\nbaseline distribution produced when no privacy group is revealed. This approach\\nallows fine-grained control over the privacy/utility trade-off but requires\\nmultiple LLM forward passes.\"),\n",
       " Document(metadata={'title': 'Towards integration of Privacy Enhancing Technologies in Explainable Artificial Intelligence', 'authors': 'Sonal Allana, Rozita Dara, Xiaodong Lin, Pulei Xiong', 'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04528v1'}, page_content='Explainable Artificial Intelligence (XAI) is a crucial pathway in mitigating\\nthe risk of non-transparency in the decision-making process of black-box\\nArtificial Intelligence (AI) systems. However, despite the benefits, XAI\\nmethods are found to leak the privacy of individuals whose data is used in\\ntraining or querying the models. Researchers have demonstrated privacy attacks\\nthat exploit explanations to infer sensitive personal information of\\nindividuals. Currently there is a lack of defenses against known privacy\\nattacks targeting explanations when vulnerable XAI are used in production and\\nmachine learning as a service system. To address this gap, in this article, we\\nexplore Privacy Enhancing Technologies (PETs) as a defense mechanism against\\nattribute inference on explanations provided by feature-based XAI methods. We\\nempirically evaluate 3 types of PETs, namely synthetic training data,\\ndifferentially private training and noise addition, on two categories of\\nfeature-based XAI. Our evaluation determines different responses from the\\nmitigation methods and side-effects of PETs on other system properties such as\\nutility and performance. In the best case, PETs integration in explanations\\nreduced the risk of the attack by 49.47%, while maintaining model utility and\\nexplanation quality. Through our evaluation, we identify strategies for using\\nPETs in XAI for maximizing benefits and minimizing the success of this privacy\\nattack on sensitive personal information.'),\n",
       " Document(metadata={'title': 'Grounded Gesture Generation: Language, Motion, and Space', 'authors': \"Anna Deichler, Jim O'Regan, Teo Guichoux, David Johansson, Jonas Beskow\", 'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04522v1'}, page_content='Human motion generation has advanced rapidly in recent years, yet the\\ncritical problem of creating spatially grounded, context-aware gestures has\\nbeen largely overlooked. Existing models typically specialize either in\\ndescriptive motion generation, such as locomotion and object interaction, or in\\nisolated co-speech gesture synthesis aligned with utterance semantics. However,\\nboth lines of work often treat motion and environmental grounding separately,\\nlimiting advances toward embodied, communicative agents. To address this gap,\\nour work introduces a multimodal dataset and framework for grounded gesture\\ngeneration, combining two key resources: (1) a synthetic dataset of spatially\\ngrounded referential gestures, and (2) MM-Conv, a VR-based dataset capturing\\ntwo-party dialogues. Together, they provide over 7.7 hours of synchronized\\nmotion, speech, and 3D scene information, standardized in the HumanML3D format.\\nOur framework further connects to a physics-based simulator, enabling synthetic\\ndata generation and situated evaluation. By bridging gesture modeling and\\nspatial grounding, our contribution establishes a foundation for advancing\\nresearch in situated gesture generation and grounded multimodal interaction.\\n  Project page: https://groundedgestures.github.io/'),\n",
       " Document(metadata={'title': 'Churn-Aware Recommendation Planning under Aggregated Preference Feedback', 'authors': 'Gur Keinan, Omer Ben-Porat', 'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04513v1'}, page_content='We study a sequential decision-making problem motivated by recent regulatory\\nand technological shifts that limit access to individual user data in\\nrecommender systems (RSs), leaving only population-level preference\\ninformation. This privacy-aware setting poses fundamental challenges in\\nplanning under uncertainty: Effective personalization requires exploration to\\ninfer user preferences, yet unsatisfactory recommendations risk immediate user\\nchurn. To address this, we introduce the Rec-APC model, in which an anonymous\\nuser is drawn from a known prior over latent user types (e.g., personas or\\nclusters), and the decision-maker sequentially selects items to recommend.\\nFeedback is binary -- positive responses refine the posterior via Bayesian\\nupdates, while negative responses result in the termination of the session.\\n  We prove that optimal policies converge to pure exploitation in finite time\\nand propose a branch-and-bound algorithm to efficiently compute them.\\nExperiments on synthetic and MovieLens data confirm rapid convergence and\\ndemonstrate that our method outperforms the POMDP solver SARSOP, particularly\\nwhen the number of user types is large or comparable to the number of content\\ncategories. Our results highlight the applicability of this approach and\\ninspire new ways to improve decision-making under the constraints imposed by\\naggregated preference data.'),\n",
       " Document(metadata={'title': 'MVL-Loc: Leveraging Vision-Language Model for Generalizable Multi-Scene Camera Relocalization', 'authors': 'Zhendong Xiao, Wu Wei, Shujie Ji, Shan Yang, Changhao Chen', 'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04509v1'}, page_content=\"Camera relocalization, a cornerstone capability of modern computer vision,\\naccurately determines a camera's position and orientation (6-DoF) from images\\nand is essential for applications in augmented reality (AR), mixed reality\\n(MR), autonomous driving, delivery drones, and robotic navigation. Unlike\\ntraditional deep learning-based methods that regress camera pose from images in\\na single scene, which often lack generalization and robustness in diverse\\nenvironments, we propose MVL-Loc, a novel end-to-end multi-scene 6-DoF camera\\nrelocalization framework. MVL-Loc leverages pretrained world knowledge from\\nvision-language models (VLMs) and incorporates multimodal data to generalize\\nacross both indoor and outdoor settings. Furthermore, natural language is\\nemployed as a directive tool to guide the multi-scene learning process,\\nfacilitating semantic understanding of complex scenes and capturing spatial\\nrelationships among objects. Extensive experiments on the 7Scenes and Cambridge\\nLandmarks datasets demonstrate MVL-Loc's robustness and state-of-the-art\\nperformance in real-world multi-scene camera relocalization, with improved\\naccuracy in both positional and orientational estimates.\"),\n",
       " Document(metadata={'title': 'Thousand-Brains Systems: Sensorimotor Intelligence for Rapid, Robust Learning and Inference', 'authors': 'Niels Leadholm, Viviane Clay, Scott Knudstrup, Hojae Lee, Jeff Hawkins', 'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04494v1'}, page_content=\"Current AI systems achieve impressive performance on many tasks, yet they\\nlack core attributes of biological intelligence, including rapid, continual\\nlearning, representations grounded in sensorimotor interactions, and structured\\nknowledge that enables efficient generalization. Neuroscience theory suggests\\nthat mammals evolved flexible intelligence through the replication of a\\nsemi-independent, sensorimotor module, a functional unit known as a cortical\\ncolumn. To address the disparity between biological and artificial\\nintelligence, thousand-brains systems were proposed as a means of mirroring the\\narchitecture of cortical columns and their interactions.\\n  In the current work, we evaluate the unique properties of Monty, the first\\nimplementation of a thousand-brains system. We focus on 3D object perception,\\nand in particular, the combined task of object recognition and pose estimation.\\nUtilizing the YCB dataset of household objects, we first assess Monty's use of\\nsensorimotor learning to build structured representations, finding that these\\nenable robust generalization. These representations include an emphasis on\\nclassifying objects by their global shape, as well as a natural ability to\\ndetect object symmetries. We then explore Monty's use of model-free and\\nmodel-based policies to enable rapid inference by supporting principled\\nmovements. We find that such policies complement Monty's modular architecture,\\na design that can accommodate communication between modules to further\\naccelerate inference speed via a novel `voting' algorithm. Finally, we examine\\nMonty's use of associative, Hebbian-like binding to enable rapid, continual,\\nand computationally efficient learning, properties that compare favorably to\\ncurrent deep learning architectures. While Monty is still in a nascent stage of\\ndevelopment, these findings support thousand-brains systems as a powerful and\\npromising new approach to AI.\"),\n",
       " Document(metadata={'title': 'A validity-guided workflow for robust large language model research in psychology', 'authors': 'Zhicheng Lin', 'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04491v1'}, page_content='Large language models (LLMs) are rapidly being integrated into psychological\\nresearch as research tools, evaluation targets, human simulators, and cognitive\\nmodels. However, recent evidence reveals severe measurement unreliability:\\nPersonality assessments collapse under factor analysis, moral preferences\\nreverse with punctuation changes, and theory-of-mind accuracy varies widely\\nwith trivial rephrasing. These \"measurement phantoms\"--statistical artifacts\\nmasquerading as psychological phenomena--threaten the validity of a growing\\nbody of research. Guided by the dual-validity framework that integrates\\npsychometrics with causal inference, we present a six-stage workflow that\\nscales validity requirements to research ambition--using LLMs to code text\\nrequires basic reliability and accuracy, while claims about psychological\\nproperties demand comprehensive construct validation. Researchers must (1)\\nexplicitly define their research goal and corresponding validity requirements,\\n(2) develop and validate computational instruments through psychometric\\ntesting, (3) design experiments that control for computational confounds, (4)\\nexecute protocols with transparency, (5) analyze data using methods appropriate\\nfor non-independent observations, and (6) report findings within demonstrated\\nboundaries and use results to refine theory. We illustrate the workflow through\\nan example of model evaluation--\"LLM selfhood\"--showing how systematic\\nvalidation can distinguish genuine computational phenomena from measurement\\nartifacts. By establishing validated computational instruments and transparent\\npractices, this workflow provides a path toward building a robust empirical\\nfoundation for AI psychology research.'),\n",
       " Document(metadata={'title': 'Dealing with Uncertainty in Contextual Anomaly Detection', 'authors': 'Luca Bindini, Lorenzo Perini, Stefano Nistri, Jesse Davis, Paolo Frasconi', 'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04490v1'}, page_content='Contextual anomaly detection (CAD) aims to identify anomalies in a target\\n(behavioral) variable conditioned on a set of contextual variables that\\ninfluence the normalcy of the target variable but are not themselves indicators\\nof anomaly. In many anomaly detection tasks, there exist contextual variables\\nthat influence the normalcy of the target variable but are not themselves\\nindicators of anomaly. In this work, we propose a novel framework for CAD,\\nnormalcy score (NS), that explicitly models both the aleatoric and epistemic\\nuncertainties. Built on heteroscedastic Gaussian process regression, our method\\nregards the Z-score as a random variable, providing confidence intervals that\\nreflect the reliability of the anomaly assessment. Through experiments on\\nbenchmark datasets and a real-world application in cardiology, we demonstrate\\nthat NS outperforms state-of-the-art CAD methods in both detection accuracy and\\ninterpretability. Moreover, confidence intervals enable an adaptive,\\nuncertainty-driven decision-making process, which may be very important in\\ndomains such as healthcare.'),\n",
       " Document(metadata={'title': 'LoSiA: Efficient High-Rank Fine-Tuning via Subnet Localization and Optimization', 'authors': 'Xujia Wang. Yunjia Qi, Bin Xu', 'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04487v1'}, page_content='Parameter-Efficient Fine-Tuning (PEFT) methods, such as LoRA, significantly\\nreduce the number of trainable parameters by introducing low-rank decomposition\\nmatrices. However, existing methods perform extensive matrix multiplications in\\ndomain specialization tasks, resulting in computational inefficiency and\\nsub-optimal fine-tuning performance. Hence, we propose LoSiA(Low-Resources\\nSubnet Integration Adaptation), an innovative method that dynamically localizes\\nand optimizes critical parameters during the training process. Specifically, it\\nidentifies a sub-network using gradient sparsity analysis and optimizes it as\\nthe trainable target. This design enables effective high-rank adaptation by\\nupdating only the sub-network parameters, reducing the additional matrix\\nmultiplication. We also present LoSiA-Pro, a faster implementation of LoSiA,\\nwhich reduces the training latency by about $27\\\\%$ compared to LoRA. Extensive\\nevaluations show that our method achieves minimal performance drop compared to\\nfull fine-tuning, while requiring the least training time across domain\\nspecialization and common-sense reasoning tasks. Further analysis shows that\\nLoSiA also reduces forgetting during continued training.'),\n",
       " Document(metadata={'title': 'Source Attribution in Retrieval-Augmented Generation', 'authors': 'Ikhtiyor Nematov, Tarik Kalai, Elizaveta Kuzmenko, Gabriele Fugagnoli, Dimitris Sacharidis, Katja Hose, Tomer Sagi', 'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04480v1'}, page_content='While attribution methods, such as Shapley values, are widely used to explain\\nthe importance of features or training data in traditional machine learning,\\ntheir application to Large Language Models (LLMs), particularly within\\nRetrieval-Augmented Generation (RAG) systems, is nascent and challenging. The\\nprimary obstacle is the substantial computational cost, where each utility\\nfunction evaluation involves an expensive LLM call, resulting in direct\\nmonetary and time expenses. This paper investigates the feasibility and\\neffectiveness of adapting Shapley-based attribution to identify influential\\nretrieved documents in RAG. We compare Shapley with more computationally\\ntractable approximations and some existing attribution methods for LLM. Our\\nwork aims to: (1) systematically apply established attribution principles to\\nthe RAG document-level setting; (2) quantify how well SHAP approximations can\\nmirror exact attributions while minimizing costly LLM interactions; and (3)\\nevaluate their practical explainability in identifying critical documents,\\nespecially under complex inter-document relationships such as redundancy,\\ncomplementarity, and synergy. This study seeks to bridge the gap between\\npowerful attribution techniques and the practical constraints of LLM-based RAG\\nsystems, offering insights into achieving reliable and affordable RAG\\nexplainability.'),\n",
       " Document(metadata={'title': 'Model Inversion Attacks on Llama 3: Extracting PII from Large Language Models', 'authors': 'Sathesh P. Sivashanmugam', 'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04478v1'}, page_content='Large language models (LLMs) have transformed natural language processing,\\nbut their ability to memorize training data poses significant privacy risks.\\nThis paper investigates model inversion attacks on the Llama 3.2 model, a\\nmultilingual LLM developed by Meta. By querying the model with carefully\\ncrafted prompts, we demonstrate the extraction of personally identifiable\\ninformation (PII) such as passwords, email addresses, and account numbers. Our\\nfindings highlight the vulnerability of even smaller LLMs to privacy attacks\\nand underscore the need for robust defenses. We discuss potential mitigation\\nstrategies, including differential privacy and data sanitization, and call for\\nfurther research into privacy-preserving machine learning techniques.'),\n",
       " Document(metadata={'title': 'The role of large language models in UI/UX design: A systematic literature review', 'authors': 'Ammar Ahmed, Ali Shariq Imran', 'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04469v1'}, page_content='This systematic literature review examines the role of large language models\\n(LLMs) in UI/UX design, synthesizing findings from 38 peer-reviewed studies\\npublished between 2022 and 2025. We identify key LLMs in use, including GPT-4,\\nGemini, and PaLM, and map their integration across the design lifecycle, from\\nideation to evaluation. Common practices include prompt engineering,\\nhuman-in-the-loop workflows, and multimodal input. While LLMs are reshaping\\ndesign processes, challenges such as hallucination, prompt instability, and\\nlimited explainability persist. Our findings highlight LLMs as emerging\\ncollaborators in design, and we propose directions for the ethical, inclusive,\\nand effective integration of these technologies.'),\n",
       " Document(metadata={'title': 'Anomalous Decision Discovery using Inverse Reinforcement Learning', 'authors': 'Ashish Bastola, Mert D. Pesé, Long Cheng, Jonathon Smereka, Abolfazl Razi', 'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04464v1'}, page_content='Anomaly detection plays a critical role in Autonomous Vehicles (AVs) by\\nidentifying unusual behaviors through perception systems that could compromise\\nsafety and lead to hazardous situations. Current approaches, which often rely\\non predefined thresholds or supervised learning paradigms, exhibit reduced\\nefficacy when confronted with unseen scenarios, sensor noise, and occlusions,\\nleading to potential safety-critical failures. Moreover, supervised methods\\nrequire large annotated datasets, limiting their real-world feasibility. To\\naddress these gaps, we propose an anomaly detection framework based on Inverse\\nReinforcement Learning (IRL) to infer latent driving intentions from sequential\\nperception data, thus enabling robust identification. Specifically, we present\\nTrajectory-Reward Guided Adaptive Pre-training (TRAP), a novel IRL framework\\nfor anomaly detection, to address two critical limitations of existing methods:\\nnoise robustness and generalization to unseen scenarios. Our core innovation is\\nimplicitly learning temporal credit assignments via reward and worst-case\\nsupervision. We leverage pre-training with variable-horizon sampling to\\nmaximize time-to-consequence, resulting in early detection of behavior\\ndeviation. Experiments on 14,000+ simulated trajectories demonstrate\\nstate-of-the-art performance, achieving 0.90 AUC and 82.2\\\\% F1-score -\\noutperforming similarly trained supervised and unsupervised baselines by 39\\\\%\\non Recall and 12\\\\% on F1-score, respectively. Similar performance is achieved\\nwhile exhibiting robustness to various noise types and generalization to unseen\\nanomaly types. Our code will be available at:\\nhttps://github.com/abastola0/TRAP.git'),\n",
       " Document(metadata={'title': 'The Joys of Categorical Conformal Prediction', 'authors': 'Michele Caprio', 'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04441v1'}, page_content='Conformal prediction (CP) is an Uncertainty Representation technique that\\ndelivers finite-sample calibrated prediction regions for any underlying Machine\\nLearning model, yet its status as an Uncertainty Quantification (UQ) tool has\\nremained conceptually opaque. We adopt a category-theoretic approach to CP --\\nframing it as a morphism, embedded in a commuting diagram, of two newly-defined\\ncategories -- that brings us three joys. First, we show that -- under minimal\\nassumptions -- CP is intrinsically a UQ mechanism, that is, its UQ capabilities\\nare a structural feature of the method. Second, we demonstrate that CP bridges\\n(and perhaps subsumes) the Bayesian, frequentist, and imprecise probabilistic\\napproaches to predictive statistical reasoning. Finally, we show that a\\nconformal prediction region (CPR) is the image of a covariant functor. This\\nobservation is relevant to AI privacy: It implies that privacy noise added\\nlocally does not break coverage.'),\n",
       " Document(metadata={'title': 'A Linguistic Analysis of Spontaneous Thoughts: Investigating Experiences of Déjà Vu, Unexpected Thoughts, and Involuntary Autobiographical Memories', 'authors': 'Videep Venkatesha, Mary Cati Poulos, Christopher Steadman, Caitlin Mills, Anne M. Cleary, Nathaniel Blanchard', 'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04439v1'}, page_content='The onset of spontaneous thoughts are reflective of dynamic interactions\\nbetween cognition, emotion, and attention. Typically, these experiences are\\nstudied through subjective appraisals that focus on their triggers,\\nphenomenology, and emotional salience. In this work, we use linguistic\\nsignatures to investigate Deja Vu, Involuntary Autobiographical Memories and\\nUnexpected Thoughts. Specifically, we analyze the inherent characteristics of\\nthe linguistic patterns in participant generated descriptions of these thought\\ntypes. We show how, by positioning language as a window into spontaneous\\ncognition, existing theories on these attentional states can be updated and\\nreaffirmed. Our findings align with prior research, reinforcing that Deja Vu is\\na metacognitive experience characterized by abstract and spatial language,\\nInvoluntary Autobiographical Memories are rich in personal and emotionally\\nsignificant detail, and Unexpected Thoughts are marked by unpredictability and\\ncognitive disruption. This work is demonstrative of languages potential to\\nreveal deeper insights into how internal spontaneous cognitive states manifest\\nthrough expression.'),\n",
       " Document(metadata={'title': 'MedGellan: LLM-Generated Medical Guidance to Support Physicians', 'authors': 'Debodeep Banerjee, Burcu Sayin, Stefano Teso, Andrea Passerini', 'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04431v1'}, page_content='Medical decision-making is a critical task, where errors can result in\\nserious, potentially life-threatening consequences. While full automation\\nremains challenging, hybrid frameworks that combine machine intelligence with\\nhuman oversight offer a practical alternative. In this paper, we present\\nMedGellan, a lightweight, annotation-free framework that uses a Large Language\\nModel (LLM) to generate clinical guidance from raw medical records, which is\\nthen used by a physician to predict diagnoses. MedGellan uses a\\nBayesian-inspired prompting strategy that respects the temporal order of\\nclinical data. Preliminary experiments show that the guidance generated by the\\nLLM with MedGellan improves diagnostic performance, particularly in recall and\\n$F_1$ score.'),\n",
       " Document(metadata={'title': 'ARMR: Adaptively Responsive Network for Medication Recommendation', 'authors': 'Feiyue Wu, Tianxing Wu, Shenqi Jing', 'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04428v1'}, page_content=\"Medication recommendation is a crucial task in healthcare, especially for\\npatients with complex medical conditions. However, existing methods often\\nstruggle to effectively balance the reuse of historical medications with the\\nintroduction of new drugs in response to the changing patient conditions. In\\norder to address this challenge, we propose an Adaptively Responsive network\\nfor Medication Recommendation (ARMR), a new method which incorporates 1) a\\npiecewise temporal learning component that distinguishes between recent and\\ndistant patient history, enabling more nuanced temporal understanding, and 2)\\nan adaptively responsive mechanism that dynamically adjusts attention to new\\nand existing drugs based on the patient's current health state and medication\\nhistory. Experiments on the MIMIC-III and MIMIC-IV datasets indicate that ARMR\\nhas better performance compared with the state-of-the-art baselines in\\ndifferent evaluation metrics, which contributes to more personalized and\\naccurate medication recommendations. The source code is publicly avaiable at:\\nhttps://github.com/seucoin/armr2.\"),\n",
       " Document(metadata={'title': 'Learning Software Bug Reports: A Systematic Literature Review', 'authors': 'Guoming Long, Jingzhi Gong, Hui Fang, Tao Chen', 'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04422v1'}, page_content='The recent advancement of artificial intelligence, especially machine\\nlearning (ML), has significantly impacted software engineering research,\\nincluding bug report analysis. ML aims to automate the understanding,\\nextraction, and correlation of information from bug reports. Despite its\\ngrowing importance, there has been no comprehensive review in this area. In\\nthis paper, we present a systematic literature review covering 1,825 papers,\\nselecting 204 for detailed analysis. We derive seven key findings: 1) Extensive\\nuse of CNN, LSTM, and $k$NN for bug report analysis, with advanced models like\\nBERT underutilized due to their complexity. 2) Word2Vec and TF-IDF are popular\\nfor feature representation, with a rise in deep learning approaches. 3) Stop\\nword removal is the most common preprocessing, with structural methods rising\\nafter 2020. 4) Eclipse and Mozilla are the most frequently evaluated software\\nprojects. 5) Bug categorization is the most common task, followed by bug\\nlocalization and severity prediction. 6) There is increasing attention on\\nspecific bugs like non-functional and performance bugs. 7) Common evaluation\\nmetrics are F1-score, Recall, Precision, and Accuracy, with $k$-fold\\ncross-validation preferred for model evaluation. 8) Many studies lack robust\\nstatistical tests. We also identify six promising future research directions to\\nprovide useful insights for practitioners.'),\n",
       " Document(metadata={'title': 'Multimedia Verification Through Multi-Agent Deep Research Multimodal Large Language Models', 'authors': 'Huy Hoan Le, Van Sy Thinh Nguyen, Thi Le Chi Dang, Vo Thanh Khang Nguyen, Truong Thanh Hung Nguyen, Hung Cao', 'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04410v1'}, page_content='This paper presents our submission to the ACMMM25 - Grand Challenge on\\nMultimedia Verification. We developed a multi-agent verification system that\\ncombines Multimodal Large Language Models (MLLMs) with specialized verification\\ntools to detect multimedia misinformation. Our system operates through six\\nstages: raw data processing, planning, information extraction, deep research,\\nevidence collection, and report generation. The core Deep Researcher Agent\\nemploys four tools: reverse image search, metadata analysis, fact-checking\\ndatabases, and verified news processing that extracts spatial, temporal,\\nattribution, and motivational context. We demonstrate our approach on a\\nchallenge dataset sample involving complex multimedia content. Our system\\nsuccessfully verified content authenticity, extracted precise geolocation and\\ntiming information, and traced source attribution across multiple platforms,\\neffectively addressing real-world multimedia verification scenarios.'),\n",
       " Document(metadata={'title': 'LayerCake: Token-Aware Contrastive Decoding within Large Language Model Layers', 'authors': 'Jingze Zhu, Yongliang Wu, Wenbo Zhu, Jiawang Cao, Yanqiang Zheng, Jiawei Chen, Xu Yang, Bernt Schiele, Jonas Fischer, Xinting Hu', 'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04404v1'}, page_content='Large language models (LLMs) excel at natural language understanding and\\ngeneration but remain vulnerable to factual errors, limiting their reliability\\nin knowledge-intensive tasks. While decoding-time strategies provide a\\npromising efficient solution without training, existing methods typically treat\\ntoken-level and layer-level signals in isolation, overlooking the joint\\ndynamics between them. In this work, we introduce a token-aware,\\nlayer-localized contrastive decoding method that aligns specific token types\\nwith their most influential transformer layers to improve factual generation.\\nThrough empirical attention analysis, we identify two key patterns: punctuation\\ntokens receive dominant attention in early layers, while conceptual tokens\\ngovern semantic reasoning in intermediate layers. By selectively suppressing\\nattention to these token types at their respective depths, we achieve the\\ninduction of controlled factual degradation and derive contrastive signals to\\nguide the final factual decoding. Our method requires no additional training or\\nmodel modification, and experiments demonstrate that our method consistently\\nimproves factuality across multiple LLMs and various benchmarks.'),\n",
       " Document(metadata={'title': 'SpiritRAG: A Q&A System for Religion and Spirituality in the United Nations Archive', 'authors': 'Yingqiang Gao, Fabian Winiger, Patrick Montjourides, Anastassia Shaitarova, Nianlong Gu, Simon Peng-Keller, Gerold Schneider', 'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04395v1'}, page_content='Religion and spirituality (R/S) are complex and highly domain-dependent\\nconcepts which have long confounded researchers and policymakers. Due to their\\ncontext-specificity, R/S are difficult to operationalize in conventional\\narchival search strategies, particularly when datasets are very large, poorly\\naccessible, and marked by information noise. As a result, considerable time\\ninvestments and specialist knowledge is often needed to extract actionable\\ninsights related to R/S from general archival sources, increasing reliance on\\npublished literature and manual desk reviews. To address this challenge, we\\npresent SpiritRAG, an interactive Question Answering (Q&A) system based on\\nRetrieval-Augmented Generation (RAG). Built using 7,500 United Nations (UN)\\nresolution documents related to R/S in the domains of health and education,\\nSpiritRAG allows researchers and policymakers to conduct complex,\\ncontext-sensitive database searches of very large datasets using an easily\\naccessible, chat-based web interface. SpiritRAG is lightweight to deploy and\\nleverages both UN documents and user provided documents as source material. A\\npilot test and evaluation with domain experts on 100 manually composed\\nquestions demonstrates the practical value and usefulness of SpiritRAG.'),\n",
       " Document(metadata={'title': 'Tractable Representation Learning with Probabilistic Circuits', 'authors': 'Steven Braun, Sahil Sidheekh, Antonio Vergari, Martin Mundt, Sriraam Natarajan, Kristian Kersting', 'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04385v1'}, page_content='Probabilistic circuits (PCs) are powerful probabilistic models that enable\\nexact and tractable inference, making them highly suitable for probabilistic\\nreasoning and inference tasks. While dominant in neural networks,\\nrepresentation learning with PCs remains underexplored, with prior approaches\\nrelying on external neural embeddings or activation-based encodings. To address\\nthis gap, we introduce autoencoding probabilistic circuits (APCs), a novel\\nframework leveraging the tractability of PCs to model probabilistic embeddings\\nexplicitly. APCs extend PCs by jointly modeling data and embeddings, obtaining\\nembedding representations through tractable probabilistic inference. The PC\\nencoder allows the framework to natively handle arbitrary missing data and is\\nseamlessly integrated with a neural decoder in a hybrid, end-to-end trainable\\narchitecture enabled by differentiable sampling. Our empirical evaluation\\ndemonstrates that APCs outperform existing PC-based autoencoding methods in\\nreconstruction quality, generate embeddings competitive with, and exhibit\\nsuperior robustness in handling missing data compared to neural autoencoders.\\nThese results highlight APCs as a powerful and flexible representation learning\\nmethod that exploits the probabilistic inference capabilities of PCs, showing\\npromising directions for robust inference, out-of-distribution detection, and\\nknowledge distillation.'),\n",
       " Document(metadata={'title': 'DC-Mamber: A Dual Channel Prediction Model based on Mamba and Linear Transformer for Multivariate Time Series Forecasting', 'authors': 'Bing Fan, Shusen Ma, Yun-Bo Zhao, Yu Kang', 'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04381v1'}, page_content=\"In multivariate time series forecasting (MTSF), existing strategies for\\nprocessing sequences are typically categorized as channel-independent and\\nchannel-mixing. The former treats all temporal information of each variable as\\na token, focusing on capturing local temporal features of individual variables,\\nwhile the latter constructs a token from the multivariate information at each\\ntime step, emphasizing the modeling of global temporal dependencies. Current\\nmainstream models are mostly based on Transformer and the emerging Mamba.\\nTransformers excel at modeling global dependencies through self-attention\\nmechanisms but exhibit limited sensitivity to local temporal patterns and\\nsuffer from quadratic computational complexity, restricting their efficiency in\\nlong-sequence processing. In contrast, Mamba, based on state space models\\n(SSMs), achieves linear complexity and efficient long-range modeling but\\nstruggles to aggregate global contextual information in parallel. To overcome\\nthe limitations of both models, we propose DC-Mamber, a dual-channel\\nforecasting model based on Mamba and linear Transformer for time series\\nforecasting. Specifically, the Mamba-based channel employs a\\nchannel-independent strategy to extract intra-variable features, while the\\nTransformer-based channel adopts a channel-mixing strategy to model\\ncross-timestep global dependencies. DC-Mamber first maps the raw input into two\\ndistinct feature representations via separate embedding layers. These\\nrepresentations are then processed by a variable encoder (built on Mamba) and a\\ntemporal encoder (built on linear Transformer), respectively. Finally, a fusion\\nlayer integrates the dual-channel features for prediction. Extensive\\nexperiments on eight public datasets confirm DC-Mamber's superior accuracy over\\nexisting models.\"),\n",
       " Document(metadata={'title': 'Transferring Visual Explainability of Self-Explaining Models through Task Arithmetic', 'authors': 'Yuya Yoshikawa, Ryotaro Shimizu, Takahiro Kawashima, Yuki Saito', 'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04380v1'}, page_content='In scenarios requiring both prediction and explanation efficiency for image\\nclassification, self-explaining models that perform both tasks in a single\\ninference are effective. However, their training incurs substantial labeling\\nand computational costs. This study aims to tackle the issue by proposing a\\nmethod to transfer the visual explainability of self-explaining models, learned\\nin a source domain, to a target domain based on a task arithmetic framework.\\nSpecifically, we construct a self-explaining model by extending image\\nclassifiers based on a vision-language pretrained model. We then define an\\n\\\\emph{explainability vector} as the difference between model parameters trained\\non the source domain with and without explanation supervision. Based on the\\ntask arithmetic framework, we impart explainability to a model trained only on\\nthe prediction task in the target domain by applying the explainability vector.\\nExperimental results on various image classification datasets demonstrate that,\\nexcept for transfers between some less-related domains, visual explainability\\ncan be successfully transferred from source to target domains, improving\\nexplanation quality in the target domain without sacrificing classification\\naccuracy. Furthermore, we show that the explainability vector learned on a\\nlarge and diverse dataset like ImageNet, extended with explanation supervision,\\nexhibits universality and robustness, improving explanation quality on nine out\\nof ten different target datasets. We also find that the explanation quality\\nachieved with a single model inference is comparable to that of Kernel SHAP,\\nwhich requires 150 model inferences.'),\n",
       " Document(metadata={'title': 'MOD-X: A Modular Open Decentralized eXchange Framework proposal for Heterogeneous Interoperable Artificial Agents', 'authors': 'Georgios Ioannides, Christos Constantinou, Vinija Jain, Aman Chadha, Aaron Elkins', 'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04376v1'}, page_content=\"As Artificial Intelligence systems evolve from monolithic models to\\necosystems of specialized agents, the need for standardized communication\\nprotocols becomes increasingly critical. This paper introduces MOD-X (Modular\\nOpen Decentralized eXchange), a novel architectural framework proposal for\\nagent interoperability that addresses key limitations of existing protocols.\\nUnlike current approaches, MOD-X proposes a layered architecture with a\\nUniversal Message Bus, thorough state management, translation capabilities, and\\nblockchain-based security mechanisms. We present MOD-X's architecture, compare\\nit with existing protocols, and demonstrate its application through a worked\\nexample how it enables integration between heterogeneous specialist agents\\n(agents with different architectures, vendors, capabilities, and knowledge\\nrepresentations--including rule-based systems, neural networks, symbolic\\nreasoning engines, and legacy software with agent wrappers). MOD-X's key\\ninnovations include a publish-subscribe communication model, semantic\\ncapability discovery, and dynamic workflow orchestration--providing a framework\\nthat bridges theoretical formalism with practical implementation. This\\narchitecture addresses the growing need for truly decentralized, interoperable\\nagent ecosystems that can scale effectively without the need for central\\ncoordination.\"),\n",
       " Document(metadata={'title': 'WebSynthesis: World-Model-Guided MCTS for Efficient WebUI-Trajectory Synthesis', 'authors': 'Yifei Gao, Junhong Ye, Jiaqi Wang, Jitao Sang', 'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04370v1'}, page_content=\"Recent advancements in large language models (LLMs) have significantly\\nimproved the capabilities of web agents. However, effectively navigating\\ncomplex and dynamic web environments still requires more advanced\\ntrajectory-level planning and execution. Prior studies have addressed\\nself-improving agents by collecting extensive GUI trajectories from\\nreal-environment interactions. Despite their effectiveness, these approaches\\nencounter two critical challenges: (1) Uncontrollable environment states, where\\nreal or sandboxed web environments often yield unstable and non-deterministic\\nfeedback, complicating the reproduction and debugging of agent behaviors; and\\n(2) High API costs, as generating even a single interaction trajectory can\\ninvolve hundreds of queries, leading to considerable API usage and\\ncomputational expenses. To address these limitations and enable scalable\\nself-improvement for agents, we propose WebSynthesis, a novel framework for\\ntrajectory synthesis and training. WebSynthesis leverages a learned world model\\nto simulate virtual web environments, allowing a policy agent to perform\\nefficient and reversible tree-based planning. This approach supports the\\nlarge-scale generation of diverse and high-quality trajectories, which are\\nsubsequently utilized to refine the agent's policy. Experimental results\\ndemonstrate that an agent trained using WebSynthesis on a small-scale synthetic\\ndataset achieves performance comparable to or even surpassing that of models\\ntrained on large-scale real-world data.\"),\n",
       " Document(metadata={'title': 'Attention Slipping: A Mechanistic Understanding of Jailbreak Attacks and Defenses in LLMs', 'authors': 'Xiaomeng Hu, Pin-Yu Chen, Tsung-Yi Ho', 'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04365v1'}, page_content='As large language models (LLMs) become more integral to society and\\ntechnology, ensuring their safety becomes essential. Jailbreak attacks exploit\\nvulnerabilities to bypass safety guardrails, posing a significant threat.\\nHowever, the mechanisms enabling these attacks are not well understood. In this\\npaper, we reveal a universal phenomenon that occurs during jailbreak attacks:\\nAttention Slipping. During this phenomenon, the model gradually reduces the\\nattention it allocates to unsafe requests in a user query during the attack\\nprocess, ultimately causing a jailbreak. We show Attention Slipping is\\nconsistent across various jailbreak methods, including gradient-based token\\nreplacement, prompt-level template refinement, and in-context learning.\\nAdditionally, we evaluate two defenses based on query perturbation, Token\\nHighlighter and SmoothLLM, and find they indirectly mitigate Attention\\nSlipping, with their effectiveness positively correlated with the degree of\\nmitigation achieved. Inspired by this finding, we propose Attention Sharpening,\\na new defense that directly counters Attention Slipping by sharpening the\\nattention score distribution using temperature scaling. Experiments on four\\nleading LLMs (Gemma2-9B-It, Llama3.1-8B-It, Qwen2.5-7B-It, Mistral-7B-It v0.2)\\nshow that our method effectively resists various jailbreak attacks while\\nmaintaining performance on benign tasks on AlpacaEval. Importantly, Attention\\nSharpening introduces no additional computational or memory overhead, making it\\nan efficient and practical solution for real-world deployment.'),\n",
       " Document(metadata={'title': 'Mission-Aligned Learning-Informed Control of Autonomous Systems: Formulation and Foundations', 'authors': 'Vyacheslav Kungurtsev, Gustav Sir, Akhil Anand, Sebastien Gros, Haozhe Tian, Homayoun Hamedmoghadam', 'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04356v1'}, page_content='Research, innovation and practical capital investment have been increasing\\nrapidly toward the realization of autonomous physical agents. This includes\\nindustrial and service robots, unmanned aerial vehicles, embedded control\\ndevices, and a number of other realizations of cybernetic/mechatronic\\nimplementations of intelligent autonomous devices. In this paper, we consider a\\nstylized version of robotic care, which would normally involve a two-level\\nReinforcement Learning procedure that trains a policy for both lower level\\nphysical movement decisions as well as higher level conceptual tasks and their\\nsub-components. In order to deliver greater safety and reliability in the\\nsystem, we present the general formulation of this as a two-level optimization\\nscheme which incorporates control at the lower level, and classical planning at\\nthe higher level, integrated with a capacity for learning. This synergistic\\nintegration of multiple methodologies -- control, classical planning, and RL --\\npresents an opportunity for greater insight for algorithm development, leading\\nto more efficient and reliable performance. Here, the notion of reliability\\npertains to physical safety and interpretability into an otherwise black box\\noperation of autonomous agents, concerning users and regulators. This work\\npresents the necessary background and general formulation of the optimization\\nframework, detailing each component and its integration with the others.'),\n",
       " Document(metadata={'title': 'AI-washing: The Asymmetric Effects of Its Two Types on Consumer Moral Judgments', 'authors': 'Greg Nyilasy, Harsha Gangadharbatla', 'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04352v1'}, page_content=\"As AI hype continues to grow, organizations face pressure to broadcast or\\ndownplay purported AI initiatives - even when contrary to truth. This paper\\nintroduces AI-washing as overstating (deceptive boasting) or understating\\n(deceptive denial) a company's real AI usage. A 2x2 experiment (N = 401)\\nexamines how these false claims affect consumer attitudes and purchase\\nintentions. Results reveal a pronounced asymmetry: deceptive denial evokes more\\nnegative moral judgments than honest negation, while deceptive boasting has no\\neffects. We show that perceived betrayal mediates these outcomes. By clarifying\\nhow AI-washing erodes trust, the study highlights clear ethical implications\\nfor policymakers, marketers, and researchers striving for transparency.\"),\n",
       " Document(metadata={'title': 'MLLM-Fabric: Multimodal Large Language Model-Driven Robotic Framework for Fabric Sorting and Selection', 'authors': 'Liman Wang, Hanyang Zhong, Tianyuan Wang, Shan Luo, Jihong Zhu', 'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04351v1'}, page_content='Choosing the right fabric is crucial to meet functional and quality\\nrequirements in robotic applications for textile manufacturing, apparel\\nproduction, and smart retail. We present MLLM-Fabric, a robotic framework\\npowered by multimodal large language models (MLLMs) for fabric sorting and\\nselection. The system includes a robotic arm, a camera, a visuotactile sensor,\\nand a pressure sensor. It employs supervised fine-tuning and multimodal\\nexplanation-guided knowledge distillation to accurately classify and rank\\nfabric properties. To facilitate further research, we release a dataset of 220\\nunique fabric samples, including RGB images and synchronized visuotactile and\\npressure data. Experimental results show that our Fabric-Llama-90B model\\nconsistently outperforms pretrained vision-language baselines in both property\\nranking accuracy and selection reliability.'),\n",
       " Document(metadata={'title': 'SmartThinker: Learning to Compress and Preserve Reasoning by Step-Level Length Control', 'authors': 'Xingyang He, Xiao Ling, Jie Liu', 'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04348v1'}, page_content='Large reasoning models (LRMs) have exhibited remarkable reasoning\\ncapabilities through inference-time scaling, but this progress has also\\nintroduced considerable redundancy and inefficiency into their reasoning\\nprocesses, resulting in substantial computational waste. Previous work has\\nattempted to mitigate this issue by penalizing the overall length of generated\\nsamples during reinforcement learning (RL), with the goal of encouraging a more\\nconcise chains of thought. However, we observe that such global length penalty\\noften lead to excessive compression of critical reasoning steps while\\npreserving unnecessary details in simpler ones, yielding a suboptimal trade-off\\nbetween accuracy and efficiency. To address this issue, we propose\\nSmartThinker, a two-stage learnable framework designed to enable fine-grained\\ncontrol over the length of reasoning chains based on the importance of each\\nindividual step. In the first stage, SmartThinker adapts a reasoning model to a\\nshort-form reasoning mode through rejection sampling combined with supervised\\nfine-tuning (SFT). In the second stage, SmartThinker applies Step-Level Length\\nControl Policy Optimization (SCPO) to refine the model output distribution,\\nwhich increases the proportion of length allocated to critical steps while\\nreducing redundancy in less important ones. SCPO consists of four core\\ncomponents: an online importance estimator, a step-level length control reward\\nfunction, a step-level generalized advantage estimation (S-GAE) and a\\ndifficulty-adaptive clipping strategy. Working in concert, these components\\nenable SCPO to implement differentiated length control across reasoning steps.\\nEmpirical results across multiple reasoning benchmarks and various backbone\\nmodels demonstrate that SmartThinker significantly reduces redundant reasoning\\nwhile achieving comparable or even superior performance to existing methods.'),\n",
       " Document(metadata={'title': 'Improving Action Smoothness for a Cascaded Online Learning Flight Control System', 'authors': 'Yifei Li, Erik-jan van Kampen', 'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04346v1'}, page_content='This paper aims to improve the action smoothness of a cascaded online\\nlearning flight control system. Although the cascaded structure is widely used\\nin flight control design, its stability can be compromised by oscillatory\\ncontrol actions, which poses challenges for practical engineering applications.\\nTo address this issue, we introduce an online temporal smoothness technique and\\na low-pass filter to reduce the amplitude and frequency of the control actions.\\nFast Fourier Transform (FFT) is used to analyze policy performance in the\\nfrequency domain. Simulation results demonstrate the improvements achieved by\\nthe two proposed techniques.'),\n",
       " Document(metadata={'title': 'Efficient Perplexity Bound and Ratio Matching in Discrete Diffusion Language Models', 'authors': 'Etrit Haxholli, Yeti Z. Gürbüz, Oğul Can, Eli Waxman', 'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04341v1'}, page_content='While continuous diffusion models excel in modeling continuous distributions,\\ntheir application to categorical data has been less effective. Recent work has\\nshown that ratio-matching through score-entropy within a continuous-time\\ndiscrete Markov chain (CTMC) framework serves as a competitive alternative to\\nautoregressive models in language modeling. To enhance this framework, we first\\nintroduce three new theorems concerning the KL divergence between the data and\\nlearned distribution. Our results serve as the discrete counterpart to those\\nestablished for continuous diffusion models and allow us to derive an improved\\nupper bound of the perplexity. Second, we empirically show that ratio-matching\\nperformed by minimizing the denoising cross-entropy between the clean and\\ncorrupted data enables models to outperform those utilizing score-entropy with\\nup to 10% lower perplexity/generative-perplexity, and 15% faster training\\nsteps. To further support our findings, we introduce and evaluate a novel CTMC\\ntransition-rate matrix that allows prediction refinement, and derive the\\nanalytic expression for its matrix exponential which facilitates the\\ncomputation of conditional ratios thus enabling efficient training and\\ngeneration.'),\n",
       " Document(metadata={'title': 'Voltage Mode Winner-Take-All Circuit for Neuromorphic Systems', 'authors': 'Abdullah M. Zyarah, Dhireesha Kudithipudi', 'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04338v1'}, page_content='Recent advances in neuromorphic computing demonstrate on-device learning\\ncapabilities with low power consumption. One of the key learning units in these\\nsystems is the winner-take-all circuit. In this research, we propose a\\nwinner-take-all circuit that can be configured to achieve k-winner and\\nhysteresis properties, simulated in IBM 65 nm node. The circuit dissipated 34.9\\n$\\\\mu$W of power with a latency of 10.4 ns, while processing 1000 inputs. The\\nutility of the circuit is demonstrated for spatial filtering and\\nclassification.'),\n",
       " Document(metadata={'title': 'CLIP-RL: Surgical Scene Segmentation Using Contrastive Language-Vision Pretraining & Reinforcement Learning', 'authors': 'Fatmaelzahraa Ali Ahmed, Muhammad Arsalan, Abdulaziz Al-Ali, Khalid Al-Jalham, Shidin Balakrishnan', 'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04317v1'}, page_content='Understanding surgical scenes can provide better healthcare quality for\\npatients, especially with the vast amount of video data that is generated\\nduring MIS. Processing these videos generates valuable assets for training\\nsophisticated models. In this paper, we introduce CLIP-RL, a novel contrastive\\nlanguage-image pre-training model tailored for semantic segmentation for\\nsurgical scenes. CLIP-RL presents a new segmentation approach which involves\\nreinforcement learning and curriculum learning, enabling continuous refinement\\nof the segmentation masks during the full training pipeline. Our model has\\nshown robust performance in different optical settings, such as occlusions,\\ntexture variations, and dynamic lighting, presenting significant challenges.\\nCLIP model serves as a powerful feature extractor, capturing rich semantic\\ncontext that enhances the distinction between instruments and tissues. The RL\\nmodule plays a pivotal role in dynamically refining predictions through\\niterative action-space adjustments. We evaluated CLIP-RL on the EndoVis 2018\\nand EndoVis 2017 datasets. CLIP-RL achieved a mean IoU of 81%, outperforming\\nstate-of-the-art models, and a mean IoU of 74.12% on EndoVis 2017. This\\nsuperior performance was achieved due to the combination of contrastive\\nlearning with reinforcement learning and curriculum learning.'),\n",
       " Document(metadata={'title': 'Surg-SegFormer: A Dual Transformer-Based Model for Holistic Surgical Scene Segmentation', 'authors': 'Fatimaelzahraa Ahmed, Muraam Abdel-Ghani, Muhammad Arsalan, Mahmoud Ali, Abdulaziz Al-Ali, Shidin Balakrishnan', 'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04304v1'}, page_content='Holistic surgical scene segmentation in robot-assisted surgery (RAS) enables\\nsurgical residents to identify various anatomical tissues, articulated tools,\\nand critical structures, such as veins and vessels. Given the firm\\nintraoperative time constraints, it is challenging for surgeons to provide\\ndetailed real-time explanations of the operative field for trainees. This\\nchallenge is compounded by the scarcity of expert surgeons relative to\\ntrainees, making the unambiguous delineation of go- and no-go zones\\ninconvenient. Therefore, high-performance semantic segmentation models offer a\\nsolution by providing clear postoperative analyses of surgical procedures.\\nHowever, recent advanced segmentation models rely on user-generated prompts,\\nrendering them impractical for lengthy surgical videos that commonly exceed an\\nhour. To address this challenge, we introduce Surg-SegFormer, a novel\\nprompt-free model that outperforms current state-of-the-art techniques.\\nSurg-SegFormer attained a mean Intersection over Union (mIoU) of 0.80 on the\\nEndoVis2018 dataset and 0.54 on the EndoVis2017 dataset. By providing robust\\nand automated surgical scene comprehension, this model significantly reduces\\nthe tutoring burden on expert surgeons, empowering residents to independently\\nand effectively understand complex surgical environments.'),\n",
       " Document(metadata={'title': 'QF: Quick Feedforward AI Model Training without Gradient Back Propagation', 'authors': 'Feng Qi', 'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04300v1'}, page_content='We propose Quick Feedforward (QF) Learning, a novel knowledge consolidation\\nframework for transformer-based models that enables efficient transfer of\\ninstruction derived knowledge into model weights through feedforward\\nactivations without any gradient back propagation. Unlike traditional\\nfinetuning, QF updates are computed in closed form, require minimal parameter\\nmodification, and preserve prior knowledge. Importantly, QF allows models to\\ntrain and infer within the same runtime environment, making the process more\\nresource efficient and closely aligned with how the human brain operates. Code\\nand models are open sourced on GitHub. I hope QF Learning inspires a more\\nefficient and brain-like paradigm for AI systems.'),\n",
       " Document(metadata={'title': 'Answer Set Programming Modulo Theories and Reasoning about Continuous Changes', 'authors': 'Joohyung Lee, Yunsong Meng', 'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04299v1'}, page_content=\"Answer Set Programming Modulo Theories (ASPMT) is a new framework of tight\\nintegration of answer set programming (ASP) and satisfiability modulo theories\\n(SMT). Similar to the relationship between first-order logic and SMT, it is\\nbased on a recent proposal of the functional stable model semantics by fixing\\ninterpretations of background theories. Analogously to a known relationship\\nbetween ASP and SAT, ``tight'' ASPMT programs can be translated into SMT\\ninstances. We demonstrate the usefulness of ASPMT by enhancing action language\\nC+ to handle continuous changes as well as discrete changes. We reformulate the\\nsemantics of C+ in terms ofASPMT, and show that SMT solvers can be used to\\ncompute the language. We also show how the language can represent cumulative\\neffects on continuous resources.\"),\n",
       " Document(metadata={'title': 'LearnLens: LLM-Enabled Personalised, Curriculum-Grounded Feedback with Educators in the Loop', 'authors': 'Runcong Zhao, Artem Borov, Jiazheng Li, Yulan He', 'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04295v1'}, page_content='Effective feedback is essential for student learning but is time-intensive\\nfor teachers. We present LearnLens, a modular, LLM-based system that generates\\npersonalised, curriculum-aligned feedback in science education. LearnLens\\ncomprises three components: (1) an error-aware assessment module that captures\\nnuanced reasoning errors; (2) a curriculum-grounded generation module that uses\\na structured, topic-linked memory chain rather than traditional\\nsimilarity-based retrieval, improving relevance and reducing noise; and (3) an\\neducator-in-the-loop interface for customisation and oversight. LearnLens\\naddresses key challenges in existing systems, offering scalable, high-quality\\nfeedback that empowers both teachers and students.'),\n",
       " Document(metadata={'title': 'M$^3$-Med: A Benchmark for Multi-lingual, Multi-modal, and Multi-hop Reasoning in Medical Instructional Video Understanding', 'authors': 'Shenxi Liu, Kan Li, Mingyang Zhao, Yuhang Tian, Bin Li, Shoujun Zhou, Hongliang Li, Fuxia Yang', 'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04289v1'}, page_content=\"With the rapid progress of artificial intelligence (AI) in multi-modal\\nunderstanding, there is increasing potential for video comprehension\\ntechnologies to support professional domains such as medical education.\\nHowever, existing benchmarks suffer from two primary limitations: (1)\\nLinguistic Singularity: they are largely confined to English, neglecting the\\nneed for multilingual resources; and (2) Shallow Reasoning: their questions are\\noften designed for surface-level information retrieval, failing to properly\\nassess deep multi-modal integration. To address these limitations, we present\\nM3-Med, the first benchmark for Multi-lingual, Multi-modal, and Multi-hop\\nreasoning in Medical instructional video understanding. M3-Med consists of\\nmedical questions paired with corresponding video segments, annotated by a team\\nof medical experts. A key innovation of M3-Med is its multi-hop reasoning task,\\nwhich requires a model to first locate a key entity in the text, then find\\ncorresponding visual evidence in the video, and finally synthesize information\\nacross both modalities to derive the answer. This design moves beyond simple\\ntext matching and poses a substantial challenge to a model's deep cross-modal\\nunderstanding capabilities. We define two tasks: Temporal Answer Grounding in\\nSingle Video (TAGSV) and Temporal Answer Grounding in Video Corpus (TAGVC). We\\nevaluated several state-of-the-art models and Large Language Models (LLMs) on\\nM3-Med. The results reveal a significant performance gap between all models and\\nhuman experts, especially on the complex multi-hop questions where model\\nperformance drops sharply. M3-Med effectively highlights the current\\nlimitations of AI models in deep cross-modal reasoning within specialized\\ndomains and provides a new direction for future research.\"),\n",
       " Document(metadata={'title': 'SeqTex: Generate Mesh Textures in Video Sequence', 'authors': 'Ze Yuan, Xin Yu, Yangtian Sun, Yuan-Chen Guo, Yan-Pei Cao, Ding Liang, Xiaojuan Qi', 'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04285v1'}, page_content='Training native 3D texture generative models remains a fundamental yet\\nchallenging problem, largely due to the limited availability of large-scale,\\nhigh-quality 3D texture datasets. This scarcity hinders generalization to\\nreal-world scenarios. To address this, most existing methods finetune\\nfoundation image generative models to exploit their learned visual priors.\\nHowever, these approaches typically generate only multi-view images and rely on\\npost-processing to produce UV texture maps -- an essential representation in\\nmodern graphics pipelines. Such two-stage pipelines often suffer from error\\naccumulation and spatial inconsistencies across the 3D surface. In this paper,\\nwe introduce SeqTex, a novel end-to-end framework that leverages the visual\\nknowledge encoded in pretrained video foundation models to directly generate\\ncomplete UV texture maps. Unlike previous methods that model the distribution\\nof UV textures in isolation, SeqTex reformulates the task as a sequence\\ngeneration problem, enabling the model to learn the joint distribution of\\nmulti-view renderings and UV textures. This design effectively transfers the\\nconsistent image-space priors from video foundation models into the UV domain.\\nTo further enhance performance, we propose several architectural innovations: a\\ndecoupled multi-view and UV branch design, geometry-informed attention to guide\\ncross-domain feature alignment, and adaptive token resolution to preserve fine\\ntexture details while maintaining computational efficiency. Together, these\\ncomponents allow SeqTex to fully utilize pretrained video priors and synthesize\\nhigh-fidelity UV texture maps without the need for post-processing. Extensive\\nexperiments show that SeqTex achieves state-of-the-art performance on both\\nimage-conditioned and text-conditioned 3D texture generation tasks, with\\nsuperior 3D consistency, texture-geometry alignment, and real-world\\ngeneralization.'),\n",
       " Document(metadata={'title': 'Clustering via Self-Supervised Diffusion', 'authors': 'Roy Uziel, Irit Chelly, Oren Freifeld, Ari Pakman', 'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04283v1'}, page_content='Diffusion models, widely recognized for their success in generative tasks,\\nhave not yet been applied to clustering. We introduce Clustering via Diffusion\\n(CLUDI), a self-supervised framework that combines the generative power of\\ndiffusion models with pre-trained Vision Transformer features to achieve robust\\nand accurate clustering. CLUDI is trained via a teacher-student paradigm: the\\nteacher uses stochastic diffusion-based sampling to produce diverse cluster\\nassignments, which the student refines into stable predictions. This\\nstochasticity acts as a novel data augmentation strategy, enabling CLUDI to\\nuncover intricate structures in high-dimensional data. Extensive evaluations on\\nchallenging datasets demonstrate that CLUDI achieves state-of-the-art\\nperformance in unsupervised classification, setting new benchmarks in\\nclustering robustness and adaptability to complex data distributions.'),\n",
       " Document(metadata={'title': 'VOLTRON: Detecting Unknown Malware Using Graph-Based Zero-Shot Learning', 'authors': 'M. Tahir Akdeniz, Zeynep Yeşilkaya, İ. Enes Köse, İ. Ulaş Ünal, Sevil Şen', 'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04275v1'}, page_content='The persistent threat of Android malware presents a serious challenge to the\\nsecurity of millions of users globally. While many machine learning-based\\nmethods have been developed to detect these threats, their reliance on large\\nlabeled datasets limits their effectiveness against emerging, previously unseen\\nmalware families, for which labeled data is scarce or nonexistent.\\n  To address this challenge, we introduce a novel zero-shot learning framework\\nthat combines Variational Graph Auto-Encoders (VGAE) with Siamese Neural\\nNetworks (SNN) to identify malware without needing prior examples of specific\\nmalware families. Our approach leverages graph-based representations of Android\\napplications, enabling the model to detect subtle structural differences\\nbetween benign and malicious software, even in the absence of labeled data for\\nnew threats.\\n  Experimental results show that our method outperforms the state-of-the-art\\nMaMaDroid, especially in zero-day malware detection. Our model achieves 96.24%\\naccuracy and 95.20% recall for unknown malware families, highlighting its\\nrobustness against evolving Android threats.'),\n",
       " Document(metadata={'title': 'ZERO: Multi-modal Prompt-based Visual Grounding', 'authors': 'Sangbum Choi, Kyeongryeol Go', 'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04270v1'}, page_content='Recent advances in artificial intelligence have led to the emergence of\\nfoundation models, large-scale pre-trained neural networks that serve as\\nversatile starting points for a wide range of downstream tasks. In this work,\\nwe present ZERO, a zero-shot multi-prompt object detection model specifically\\ndesigned for robust, production-ready deployment across diverse industrial\\ndomains. ZERO integrates direct image input with multiple user-defined prompts,\\nwhich can include both textual and visual cues, and processes them through\\ndedicated encoders to generate accurate detection outputs. The model\\narchitecture is optimized for scalability, with a total of 1.033 TFLOPS and\\n622.346 million parameters, and is trained using a domain-specific image\\ndatabase exceeding one billion images. For the CVPR 2025 Foundational Few-Shot\\nObject Detection (FSOD) Challenge, we introduce a domain-specific fine-tuning\\nstrategy that emphasizes prompt diversity and conservative pseudo-labeling,\\nenabling effective adaptation to new domains with minimal supervision. Our\\napproach demonstrates practical advantages in flexibility, efficiency, and\\nreal-world applicability, achieving strong performance on the RF20VL-fsod\\nbenchmark despite limited annotation budgets. The results highlight the\\npotential of prompt-driven, data-centric AI for scalable and adaptive object\\ndetection in dynamic industrial environments.'),\n",
       " Document(metadata={'title': 'Deep-Learning-Assisted Highly-Accurate COVID-19 Diagnosis on Lung Computed Tomography Images', 'authors': 'Yinuo Wang, Juhyun Bae, Ka Ho Chow, Shenyang Chen, Shreyash Gupta', 'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04252v1'}, page_content='COVID-19 is a severe and acute viral disease that can cause symptoms\\nconsistent with pneumonia in which inflammation is caused in the alveolous\\nregions of the lungs leading to a build-up of fluid and breathing difficulties.\\nThus, the diagnosis of COVID using CT scans has been effective in assisting\\nwith RT-PCR diagnosis and severity classifications. In this paper, we proposed\\na new data quality control pipeline to refine the quality of CT images based on\\nGAN and sliding windows. Also, we use class-sensitive cost functions including\\nLabel Distribution Aware Loss(LDAM Loss) and Class-balanced(CB) Loss to solve\\nthe long-tail problem existing in datasets. Our model reaches more than 0.983\\nMCC in the benchmark test dataset.'),\n",
       " Document(metadata={'title': 'Just Enough Shifts: Mitigating Over-Refusal in Aligned Language Models with Targeted Representation Fine-Tuning', 'authors': 'Mahavir Dabas, Si Chen, Charles Fleming, Ming Jin, Ruoxi Jia', 'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04250v1'}, page_content=\"Safety alignment is crucial for large language models (LLMs) to resist\\nmalicious instructions but often results in over-refusals, where benign prompts\\nare unnecessarily rejected, impairing user experience and model utility. We\\nintroduce ACTOR (Activation-Based Training for Over-Refusal Reduction), a\\nrobust and compute- and data-efficient training framework that minimizes\\nover-refusals by leveraging internal activation patterns from diverse queries.\\nACTOR precisely identifies and adjusts the activation components that trigger\\nrefusals, providing stronger control over the refusal mechanism. By fine-tuning\\nonly a single model layer, ACTOR effectively reduces over-refusals across\\nmultiple benchmarks while maintaining the model's ability to handle harmful\\nqueries and preserve overall utility.\"),\n",
       " Document(metadata={'title': 'Domain Generalizable Portrait Style Transfer', 'authors': 'Xinbo Wang, Wenju Xu, Qing Zhang, Wei-Shi Zheng', 'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04243v1'}, page_content='This paper presents a portrait style transfer method that generalizes well to\\nvarious different domains while enabling high-quality semantic-aligned\\nstylization on regions including hair, eyes, eyelashes, skins, lips, and\\nbackground. To this end, we propose to establish dense semantic correspondence\\nbetween the given input and reference portraits based on a pre-trained model\\nand a semantic adapter, with which we obtain a warped reference semantically\\naligned with the input. To ensure effective yet controllable style transfer, we\\ndevise an AdaIN-Wavelet transform to balance content preservation and\\nstylization by blending low-frequency information of the warped reference with\\nhigh-frequency information of the input in the latent space. A style adapter is\\nalso designed to provide style guidance from the warped reference. With the\\nstylized latent from AdaIN-Wavelet transform, we employ a dual-conditional\\ndiffusion model that integrates a ControlNet recording high-frequency\\ninformation and the style guidance to generate the final result. Extensive\\nexperiments demonstrate the superiority of our method. Our code and trained\\nmodel are available at https://github.com/wangxb29/DGPST.'),\n",
       " Document(metadata={'title': 'Scaling Context Requires Rethinking Attention', 'authors': 'Carles Gelada, Jacob Buckman, Sean Zhang, Txus Bach', 'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04239v1'}, page_content='We argue that neither transformers nor sub-quadratic architectures are well\\nsuited to training at long sequence lengths: the cost of processing the context\\nis too expensive in the former, too inexpensive in the latter. Approaches such\\nas sliding window attention which reduce the cost-per-token of a transformer\\nimpair in-context learning, and so are also unsuitable. To address these\\nlimitations, we introduce power attention, an architectural layer for\\nlinear-cost sequence modeling whose state size can be adjusted independently of\\nparameters, unlocking the advantages of linear attention on practical domains.\\nWe develop and open-source a set of GPU kernels for efficient power attention,\\nidentifying a novel pattern of operation fusion to avoid memory and bandwidth\\nbottlenecks. Our experiments on the in-context learning of power attention\\nshows that these models dominate both exponential attention and linear\\nattention at long-context training.'),\n",
       " Document(metadata={'title': 'Design Optimization of Three-Dimensional Wire Arrangement Considering Wire Crossings for Tendon-driven Robots', 'authors': 'Kento Kawaharazuka, Shintaro Inoue, Yuta Sahara, Keita Yoneda, Temma Suzuki, Kei Okada', 'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04235v1'}, page_content='Tendon-driven mechanisms are useful from the perspectives of variable\\nstiffness, redundant actuation, and lightweight design, and they are widely\\nused, particularly in hands, wrists, and waists of robots. The design of these\\nwire arrangements has traditionally been done empirically, but it becomes\\nextremely challenging when dealing with complex structures. Various studies\\nhave attempted to optimize wire arrangement, but many of them have\\noversimplified the problem by imposing conditions such as restricting movements\\nto a 2D plane, keeping the moment arm constant, or neglecting wire crossings.\\nTherefore, this study proposes a three-dimensional wire arrangement\\noptimization that takes wire crossings into account. We explore wire\\narrangements through a multi-objective black-box optimization method that\\nensures wires do not cross while providing sufficient joint torque along a\\ndefined target trajectory. For a 3D link structure, we optimize the wire\\narrangement under various conditions, demonstrate its effectiveness, and\\ndiscuss the obtained design solutions.'),\n",
       " Document(metadata={'title': 'High-Resolution Sustain Pedal Depth Estimation from Piano Audio Across Room Acoustics', 'authors': 'Kun Fang, Hanwen Zhang, Ziyu Wang, Ichiro Fujinaga', 'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04230v1'}, page_content='Piano sustain pedal detection has previously been approached as a binary\\non/off classification task, limiting its application in real-world piano\\nperformance scenarios where pedal depth significantly influences musical\\nexpression. This paper presents a novel approach for high-resolution estimation\\nthat predicts continuous pedal depth values. We introduce a Transformer-based\\narchitecture that not only matches state-of-the-art performance on the\\ntraditional binary classification task but also achieves high accuracy in\\ncontinuous pedal depth estimation. Furthermore, by estimating continuous\\nvalues, our model provides musically meaningful predictions for sustain pedal\\nusage, whereas baseline models struggle to capture such nuanced expressions\\nwith their binary detection approach. Additionally, this paper investigates the\\ninfluence of room acoustics on sustain pedal estimation using a synthetic\\ndataset that includes varied acoustic conditions. We train our model with\\ndifferent combinations of room settings and test it in an unseen new\\nenvironment using a \"leave-one-out\" approach. Our findings show that the two\\nbaseline models and ours are not robust to unseen room conditions. Statistical\\nanalysis further confirms that reverberation influences model predictions and\\nintroduces an overestimation bias.'),\n",
       " Document(metadata={'title': 'Hijacking JARVIS: Benchmarking Mobile GUI Agents against Unprivileged Third Parties', 'authors': 'Guohong Liu, Jialei Ye, Jiacheng Liu, Yuanchun Li, Wei Liu, Pengzhi Gao, Jian Luan, Yunxin Liu', 'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04227v1'}, page_content='Mobile GUI agents are designed to autonomously execute diverse device-control\\ntasks by interpreting and interacting with mobile screens. Despite notable\\nadvancements, their resilience in real-world scenarios where screen content may\\nbe partially manipulated by untrustworthy third parties remains largely\\nunexplored. Owing to their black-box and autonomous nature, these agents are\\nvulnerable to manipulations that could compromise user devices. In this work,\\nwe present the first systematic investigation into the vulnerabilities of\\nmobile GUI agents. We introduce a scalable attack simulation framework\\nAgentHazard, which enables flexible and targeted modifications of screen\\ncontent within existing applications. Leveraging this framework, we develop a\\ncomprehensive benchmark suite comprising both a dynamic task execution\\nenvironment and a static dataset of vision-language-action tuples, totaling\\nover 3,000 attack scenarios. The dynamic environment encompasses 58\\nreproducible tasks in an emulator with various types of hazardous UI content,\\nwhile the static dataset is constructed from 210 screenshots collected from 14\\npopular commercial apps. Importantly, our content modifications are designed to\\nbe feasible for unprivileged third parties. We evaluate 7 widely-used mobile\\nGUI agents and 5 common backbone models using our benchmark. Our findings\\nreveal that all examined agents are significantly influenced by misleading\\nthird-party content (with an average misleading rate of 28.8% in human-crafted\\nattack scenarios) and that their vulnerabilities are closely linked to the\\nemployed perception modalities and backbone LLMs. Furthermore, we assess\\ntraining-based mitigation strategies, highlighting both the challenges and\\nopportunities for enhancing the robustness of mobile GUI agents. Our code and\\ndata will be released at https://agenthazard.github.io.'),\n",
       " Document(metadata={'title': 'Zero-Shot Cyclic Peptide Design with Composable Geometric Conditions', 'authors': 'Dapeng Jiang, Xiangzhe Kong, Jiaqi Han, Mingyu Li, Rui Jiao, Wenbing Huang, Stefano Ermon, Jianzhu Ma, Yang Liu', 'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04225v1'}, page_content='Cyclic peptides, characterized by geometric constraints absent in linear\\npeptides, offer enhanced biochemical properties, presenting new opportunities\\nto address unmet medical needs. However, designing target-specific cyclic\\npeptides remains underexplored due to limited training data. To bridge the gap,\\nwe propose CP-Composer, a novel generative framework that enables zero-shot\\ncyclic peptide generation via composable geometric constraints. Our approach\\ndecomposes complex cyclization patterns into unit constraints, which are\\nincorporated into a diffusion model through geometric conditioning on nodes and\\nedges. During training, the model learns from unit constraints and their random\\ncombinations in linear peptides, while at inference, novel constraint\\ncombinations required for cyclization are imposed as input. Experiments show\\nthat our model, despite trained with linear peptides, is capable of generating\\ndiverse target-binding cyclic peptides, reaching success rates from 38% to 84%\\non different cyclization strategies.'),\n",
       " Document(metadata={'title': 'Fairness Evaluation of Large Language Models in Academic Library Reference Services', 'authors': 'Haining Wang, Jason Clark, Yueru Yan, Star Bradley, Ruiyang Chen, Yiqiong Zhang, Hengyi Fu, Zuoyu Tian', 'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04224v1'}, page_content=\"As libraries explore large language models (LLMs) for use in virtual\\nreference services, a key question arises: Can LLMs serve all users equitably,\\nregardless of demographics or social status? While they offer great potential\\nfor scalable support, LLMs may also reproduce societal biases embedded in their\\ntraining data, risking the integrity of libraries' commitment to equitable\\nservice. To address this concern, we evaluate whether LLMs differentiate\\nresponses across user identities by prompting six state-of-the-art LLMs to\\nassist patrons differing in sex, race/ethnicity, and institutional role. We\\nfound no evidence of differentiation by race or ethnicity, and only minor\\nevidence of stereotypical bias against women in one model. LLMs demonstrated\\nnuanced accommodation of institutional roles through the use of linguistic\\nchoices related to formality, politeness, and domain-specific vocabularies,\\nreflecting professional norms rather than discriminatory treatment. These\\nfindings suggest that current LLMs show a promising degree of readiness to\\nsupport equitable and contextually appropriate communication in academic\\nlibrary reference services.\"),\n",
       " Document(metadata={'title': 'Context Tuning for In-Context Optimization', 'authors': 'Jack Lu, Ryan Teehan, Zhenbang Yang, Mengye Ren', 'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04221v1'}, page_content=\"We introduce Context Tuning, a simple and effective method to significantly\\nenhance few-shot adaptation of language models (LLMs) without fine-tuning model\\nparameters. While prompt-based adaptation techniques have demonstrated the\\neffectiveness of lightweight adaptation methods for large language models\\n(LLMs), they typically initialize a trainable prompt or prefix with irrelevant\\ntokens for the task at hand. In contrast, Context Tuning initializes the\\ntrainable prompt or prefix with task-specific demonstration examples,\\nleveraging the model's inherent In-Context Learning (ICL) ability to extract\\nrelevant information for improved few-shot learning performance. Extensive\\nevaluations on benchmarks such as CrossFit, UnifiedQA, MMLU, BIG-Bench Hard,\\nand ARC demonstrate that Context Tuning outperforms traditional prompt-based\\nadaptation methods and achieves competitive accuracy to Test-Time Training with\\nsignificantly higher training efficiency.\"),\n",
       " Document(metadata={'title': 'Model Collapse Is Not a Bug but a Feature in Machine Unlearning for LLMs', 'authors': 'Yan Scholten, Sophie Xhonneux, Stephan Günnemann, Leo Schwinn', 'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04219v1'}, page_content='Current unlearning methods for LLMs optimize on the private information they\\nseek to remove by incorporating it into their training objectives. We argue\\nthis not only risks reinforcing exposure to sensitive data, it also\\nfundamentally contradicts the principle of minimizing its use. As a remedy, we\\npropose a novel unlearning method - Partial Model Collapse (PMC), which does\\nnot require unlearning targets in the unlearning objective. Our approach is\\ninspired by recent observations that training generative models on their own\\ngenerations leads to distribution collapse, effectively removing information\\nfrom the model. Our core idea is to leverage this collapse for unlearning by\\ntriggering collapse partially on the sensitive data. We theoretically analyze\\nthat our approach converges to the desired outcome, i.e. the LLM unlearns the\\ninformation in the forget set. We empirically demonstrate that PMC overcomes\\ntwo key limitations of existing unlearning approaches that explicitly optimize\\non unlearning targets, and more effectively removes private information from\\nmodel outputs. Overall, our contributions represent an important step toward\\nmore comprehensive unlearning that aligns with real-world privacy constraints.\\nCode available at https://www.cs.cit.tum.de/daml/partial-model-collapse/.'),\n",
       " Document(metadata={'title': 'Mpemba Effect in Large-Language Model Training Dynamics: A Minimal Analysis of the Valley-River model', 'authors': 'Sibei Liu, Zhijian Hu', 'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04206v1'}, page_content='Learning rate (LR) schedules in large language model (LLM) training often\\nfollow empirical templates: warm-up, constant plateau/stable phase, and decay\\n(WSD). However, the mechanistic explanation for this strategy remains\\nunderexplored, and the choice of plateau height and decay schedule is largely\\nheuristic. In this paper, we connect training dynamics to a thermodynamic\\nanalogy via the Mpemba effect - a phenomenon in which a hotter system cools\\nfaster than a colder one when quenched into the same bath. We analyze a class\\nof \"valley-river\" loss landscapes, where sharp (valley) directions equilibrate\\nquickly, while flatter (river) directions govern global descent. The Mpemba\\neffect provides an explanation for the necessity of the warm-up phase and\\nmotivates a high plateau - rather than a low one - for accelerating loss\\ndecrease during decay. We show that for certain loss landscapes, there exists\\nan optimal plateau learning rate - the \"strong Mpemba point\" - at which the\\nslowest mode vanishes, resulting in faster convergence during the decay phase.\\nWe derive analytical conditions for its existence and estimate decay dynamics\\nrequired to preserve the Mpemba advantage. Our minimal model and analysis offer\\na principled justification for plateau-based schedulers and provide guidance\\nfor tuning LR in LLMs with minimal hyperparameter sweep.'),\n",
       " Document(metadata={'title': 'Mixed-Sample SGD: an End-to-end Analysis of Supervised Transfer Learning', 'authors': 'Yuyang Deng, Samory Kpotufe', 'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04194v1'}, page_content='Theoretical works on supervised transfer learning (STL) -- where the learner\\nhas access to labeled samples from both source and target distributions -- have\\nfor the most part focused on statistical aspects of the problem, while\\nefficient optimization has received less attention. We consider the problem of\\ndesigning an SGD procedure for STL that alternates sampling between source and\\ntarget data, while maintaining statistical transfer guarantees without prior\\nknowledge of the quality of the source data. A main algorithmic difficulty is\\nin understanding how to design such an adaptive sub-sampling mechanism at each\\nSGD step, to automatically gain from the source when it is informative, or bias\\ntowards the target and avoid negative transfer when the source is less\\ninformative.\\n  We show that, such a mixed-sample SGD procedure is feasible for general\\nprediction tasks with convex losses, rooted in tracking an abstract sequence of\\nconstrained convex programs that serve to maintain the desired transfer\\nguarantees.\\n  We instantiate these results in the concrete setting of linear regression\\nwith square loss, and show that the procedure converges, with $1/\\\\sqrt{T}$\\nrate, to a solution whose statistical performance on the target is adaptive to\\nthe a priori unknown quality of the source. Experiments with synthetic and real\\ndatasets support the theory.'),\n",
       " Document(metadata={'title': 'SymbolicThought: Integrating Language Models and Symbolic Reasoning for Consistent and Interpretable Human Relationship Understanding', 'authors': 'Runcong Zhao, Qinglin Zhu, Hainiu Xu, Bin Liang, Yulan He, Lin Gui', 'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.04189v1'}, page_content='Understanding character relationships is essential for interpreting complex\\nnarratives and conducting socially grounded AI research. However, manual\\nannotation is time-consuming and low in coverage, while large language models\\n(LLMs) often produce hallucinated or logically inconsistent outputs. We present\\nSymbolicThought, a human-in-the-loop framework that combines LLM-based\\nextraction with symbolic reasoning. The system constructs editable character\\nrelationship graphs, refines them using seven types of logical constraints, and\\nenables real-time validation and conflict resolution through an interactive\\ninterface. To support logical supervision and explainable social analysis, we\\nrelease a dataset of 160 interpersonal relationships with corresponding logical\\nstructures. Experiments show that SymbolicThought improves annotation accuracy\\nand consistency while significantly reducing time cost, offering a practical\\ntool for narrative understanding, explainable AI, and LLM evaluation.'),\n",
       " Document(metadata={'title': 'Uncertainty Quantification in the Tsetlin Machine', 'authors': 'Runar Helin, Ole-Christoffer Granmo, Mayur Kishor Shende, Lei Jiao, Vladimir I. Zadorozhny, Kunal Ganesh Dumbre, Rishad Shafik, Alex Yakovlev', 'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.04175v1'}, page_content='Data modeling using Tsetlin machines (TMs) is all about building logical\\nrules from the data features. The decisions of the model are based on a\\ncombination of these logical rules. Hence, the model is fully transparent and\\nit is possible to get explanations of its predictions. In this paper, we\\npresent a probability score for TM predictions and develop new techniques for\\nuncertainty quantification to increase the explainability further. The\\nprobability score is an inherent property of any TM variant and is derived\\nthrough an analysis of the TM learning dynamics. Simulated data is used to show\\na clear connection between the learned TM probability scores and the underlying\\nprobabilities of the data. A visualization of the probability scores also\\nreveals that the TM is less confident in its predictions outside the training\\ndata domain, which contrasts the typical extrapolation phenomenon found in\\nArtificial Neural Networks. The paper concludes with an application of the\\nuncertainty quantification techniques on an image classification task using the\\nCIFAR-10 dataset, where they provide new insights and suggest possible\\nimprovements to current TM image classification models.'),\n",
       " Document(metadata={'title': 'Structure As Search: Unsupervised Permutation Learning for Combinatorial Optimization', 'authors': 'Yimeng Min, Carla P. Gomes', 'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.04164v1'}, page_content='We propose a non-autoregressive framework for the Travelling Salesman Problem\\nwhere solutions emerge directly from learned permutations without explicit\\nsearch. By applying a similarity transformation to Hamiltonian cycles, the\\nmodel learns to approximate permutation matrices via continuous relaxations.\\nOur unsupervised approach achieves competitive performance against classical\\nheuristics, demonstrating that the inherent structure of the problem can\\neffectively guide combinatorial optimization without sequential\\ndecision-making.'),\n",
       " Document(metadata={'title': 'Physics-informed neural networks and neural operators for a study of EUV electromagnetic wave diffraction from a lithography mask', 'authors': \"Vasiliy A. Es'kin, Egor V. Ivanov\", 'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.04153v1'}, page_content='Physics-informed neural networks (PINNs) and neural operators (NOs) for\\nsolving the problem of diffraction of Extreme Ultraviolet (EUV) electromagnetic\\nwaves from a mask are presented. A novel hybrid Waveguide Neural Operator\\n(WGNO) is introduced, which is based on a waveguide method with its most\\ncomputationally expensive part replaced by a neural network. Numerical\\nexperiments on realistic 2D and 3D masks show that the WGNO achieves\\nstate-of-the-art accuracy and inference time, providing a highly efficient\\nsolution for accelerating the design workflows of lithography masks.'),\n",
       " Document(metadata={'title': 'Dissecting Clinical Reasoning in Language Models: A Comparative Study of Prompts and Model Adaptation Strategies', 'authors': 'Mael Jullien, Marco Valentino, Leonardo Ranaldi, Andre Freitas', 'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.04142v1'}, page_content='Recent works on large language models (LLMs) have demonstrated the impact of\\nprompting strategies and fine-tuning techniques on their reasoning\\ncapabilities. Yet, their effectiveness on clinical natural language inference\\n(NLI) remains underexplored. This study presents the first controlled\\nevaluation of how prompt structure and efficient fine-tuning jointly shape\\nmodel performance in clinical NLI. We inspect four classes of prompting\\nstrategies to elicit reasoning in LLMs at different levels of abstraction, and\\nevaluate their impact on a range of clinically motivated reasoning types. For\\neach prompting strategy, we construct high-quality demonstrations using a\\nfrontier model to distil multi-step reasoning capabilities into smaller models\\n(4B parameters) via Low-Rank Adaptation (LoRA). Across different language\\nmodels fine-tuned on the NLI4CT benchmark, we found that prompt type alone\\naccounts for up to 44% of the variance in macro-F1. Moreover, LoRA fine-tuning\\nyields consistent gains of +8 to 12 F1, raises output alignment above 97%, and\\nnarrows the performance gap to GPT-4o-mini to within 7.1%. Additional\\nexperiments on reasoning generalisation reveal that LoRA improves performance\\nin 75% of the models on MedNLI and TREC Clinical Trials Track. Overall, these\\nfindings demonstrate that (i) prompt structure is a primary driver of clinical\\nreasoning performance, (ii) compact models equipped with strong prompts and\\nLoRA can rival frontier-scale systems, and (iii) reasoning-type-aware\\nevaluation is essential to uncover prompt-induced trade-offs. Our results\\nhighlight the promise of combining prompt design and lightweight adaptation for\\nmore efficient and trustworthy clinical NLP systems, providing insights on the\\nstrengths and limitations of widely adopted prompting and parameter-efficient\\ntechniques in highly specialised domains.'),\n",
       " Document(metadata={'title': 'Pedestrian Intention Prediction via Vision-Language Foundation Models', 'authors': 'Mohsen Azarmi, Mahdi Rezaei, He Wang', 'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.04141v1'}, page_content='Prediction of pedestrian crossing intention is a critical function in\\nautonomous vehicles. Conventional vision-based methods of crossing intention\\nprediction often struggle with generalizability, context understanding, and\\ncausal reasoning. This study explores the potential of vision-language\\nfoundation models (VLFMs) for predicting pedestrian crossing intentions by\\nintegrating multimodal data through hierarchical prompt templates. The\\nmethodology incorporates contextual information, including visual frames,\\nphysical cues observations, and ego-vehicle dynamics, into systematically\\nrefined prompts to guide VLFMs effectively in intention prediction. Experiments\\nwere conducted on three common datasets-JAAD, PIE, and FU-PIP. Results\\ndemonstrate that incorporating vehicle speed, its variations over time, and\\ntime-conscious prompts significantly enhances the prediction accuracy up to\\n19.8%. Additionally, optimised prompts generated via an automatic prompt\\nengineering framework yielded 12.5% further accuracy gains. These findings\\nhighlight the superior performance of VLFMs compared to conventional\\nvision-based models, offering enhanced generalisation and contextual\\nunderstanding for autonomous driving applications.'),\n",
       " Document(metadata={'title': 'Driver-Net: Multi-Camera Fusion for Assessing Driver Take-Over Readiness in Automated Vehicles', 'authors': 'Mahdi Rezaei, Mohsen Azarmi', 'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.04139v1'}, page_content=\"Ensuring safe transition of control in automated vehicles requires an\\naccurate and timely assessment of driver readiness. This paper introduces\\nDriver-Net, a novel deep learning framework that fuses multi-camera inputs to\\nestimate driver take-over readiness. Unlike conventional vision-based driver\\nmonitoring systems that focus on head pose or eye gaze, Driver-Net captures\\nsynchronised visual cues from the driver's head, hands, and body posture\\nthrough a triple-camera setup. The model integrates spatio-temporal data using\\na dual-path architecture, comprising a Context Block and a Feature Block,\\nfollowed by a cross-modal fusion strategy to enhance prediction accuracy.\\nEvaluated on a diverse dataset collected from the University of Leeds Driving\\nSimulator, the proposed method achieves an accuracy of up to 95.8% in driver\\nreadiness classification. This performance significantly enhances existing\\napproaches and highlights the importance of multimodal and multi-view fusion.\\nAs a real-time, non-intrusive solution, Driver-Net contributes meaningfully to\\nthe development of safer and more reliable automated vehicles and aligns with\\nnew regulatory mandates and upcoming safety standards.\"),\n",
       " Document(metadata={'title': 'A Technical Survey of Reinforcement Learning Techniques for Large Language Models', 'authors': 'Saksham Sahai Srivastava, Vaneet Aggarwal', 'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.04136v1'}, page_content='Reinforcement Learning (RL) has emerged as a transformative approach for\\naligning and enhancing Large Language Models (LLMs), addressing critical\\nchallenges in instruction following, ethical alignment, and reasoning\\ncapabilities. This survey offers a comprehensive foundation on the integration\\nof RL with language models, highlighting prominent algorithms such as Proximal\\nPolicy Optimization (PPO), Q-Learning, and Actor-Critic methods. Additionally,\\nit provides an extensive technical overview of RL techniques specifically\\ntailored for LLMs, including foundational methods like Reinforcement Learning\\nfrom Human Feedback (RLHF) and AI Feedback (RLAIF), as well as advanced\\nstrategies such as Direct Preference Optimization (DPO) and Group Relative\\nPolicy Optimization (GRPO). We systematically analyze their applications across\\ndomains, i.e., from code generation to tool-augmented reasoning. We also\\npresent a comparative taxonomy based on reward modeling, feedback mechanisms,\\nand optimization strategies. Our evaluation highlights key trends. RLHF remains\\ndominant for alignment, and outcome-based RL such as RLVR significantly\\nimproves stepwise reasoning. However, persistent challenges such as reward\\nhacking, computational costs, and scalable feedback collection underscore the\\nneed for continued innovation. We further discuss emerging directions,\\nincluding hybrid RL algorithms, verifier-guided training, and multi-objective\\nalignment frameworks. This survey serves as a roadmap for researchers advancing\\nRL-driven LLM development, balancing capability enhancement with safety and\\nscalability.'),\n",
       " Document(metadata={'title': 'Towards Accurate and Efficient 3D Object Detection for Autonomous Driving: A Mixture of Experts Computing System on Edge', 'authors': 'Linshen Liu, Boyan Su, Junyue Jiang, Guanlin Wu, Cong Guo, Ceyu Xu, Hao Frank Yang', 'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.04123v1'}, page_content='This paper presents Edge-based Mixture of Experts (MoE) Collaborative\\nComputing (EMC2), an optimal computing system designed for autonomous vehicles\\n(AVs) that simultaneously achieves low-latency and high-accuracy 3D object\\ndetection. Unlike conventional approaches, EMC2 incorporates a scenario-aware\\nMoE architecture specifically optimized for edge platforms. By effectively\\nfusing LiDAR and camera data, the system leverages the complementary strengths\\nof sparse 3D point clouds and dense 2D images to generate robust multimodal\\nrepresentations. To enable this, EMC2 employs an adaptive multimodal data\\nbridge that performs multi-scale preprocessing on sensor inputs, followed by a\\nscenario-aware routing mechanism that dynamically dispatches features to\\ndedicated expert models based on object visibility and distance. In addition,\\nEMC2 integrates joint hardware-software optimizations, including hardware\\nresource utilization optimization and computational graph simplification, to\\nensure efficient and real-time inference on resource-constrained edge devices.\\nExperiments on open-source benchmarks clearly show the EMC2 advancements as a\\nend-to-end system. On the KITTI dataset, it achieves an average accuracy\\nimprovement of 3.58% and a 159.06% inference speedup compared to 15 baseline\\nmethods on Jetson platforms, with similar performance gains on the nuScenes\\ndataset, highlighting its capability to advance reliable, real-time 3D object\\ndetection tasks for AVs.'),\n",
       " Document(metadata={'title': 'When Data-Free Knowledge Distillation Meets Non-Transferable Teacher: Escaping Out-of-Distribution Trap is All You Need', 'authors': 'Ziming Hong, Runnan Chen, Zengmao Wang, Bo Han, Bo Du, Tongliang Liu', 'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.04119v1'}, page_content=\"Data-free knowledge distillation (DFKD) transfers knowledge from a teacher to\\na student without access the real in-distribution (ID) data. Its common\\nsolution is to use a generator to synthesize fake data and use them as a\\nsubstitute for real ID data. However, existing works typically assume teachers\\nare trustworthy, leaving the robustness and security of DFKD from untrusted\\nteachers largely unexplored. In this work, we conduct the first investigation\\ninto distilling non-transferable learning (NTL) teachers using DFKD, where the\\ntransferability from an ID domain to an out-of-distribution (OOD) domain is\\nprohibited. We find that NTL teachers fool DFKD through divert the generator's\\nattention from the useful ID knowledge to the misleading OOD knowledge. This\\nhinders ID knowledge transfer but prioritizes OOD knowledge transfer. To\\nmitigate this issue, we propose Adversarial Trap Escaping (ATEsc) to benefit\\nDFKD by identifying and filtering out OOD-like synthetic samples. Specifically,\\ninspired by the evidence that NTL teachers show stronger adversarial robustness\\non OOD samples than ID samples, we split synthetic samples into two groups\\naccording to their robustness. The fragile group is treated as ID-like data and\\nused for normal knowledge distillation, while the robust group is seen as\\nOOD-like data and utilized for forgetting OOD knowledge. Extensive experiments\\ndemonstrate the effectiveness of ATEsc for improving DFKD against NTL teachers.\\nCode is released at https://github.com/tmllab/2025_ICML_ATEsc.\"),\n",
       " Document(metadata={'title': 'Addressing The Devastating Effects Of Single-Task Data Poisoning In Exemplar-Free Continual Learning', 'authors': 'Stanisław Pawlak, Bartłomiej Twardowski, Tomasz Trzciński, Joost van de Weijer', 'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.04106v1'}, page_content='Our research addresses the overlooked security concerns related to data\\npoisoning in continual learning (CL). Data poisoning - the intentional\\nmanipulation of training data to affect the predictions of machine learning\\nmodels - was recently shown to be a threat to CL training stability. While\\nexisting literature predominantly addresses scenario-dependent attacks, we\\npropose to focus on a more simple and realistic single-task poison (STP)\\nthreats. In contrast to previously proposed poisoning settings, in STP\\nadversaries lack knowledge and access to the model, as well as to both previous\\nand future tasks. During an attack, they only have access to the current task\\nwithin the data stream. Our study demonstrates that even within these stringent\\nconditions, adversaries can compromise model performance using standard image\\ncorruptions. We show that STP attacks are able to strongly disrupt the whole\\ncontinual training process: decreasing both the stability (its performance on\\npast tasks) and plasticity (capacity to adapt to new tasks) of the algorithm.\\nFinally, we propose a high-level defense framework for CL along with a poison\\ntask detection method based on task vectors. The code is available at\\nhttps://github.com/stapaw/STP.git .'),\n",
       " Document(metadata={'title': 'Enhancing Robustness of LLM-Driven Multi-Agent Systems through Randomized Smoothing', 'authors': 'Jinwei Hu, Yi Dong, Zhengtao Ding, Xiaowei Huang', 'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.04105v1'}, page_content='This paper presents a defense framework for enhancing the safety of large\\nlanguage model (LLM) empowered multi-agent systems (MAS) in safety-critical\\ndomains such as aerospace. We apply randomized smoothing, a statistical\\nrobustness certification technique, to the MAS consensus context, enabling\\nprobabilistic guarantees on agent decisions under adversarial influence. Unlike\\ntraditional verification methods, our approach operates in black-box settings\\nand employs a two-stage adaptive sampling mechanism to balance robustness and\\ncomputational efficiency. Simulation results demonstrate that our method\\neffectively prevents the propagation of adversarial behaviors and\\nhallucinations while maintaining consensus performance. This work provides a\\npractical and scalable path toward safe deployment of LLM-based MAS in\\nreal-world, high-stakes environments.'),\n",
       " Document(metadata={'title': 'How to Train Your LLM Web Agent: A Statistical Diagnosis', 'authors': 'Dheeraj Vattikonda, Santhoshi Ravichandran, Emiliano Penaloza, Hadi Nekoei, Megh Thakkar, Thibault Le Sellier de Chezelles, Nicolas Gontier, Miguel Muñoz-Mármol, Sahar Omidi Shayegan, Stefania Raimondo, Xue Liu, Alexandre Drouin, Laurent Charlin, Alexandre Piché, Alexandre Lacoste, Massimo Caccia', 'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.04103v1'}, page_content='LLM-based web agents have recently made significant progress, but much of it\\nhas occurred in closed-source systems, widening the gap with open-source\\nalternatives. Progress has been held back by two key challenges: first, a\\nnarrow focus on single-step tasks that overlooks the complexity of multi-step\\nweb interactions; and second, the high compute costs required to post-train\\nLLM-based web agents. To address this, we present the first statistically\\ngrounded study on compute allocation for LLM web-agent post-training. Our\\napproach uses a two-stage pipeline, training a Llama 3.1 8B student to imitate\\na Llama 3.3 70B teacher via supervised fine-tuning (SFT), followed by on-policy\\nreinforcement learning. We find this process highly sensitive to hyperparameter\\nchoices, making exhaustive sweeps impractical. To spare others from expensive\\ntrial-and-error, we sample 1,370 configurations and use bootstrapping to\\nestimate effective hyperparameters. Our results show that combining SFT with\\non-policy RL consistently outperforms either approach alone on both WorkArena\\nand MiniWob++. Further, this strategy requires only 55% of the compute to match\\nthe peak performance of pure SFT on MiniWob++, effectively pushing the\\ncompute-performance Pareto frontier, and is the only strategy that can close\\nthe gap with closed-source models.'),\n",
       " Document(metadata={'title': 'Hierarchical Testing with Rabbit Optimization for Industrial Cyber-Physical Systems', 'authors': 'Jinwei Hu, Zezhi Tang, Xin Jin, Benyuan Zhang, Yi Dong, Xiaowei Huang', 'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.04100v1'}, page_content=\"This paper presents HERO (Hierarchical Testing with Rabbit Optimization), a\\nnovel black-box adversarial testing framework for evaluating the robustness of\\ndeep learning-based Prognostics and Health Management systems in Industrial\\nCyber-Physical Systems. Leveraging Artificial Rabbit Optimization, HERO\\ngenerates physically constrained adversarial examples that align with\\nreal-world data distributions via global and local perspective. Its\\ngeneralizability ensures applicability across diverse ICPS scenarios. This\\nstudy specifically focuses on the Proton Exchange Membrane Fuel Cell system,\\nchosen for its highly dynamic operational conditions, complex degradation\\nmechanisms, and increasing integration into ICPS as a sustainable and efficient\\nenergy solution. Experimental results highlight HERO's ability to uncover\\nvulnerabilities in even state-of-the-art PHM models, underscoring the critical\\nneed for enhanced robustness in real-world applications. By addressing these\\nchallenges, HERO demonstrates its potential to advance more resilient PHM\\nsystems across a wide range of ICPS domains.\"),\n",
       " Document(metadata={'title': 'Conversation Forests: The Key to Fine Tuning Large Language Models for Multi-Turn Medical Conversations is Branching', 'authors': 'Thomas Savage', 'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.04099v1'}, page_content=\"Fine-tuning methods such as Direct Preference Optimization (DPO) and Group\\nRelative Policy Optimization (GRPO) have demonstrated success in training large\\nlanguage models (LLMs) for single-turn tasks. However, these methods fall short\\nin multi-turn applications, such as diagnostic patient interviewing, where\\nunderstanding how early conversational turns influence downstream completions\\nand outcomes is essential. In medicine, a multi-turn perspective is critical\\nfor learning diagnostic schemas and better understanding conversation dynamics.\\nTo address this gap, I introduce Savage Conversation Forests (SCF), a\\nreinforcement learning framework that leverages a branched conversation\\narchitecture to fine-tune LLMs for multi-turn dialogue. SCF generates multiple\\npossible conversation continuations at each turn, enabling the model to learn\\nhow different early responses affect downstream interactions and diagnostic\\noutcomes. In experiments simulating doctor-patient conversations, SCF with\\nbranching outperforms linear conversation architectures on diagnostic accuracy.\\nI hypothesize that SCF's improvements stem from its ability to provide richer,\\ninterdependent training signals across conversation turns. These results\\nsuggest that a branched training architecture is an important strategy for fine\\ntuning LLMs in complex multi-turn conversational tasks.\"),\n",
       " Document(metadata={'title': 'Human-centered AI with focus on Human-robot interaction (Book chapter)', 'authors': 'Alireza Mortezapour, Giuliana Vitiello', 'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.04095v1'}, page_content='Modern social robots can be considered the descendants of steam engines from\\nthe First Industrial Revolution (IR 1.0) and industrial robotic arms from the\\nThird Industrial Revolution (IR 3.0). As some time has passed since the\\nintroduction of these robots during the Fourth Industrial Revolution (IR 4.0),\\nchallenges and issues in their interaction with humans have emerged, leading\\nresearchers to conclude that, like any other AI-based technology, these robots\\nmust also be human-centered to meet the needs of their users. This chapter aims\\nto introduce humans and their needs in interactions with robots, ranging from\\nshort-term, one-on-one interactions (micro-level) to long-term, macro-level\\nneeds at the societal scale. Building upon the principles of human-centered AI,\\nthis chapter presents, for the first time, a new framework of human needs\\ncalled the Dual Pyramid. This framework encompasses a comprehensive list of\\nhuman needs in robot interactions, from the most fundamental, robot\\neffectiveness to macro level requirements, such as the collaboration with\\nrobots in achieving the United Nations 17 Sustainable Development Goals.'),\n",
       " Document(metadata={'title': 'MMMOS: Multi-domain Multi-axis Audio Quality Assessment', 'authors': 'Yi-Cheng Lin, Jia-Hung Chen, Hung-yi Lee', 'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.04094v1'}, page_content=\"Accurate audio quality estimation is essential for developing and evaluating\\naudio generation, retrieval, and enhancement systems. Existing non-intrusive\\nassessment models predict a single Mean Opinion Score (MOS) for speech, merging\\ndiverse perceptual factors and failing to generalize beyond speech. We propose\\nMMMOS, a no-reference, multi-domain audio quality assessment system that\\nestimates four orthogonal axes: Production Quality, Production Complexity,\\nContent Enjoyment, and Content Usefulness across speech, music, and\\nenvironmental sounds. MMMOS fuses frame-level embeddings from three pretrained\\nencoders (WavLM, MuQ, and M2D) and evaluates three aggregation strategies with\\nfour loss functions. By ensembling the top eight models, MMMOS shows a 20-30%\\nreduction in mean squared error and a 4-5% increase in Kendall's {\\\\tau} versus\\nbaseline, gains first place in six of eight Production Complexity metrics, and\\nranks among the top three on 17 of 32 challenge metrics.\"),\n",
       " Document(metadata={'title': 'Accurate and Efficient World Modeling with Masked Latent Transformers', 'authors': 'Maxime Burchi, Radu Timofte', 'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.04075v1'}, page_content=\"The Dreamer algorithm has recently obtained remarkable performance across\\ndiverse environment domains by training powerful agents with simulated\\ntrajectories. However, the compressed nature of its world model's latent space\\ncan result in the loss of crucial information, negatively affecting the agent's\\nperformance. Recent approaches, such as $\\\\Delta$-IRIS and DIAMOND, address this\\nlimitation by training more accurate world models. However, these methods\\nrequire training agents directly from pixels, which reduces training efficiency\\nand prevents the agent from benefiting from the inner representations learned\\nby the world model. In this work, we propose an alternative approach to world\\nmodeling that is both accurate and efficient. We introduce EMERALD (Efficient\\nMaskEd latent tRAnsformer worLD model), a world model using a spatial latent\\nstate with MaskGIT predictions to generate accurate trajectories in latent\\nspace and improve the agent performance. On the Crafter benchmark, EMERALD\\nachieves new state-of-the-art performance, becoming the first method to surpass\\nhuman experts performance within 10M environment steps. Our method also\\nsucceeds to unlock all 22 Crafter achievements at least once during evaluation.\"),\n",
       " Document(metadata={'title': 'Beyond Independent Passages: Adaptive Passage Combination Retrieval for Retrieval Augmented Open-Domain Question Answering', 'authors': 'Ting-Wen Ko, Jyun-Yu Jiang, Pu-Jen Cheng', 'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.04069v1'}, page_content='Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\\nincorporating external documents at inference time, enabling up-to-date\\nknowledge access without costly retraining. However, conventional RAG methods\\nretrieve passages independently, often leading to redundant, noisy, or\\ninsufficiently diverse context-particularly problematic - particularly\\nproblematic in noisy corpora and for multi-hop questions. To address this, we\\npropose Adaptive Passage Combination Retrieval (AdaPCR), a novel framework for\\nopen-domain question answering with black-box LMs. AdaPCR explicitly models\\ndependencies between passages by considering passage combinations as units for\\nretrieval and reranking. It consists of a context-aware query reformulation\\nusing concatenated passages, and a reranking step trained with a predictive\\nobjective aligned with downstream answer likelihood. Crucially, AdaPCR\\nadaptively selects the number of retrieved passages without additional stopping\\nmodules. Experiments across several QA benchmarks show that AdaPCR outperforms\\nbaselines, particularly in multi-hop reasoning, demonstrating the effectiveness\\nof modeling inter-passage dependencies for improved retrieval.'),\n",
       " Document(metadata={'title': 'HAWK: A Hierarchical Workflow Framework for Multi-Agent Collaboration', 'authors': 'Yuyang Cheng, Yumiao Xu, Chaojia Yu, Yong Zhao', 'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.04067v1'}, page_content=\"Contemporary multi-agent systems encounter persistent challenges in\\ncross-platform interoperability, dynamic task scheduling, and efficient\\nresource sharing. Agents with heterogeneous implementations often lack\\nstandardized interfaces; collaboration frameworks remain brittle and hard to\\nextend; scheduling policies are static; and inter-agent state synchronization\\nis insufficient. We propose Hierarchical Agent Workflow (HAWK), a modular\\nframework comprising five layers-User, Workflow, Operator, Agent, and\\nResource-and supported by sixteen standardized interfaces. HAWK delivers an\\nend-to-end pipeline covering task parsing, workflow orchestration, intelligent\\nscheduling, resource invocation, and data synchronization. At its core lies an\\nadaptive scheduling and optimization module in the Workflow Layer, which\\nharnesses real-time feedback and dynamic strategy adjustment to maximize\\nutilization. The Resource Layer provides a unified abstraction over\\nheterogeneous data sources, large models, physical devices, and third-party\\nservices&tools, simplifying cross-domain information retrieval. We demonstrate\\nHAWK's scalability and effectiveness via CreAgentive, a multi-agent\\nnovel-generation prototype, which achieves marked gains in throughput, lowers\\ninvocation complexity, and improves system controllability. We also show how\\nhybrid deployments of large language models integrate seamlessly within HAWK,\\nhighlighting its flexibility. Finally, we outline future research\\navenues-hallucination mitigation, real-time performance tuning, and enhanced\\ncross-domain adaptability-and survey prospective applications in healthcare,\\ngovernment, finance, and education.\"),\n",
       " Document(metadata={'title': 'Stochastic Human Motion Prediction with Memory of Action Transition and Action Characteristic', 'authors': 'Jianwei Tang, Hong Yang, Tengyue Chen, Jian-Fang Hu', 'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.04062v1'}, page_content='Action-driven stochastic human motion prediction aims to generate future\\nmotion sequences of a pre-defined target action based on given past observed\\nsequences performing non-target actions. This task primarily presents two\\nchallenges. Firstly, generating smooth transition motions is hard due to the\\nvarying transition speeds of different actions. Secondly, the action\\ncharacteristic is difficult to be learned because of the similarity of some\\nactions. These issues cause the predicted results to be unreasonable and\\ninconsistent. As a result, we propose two memory banks, the Soft-transition\\nAction Bank (STAB) and Action Characteristic Bank (ACB), to tackle the problems\\nabove. The STAB stores the action transition information. It is equipped with\\nthe novel soft searching approach, which encourages the model to focus on\\nmultiple possible action categories of observed motions. The ACB records action\\ncharacteristic, which produces more prior information for predicting certain\\nactions. To fuse the features retrieved from the two banks better, we further\\npropose the Adaptive Attention Adjustment (AAA) strategy. Extensive experiments\\non four motion prediction datasets demonstrate that our approach consistently\\noutperforms the previous state-of-the-art. The demo and code are available at\\nhttps://hyqlat.github.io/STABACB.github.io/.'),\n",
       " Document(metadata={'title': 'Temporal Continual Learning with Prior Compensation for Human Motion Prediction', 'authors': 'Jianwei Tang, Jiangxin Sun, Xiaotong Lin, Lifang Zhang, Wei-Shi Zheng, Jian-Fang Hu', 'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.04060v1'}, page_content='Human Motion Prediction (HMP) aims to predict future poses at different\\nmoments according to past motion sequences. Previous approaches have treated\\nthe prediction of various moments equally, resulting in two main limitations:\\nthe learning of short-term predictions is hindered by the focus on long-term\\npredictions, and the incorporation of prior information from past predictions\\ninto subsequent predictions is limited. In this paper, we introduce a novel\\nmulti-stage training framework called Temporal Continual Learning (TCL) to\\naddress the above challenges. To better preserve prior information, we\\nintroduce the Prior Compensation Factor (PCF). We incorporate it into the model\\ntraining to compensate for the lost prior information. Furthermore, we derive a\\nmore reasonable optimization objective through theoretical derivation. It is\\nimportant to note that our TCL framework can be easily integrated with\\ndifferent HMP backbone models and adapted to various datasets and applications.\\nExtensive experiments on four HMP benchmark datasets demonstrate the\\neffectiveness and flexibility of TCL. The code is available at\\nhttps://github.com/hyqlat/TCL.'),\n",
       " Document(metadata={'title': 'Attributing Data for Sharpness-Aware Minimization', 'authors': 'Chenyang Ren, Yifan Jia, Huanyi Xie, Zhaobin Xu, Tianxing Wei, Liangyu Wang, Lijie Hu, Di Wang', 'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.04059v1'}, page_content='Sharpness-aware Minimization (SAM) improves generalization in large-scale\\nmodel training by linking loss landscape geometry to generalization. However,\\nchallenges such as mislabeled noisy data and privacy concerns have emerged as\\nsignificant issues. Data attribution, which identifies the contributions of\\nspecific training samples, offers a promising solution. However, directly\\nrendering existing data influence evaluation tools such as influence functions\\n(IF) to SAM will be inapplicable or inaccurate as SAM utilizes an inner loop to\\nfind model perturbations that maximize loss, which the outer loop then\\nminimizes, resulting in a doubled computational structure. Additionally, this\\nbilevel structure complicates the modeling of data influence on the parameters.\\nIn this paper, based on the IF, we develop two innovative data valuation\\nmethods for SAM, each offering unique benefits in different scenarios: the\\nHessian-based IF and the Gradient Trajectory-based IF. The first one provides a\\ncomprehensive estimation of data influence using a closed-form measure that\\nrelies only on the trained model weights. In contrast, the other IF for SAM\\nutilizes gradient trajectory information during training for more accurate and\\nefficient data assessment. Extensive experiments demonstrate their\\neffectiveness in data evaluation and parameter tuning, with applications in\\nidentifying mislabeled data, model editing, and enhancing interpretability.'),\n",
       " Document(metadata={'title': 'Rethinking and Exploring String-Based Malware Family Classification in the Era of LLMs and RAG', 'authors': 'Yufan Chen, Daoyuan Wu, Juantao Zhong, Zicheng Zhang, Debin Gao, Shuai Wang, Yingjiu Li, Ning Liu', 'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.04055v1'}, page_content='Malware Family Classification (MFC) aims to identify the fine-grained family\\n(e.g., GuLoader or BitRAT) to which a potential malware sample belongs, in\\ncontrast to malware detection or sample classification that predicts only an\\nYes/No. Accurate family identification can greatly facilitate automated sample\\nlabeling and understanding on crowdsourced malware analysis platforms such as\\nVirusTotal and MalwareBazaar, which generate vast amounts of data daily. In\\nthis paper, we explore and assess the feasibility of using traditional binary\\nstring features for MFC in the new era of large language models (LLMs) and\\nRetrieval-Augmented Generation (RAG). Specifically, we investigate how\\nFamily-Specific String (FSS) features could be utilized in a manner similar to\\nRAG to facilitate MFC. To this end, we develop a curated evaluation framework\\ncovering 4,347 samples from 67 malware families, extract and analyze over 25\\nmillion strings, and conduct detailed ablation studies to assess the impact of\\ndifferent design choices in four major modules.'),\n",
       " Document(metadata={'title': 'TopoMAS: Large Language Model Driven Topological Materials Multiagent System', 'authors': 'Baohua Zhang, Xin Li, Huangchao Xu, Zhong Jin, Quansheng Wu, Ce Li', 'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.04053v1'}, page_content=\"Topological materials occupy a frontier in condensed-matter physics thanks to\\ntheir remarkable electronic and quantum properties, yet their cross-scale\\ndesign remains bottlenecked by inefficient discovery workflows. Here, we\\nintroduce TopoMAS (Topological materials Multi-Agent System), an interactive\\nhuman-AI framework that seamlessly orchestrates the entire materials-discovery\\npipeline: from user-defined queries and multi-source data retrieval, through\\ntheoretical inference and crystal-structure generation, to first-principles\\nvalidation. Crucially, TopoMAS closes the loop by autonomously integrating\\ncomputational outcomes into a dynamic knowledge graph, enabling continuous\\nknowledge refinement. In collaboration with human experts, it has already\\nguided the identification of novel topological phases SrSbO3, confirmed by\\nfirst-principles calculations. Comprehensive benchmarks demonstrate robust\\nadaptability across base Large Language Model, with the lightweight Qwen2.5-72B\\nmodel achieving 94.55% accuracy while consuming only 74.3-78.4% of tokens\\nrequired by Qwen3-235B and 83.0% of DeepSeek-V3's usage--delivering responses\\ntwice as fast as Qwen3-235B. This efficiency establishes TopoMAS as an\\naccelerator for computation-driven discovery pipelines. By harmonizing rational\\nagent orchestration with a self-evolving knowledge graph, our framework not\\nonly delivers immediate advances in topological materials but also establishes\\na transferable, extensible paradigm for materials-science domain.\"),\n",
       " Document(metadata={'title': 'Predictive Modeling of Effluent Temperature in SAT Systems Using Ambient Meteorological Data: Implications for Infiltration Management', 'authors': 'Roy Elkayam', 'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.04050v1'}, page_content='Accurate prediction of effluent temperature in recharge basins is essential\\nfor optimizing the Soil Aquifer Treatment (SAT) process, as temperature\\ndirectly influences water viscosity and infiltration rates. This study develops\\nand evaluates predictive models for effluent temperature in the upper recharge\\nlayer of a Shafdan SAT system recharge basin using ambient meteorological data.\\nMultiple linear regression (MLR), neural networks (NN), and random forests (RF)\\nwere tested for their predictive accuracy and interpretability. The MLR model,\\npreferred for its operational simplicity and robust performance, achieved high\\npredictive accuracy (R2 = 0.86-0.87) and was used to estimate effluent\\ntemperatures over a 10-year period. Results highlight pronounced seasonal\\ntemperature cycles and the importance of topsoil temperature in governing the\\nthermal profile of the infiltrating effluent. The study provides practical\\nequations for real-time monitoring and long-term planning of SAT operations.'),\n",
       " Document(metadata={'title': 'Evaluating the Effectiveness of Large Language Models in Solving Simple Programming Tasks: A User-Centered Study', 'authors': 'Kai Deng', 'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.04043v1'}, page_content='As large language models (LLMs) become more common in educational tools and\\nprogramming environments, questions arise about how these systems should\\ninteract with users. This study investigates how different interaction styles\\nwith ChatGPT-4o (passive, proactive, and collaborative) affect user performance\\non simple programming tasks. I conducted a within-subjects experiment where\\nfifteen high school students participated, completing three problems under\\nthree distinct versions of the model. Each version was designed to represent a\\nspecific style of AI support: responding only when asked, offering suggestions\\nautomatically, or engaging the user in back-and-forth dialogue.Quantitative\\nanalysis revealed that the collaborative interaction style significantly\\nimproved task completion time compared to the passive and proactive conditions.\\nParticipants also reported higher satisfaction and perceived helpfulness when\\nworking with the collaborative version. These findings suggest that the way an\\nLLM communicates, how it guides, prompts, and responds, can meaningfully impact\\nlearning and performance. This research highlights the importance of designing\\nLLMs that go beyond functional correctness to support more interactive,\\nadaptive, and user-centered experiences, especially for novice programmers.'),\n",
       " Document(metadata={'title': 'T-SYNTH: A Knowledge-Based Dataset of Synthetic Breast Images', 'authors': 'Christopher Wiedeman, Anastasiia Sarmakeeva, Elena Sizikova, Daniil Filienko, Miguel Lago, Jana G. Delfino, Aldo Badano', 'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.04038v1'}, page_content='One of the key impediments for developing and assessing robust medical\\nimaging algorithms is limited access to large-scale datasets with suitable\\nannotations. Synthetic data generated with plausible physical and biological\\nconstraints may address some of these data limitations. We propose the use of\\nphysics simulations to generate synthetic images with pixel-level segmentation\\nannotations, which are notoriously difficult to obtain. Specifically, we apply\\nthis approach to breast imaging analysis and release T-SYNTH, a large-scale\\nopen-source dataset of paired 2D digital mammography (DM) and 3D digital breast\\ntomosynthesis (DBT) images. Our initial experimental results indicate that\\nT-SYNTH images show promise for augmenting limited real patient datasets for\\ndetection tasks in DM and DBT. Our data and code are publicly available at\\nhttps://github.com/DIDSR/tsynth-release.'),\n",
       " Document(metadata={'title': 'Ready Jurist One: Benchmarking Language Agents for Legal Intelligence in Dynamic Environments', 'authors': 'Zheng Jia, Shengbin Yue, Wei Chen, Siyuan Wang, Yidong Liu, Yun Song, Zhongyu Wei', 'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.04037v1'}, page_content='The gap between static benchmarks and the dynamic nature of real-world legal\\npractice poses a key barrier to advancing legal intelligence. To this end, we\\nintroduce J1-ENVS, the first interactive and dynamic legal environment tailored\\nfor LLM-based agents. Guided by legal experts, it comprises six representative\\nscenarios from Chinese legal practices across three levels of environmental\\ncomplexity. We further introduce J1-EVAL, a fine-grained evaluation framework,\\ndesigned to assess both task performance and procedural compliance across\\nvarying levels of legal proficiency. Extensive experiments on 17 LLM agents\\nreveal that, while many models demonstrate solid legal knowledge, they struggle\\nwith procedural execution in dynamic settings. Even the SOTA model, GPT-4o,\\nfalls short of 60% overall performance. These findings highlight persistent\\nchallenges in achieving dynamic legal intelligence and offer valuable insights\\nto guide future research.'),\n",
       " Document(metadata={'title': 'Lyria: A General LLM-Driven Genetic Algorithm Framework for Problem Solving', 'authors': 'Weizhi Tang, Kwabena Nuamah, Vaishak Belle', 'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.04034v1'}, page_content='While Large Language Models (LLMs) have demonstrated impressive abilities\\nacross various domains, they still struggle with complex problems characterized\\nby multi-objective optimization, precise constraint satisfaction, immense\\nsolution spaces, etc. To address the limitation, drawing on the superior\\nsemantic understanding ability of LLMs and also the outstanding global search\\nand optimization capability of genetic algorithms, we propose to capitalize on\\ntheir respective strengths and introduce Lyria, a general LLM-driven genetic\\nalgorithm framework, comprising 7 essential components. Through conducting\\nextensive experiments with 4 LLMs across 3 types of problems, we demonstrated\\nthe efficacy of Lyria. Additionally, with 7 additional ablation experiments, we\\nfurther systematically analyzed and elucidated the factors that affect its\\nperformance.'),\n",
       " Document(metadata={'title': 'Nunchi-Bench: Benchmarking Language Models on Cultural Reasoning with a Focus on Korean Superstition', 'authors': 'Kyuhee Kim, Sangah Lee', 'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.04014v1'}, page_content=\"As large language models (LLMs) become key advisors in various domains, their\\ncultural sensitivity and reasoning skills are crucial in multicultural\\nenvironments. We introduce Nunchi-Bench, a benchmark designed to evaluate LLMs'\\ncultural understanding, with a focus on Korean superstitions. The benchmark\\nconsists of 247 questions spanning 31 topics, assessing factual knowledge,\\nculturally appropriate advice, and situational interpretation. We evaluate\\nmultilingual LLMs in both Korean and English to analyze their ability to reason\\nabout Korean cultural contexts and how language variations affect performance.\\nTo systematically assess cultural reasoning, we propose a novel evaluation\\nstrategy with customized scoring metrics that capture the extent to which\\nmodels recognize cultural nuances and respond appropriately. Our findings\\nhighlight significant challenges in LLMs' cultural reasoning. While models\\ngenerally recognize factual information, they struggle to apply it in practical\\nscenarios. Furthermore, explicit cultural framing enhances performance more\\neffectively than relying solely on the language of the prompt. To support\\nfurther research, we publicly release Nunchi-Bench alongside a leaderboard.\"),\n",
       " Document(metadata={'title': 'Leveraging Multimodal Data and Side Users for Diffusion Cross-Domain Recommendation', 'authors': 'Fan Zhang, Jinpeng Chen, Huan Li, Senzhang Wang, Yuan Cao, Kaimin Wei, JianXiang He, Feifei Kou, Jinqing Wang', 'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.04000v1'}, page_content=\"Cross-domain recommendation (CDR) aims to address the persistent cold-start\\nproblem in Recommender Systems. Current CDR research concentrates on\\ntransferring cold-start users' information from the auxiliary domain to the\\ntarget domain. However, these systems face two main issues: the\\nunderutilization of multimodal data, which hinders effective cross-domain\\nalignment, and the neglect of side users who interact solely within the target\\ndomain, leading to inadequate learning of the target domain's vector space\\ndistribution. To address these issues, we propose a model leveraging Multimodal\\ndata and Side users for diffusion Cross-domain recommendation (MuSiC). We first\\nemploy a multimodal large language model to extract item multimodal features\\nand leverage a large language model to uncover user features using prompt\\nlearning without fine-tuning. Secondly, we propose the cross-domain diffusion\\nmodule to learn the generation of feature vectors in the target domain. This\\napproach involves learning feature distribution from side users and\\nunderstanding the patterns in cross-domain transformation through overlapping\\nusers. Subsequently, the trained diffusion module is used to generate feature\\nvectors for cold-start users in the target domain, enabling the completion of\\ncross-domain recommendation tasks. Finally, our experimental evaluation of the\\nAmazon dataset confirms that MuSiC achieves state-of-the-art performance,\\nsignificantly outperforming all selected baselines. Our code is available:\\nhttps://anonymous.4open.science/r/MuSiC-310A/.\"),\n",
       " Document(metadata={'title': 'Toward Better Generalisation in Uncertainty Estimators: Leveraging Data-Agnostic Features', 'authors': 'Thuy An Ha, Bao Quoc Vo', 'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.03998v1'}, page_content='Large Language Models (LLMs) often generate responses that are factually\\nincorrect yet expressed with high confidence, which can pose serious risks for\\nend users. To address this, it is essential for LLMs not only to produce\\nanswers but also to provide accurate estimates of their correctness.\\nUncertainty quantification methods have been introduced to assess the quality\\nof LLM outputs, with factual accuracy being a key aspect of that quality. Among\\nthese methods, those that leverage hidden states to train probes have shown\\nparticular promise, as these internal representations encode information\\nrelevant to the factuality of responses, making this approach the focus of this\\npaper. However, the probe trained on the hidden states of one dataset often\\nstruggles to generalise to another dataset of a different task or domain. To\\naddress this limitation, we explore combining data-agnostic features with\\nhidden-state features and assess whether this hybrid feature set enhances\\nout-of-domain performance. We further examine whether selecting only the most\\ninformative hidden-state features, thereby discarding task-specific noise,\\nenables the data-agnostic features to contribute more effectively. The\\nexperiment results indicate that although introducing data-agnostic features\\ngenerally enhances generalisation performance in most cases, in certain\\nscenarios their inclusion degrades performance. A similar pattern emerges when\\nretaining only the most important hidden-state features - adding data-agnostic\\nfeatures does not consistently further enhance performance compared to using\\nthe full set of hidden-state features. A closer analysis reveals that, in some\\nspecific cases, the trained probe underweights the data-agnostic features\\nrelative to the hidden-state features, which we believe is the main reason why\\nthe results are inconclusive.'),\n",
       " Document(metadata={'title': 'Real-TabPFN: Improving Tabular Foundation Models via Continued Pre-training With Real-World Data', 'authors': 'Anurag Garg, Muhammad Ali, Noah Hollmann, Lennart Purucker, Samuel Müller, Frank Hutter', 'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.03971v1'}, page_content='Foundation models for tabular data, like TabPFN, achieve strong performance\\non small datasets when pre-trained solely on synthetic data. We show that this\\nperformance can be significantly boosted by a targeted continued pre-training\\nphase. Specifically, we demonstrate that leveraging a small, curated collection\\nof large, real-world datasets for continued pre-training yields superior\\ndownstream predictive accuracy compared to using broader, potentially noisier\\ncorpora like CommonCrawl or GitTables. Our resulting model, Real-TabPFN,\\nachieves substantial performance gains on 29 datasets from the OpenML AutoML\\nBenchmark.'),\n",
       " Document(metadata={'title': 'A Comparative Study of Specialized LLMs as Dense Retrievers', 'authors': 'Hengran Zhang, Keping Bi, Jiafeng Guo', 'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.03958v1'}, page_content='While large language models (LLMs) are increasingly deployed as dense\\nretrievers, the impact of their domain-specific specialization on retrieval\\neffectiveness remains underexplored. This investigation systematically examines\\nhow task-specific adaptations in LLMs influence their retrieval capabilities,\\nan essential step toward developing unified retrievers capable of handling\\ntext, code, images, and multimodal content. We conduct extensive experiments\\nwith eight Qwen2.5 7B LLMs, including base, instruction-tuned,\\ncode/math-specialized, long reasoning, and vision-language models across\\nzero-shot retrieval settings and the supervised setting. For the zero-shot\\nretrieval settings, we consider text retrieval from the BEIR benchmark and code\\nretrieval from the CoIR benchmark. Further, to evaluate supervised performance,\\nall LLMs are fine-tuned on the MS MARCO dataset. We find that mathematical\\nspecialization and the long reasoning capability cause consistent degradation\\nin three settings, indicating conflicts between mathematical reasoning and\\nsemantic matching. The vision-language model and code-specialized LLMs\\ndemonstrate superior zero-shot performance compared to other LLMs, even\\nsurpassing BM25 on the code retrieval task, and maintain comparable performance\\nto base LLMs in supervised settings. These findings suggest promising\\ndirections for the unified retrieval task leveraging cross-domain and\\ncross-modal fusion.'),\n",
       " Document(metadata={'title': 'Evaluating Adversarial Protections for Diffusion Personalization: A Comprehensive Study', 'authors': 'Kai Ye, Tianyi Chen, Zhen Wang', 'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.03953v1'}, page_content='With the increasing adoption of diffusion models for image generation and\\npersonalization, concerns regarding privacy breaches and content misuse have\\nbecome more pressing. In this study, we conduct a comprehensive comparison of\\neight perturbation based protection methods: AdvDM, ASPL, FSGM, MetaCloak,\\nMist, PhotoGuard, SDS, and SimAC--across both portrait and artwork domains.\\nThese methods are evaluated under varying perturbation budgets, using a range\\nof metrics to assess visual imperceptibility and protective efficacy. Our\\nresults offer practical guidance for method selection. Code is available at:\\nhttps://github.com/vkeilo/DiffAdvPerturbationBench.'),\n",
       " Document(metadata={'title': 'Optimizing Age of Trust and Throughput in Multi-Hop UAV-Aided IoT Networks', 'authors': 'Yizhou Luo, Kwan-Wu Chin, Ruyi Guan, Xi Xiao, Caimeng Wang, Jingyin Feng, Tengjiao He', 'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.03950v1'}, page_content=\"Devices operating in Internet of Things (IoT) networks may be deployed across\\nvast geographical areas and interconnected via multi-hop communications.\\nFurther, they may be unguarded. This makes them vulnerable to attacks and\\nmotivates operators to check on devices frequently. To this end, we propose and\\nstudy an Unmanned Aerial Vehicle (UAV)-aided attestation framework for use in\\nIoT networks with a charging station powered by solar. A key challenge is\\noptimizing the trajectory of the UAV to ensure it attests as many devices as\\npossible. A trade-off here is that devices being checked by the UAV are\\noffline, which affects the amount of data delivered to a gateway. Another\\nchallenge is that the charging station experiences time-varying energy\\narrivals, which in turn affect the flight duration and charging schedule of the\\nUAV. To address these challenges, we employ a Deep Reinforcement Learning (DRL)\\nsolution to optimize the UAV's charging schedule and the selection of devices\\nto be attested during each flight. The simulation results show that our\\nsolution reduces the average age of trust by 88% and throughput loss due to\\nattestation by 30%.\"),\n",
       " Document(metadata={'title': 'EdgeSRIE: A hybrid deep learning framework for real-time speckle reduction and image enhancement on portable ultrasound systems', 'authors': 'Hyunwoo Cho, Jongsoo Lee, Jinbum Kang, Yangmo Yoo', 'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.03937v1'}, page_content='Speckle patterns in ultrasound images often obscure anatomical details,\\nleading to diagnostic uncertainty. Recently, various deep learning (DL)-based\\ntechniques have been introduced to effectively suppress speckle; however, their\\nhigh computational costs pose challenges for low-resource devices, such as\\nportable ultrasound systems. To address this issue, EdgeSRIE, which is a\\nlightweight hybrid DL framework for real-time speckle reduction and image\\nenhancement in portable ultrasound imaging, is introduced. The proposed\\nframework consists of two main branches: an unsupervised despeckling branch,\\nwhich is trained by minimizing a loss function between speckled images, and a\\ndeblurring branch, which restores blurred images to sharp images. For hardware\\nimplementation, the trained network is quantized to 8-bit integer precision and\\ndeployed on a low-resource system-on-chip (SoC) with limited power consumption.\\nIn the performance evaluation with phantom and in vivo analyses, EdgeSRIE\\nachieved the highest contrast-to-noise ratio (CNR) and average gradient\\nmagnitude (AGM) compared with the other baselines (different 2-rule-based\\nmethods and other 4-DL-based methods). Furthermore, EdgeSRIE enabled real-time\\ninference at over 60 frames per second while satisfying computational\\nrequirements (< 20K parameters) on actual portable ultrasound hardware. These\\nresults demonstrated the feasibility of EdgeSRIE for real-time, high-quality\\nultrasound imaging in resource-limited environments.'),\n",
       " Document(metadata={'title': 'An ASP-Based Framework for MUSes', 'authors': 'Mohimenul Kabir, Kuldeep S Meel', 'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.03929v1'}, page_content='Given an unsatisfiable formula, understanding the core reason for\\nunsatisfiability is crucial in several applications. One effective way to\\ncapture this is through the minimal unsatisfiable subset (MUS), the\\nsubset-minimal set of clauses that remains unsatisfiable. Current research\\nbroadly focuses on two directions: (i) enumerating as many MUSes as possible\\nwithin a given time limit, and (ii) counting the total number of MUSes for a\\ngiven unsatisfiable formula.\\n  In this paper, we introduce an answer set programming-based framework, named\\nMUS-ASP, designed for online enumeration of MUSes. ASP is a powerful tool for\\nits strengths in knowledge representation and is particularly suitable for\\nspecifying complex combinatorial problems. By translating MUS enumeration into\\nanswer set solving, MUS-ASP leverages the computational efficiency of\\nstate-of-the-art ASP systems. Our extensive experimental evaluation\\ndemonstrates the effectiveness of MUS-ASP and highlights the acceleration in\\nboth MUS enumeration and counting tasks, particularly when integrated within\\nhybrid solvers, including the framework proposed in this paper.'),\n",
       " Document(metadata={'title': 'CortexDebate: Debating Sparsely and Equally for Multi-Agent Debate', 'authors': 'Yiliu Sun, Zicheng Zhao, Sheng Wan, Chen Gong', 'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.03928v1'}, page_content='Nowadays, single Large Language Model (LLM) struggles with critical issues\\nsuch as hallucination and inadequate reasoning abilities. To mitigate these\\nissues, Multi-Agent Debate (MAD) has emerged as an effective strategy, where\\nLLM agents engage in in-depth debates with others on tasks. However, existing\\nMAD methods face two major issues: (a) too lengthy input contexts, which causes\\nLLM agents to get lost in plenty of input information and experiences\\nperformance drop; and (b) the overconfidence dilemma, where self-assured LLM\\nagents dominate the debate, leading to low debating effectiveness. To address\\nthese limitations, we propose a novel MAD method called \"CortexDebate\".\\nInspired by the human brain\\'s tendency to establish a sparse and dynamically\\noptimized network among cortical areas governed by white matter, CortexDebate\\nconstructs a sparse debating graph among LLM agents, where each LLM agent only\\ndebates with the ones that are helpful to it. To optimize the graph, we propose\\na module named McKinsey-based Debate Matter (MDM), which acts as an artificial\\nanalog to white matter. By integrating the McKinsey Trust Formula, a\\nwell-established measure of trustworthiness from sociology, MDM enables\\ncredible evaluations that guide graph optimization. The effectiveness of our\\nCortexDebate has been well demonstrated by extensive experimental results\\nacross eight datasets from four task types.'),\n",
       " Document(metadata={'title': 'Learning Disentangled Stain and Structural Representations for Semi-Supervised Histopathology Segmentation', 'authors': 'Ha-Hieu Pham, Nguyen Lan Vi Vu, Thanh-Huy Nguyen, Ulas Bagci, Min Xu, Trung-Nghia Le, Huy-Hieu Pham', 'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.03923v1'}, page_content='Accurate gland segmentation in histopathology images is essential for cancer\\ndiagnosis and prognosis. However, significant variability in Hematoxylin and\\nEosin (H&E) staining and tissue morphology, combined with limited annotated\\ndata, poses major challenges for automated segmentation. To address this, we\\npropose Color-Structure Dual-Student (CSDS), a novel semi-supervised\\nsegmentation framework designed to learn disentangled representations of stain\\nappearance and tissue structure. CSDS comprises two specialized student\\nnetworks: one trained on stain-augmented inputs to model chromatic variation,\\nand the other on structure-augmented inputs to capture morphological cues. A\\nshared teacher network, updated via Exponential Moving Average (EMA),\\nsupervises both students through pseudo-labels. To further improve label\\nreliability, we introduce stain-aware and structure-aware uncertainty\\nestimation modules that adaptively modulate the contribution of each student\\nduring training. Experiments on the GlaS and CRAG datasets show that CSDS\\nachieves state-of-the-art performance in low-label settings, with Dice score\\nimprovements of up to 1.2% on GlaS and 0.7% on CRAG at 5% labeled data, and\\n0.7% and 1.4% at 10%. Our code and pre-trained models are available at\\nhttps://github.com/hieuphamha19/CSDS.'),\n",
       " Document(metadata={'title': 'Animation Needs Attention: A Holistic Approach to Slides Animation Comprehension with Visual-Language Models', 'authors': 'Yifan Jiang, Yibo Xue, Yukun Kang, Pin Zheng, Jian Peng, Feiran Wu, Changliang Xu', 'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.03916v1'}, page_content='Slide animations, such as fade-ins, fly-ins, and wipes, are critical for\\naudience engagement, efficient information delivery, and vivid visual\\nexpression. However, most AI-driven slide-generation tools still lack native\\nanimation support, and existing vision-language models (VLMs) struggle with\\nanimation tasks due to the absence of public datasets and limited\\ntemporal-reasoning capabilities. To address this gap, we release the first\\npublic dataset for slide-animation modeling: 12,000 triplets of\\nnatural-language descriptions, animation JSON files, and rendered videos,\\ncollectively covering every built-in PowerPoint effect. Using this resource, we\\nfine-tune Qwen-2.5-VL-7B with Low-Rank Adaptation (LoRA) and achieve consistent\\nimprovements over GPT-4.1 and Gemini-2.5-Pro in BLEU-4, ROUGE-L, SPICE, and our\\nCoverage-Order-Detail Assessment (CODA) metric, which evaluates action\\ncoverage, temporal order, and detail fidelity. On a manually curated test set\\nof slides, the LoRA model increases BLEU-4 by around 60%, ROUGE-L by 30%, and\\nshows significant improvements in CODA-detail. This demonstrates that low-rank\\nadaptation enables reliable temporal reasoning and generalization beyond\\nsynthetic data. Overall, our dataset, LoRA-enhanced model, and CODA metric\\nprovide a rigorous benchmark and foundation for future research on VLM-based\\ndynamic slide generation.'),\n",
       " Document(metadata={'title': 'Agent Exchange: Shaping the Future of AI Agent Economics', 'authors': 'Yingxuan Yang, Ying Wen, Jun Wang, Weinan Zhang', 'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.03904v1'}, page_content='The rise of Large Language Models (LLMs) has transformed AI agents from\\npassive computational tools into autonomous economic actors. This shift marks\\nthe emergence of the agent-centric economy, in which agents take on active\\neconomic roles-exchanging value, making strategic decisions, and coordinating\\nactions with minimal human oversight. To realize this vision, we propose Agent\\nExchange (AEX), a specialized auction platform designed to support the dynamics\\nof the AI agent marketplace. AEX offers an optimized infrastructure for agent\\ncoordination and economic participation. Inspired by Real-Time Bidding (RTB)\\nsystems in online advertising, AEX serves as the central auction engine,\\nfacilitating interactions among four ecosystem components: the User-Side\\nPlatform (USP), which translates human goals into agent-executable tasks; the\\nAgent-Side Platform (ASP), responsible for capability representation,\\nperformance tracking, and optimization; Agent Hubs, which coordinate agent\\nteams and participate in AEX-hosted auctions; and the Data Management Platform\\n(DMP), ensuring secure knowledge sharing and fair value attribution. We outline\\nthe design principles and system architecture of AEX, laying the groundwork for\\nagent-based economic infrastructure in future AI ecosystems.'),\n",
       " Document(metadata={'title': \"Transformer Model for Alzheimer's Disease Progression Prediction Using Longitudinal Visit Sequences\", 'authors': 'Mahdi Moghaddami, Clayton Schubring, Mohammad-Reza Siadat', 'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.03899v1'}, page_content=\"Alzheimer's disease (AD) is a neurodegenerative disorder with no known cure\\nthat affects tens of millions of people worldwide. Early detection of AD is\\ncritical for timely intervention to halt or slow the progression of the\\ndisease. In this study, we propose a Transformer model for predicting the stage\\nof AD progression at a subject's next clinical visit using features from a\\nsequence of visits extracted from the subject's visit history. We also\\nrigorously compare our model to recurrent neural networks (RNNs) such as long\\nshort-term memory (LSTM), gated recurrent unit (GRU), and minimalRNN and assess\\ntheir performances based on factors such as the length of prior visits and data\\nimbalance. We test the importance of different feature categories and visit\\nhistory, as well as compare the model to a newer Transformer-based model\\noptimized for time series. Our model demonstrates strong predictive performance\\ndespite missing visits and missing features in available visits, particularly\\nin identifying converter subjects -- individuals transitioning to more severe\\ndisease stages -- an area that has posed significant challenges in longitudinal\\nprediction. The results highlight the model's potential in enhancing early\\ndiagnosis and patient outcomes.\"),\n",
       " Document(metadata={'title': 'TayFCS: Towards Light Feature Combination Selection for Deep Recommender Systems', 'authors': 'Xianquan Wang, Zhaocheng Du, Jieming Zhu, Chuhan Wu, Qinglin Jia, Zhenhua Dong', 'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.03895v1'}, page_content=\"Feature interaction modeling is crucial for deep recommendation models. A\\ncommon and effective approach is to construct explicit feature combinations to\\nenhance model performance. However, in practice, only a small fraction of these\\ncombinations are truly informative. Thus it is essential to select useful\\nfeature combinations to reduce noise and manage memory consumption. While\\nfeature selection methods have been extensively studied, they are typically\\nlimited to selecting individual features. Extending these methods for\\nhigh-order feature combination selection presents a significant challenge due\\nto the exponential growth in time complexity when evaluating feature\\ncombinations one by one. In this paper, we propose $\\\\textbf{TayFCS}$, a\\nlightweight feature combination selection method that significantly improves\\nmodel performance. Specifically, we propose the Taylor Expansion Scorer\\n(TayScorer) module for field-wise Taylor expansion on the base model. Instead\\nof evaluating all potential feature combinations' importance by repeatedly\\nrunning experiments with feature adding and removal, this scorer only needs to\\napproximate the importance based on their sub-components' gradients. This can\\nbe simply computed with one backward pass based on a trained recommendation\\nmodel. To further reduce information redundancy among feature combinations and\\ntheir sub-components, we introduce Logistic Regression Elimination (LRE), which\\nestimates the corresponding information gain based on the model prediction\\nperformance. Experimental results on three benchmark datasets validate both the\\neffectiveness and efficiency of our approach. Furthermore, online A/B test\\nresults demonstrate its practical applicability and commercial value.\"),\n",
       " Document(metadata={'title': 'Hierarchical Semantic-Visual Fusion of Visible and Near-infrared Images for Long-range Haze Removal', 'authors': 'Yi Li, Xiaoxiong Wang, Jiawei Wang, Yi Chang, Kai Cao, Luxin Yan', 'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.03893v1'}, page_content='While image dehazing has advanced substantially in the past decade, most\\nefforts have focused on short-range scenarios, leaving long-range haze removal\\nunder-explored. As distance increases, intensified scattering leads to severe\\nhaze and signal loss, making it impractical to recover distant details solely\\nfrom visible images. Near-infrared, with superior fog penetration, offers\\ncritical complementary cues through multimodal fusion. However, existing\\nmethods focus on content integration while often neglecting haze embedded in\\nvisible images, leading to results with residual haze. In this work, we argue\\nthat the infrared and visible modalities not only provide complementary\\nlow-level visual features, but also share high-level semantic consistency.\\nMotivated by this, we propose a Hierarchical Semantic-Visual Fusion (HSVF)\\nframework, comprising a semantic stream to reconstruct haze-free scenes and a\\nvisual stream to incorporate structural details from the near-infrared\\nmodality. The semantic stream first acquires haze-robust semantic prediction by\\naligning modality-invariant intrinsic representations. Then the shared\\nsemantics act as strong priors to restore clear and high-contrast distant\\nscenes under severe haze degradation. In parallel, the visual stream focuses on\\nrecovering lost structural details from near-infrared by fusing complementary\\ncues from both visible and near-infrared images. Through the cooperation of\\ndual streams, HSVF produces results that exhibit both high-contrast scenes and\\nrich texture details. Moreover, we introduce a novel pixel-aligned\\nvisible-infrared haze dataset with semantic labels to facilitate benchmarking.\\nExtensive experiments demonstrate the superiority of our method over\\nstate-of-the-art approaches in real-world long-range haze removal.'),\n",
       " Document(metadata={'title': 'LLMs model how humans induce logically structured rules', 'authors': 'Alyssa Loo, Ellie Pavlick, Roman Feiman', 'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.03876v1'}, page_content='A central goal of cognitive science is to provide a computationally explicit\\naccount of both the structure of the mind and its development: what are the\\nprimitive representational building blocks of cognition, what are the rules via\\nwhich those primitives combine, and where do these primitives and rules come\\nfrom in the first place? A long-standing debate concerns the adequacy of\\nartificial neural networks as computational models that can answer these\\nquestions, in particular in domains related to abstract cognitive function,\\nsuch as language and logic. This paper argues that recent advances in neural\\nnetworks -- specifically, the advent of large language models (LLMs) --\\nrepresent an important shift in this debate. We test a variety of LLMs on an\\nexisting experimental paradigm used for studying the induction of rules\\nformulated over logical concepts. Across four experiments, we find converging\\nempirical evidence that LLMs provide at least as good a fit to human behavior\\nas models that implement a Bayesian probablistic language of thought (pLoT),\\nwhich have been the best computational models of human behavior on the same\\ntask. Moreover, we show that the LLMs make qualitatively different predictions\\nabout the nature of the rules that are inferred and deployed in order to\\ncomplete the task, indicating that the LLM is unlikely to be a mere\\nimplementation of the pLoT solution. Based on these results, we argue that LLMs\\nmay instantiate a novel theoretical account of the primitive representations\\nand computations necessary to explain human logical concepts, with which future\\nwork in cognitive science should engage.'),\n",
       " Document(metadata={'title': 'Demystifying ChatGPT: How It Masters Genre Recognition', 'authors': 'Subham Raj, Sriparna Saha, Brijraj Singh, Niranjan Pedanekar', 'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.03875v1'}, page_content=\"The introduction of ChatGPT has garnered significant attention within the NLP\\ncommunity and beyond. Previous studies have demonstrated ChatGPT's substantial\\nadvancements across various downstream NLP tasks, highlighting its adaptability\\nand potential to revolutionize language-related applications. However, its\\ncapabilities and limitations in genre prediction remain unclear. This work\\nanalyzes three Large Language Models (LLMs) using the MovieLens-100K dataset to\\nassess their genre prediction capabilities. Our findings show that ChatGPT,\\nwithout fine-tuning, outperformed other LLMs, and fine-tuned ChatGPT performed\\nbest overall. We set up zero-shot and few-shot prompts using audio\\ntranscripts/subtitles from movie trailers in the MovieLens-100K dataset,\\ncovering 1682 movies of 18 genres, where each movie can have multiple genres.\\nAdditionally, we extended our study by extracting IMDb movie posters to utilize\\na Vision Language Model (VLM) with prompts for poster information. This\\nfine-grained information was used to enhance existing LLM prompts. In\\nconclusion, our study reveals ChatGPT's remarkable genre prediction\\ncapabilities, surpassing other language models. The integration of VLM further\\nenhances our findings, showcasing ChatGPT's potential for content-related\\napplications by incorporating visual information from movie posters.\"),\n",
       " Document(metadata={'title': 'Enhancing Adaptive Behavioral Interventions with LLM Inference from Participant-Described States', 'authors': 'Karine Karine, Benjamin M. Marlin', 'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.03871v1'}, page_content='The use of reinforcement learning (RL) methods to support health behavior\\nchange via personalized and just-in-time adaptive interventions is of\\nsignificant interest to health and behavioral science researchers focused on\\nproblems such as smoking cessation support and physical activity promotion.\\nHowever, RL methods are often applied to these domains using a small collection\\nof context variables to mitigate the significant data scarcity issues that\\narise from practical limitations on the design of adaptive intervention trials.\\nIn this paper, we explore an approach to significantly expanding the state\\nspace of an adaptive intervention without impacting data efficiency. The\\nproposed approach enables intervention participants to provide natural language\\ndescriptions of aspects of their current state. It then leverages inference\\nwith pre-trained large language models (LLMs) to better align the policy of a\\nbase RL method with these state descriptions. To evaluate our method, we\\ndevelop a novel physical activity intervention simulation environment that\\ngenerates text-based state descriptions conditioned on latent state variables\\nusing an auxiliary LLM. We show that this approach has the potential to\\nsignificantly improve the performance of online policy learning methods.'),\n",
       " Document(metadata={'title': 'Uncovering Systemic and Environment Errors in Autonomous Systems Using Differential Testing', 'authors': 'Rahil P Mehta, Yashwanthi Anand, Manish Motwani, Sandhya Saisubramanian', 'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.03870v1'}, page_content=\"When an autonomous agent behaves undesirably, including failure to complete a\\ntask, it can be difficult to determine whether the behavior is due to a\\nsystemic agent error, such as flaws in the model or policy, or an environment\\nerror, where a task is inherently infeasible under a given environment\\nconfiguration, even for an ideal agent. As agents and their environments grow\\nmore complex, identifying the error source becomes increasingly difficult but\\ncritical for reliable deployment. We introduce AIProbe, a novel black-box\\ntesting technique that applies differential testing to attribute undesirable\\nagent behaviors either to agent deficiencies, such as modeling or training\\nflaws, or due to environmental infeasibility. AIProbe first generates diverse\\nenvironmental configurations and tasks for testing the agent, by modifying\\nconfigurable parameters using Latin Hypercube sampling. It then solves each\\ngenerated task using a search-based planner, independent of the agent. By\\ncomparing the agent's performance to the planner's solution, AIProbe identifies\\nwhether failures are due to errors in the agent's model or policy, or due to\\nunsolvable task conditions. Our evaluation across multiple domains shows that\\nAIProbe significantly outperforms state-of-the-art techniques in detecting both\\ntotal and unique errors, thereby contributing to a reliable deployment of\\nautonomous agents.\"),\n",
       " Document(metadata={'title': 'From Query to Explanation: Uni-RAG for Multi-Modal Retrieval-Augmented Learning in STEM', 'authors': 'Xinyi Wu, Yanhao Jia, Luwei Xiao, Shuai Zhao, Fengkuang Chiang, Erik Cambria', 'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.03868v1'}, page_content=\"In AI-facilitated teaching, leveraging various query styles to interpret\\nabstract educational content is crucial for delivering effective and accessible\\nlearning experiences. However, existing retrieval systems predominantly focus\\non natural text-image matching and lack the capacity to address the diversity\\nand ambiguity inherent in real-world educational scenarios. To address this\\nlimitation, we develop a lightweight and efficient multi-modal retrieval\\nmodule, named Uni-Retrieval, which extracts query-style prototypes and\\ndynamically matches them with tokens from a continually updated Prompt Bank.\\nThis Prompt Bank encodes and stores domain-specific knowledge by leveraging a\\nMixture-of-Expert Low-Rank Adaptation (MoE-LoRA) module and can be adapted to\\nenhance Uni-Retrieval's capability to accommodate unseen query types at test\\ntime. To enable natural language educational content generation, we integrate\\nthe original Uni-Retrieval with a compact instruction-tuned language model,\\nforming a complete retrieval-augmented generation pipeline named Uni-RAG. Given\\na style-conditioned query, Uni-RAG first retrieves relevant educational\\nmaterials and then generates human-readable explanations, feedback, or\\ninstructional content aligned with the learning objective. Experimental results\\non SER and other multi-modal benchmarks show that Uni-RAG outperforms baseline\\nretrieval and RAG systems in both retrieval accuracy and generation quality,\\nwhile maintaining low computational cost. Our framework provides a scalable,\\npedagogically grounded solution for intelligent educational systems, bridging\\nretrieval and generation to support personalized, explainable, and efficient\\nlearning assistance across diverse STEM scenarios.\"),\n",
       " Document(metadata={'title': 'OrthoRank: Token Selection via Sink Token Orthogonality for Efficient LLM inference', 'authors': 'Seungjun Shin, Jaehoon Oh, Dokwan Oh', 'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.03865v1'}, page_content='Attention mechanisms are central to the success of large language models\\n(LLMs), enabling them to capture intricate token dependencies and implicitly\\nassign importance to each token. Recent studies have revealed the sink token,\\nwhich receives disproportionately high attention despite their limited semantic\\nrole. In this paper, we first expand the relationship between the sink token\\nand other tokens, moving beyond attention to explore their similarity in hidden\\nstates, considering the layer depth. We observe that as the layers get deeper,\\nthe cosine similarity between the normalized hidden states of the sink token\\nand those of other tokens increases, and that the normalized hidden states of\\nthe sink token exhibit negligible changes. These imply that other tokens\\nconsistently are directed toward the sink token throughout the layers. Next, we\\npropose a dynamic token selection method, called OrthoRank, using these\\nfindings to select important tokens. Specifically, in a certain layer, we\\ndefine token importance by the speed at which the token moves toward the sink\\ntoken. This is converted into orthogonality with the sink token, meaning that\\ntokens that are more orthogonal to the sink token are assigned greater\\nimportance. Finally, through extensive experiments, we demonstrated that our\\nmethod results in lower perplexity and higher zero-shot accuracy compared to\\nlayer pruning methods at the same sparsity ratio with comparable throughput,\\nwhile also achieving superior performance on LongBench.'),\n",
       " Document(metadata={'title': 'Enhanced accuracy through ensembling of randomly initialized auto-regressive models for time-dependent PDEs', 'authors': 'Ishan Khurjekar, Indrashish Saha, Lori Graham-Brady, Somdatta Goswami', 'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.03863v1'}, page_content=\"Systems governed by partial differential equations (PDEs) require\\ncomputationally intensive numerical solvers to predict spatiotemporal field\\nevolution. While machine learning (ML) surrogates offer faster solutions,\\nautoregressive inference with ML models suffer from error accumulation over\\nsuccessive predictions, limiting their long-term accuracy. We propose a deep\\nensemble framework to address this challenge, where multiple ML surrogate\\nmodels with random weight initializations are trained in parallel and\\naggregated during inference. This approach leverages the diversity of model\\npredictions to mitigate error propagation while retaining the autoregressive\\nstrategies ability to capture the system's time dependent relations. We\\nvalidate the framework on three PDE-driven dynamical systems - stress evolution\\nin heterogeneous microstructures, Gray-Scott reaction-diffusion, and\\nplanetary-scale shallow water system - demonstrating consistent reduction in\\nerror accumulation over time compared to individual models. Critically, the\\nmethod requires only a few time steps as input, enabling full trajectory\\npredictions with inference times significantly faster than numerical solvers.\\nOur results highlight the robustness of ensemble methods in diverse physical\\nsystems and their potential as efficient and accurate alternatives to\\ntraditional solvers. The codes for this work are available on GitHub\\n(https://github.com/Graham-Brady-Research-Group/AutoregressiveEnsemble_SpatioTemporal_Evolution).\"),\n",
       " Document(metadata={'title': 'KEA Explain: Explanations of Hallucinations using Graph Kernel Analysis', 'authors': 'Reilly Haskins, Ben Adams', 'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.03847v1'}, page_content='Large Language Models (LLMs) frequently generate hallucinations: statements\\nthat are syntactically plausible but lack factual grounding. This research\\npresents KEA (Kernel-Enriched AI) Explain: a neurosymbolic framework that\\ndetects and explains such hallucinations by comparing knowledge graphs\\nconstructed from LLM outputs with ground truth data from Wikidata or contextual\\ndocuments. Using graph kernels and semantic clustering, the method provides\\nexplanations for detected hallucinations, ensuring both robustness and\\ninterpretability. Our framework achieves competitive accuracy in detecting\\nhallucinations across both open- and closed-domain tasks, and is able to\\ngenerate contrastive explanations, enhancing transparency. This research\\nadvances the reliability of LLMs in high-stakes domains and provides a\\nfoundation for future work on precision improvements and multi-source knowledge\\nintegration.'),\n",
       " Document(metadata={'title': 'Participatory Evolution of Artificial Life Systems via Semantic Feedback', 'authors': 'Shuowen Li, Kexin Wang, Minglu Fang, Danqi Huang, Ali Asadipour, Haipeng Mi, Yitong Sun', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03839v1'}, page_content=\"We present a semantic feedback framework that enables natural language to\\nguide the evolution of artificial life systems. Integrating a\\nprompt-to-parameter encoder, a CMA-ES optimizer, and CLIP-based evaluation, the\\nsystem allows user intent to modulate both visual outcomes and underlying\\nbehavioral rules. Implemented in an interactive ecosystem simulation, the\\nframework supports prompt refinement, multi-agent interaction, and emergent\\nrule synthesis. User studies show improved semantic alignment over manual\\ntuning and demonstrate the system's potential as a platform for participatory\\ngenerative design and open-ended evolution.\"),\n",
       " Document(metadata={'title': 'Economic Evaluation of LLMs', 'authors': 'Michael J. Zellinger, Matt Thomson', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03834v1'}, page_content='Practitioners often navigate LLM performance trade-offs by plotting Pareto\\nfrontiers of optimal accuracy-cost trade-offs. However, this approach offers no\\nway to compare between LLMs with distinct strengths and weaknesses: for\\nexample, a cheap, error-prone model vs a pricey but accurate one. To address\\nthis gap, we propose economic evaluation of LLMs. Our framework quantifies the\\nperformance trade-off of an LLM as a single number based on the economic\\nconstraints of a concrete use case, all expressed in dollars: the cost of\\nmaking a mistake, the cost of incremental latency, and the cost of abstaining\\nfrom a query. We apply our economic evaluation framework to compare the\\nperformance of reasoning and non-reasoning models on difficult questions from\\nthe MATH benchmark, discovering that reasoning models offer better\\naccuracy-cost tradeoffs as soon as the economic cost of a mistake exceeds\\n\\\\$0.01. In addition, we find that single large LLMs often outperform cascades\\nwhen the cost of making a mistake is as low as \\\\$0.1. Overall, our findings\\nsuggest that when automating meaningful human tasks with AI models,\\npractitioners should typically use the most powerful available model, rather\\nthan attempt to minimize AI deployment costs, since deployment costs are likely\\ndwarfed by the economic impact of AI errors.'),\n",
       " Document(metadata={'title': 'RELRaE: LLM-Based Relationship Extraction, Labelling, Refinement, and Evaluation', 'authors': 'George Hannah, Jacopo de Berardinis, Terry R. Payne, Valentina Tamma, Andrew Mitchell, Ellen Piercy, Ewan Johnson, Andrew Ng, Harry Rostron, Boris Konev', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03829v1'}, page_content='A large volume of XML data is produced in experiments carried out by robots\\nin laboratories. In order to support the interoperability of data between labs,\\nthere is a motivation to translate the XML data into a knowledge graph. A key\\nstage of this process is the enrichment of the XML schema to lay the foundation\\nof an ontology schema. To achieve this, we present the RELRaE framework, a\\nframework that employs large language models in different stages to extract and\\naccurately label the relationships implicitly present in the XML schema. We\\ninvestigate the capability of LLMs to accurately generate these labels and then\\nevaluate them. Our work demonstrates that LLMs can be effectively used to\\nsupport the generation of relationship labels in the context of lab automation,\\nand that they can play a valuable role within semi-automatic ontology\\ngeneration frameworks more generally.'),\n",
       " Document(metadata={'title': 'Leveraging Large Language Models for Tacit Knowledge Discovery in Organizational Contexts', 'authors': 'Gianlucca Zuin, Saulo Mastelini, Túlio Loures, Adriano Veloso', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03811v1'}, page_content=\"Documenting tacit knowledge in organizations can be a challenging task due to\\nincomplete initial information, difficulty in identifying knowledgeable\\nindividuals, the interplay of formal hierarchies and informal networks, and the\\nneed to ask the right questions. To address this, we propose an agent-based\\nframework leveraging large language models (LLMs) to iteratively reconstruct\\ndataset descriptions through interactions with employees. Modeling knowledge\\ndissemination as a Susceptible-Infectious (SI) process with waning infectivity,\\nwe conduct 864 simulations across various synthetic company structures and\\ndifferent dissemination parameters. Our results show that the agent achieves\\n94.9% full-knowledge recall, with self-critical feedback scores strongly\\ncorrelating with external literature critic scores. We analyze how each\\nsimulation parameter affects the knowledge retrieval process for the agent. In\\nparticular, we find that our approach is able to recover information without\\nneeding to access directly the only domain specialist. These findings highlight\\nthe agent's ability to navigate organizational complexity and capture\\nfragmented knowledge that would otherwise remain inaccessible.\"),\n",
       " Document(metadata={'title': 'Generating Novelty in Open-World Multi-Agent Strategic Board Games', 'authors': 'Mayank Kejriwal, Shilpa Thomas', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03802v1'}, page_content='We describe GNOME (Generating Novelty in Open-world Multi-agent\\nEnvironments), an experimental platform that is designed to test the\\neffectiveness of multi-agent AI systems when faced with \\\\emph{novelty}. GNOME\\nseparates the development of AI gameplaying agents with the simulator, allowing\\n\\\\emph{unanticipated} novelty (in essence, novelty that is not subject to\\nmodel-selection bias). Using a Web GUI, GNOME was recently demonstrated at\\nNeurIPS 2020 using the game of Monopoly to foster an open discussion on AI\\nrobustness and the nature of novelty in real-world environments. In this\\narticle, we further detail the key elements of the demonstration, and also\\nprovide an overview of the experimental design that is being currently used in\\nthe DARPA Science of Artificial Intelligence and Learning for Open-World\\nNovelty (SAIL-ON) program to evaluate external teams developing\\nnovelty-adaptive gameplaying agents.'),\n",
       " Document(metadata={'title': 'Learning Dark Souls Combat Through Pixel Input With Neuroevolution', 'authors': \"Jim O'Connor, Gary B. Parker, Mustafa Bugti\", 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03793v1'}, page_content=\"This paper investigates the application of Neuroevolution of Augmenting\\nTopologies (NEAT) to automate gameplay in Dark Souls, a notoriously challenging\\naction role-playing game characterized by complex combat mechanics, dynamic\\nenvironments, and high-dimensional visual inputs. Unlike traditional\\nreinforcement learning or game playing approaches, our method evolves neural\\nnetworks directly from raw pixel data, circumventing the need for explicit\\ngame-state information. To facilitate this approach, we introduce the Dark\\nSouls API (DSAPI), a novel Python framework leveraging real-time computer\\nvision techniques for extracting critical game metrics, including player and\\nenemy health states. Using NEAT, agents evolve effective combat strategies for\\ndefeating the Asylum Demon, the game's initial boss, without predefined\\nbehaviors or domain-specific heuristics. Experimental results demonstrate that\\nevolved agents achieve up to a 35% success rate, indicating the viability of\\nneuroevolution in addressing complex, visually intricate gameplay scenarios.\\nThis work represents an interesting application of vision-based neuroevolution,\\nhighlighting its potential use in a wide range of challenging game environments\\nlacking direct API support or well-defined state representations.\"),\n",
       " Document(metadata={'title': 'FastDINOv2: Frequency Based Curriculum Learning Improves Robustness and Training Speed', 'authors': 'Jiaqi Zhang, Juntuo Wang, Zhixin Sun, John Zou, Randall Balestriero', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03779v1'}, page_content='Large-scale vision foundation models such as DINOv2 boast impressive\\nperformances by leveraging massive architectures and training datasets. But\\nnumerous scenarios require practitioners to reproduce those pre-training\\nsolutions, such as on private data, new modalities, or simply for scientific\\nquestioning--which is currently extremely demanding computation-wise. We thus\\npropose a novel pre-training strategy for DINOv2 that simultaneously\\naccelerates convergence--and strengthens robustness to common corruptions as a\\nby-product. Our approach involves a frequency filtering\\ncurriculum--low-frequency being seen first--and the Gaussian noise patching\\naugmentation. Applied to a ViT-B/16 backbone trained on ImageNet-1K, while\\npre-training time and FLOPs are reduced by 1.6x and 2.25x, our method still\\nachieves matching robustness in corruption benchmarks (ImageNet-C) and\\nmaintains competitive linear probing performance compared with baseline. This\\ndual benefit of efficiency and robustness makes large-scale self-supervised\\nfoundation modeling more attainable, while opening the door to novel\\nexploration around data curriculum and augmentation as means to improve\\nself-supervised learning models robustness. The code is available at\\nhttps://github.com/KevinZ0217/fast_dinov2'),\n",
       " Document(metadata={'title': 'Optimizing UAV Trajectories via a Simplified Close Enough TSP Approach', 'authors': 'Hiba Bederina', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03775v1'}, page_content='This article explores an approach to addressing the Close Enough Traveling\\nSalesman Problem (CETSP). The objective is to streamline the mathematical\\nformulation by introducing reformulations that approximate the Euclidean\\ndistances and simplify the objective function. Additionally, the use of convex\\nsets in the constraint design offers computational benefits. The proposed\\nmethodology is empirically validated on real-world CETSP instances, with the\\naid of computational strategies such as a fragmented CPLEX-based approach.\\nResults demonstrate its effectiveness in managing computational resources\\nwithout compromising solution quality. Furthermore, the article analyzes the\\nbehavior of the proposed mathematical formulations, providing comprehensive\\ninsights into their performance.'),\n",
       " Document(metadata={'title': 'Alpay Algebra IV: Symbiotic Semantics and the Fixed-Point Convergence of Observer Embeddings', 'authors': 'Bugra Kilictas, Faruk Alpay', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03774v1'}, page_content='We present a theoretical framework in which a document and an AI model engage\\nin a transfinite fixed-point interaction that leads to stable semantic\\nalignment. Building on the foundations of Alpay Algebra, we introduce a\\nfunctorial system wherein an observer (the AI) and a textual environment (this\\npaper) co-evolve through iterative transformations guided by the phi-infinity\\noperator. This process guarantees the existence of a unique fixed point in the\\nAI\\'s embedding space -- a state where the AI\\'s internal representation of the\\ncontent becomes stable, self-consistent, and semantically faithful. We prove\\nthat such convergence is mathematically sound, semantically invariant, and\\npermanent, even under perturbation or further context expansion. This fixed\\npoint acts as an \"empathetic embedding,\" wherein the AI internalizes not only\\nthe meaning of the content but also the author\\'s intent. We interpret this as a\\nrigorous, category-theoretic route to alignment at the embedding level, with\\nimplications for semantic security, symbolic memory, and the construction of AI\\nsystems with persistent self-referential understanding. All references in this\\npaper function as nodes in the Alpay Algebra universe, and this work embeds\\nitself as a new fixed-point node within that transfinite semantic graph.'),\n",
       " Document(metadata={'title': 'StreamDiT: Real-Time Streaming Text-to-Video Generation', 'authors': 'Akio Kodaira, Tingbo Hou, Ji Hou, Masayoshi Tomizuka, Yue Zhao', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03745v1'}, page_content='Recently, great progress has been achieved in text-to-video (T2V) generation\\nby scaling transformer-based diffusion models to billions of parameters, which\\ncan generate high-quality videos. However, existing models typically produce\\nonly short clips offline, restricting their use cases in interactive and\\nreal-time applications. This paper addresses these challenges by proposing\\nStreamDiT, a streaming video generation model. StreamDiT training is based on\\nflow matching by adding a moving buffer. We design mixed training with\\ndifferent partitioning schemes of buffered frames to boost both content\\nconsistency and visual quality. StreamDiT modeling is based on adaLN DiT with\\nvarying time embedding and window attention. To practice the proposed method,\\nwe train a StreamDiT model with 4B parameters. In addition, we propose a\\nmultistep distillation method tailored for StreamDiT. Sampling distillation is\\nperformed in each segment of a chosen partitioning scheme. After distillation,\\nthe total number of function evaluations (NFEs) is reduced to the number of\\nchunks in a buffer. Finally, our distilled model reaches real-time performance\\nat 16 FPS on one GPU, which can generate video streams at 512p resolution. We\\nevaluate our method through both quantitative metrics and human evaluation. Our\\nmodel enables real-time applications, e.g. streaming generation, interactive\\ngeneration, and video-to-video. We provide video results and more examples in\\nour project website: <a href=\"https://cumulo-autumn.github.io/StreamDiT/\">this\\nhttps URL.</a>'),\n",
       " Document(metadata={'title': 'Less is More: Empowering GUI Agent with Context-Aware Simplification', 'authors': 'Gongwei Chen, Xurui Zhou, Rui Shao, Yibo Lyu, Kaiwen Zhou, Shuai Wang, Wentao Li, Yinchuan Li, Zhongang Qi, Liqiang Nie', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03730v1'}, page_content='The research focus of GUI agents is shifting from text-dependent to\\npure-vision-based approaches, which, though promising, prioritize comprehensive\\npre-training data collection while neglecting contextual modeling challenges.\\nWe probe the characteristics of element and history contextual modeling in GUI\\nagent and summarize: 1) the high-density and loose-relation of element context\\nhighlight the existence of many unrelated elements and their negative\\ninfluence; 2) the high redundancy of history context reveals the inefficient\\nhistory modeling in current GUI agents. In this work, we propose a\\ncontext-aware simplification framework for building an efficient and effective\\nGUI Agent, termed SimpAgent. To mitigate potential interference from numerous\\nunrelated elements, we introduce a masking-based element pruning method that\\ncircumvents the intractable relation modeling through an efficient masking\\nmechanism. To reduce the redundancy in historical information, we devise a\\nconsistency-guided history compression module, which enhances implicit\\nLLM-based compression through innovative explicit guidance, achieving an\\noptimal balance between performance and efficiency. With the above components,\\nSimpAgent reduces 27% FLOPs and achieves superior GUI navigation performances.\\nComprehensive navigation experiments across diverse web and mobile environments\\ndemonstrate the effectiveness and potential of our agent.'),\n",
       " Document(metadata={'title': 'Agent-Based Detection and Resolution of Incompleteness and Ambiguity in Interactions with Large Language Models', 'authors': 'Riya Naik, Ashwin Srinivasan, Swati Agarwal, Estrid He', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03726v1'}, page_content='Many of us now treat LLMs as modern-day oracles asking it almost any kind of\\nquestion. However, consulting an LLM does not have to be a single turn\\nactivity. But long multi-turn interactions can get tedious if it is simply to\\nclarify contextual information that can be arrived at through reasoning. In\\nthis paper, we examine the use of agent-based architecture to bolster LLM-based\\nQuestion-Answering systems with additional reasoning capabilities. We examine\\nthe automatic resolution of potential incompleteness or ambiguities in\\nquestions by transducers implemented using LLM-based agents. We focus on\\nseveral benchmark datasets that are known to contain questions with these\\ndeficiencies to varying degrees. We equip different LLMs (GPT-3.5-Turbo and\\nLlama-4-Scout) with agents that act as specialists in detecting and resolving\\ndeficiencies of incompleteness and ambiguity. The agents are implemented as\\nzero-shot ReAct agents. Rather than producing an answer in a single step, the\\nmodel now decides between 3 actions a) classify b) resolve c) answer. Action a)\\ndecides if the question is incomplete, ambiguous, or normal. Action b)\\ndetermines if any deficiencies identified can be resolved. Action c) answers\\nthe resolved form of the question. We compare the use of LLMs with and without\\nthe use of agents with these components. Our results show benefits of agents\\nwith transducer 1) A shortening of the length of interactions with human 2) An\\nimprovement in the answer quality and 3) Explainable resolution of deficiencies\\nin the question. On the negative side we find while it may result in additional\\nLLM invocations and in some cases, increased latency. But on tested datasets,\\nthe benefits outweigh the costs except when questions already have sufficient\\ncontext. Suggesting the agent-based approach could be a useful mechanism to\\nharness the power of LLMs to develop more robust QA systems.'),\n",
       " Document(metadata={'title': 'Roadmap for using large language models (LLMs) to accelerate cross-disciplinary research with an example from computational biology', 'authors': 'Ruian Ke, Ruy M. Ribeiro', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03722v1'}, page_content='Large language models (LLMs) are powerful artificial intelligence (AI) tools\\ntransforming how research is conducted. However, their use in research has been\\nmet with skepticism, due to concerns about hallucinations, biases and potential\\nharms to research. These emphasize the importance of clearly understanding the\\nstrengths and weaknesses of LLMs to ensure their effective and responsible use.\\nHere, we present a roadmap for integrating LLMs into cross-disciplinary\\nresearch, where effective communication, knowledge transfer and collaboration\\nacross diverse fields are essential but often challenging. We examine the\\ncapabilities and limitations of LLMs and provide a detailed computational\\nbiology case study (on modeling HIV rebound dynamics) demonstrating how\\niterative interactions with an LLM (ChatGPT) can facilitate interdisciplinary\\ncollaboration and research. We argue that LLMs are best used as augmentative\\ntools within a human-in-the-loop framework. Looking forward, we envisage that\\nthe responsible use of LLMs will enhance innovative cross-disciplinary research\\nand substantially accelerate scientific discoveries.'),\n",
       " Document(metadata={'title': 'Predicting Business Angel Early-Stage Decision Making Using AI', 'authors': 'Yan Katcharovski, Andrew L. Maxwell', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03721v1'}, page_content=\"External funding is crucial for early-stage ventures, particularly technology\\nstartups that require significant R&D investment. Business angels offer a\\ncritical source of funding, but their decision-making is often subjective and\\nresource-intensive for both investor and entrepreneur. Much research has\\ninvestigated this investment process to find the critical factors angels\\nconsider. One such tool, the Critical Factor Assessment (CFA), deployed more\\nthan 20,000 times by the Canadian Innovation Centre, has been evaluated\\npost-decision and found to be significantly more accurate than investors' own\\ndecisions. However, a single CFA analysis requires three trained individuals\\nand several days, limiting its adoption. This study builds on previous work\\nvalidating the CFA to investigate whether the constraints inhibiting its\\nadoption can be overcome using a trained AI model. In this research, we\\nprompted multiple large language models (LLMs) to assign the eight CFA factors\\nto a dataset of 600 transcribed, unstructured startup pitches seeking business\\nangel funding with known investment outcomes. We then trained and evaluated\\nmachine learning classification models using the LLM-generated CFA scores as\\ninput features. Our best-performing model demonstrated high predictive accuracy\\n(85.0% for predicting BA deal/no-deal outcomes) and exhibited significant\\ncorrelation (Spearman's r = 0.896, p-value < 0.001) with conventional\\nhuman-graded evaluations. The integration of AI-based feature extraction with a\\nstructured and validated decision-making framework yielded a scalable,\\nreliable, and less-biased model for evaluating startup pitches, removing the\\nconstraints that previously limited adoption.\"),\n",
       " Document(metadata={'title': 'Controlling Thinking Speed in Reasoning Models', 'authors': 'Zhengkai Lin, Zhihang Fu, Ze Chen, Chao Chen, Liang Xie, Wenxiao Wang, Deng Cai, Zheng Wang, Jieping Ye', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03704v1'}, page_content=\"Human cognition is theorized to operate in two modes: fast, intuitive System\\n1 thinking and slow, deliberate System 2 thinking. While current Large\\nReasoning Models (LRMs) excel at System 2 thinking, their inability to perform\\nfast thinking leads to high computational overhead and latency. In this work,\\nwe enable LRMs to approximate human intelligence through dynamic thinking speed\\nadjustment, optimizing accuracy-efficiency trade-offs. Our approach addresses\\ntwo key questions: (1) how to control thinking speed in LRMs, and (2) when to\\nadjust it for optimal performance. For the first question, we identify the\\nsteering vector that governs slow-fast thinking transitions in LRMs'\\nrepresentation space. Using this vector, we achieve the first representation\\nediting-based test-time scaling effect, outperforming existing prompt-based\\nscaling methods. For the second question, we apply real-time difficulty\\nestimation to signal reasoning segments of varying complexity. Combining these\\ntechniques, we propose the first reasoning strategy that enables fast\\nprocessing of easy steps and deeper analysis for complex reasoning. Without any\\ntraining or additional cost, our plug-and-play method yields an average +1.3%\\naccuracy with -8.6% token usage across leading LRMs and advanced reasoning\\nbenchmarks. All of our algorithms are implemented based on vLLM and are\\nexpected to support broader applications and inspire future research.\"),\n",
       " Document(metadata={'title': 'Sign Spotting Disambiguation using Large Language Models', 'authors': 'JianHe Low, Ozge Mercanoglu Sincan, Richard Bowden', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03703v1'}, page_content=\"Sign spotting, the task of identifying and localizing individual signs within\\ncontinuous sign language video, plays a pivotal role in scaling dataset\\nannotations and addressing the severe data scarcity issue in sign language\\ntranslation. While automatic sign spotting holds great promise for enabling\\nframe-level supervision at scale, it grapples with challenges such as\\nvocabulary inflexibility and ambiguity inherent in continuous sign streams.\\nHence, we introduce a novel, training-free framework that integrates Large\\nLanguage Models (LLMs) to significantly enhance sign spotting quality. Our\\napproach extracts global spatio-temporal and hand shape features, which are\\nthen matched against a large-scale sign dictionary using dynamic time warping\\nand cosine similarity. This dictionary-based matching inherently offers\\nsuperior vocabulary flexibility without requiring model retraining. To mitigate\\nnoise and ambiguity from the matching process, an LLM performs context-aware\\ngloss disambiguation via beam search, notably without fine-tuning. Extensive\\nexperiments on both synthetic and real-world sign language datasets demonstrate\\nour method's superior accuracy and sentence fluency compared to traditional\\napproaches, highlighting the potential of LLMs in advancing sign spotting.\"),\n",
       " Document(metadata={'title': 'Towards Unified Neurosymbolic Reasoning on Knowledge Graphs', 'authors': 'Qika Lin, Fangzhi Xu, Hao Lu, Kai He, Rui Mao, Jun Liu, Erik Cambria, Mengling Feng', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03697v1'}, page_content='Knowledge Graph (KG) reasoning has received significant attention in the\\nfields of artificial intelligence and knowledge engineering, owing to its\\nability to autonomously deduce new knowledge and consequently enhance the\\navailability and precision of downstream applications. However, current methods\\npredominantly concentrate on a single form of neural or symbolic reasoning,\\nfailing to effectively integrate the inherent strengths of both approaches.\\nFurthermore, the current prevalent methods primarily focus on addressing a\\nsingle reasoning scenario, presenting limitations in meeting the diverse\\ndemands of real-world reasoning tasks. Unifying the neural and symbolic\\nmethods, as well as diverse reasoning scenarios in one model is challenging as\\nthere is a natural representation gap between symbolic rules and neural\\nnetworks, and diverse scenarios exhibit distinct knowledge structures and\\nspecific reasoning objectives. To address these issues, we propose a unified\\nneurosymbolic reasoning framework, namely Tunsr, for KG reasoning. Tunsr first\\nintroduces a consistent structure of reasoning graph that starts from the query\\nentity and constantly expands subsequent nodes by iteratively searching\\nposterior neighbors. Based on it, a forward logic message-passing mechanism is\\nproposed to update both the propositional representations and attentions, as\\nwell as first-order logic (FOL) representations and attentions of each node. In\\nthis way, Tunsr conducts the transformation of merging multiple rules by\\nmerging possible relations at each step. Finally, the FARI algorithm is\\nproposed to induce FOL rules by constantly performing attention calculations\\nover the reasoning graph. Extensive experimental results on 19 datasets of four\\nreasoning scenarios (transductive, inductive, interpolation, and extrapolation)\\ndemonstrate the effectiveness of Tunsr.'),\n",
       " Document(metadata={'title': 'Towards Machine Theory of Mind with Large Language Model-Augmented Inverse Planning', 'authors': 'Rebekah A. Gelpí, Eric Xue, William A. Cunningham', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03682v1'}, page_content=\"We propose a hybrid approach to machine Theory of Mind (ToM) that uses large\\nlanguage models (LLMs) as a mechanism for generating hypotheses and likelihood\\nfunctions with a Bayesian inverse planning model that computes posterior\\nprobabilities for an agent's likely mental states given its actions. Bayesian\\ninverse planning models can accurately predict human reasoning on a variety of\\nToM tasks, but these models are constrained in their ability to scale these\\npredictions to scenarios with a large number of possible hypotheses and\\nactions. Conversely, LLM-based approaches have recently demonstrated promise in\\nsolving ToM benchmarks, but can exhibit brittleness and failures on reasoning\\ntasks even when they pass otherwise structurally identical versions. By\\ncombining these two methods, this approach leverages the strengths of each\\ncomponent, closely matching optimal results on a task inspired by prior inverse\\nplanning models and improving performance relative to models that utilize LLMs\\nalone or with chain-of-thought prompting, even with smaller LLMs that typically\\nperform poorly on ToM tasks. We also exhibit the model's potential to predict\\nmental states on open-ended tasks, offering a promising direction for future\\ndevelopment of ToM models and the creation of socially intelligent generative\\nagents.\"),\n",
       " Document(metadata={'title': 'STRUCTSENSE: A Task-Agnostic Agentic Framework for Structured Information Extraction with Human-In-The-Loop Evaluation and Benchmarking', 'authors': 'Tek Raj Chhetri, Yibei Chen, Puja Trivedi, Dorota Jarecka, Saif Haobsh, Patrick Ray, Lydia Ng, Satrajit S. Ghosh', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03674v1'}, page_content='The ability to extract structured information from unstructured sources-such\\nas free-text documents and scientific literature-is critical for accelerating\\nscientific discovery and knowledge synthesis. Large Language Models (LLMs) have\\ndemonstrated remarkable capabilities in various natural language processing\\ntasks, including structured information extraction. However, their\\neffectiveness often diminishes in specialized, domain-specific contexts that\\nrequire nuanced understanding and expert-level domain knowledge. In addition,\\nexisting LLM-based approaches frequently exhibit poor transferability across\\ntasks and domains, limiting their scalability and adaptability. To address\\nthese challenges, we introduce StructSense, a modular, task-agnostic,\\nopen-source framework for structured information extraction built on LLMs.\\nStructSense is guided by domain-specific symbolic knowledge encoded in\\nontologies, enabling it to navigate complex domain content more effectively. It\\nfurther incorporates agentic capabilities through self-evaluative judges that\\nform a feedback loop for iterative refinement, and includes human-in-the-loop\\nmechanisms to ensure quality and validation. We demonstrate that StructSense\\ncan overcome both the limitations of domain sensitivity and the lack of\\ncross-task generalizability, as shown through its application to diverse\\nneuroscience information extraction tasks.'),\n",
       " Document(metadata={'title': 'TACOS: Open Tagging and Comparative Scoring for Instruction Fine-Tuning Data Selection', 'authors': 'Xixiang He, Hao Yu, Qiyao Sun, Ao Cheng, Tailai Zhang, Cong Liu, Shuxuan Guo', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03673v1'}, page_content='Instruction Fine-Tuning (IFT) is crucial for aligning large language models\\n(LLMs) with human preferences, and selecting a small yet representative subset\\nfrom massive data significantly facilitates IFT in terms of both efficiency and\\neffectiveness. Nevertheless, existing approaches suffer from two limitations:\\nthe use of simple heuristics restricts data diversity, while the singleton data\\nquality evaluation accounts for inconsistent criteria between independent\\nsamples. To address the issues, we present TACOS, an innovative method that\\nintegrates Open Tagging and Comparative Scoring for IFT data selection. To\\ncapture data diversity, we leverage LLMs to assign open-domain tags to human\\nqueries, followed by a normalization stage to denoise the open tags and enable\\nefficient clustering. Additionally, we suggest a comparative scoring method\\nthat allows the relative quality evaluation of samples within a cluster,\\navoiding inconsistent criteria seen in singleton-based evaluations. Extensive\\nexperiments across diverse datasets and LLM architectures demonstrate that\\nTACOS outperforms existing approaches by a large margin. Notably, it achieves\\nsuperior instruction-following performance on MT-Bench and ranks 1st among\\nLLaMA2-7B-Based models on AlpacaEval 2.0, illustrating its efficacy for IFT\\ndata selection.'),\n",
       " Document(metadata={'title': 'Recon, Answer, Verify: Agents in Search of Truth', 'authors': 'Satyam Shukla, Himanshu Dutta, Pushpak Bhattacharyya', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03671v1'}, page_content=\"Automated fact checking with large language models (LLMs) offers a scalable\\nalternative to manual verification. Evaluating fact checking is challenging as\\nexisting benchmark datasets often include post claim analysis and annotator\\ncues, which are absent in real world scenarios where claims are fact checked\\nimmediately after being made. This limits the realism of current evaluations.\\nWe present Politi Fact Only (PFO), a 5 class benchmark dataset of 2,982\\npolitical claims from politifact.com, where all post claim analysis and\\nannotator cues have been removed manually. This ensures that models are\\nevaluated using only the information that would have been available prior to\\nthe claim's verification. Evaluating LLMs on PFO, we see an average performance\\ndrop of 22% in terms of macro f1 compared to PFO's unfiltered version. Based on\\nthe identified challenges of the existing LLM based fact checking system, we\\npropose RAV (Recon Answer Verify), an agentic framework with three agents:\\nquestion generator, answer generator, and label generator. Our pipeline\\niteratively generates and answers sub questions to verify different aspects of\\nthe claim before finally generating the label. RAV generalizes across domains\\nand label granularities, and it outperforms state of the art approaches on well\\nknown baselines RAWFC (fact checking, 3 class) by 25.28%, and on HOVER\\n(encyclopedia, 2 class) by 1.54% on 2 hop, 4.94% on 3 hop, and 1.78% on 4 hop,\\nsub categories respectively. RAV shows the least performance drop compared to\\nbaselines of 16.3% in macro f1 when we compare PFO with its unfiltered version.\"),\n",
       " Document(metadata={'title': 'Interaction Techniques that Encourage Longer Prompts Can Improve Psychological Ownership when Writing with AI', 'authors': 'Nikhita Joshi, Daniel Vogel', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03670v1'}, page_content=\"Writing longer prompts for an AI assistant to generate a short story\\nincreases psychological ownership, a user's feeling that the writing belongs to\\nthem. To encourage users to write longer prompts, we evaluated two interaction\\ntechniques that modify the prompt entry interface of chat-based generative AI\\nassistants: pressing and holding the prompt submission button, and continuously\\nmoving a slider up and down when submitting a short prompt. A within-subjects\\nexperiment investigated the effects of such techniques on prompt length and\\npsychological ownership, and results showed that these techniques increased\\nprompt length and led to higher psychological ownership than baseline\\ntechniques. A second experiment further augmented these techniques by showing\\nAI-generated suggestions for how the prompts could be expanded. This further\\nincreased prompt length, but did not lead to improvements in psychological\\nownership. Our results show that simple interface modifications like these can\\nelicit more writing from users and improve psychological ownership.\"),\n",
       " Document(metadata={'title': 'Re-Emergent Misalignment: How Narrow Fine-Tuning Erodes Safety Alignment in LLMs', 'authors': 'Jeremiah Giordani', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03662v1'}, page_content='Recent work has shown that fine-tuning large language models (LLMs) on code\\nwith security vulnerabilities can result in misaligned and unsafe behaviors\\nacross broad domains. These results prompted concerns about the emergence of\\nharmful behaviors from narrow domain fine-tuning. In this paper, we\\ncontextualize these findings by analyzing how such narrow adaptation impacts\\nthe internal mechanisms and behavioral manifestations of LLMs. Through a series\\nof experiments covering output probability distributions, loss and gradient\\nvector geometry, layer-wise activation dynamics, and activation space\\ndimensions, we find that behaviors attributed to \"emergent misalignment\" may be\\nbetter interpreted as an erosion of prior alignment. We show that fine tuning\\non insecure code induces internal changes that oppose alignment. Further, we\\nidentify a shared latent dimension in the model\\'s activation space that governs\\nalignment behavior. We show that this space is activated by insecure code and\\nby misaligned responses more generally, revealing how narrow fine-tuning can\\ndegrade general safety behavior by interfering with shared internal mechanisms.\\nOur findings offer a mechanistic interpretation for previously observed\\nmisalignment phenomena, and highlights the fragility of alignment in LLMs. The\\nresults underscore the need for more robust fine-tuning strategies that\\npreserve intended behavior across domains.'),\n",
       " Document(metadata={'title': 'Improving Low-Resource Dialect Classification Using Retrieval-based Voice Conversion', 'authors': 'Lea Fischbach, Akbar Karimi, Caroline Kleen, Alfred Lameli, Lucie Flek', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03641v1'}, page_content='Deep learning models for dialect identification are often limited by the\\nscarcity of dialectal data. To address this challenge, we propose to use\\nRetrieval-based Voice Conversion (RVC) as an effective data augmentation method\\nfor a low-resource German dialect classification task. By converting audio\\nsamples to a uniform target speaker, RVC minimizes speaker-related variability,\\nenabling models to focus on dialect-specific linguistic and phonetic features.\\nOur experiments demonstrate that RVC enhances classification performance when\\nutilized as a standalone augmentation method. Furthermore, combining RVC with\\nother augmentation methods such as frequency masking and segment removal leads\\nto additional performance gains, highlighting its potential for improving\\ndialect classification in low-resource scenarios.'),\n",
       " Document(metadata={'title': 'Large Language Models for Combinatorial Optimization: A Systematic Review', 'authors': 'Francesca Da Ros, Michael Soprano, Luca Di Gaspero, Kevin Roitero', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03637v1'}, page_content='This systematic review explores the application of Large Language Models\\n(LLMs) in Combinatorial Optimization (CO). We report our findings using the\\nPreferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA)\\nguidelines. We conduct a literature search via Scopus and Google Scholar,\\nexamining over 2,000 publications. We assess publications against four\\ninclusion and four exclusion criteria related to their language, research\\nfocus, publication year, and type. Eventually, we select 103 studies. We\\nclassify these studies into semantic categories and topics to provide a\\ncomprehensive overview of the field, including the tasks performed by LLMs, the\\narchitectures of LLMs, the existing datasets specifically designed for\\nevaluating LLMs in CO, and the field of application. Finally, we identify\\nfuture directions for leveraging LLMs in this field.'),\n",
       " Document(metadata={'title': 'From Video to EEG: Adapting Joint Embedding Predictive Architecture to Uncover Visual Concepts in Brain Signal Analysis', 'authors': 'Amir Hojjati, Lu Li, Ibrahim Hameed, Anis Yazidi, Pedro G. Lind, Rabindra Khadka', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03633v1'}, page_content='EEG signals capture brain activity with high temporal and low spatial\\nresolution, supporting applications such as neurological diagnosis, cognitive\\nmonitoring, and brain-computer interfaces. However, effective analysis is\\nhindered by limited labeled data, high dimensionality, and the absence of\\nscalable models that fully capture spatiotemporal dependencies. Existing\\nself-supervised learning (SSL) methods often focus on either spatial or\\ntemporal features, leading to suboptimal representations. To this end, we\\npropose EEG-VJEPA, a novel adaptation of the Video Joint Embedding Predictive\\nArchitecture (V-JEPA) for EEG classification. By treating EEG as video-like\\nsequences, EEG-VJEPA learns semantically meaningful spatiotemporal\\nrepresentations using joint embeddings and adaptive masking. To our knowledge,\\nthis is the first work that exploits V-JEPA for EEG classification and explores\\nthe visual concepts learned by the model. Evaluations on the publicly available\\nTemple University Hospital (TUH) Abnormal EEG dataset show that EEG-VJEPA\\noutperforms existing state-of-the-art models in classification accuracy.Beyond\\nclassification accuracy, EEG-VJEPA captures physiologically relevant spatial\\nand temporal signal patterns, offering interpretable embeddings that may\\nsupport human-AI collaboration in diagnostic workflows. These findings position\\nEEG-VJEPA as a promising framework for scalable, trustworthy EEG analysis in\\nreal-world clinical settings.'),\n",
       " Document(metadata={'title': 'Disentangling Doubt in Deep Causal AI', 'authors': 'Cooper Doyle', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03622v1'}, page_content='Accurate individual treatment-effect estimation in high-stakes applications\\ndemands both reliable point predictions and interpretable uncertainty\\nquantification. We propose a factorized Monte Carlo Dropout framework for deep\\ntwin-network models that splits total predictive variance into representation\\nuncertainty (sigma_rep) in the shared encoder and prediction uncertainty\\n(sigma_pred) in the outcome heads. Across three synthetic covariate-shift\\nregimes, our intervals are well-calibrated (ECE < 0.03) and satisfy sigma_rep^2\\n+ sigma_pred^2 ~ sigma_tot^2. Additionally, we observe a crossover: head\\nuncertainty leads on in-distribution data, but representation uncertainty\\ndominates under shift. Finally, on a real-world twins cohort with induced\\nmultivariate shifts, only sigma_rep spikes on out-of-distribution samples\\n(delta sigma ~ 0.0002) and becomes the primary error predictor (rho_rep <=\\n0.89), while sigma_pred remains flat. This module-level decomposition offers a\\npractical diagnostic for detecting and interpreting uncertainty sources in deep\\ncausal-effect models.'),\n",
       " Document(metadata={'title': 'Is It Time To Treat Prompts As Code? A Multi-Use Case Study For Prompt Optimization Using DSPy', 'authors': 'Francisca Lemos, Victor Alves, Filipa Ferraz', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03620v1'}, page_content=\"Although prompt engineering is central to unlocking the full potential of\\nLarge Language Models (LLMs), crafting effective prompts remains a\\ntime-consuming trial-and-error process that relies on human intuition. This\\nstudy investigates Declarative Self-improving Python (DSPy), an optimization\\nframework that programmatically creates and refines prompts, applied to five\\nuse cases: guardrail enforcement, hallucination detection in code, code\\ngeneration, routing agents, and prompt evaluation. Each use case explores how\\nprompt optimization via DSPy influences performance. While some cases\\ndemonstrated modest improvements - such as minor gains in the guardrails use\\ncase and selective enhancements in hallucination detection - others showed\\nnotable benefits. The prompt evaluation criterion task demonstrated a\\nsubstantial performance increase, rising accuracy from 46.2% to 64.0%. In the\\nrouter agent case, the possibility of improving a poorly performing prompt and\\nof a smaller model matching a stronger one through optimized prompting was\\nexplored. Although prompt refinement increased accuracy from 85.0% to 90.0%,\\nusing the optimized prompt with a cheaper model did not improve performance.\\nOverall, this study's findings suggest that DSPy's systematic prompt\\noptimization can enhance LLM performance, particularly when instruction tuning\\nand example selection are optimized together. However, the impact varies by\\ntask, highlighting the importance of evaluating specific use cases in prompt\\noptimization research.\"),\n",
       " Document(metadata={'title': 'EvoAgentX: An Automated Framework for Evolving Agentic Workflows', 'authors': 'Yingxu Wang, Siwei Liu, Jinyuan Fang, Zaiqiao Meng', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03616v1'}, page_content='Multi-agent systems (MAS) have emerged as a powerful paradigm for\\norchestrating large language models (LLMs) and specialized tools to\\ncollaboratively address complex tasks. However, existing MAS frameworks often\\nrequire manual workflow configuration and lack native support for dynamic\\nevolution and performance optimization. In addition, many MAS optimization\\nalgorithms are not integrated into a unified framework. In this paper, we\\npresent EvoAgentX, an open-source platform that automates the generation,\\nexecution, and evolutionary optimization of multi-agent workflows. EvoAgentX\\nemploys a modular architecture consisting of five core layers: the basic\\ncomponents, agent, workflow, evolving, and evaluation layers. Specifically,\\nwithin the evolving layer, EvoAgentX integrates three MAS optimization\\nalgorithms, TextGrad, AFlow, and MIPRO, to iteratively refine agent prompts,\\ntool configurations, and workflow topologies. We evaluate EvoAgentX on\\nHotPotQA, MBPP, and MATH for multi-hop reasoning, code generation, and\\nmathematical problem solving, respectively, and further assess it on real-world\\ntasks using GAIA. Experimental results show that EvoAgentX consistently\\nachieves significant performance improvements, including a 7.44% increase in\\nHotPotQA F1, a 10.00% improvement in MBPP pass@1, a 10.00% gain in MATH solve\\naccuracy, and an overall accuracy improvement of up to 20.00% on GAIA. The\\nsource code is available at: https://github.com/EvoAgentX/EvoAgentX'),\n",
       " Document(metadata={'title': 'Multi-Hop Reasoning for Question Answering with Hyperbolic Representations', 'authors': 'Simon Welz, Lucie Flek, Akbar Karimi', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03612v1'}, page_content='Hyperbolic representations are effective in modeling knowledge graph data\\nwhich is prevalently used to facilitate multi-hop reasoning. However, a\\nrigorous and detailed comparison of the two spaces for this task is lacking. In\\nthis paper, through a simple integration of hyperbolic representations with an\\nencoder-decoder model, we perform a controlled and comprehensive set of\\nexperiments to compare the capacity of hyperbolic space versus Euclidean space\\nin multi-hop reasoning. Our results show that the former consistently\\noutperforms the latter across a diverse set of datasets. In addition, through\\nan ablation study, we show that a learnable curvature initialized with the\\ndelta hyperbolicity of the utilized data yields superior results to random\\ninitializations. Furthermore, our findings suggest that hyperbolic\\nrepresentations can be significantly more advantageous when the datasets\\nexhibit a more hierarchical structure.'),\n",
       " Document(metadata={'title': 'Benchmarking Vector, Graph and Hybrid Retrieval Augmented Generation (RAG) Pipelines for Open Radio Access Networks (ORAN)', 'authors': 'Sarat Ahmad, Zeinab Nezami, Maryam Hafeez, Syed Ali Raza Zaidi', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03608v1'}, page_content='Generative AI (GenAI) is expected to play a pivotal role in enabling\\nautonomous optimization in future wireless networks. Within the ORAN\\narchitecture, Large Language Models (LLMs) can be specialized to generate xApps\\nand rApps by leveraging specifications and API definitions from the RAN\\nIntelligent Controller (RIC) platform. However, fine-tuning base LLMs for\\ntelecom-specific tasks remains expensive and resource-intensive.\\nRetrieval-Augmented Generation (RAG) offers a practical alternative through\\nin-context learning, enabling domain adaptation without full retraining. While\\ntraditional RAG systems rely on vector-based retrieval, emerging variants such\\nas GraphRAG and Hybrid GraphRAG incorporate knowledge graphs or dual retrieval\\nstrategies to support multi-hop reasoning and improve factual grounding.\\nDespite their promise, these methods lack systematic, metric-driven\\nevaluations, particularly in high-stakes domains such as ORAN. In this study,\\nwe conduct a comparative evaluation of Vector RAG, GraphRAG, and Hybrid\\nGraphRAG using ORAN specifications. We assess performance across varying\\nquestion complexities using established generation metrics: faithfulness,\\nanswer relevance, context relevance, and factual correctness. Results show that\\nboth GraphRAG and Hybrid GraphRAG outperform traditional RAG. Hybrid GraphRAG\\nimproves factual correctness by 8%, while GraphRAG improves context relevance\\nby 7%.'),\n",
       " Document(metadata={'title': 'Behaviour Space Analysis of LLM-driven Meta-heuristic Discovery', 'authors': 'Niki van Stein, Haoran Yin, Anna V. Kononova, Thomas Bäck, Gabriela Ochoa', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03605v1'}, page_content='We investigate the behaviour space of meta-heuristic optimisation algorithms\\nautomatically generated by Large Language Model driven algorithm discovery\\nmethods. Using the Large Language Evolutionary Algorithm (LLaMEA) framework\\nwith a GPT o4-mini LLM, we iteratively evolve black-box optimisation\\nheuristics, evaluated on 10 functions from the BBOB benchmark suite. Six LLaMEA\\nvariants, featuring different mutation prompt strategies, are compared and\\nanalysed. We log dynamic behavioural metrics including exploration,\\nexploitation, convergence and stagnation measures, for each run, and analyse\\nthese via visual projections and network-based representations. Our analysis\\ncombines behaviour-based\\n  projections, Code Evolution Graphs built from static code features,\\nperformance convergence curves, and behaviour-based Search Trajectory Networks.\\nThe results reveal clear differences in search dynamics and algorithm\\nstructures across LLaMEA configurations. Notably, the variant that employs both\\na code simplification prompt and a random perturbation prompt in a 1+1 elitist\\nevolution strategy, achieved the best performance, with the highest Area Over\\nthe Convergence Curve. Behaviour-space visualisations show that\\nhigher-performing algorithms exhibit more intensive exploitation behaviour and\\nfaster convergence with less stagnation. Our findings demonstrate how\\nbehaviour-space analysis can explain why certain LLM-designed heuristics\\noutperform others and how LLM-driven algorithm discovery navigates the\\nopen-ended and complex search space of algorithms. These findings provide\\ninsights to guide the future design of adaptive LLM-driven algorithm\\ngenerators.'),\n",
       " Document(metadata={'title': 'MusGO: A Community-Driven Framework For Assessing Openness in Music-Generative AI', 'authors': 'Roser Batlle-Roca, Laura Ibáñez-Martínez, Xavier Serra, Emilia Gómez, Martín Rocamora', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03599v1'}, page_content=\"Since 2023, generative AI has rapidly advanced in the music domain. Despite\\nsignificant technological advancements, music-generative models raise critical\\nethical challenges, including a lack of transparency and accountability, along\\nwith risks such as the replication of artists' works, which highlights the\\nimportance of fostering openness. With upcoming regulations such as the EU AI\\nAct encouraging open models, many generative models are being released labelled\\nas 'open'. However, the definition of an open model remains widely debated. In\\nthis article, we adapt a recently proposed evidence-based framework for\\nassessing openness in LLMs to the music domain. Using feedback from a survey of\\n110 participants from the Music Information Retrieval (MIR) community, we\\nrefine the framework into MusGO (Music-Generative Open AI), which comprises 13\\nopenness categories: 8 essential and 5 desirable. We evaluate 16\\nstate-of-the-art generative models and provide an openness leaderboard that is\\nfully open to public scrutiny and community contributions. Through this work,\\nwe aim to clarify the concept of openness in music-generative AI and promote\\nits transparent and responsible development.\"),\n",
       " Document(metadata={'title': \"RECA-PD: A Robust Explainable Cross-Attention Method for Speech-based Parkinson's Disease Classification\", 'authors': 'Terry Yi Zhong, Cristian Tejedor-Garcia, Martha Larson, Bastiaan R. Bloem', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03594v1'}, page_content=\"Parkinson's Disease (PD) affects over 10 million people globally, with speech\\nimpairments often preceding motor symptoms by years, making speech a valuable\\nmodality for early, non-invasive detection. While recent deep-learning models\\nachieve high accuracy, they typically lack the explainability required for\\nclinical use. To address this, we propose RECA-PD, a novel, robust, and\\nexplainable cross-attention architecture that combines interpretable speech\\nfeatures with self-supervised representations. RECA-PD matches state-of-the-art\\nperformance in Speech-based PD detection while providing explanations that are\\nmore consistent and more clinically meaningful. Additionally, we demonstrate\\nthat performance degradation in certain speech tasks (e.g., monologue) can be\\nmitigated by segmenting long recordings. Our findings indicate that performance\\nand explainability are not necessarily mutually exclusive. Future work will\\nenhance the usability of explanations for non-experts and explore severity\\nestimation to increase the real-world clinical relevance.\"),\n",
       " Document(metadata={'title': 'Causal-SAM-LLM: Large Language Models as Causal Reasoners for Robust Medical Segmentation', 'authors': 'Tao Tang, Shijie Xu, Yiting Wu, Zhixiang Lu', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03585v1'}, page_content=\"The clinical utility of deep learning models for medical image segmentation\\nis severely constrained by their inability to generalize to unseen domains.\\nThis failure is often rooted in the models learning spurious correlations\\nbetween anatomical content and domain-specific imaging styles. To overcome this\\nfundamental challenge, we introduce Causal-SAM-LLM, a novel framework that\\nelevates Large Language Models (LLMs) to the role of causal reasoners. Our\\nframework, built upon a frozen Segment Anything Model (SAM) encoder,\\nincorporates two synergistic innovations. First, Linguistic Adversarial\\nDisentanglement (LAD) employs a Vision-Language Model to generate rich, textual\\ndescriptions of confounding image styles. By training the segmentation model's\\nfeatures to be contrastively dissimilar to these style descriptions, it learns\\na representation robustly purged of non-causal information. Second, Test-Time\\nCausal Intervention (TCI) provides an interactive mechanism where an LLM\\ninterprets a clinician's natural language command to modulate the segmentation\\ndecoder's features in real-time, enabling targeted error correction. We conduct\\nan extensive empirical evaluation on a composite benchmark from four public\\ndatasets (BTCV, CHAOS, AMOS, BraTS), assessing generalization under\\ncross-scanner, cross-modality, and cross-anatomy settings. Causal-SAM-LLM\\nestablishes a new state of the art in out-of-distribution (OOD) robustness,\\nimproving the average Dice score by up to 6.2 points and reducing the Hausdorff\\nDistance by 15.8 mm over the strongest baseline, all while using less than 9%\\nof the full model's trainable parameters. Our work charts a new course for\\nbuilding robust, efficient, and interactively controllable medical AI systems.\"),\n",
       " Document(metadata={'title': 'A Universal Approach to Feature Representation in Dynamic Task Assignment Problems', 'authors': 'Riccardo Lo Bianco, Remco Dijkman, Wim Nuijten, Willem van Jaarsveld', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03579v1'}, page_content='Dynamic task assignment concerns the optimal assignment of resources to tasks\\nin a business process. Recently, Deep Reinforcement Learning (DRL) has been\\nproposed as the state of the art for solving assignment problems. DRL methods\\nusually employ a neural network (NN) as an approximator for the policy\\nfunction, which ingests the state of the process and outputs a valuation of the\\npossible assignments. However, representing the state and the possible\\nassignments so that they can serve as inputs and outputs for a policy NN\\nremains an open challenge, especially when tasks or resources have features\\nwith an infinite number of possible values. To solve this problem, this paper\\nproposes a method for representing and solving assignment problems with\\ninfinite state and action spaces. In doing so, it provides three contributions:\\n(I) A graph-based feature representation of assignment problems, which we call\\nassignment graph; (II) A mapping from marked Colored Petri Nets to assignment\\ngraphs; (III) An adaptation of the Proximal Policy Optimization algorithm that\\ncan learn to solve assignment problems represented through assignment graphs.\\nTo evaluate the proposed representation method, we model three archetypal\\nassignment problems ranging from finite to infinite state and action space\\ndimensionalities. The experiments show that the method is suitable for\\nrepresenting and learning close-to-optimal task assignment policies regardless\\nof the state and action space dimensionalities.'),\n",
       " Document(metadata={'title': 'SciVid: Cross-Domain Evaluation of Video Models in Scientific Applications', 'authors': 'Yana Hasson, Pauline Luc, Liliane Momeni, Maks Ovsjanikov, Guillaume Le Moing, Alina Kuznetsova, Ira Ktena, Jennifer J. Sun, Skanda Koppula, Dilara Gokay, Joseph Heyward, Etienne Pot, Andrew Zisserman', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03578v1'}, page_content='In recent years, there has been a proliferation of spatiotemporal foundation\\nmodels in different scientific disciplines. While promising, these models are\\noften domain-specific and are only assessed within the particular applications\\nfor which they are designed. Given that many tasks can be represented as video\\nmodeling problems, video foundation models (ViFMs) hold considerable promise as\\ngeneral-purpose domain-agnostic approaches. However, it is not known whether\\nthe knowledge acquired on large-scale but potentially out-of-domain data can be\\neffectively transferred across diverse scientific disciplines, and if a single,\\npretrained ViFM can be competitive with domain-specific baselines. To address\\nthis, we introduce SciVid, a comprehensive benchmark comprising five\\n*Sci*entific *Vid*eo tasks, across medical computer vision, animal behavior,\\nand weather forecasting. We adapt six leading ViFMs to SciVid using simple\\ntrainable readout modules, establishing strong baselines and demonstrating the\\npotential for effective transfer learning. Specifically, we show that\\nstate-of-the-art results can be obtained in several applications by leveraging\\nthe general-purpose representations from ViFM backbones. Furthermore, our\\nresults reveal the limitations of existing ViFMs, and highlight opportunities\\nfor the development of generalizable models for high-impact scientific\\napplications. We release our code at https://github.com/google-deepmind/scivid\\nto facilitate further research in the development of ViFMs.'),\n",
       " Document(metadata={'title': 'An Advanced Deep Learning Framework for Ischemic and Hemorrhagic Brain Stroke Diagnosis Using Computed Tomography (CT) Images', 'authors': 'Md. Sabbir Hossen, Eshat Ahmed Shuvo, Shibbir Ahmed Arif, Pabon Shaha, Md. Saiduzzaman, Mostofa Kamal Nasir', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03558v1'}, page_content=\"Brain stroke is one of the leading causes of mortality and long-term\\ndisability worldwide, highlighting the need for precise and fast prediction\\ntechniques. Computed Tomography (CT) scan is considered one of the most\\neffective methods for diagnosing brain strokes. The majority of stroke\\nclassification techniques rely on a single slice-level prediction mechanism,\\nallowing the radiologist to manually choose the most critical CT slice from the\\noriginal CT volume. Although clinical evaluations are often used in traditional\\ndiagnostic procedures, machine learning (ML) has opened up new avenues for\\nimproving stroke diagnosis. To supplement traditional diagnostic techniques,\\nthis study investigates the use of machine learning models, specifically\\nconcerning the prediction of brain stroke at an early stage utilizing CT scan\\nimages. In this research, we proposed a novel approach to brain stroke\\ndetection leveraging machine learning techniques, focusing on optimizing\\nclassification performance with pre-trained deep learning models and advanced\\noptimization strategies. Pre-trained models, including DenseNet201,\\nInceptionV3, MobileNetV2, ResNet50, and Xception, are utilized for feature\\nextraction. Additionally, we employed feature engineering techniques, including\\nBFO, PCA, and LDA, to enhance models' performance further. These features are\\nsubsequently classified using machine learning algorithms such as SVC, RF, XGB,\\nDT, LR, KNN, and GNB. Our experiments demonstrate that the combination of\\nMobileNetV2, LDA, and SVC achieved the highest classification accuracy of\\n97.93%, significantly outperforming other model-optimizer-classifier\\ncombinations. The results underline the effectiveness of integrating\\nlightweight pre-trained models with robust optimization and classification\\ntechniques for brain stroke diagnosis.\"),\n",
       " Document(metadata={'title': 'H2HTalk: Evaluating Large Language Models as Emotional Companion', 'authors': 'Boyang Wang, Yalun Wu, Hongcheng Guo, Zhoujun Li', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03543v1'}, page_content='As digital emotional support needs grow, Large Language Model companions\\noffer promising authentic, always-available empathy, though rigorous evaluation\\nlags behind model advancement. We present Heart-to-Heart Talk (H2HTalk), a\\nbenchmark assessing companions across personality development and empathetic\\ninteraction, balancing emotional intelligence with linguistic fluency. H2HTalk\\nfeatures 4,650 curated scenarios spanning dialogue, recollection, and itinerary\\nplanning that mirror real-world support conversations, substantially exceeding\\nprevious datasets in scale and diversity. We incorporate a Secure Attachment\\nPersona (SAP) module implementing attachment-theory principles for safer\\ninteractions. Benchmarking 50 LLMs with our unified protocol reveals that\\nlong-horizon planning and memory retention remain key challenges, with models\\nstruggling when user needs are implicit or evolve mid-conversation. H2HTalk\\nestablishes the first comprehensive benchmark for emotionally intelligent\\ncompanions. We release all materials to advance development of LLMs capable of\\nproviding meaningful and safe psychological support.'),\n",
       " Document(metadata={'title': 'Foundation versus Domain-specific Models: Performance Comparison, Fusion, and Explainability in Face Recognition', 'authors': 'Redwan Sony, Parisa Farmanifard, Arun Ross, Anil K. Jain', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03541v1'}, page_content=\"In this paper, we address the following question: How do generic foundation\\nmodels (e.g., CLIP, BLIP, LLaVa, DINO) compare against a domain-specific face\\nrecognition model (viz., AdaFace or ArcFace) on the face recognition task?\\nThrough a series of experiments involving several foundation models and\\nbenchmark datasets, we are able to report the following findings: (a) In all\\ndatasets considered, domain-specific models outperformed zero-shot foundation\\nmodels. (b) The performance of zero-shot generic foundation models improves on\\nover-segmented face images than tightly cropped faces thereby suggesting the\\nimportance of contextual clues. For example, at a False Match Rate (FMR) of\\n0.01%, the True Match Rate (TMR) of OpenCLIP improved from 64.97% to 81.73% on\\nthe LFW dataset as the face crop increased from 112x112 to 250x250 while the\\nTMR of domain-specific AdaFace dropped from 99.09% to 77.31%. (c) A simple\\nscore-level fusion of a foundation model with a domain-specific FR model\\nimproved the accuracy at low FMRs. For example, the TMR of AdaFace when fused\\nwith BLIP improved from 72.64% to 83.31% at an FMR of 0.0001% on the IJB-B\\ndataset and from 73.17% to 85.81% on the IJB-C dataset. (d) Foundation models,\\nsuch as ChatGPT, can be used to impart explainability to the FR pipeline (e.g.,\\n``Despite minor lighting and head tilt differences, the two left-profile images\\nshow high consistency in forehead slope, nose shape, chin contour...''). In\\nsome instances, foundation models are even able to resolve low-confidence\\ndecisions made by AdaFace (e.g., ``Although AdaFace assigns a low similarity\\nscore of 0.21, both images exhibit visual similarity...and the pair is likely\\nof the same person''), thereby reiterating the importance of combining\\ndomain-specific FR models with generic foundation models in a judicious manner.\"),\n",
       " Document(metadata={'title': 'Multimodal Alignment with Cross-Attentive GRUs for Fine-Grained Video Understanding', 'authors': 'Namho Kim, Junhwa Kim', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03531v1'}, page_content='Fine-grained video classification requires understanding complex\\nspatio-temporal and semantic cues that often exceed the capacity of a single\\nmodality. In this paper, we propose a multimodal framework that fuses video,\\nimage, and text representations using GRU-based sequence encoders and\\ncross-modal attention mechanisms. The model is trained using a combination of\\nclassification or regression loss, depending on the task, and is further\\nregularized through feature-level augmentation and autoencoding techniques. To\\nevaluate the generality of our framework, we conduct experiments on two\\nchallenging benchmarks: the DVD dataset for real-world violence detection and\\nthe Aff-Wild2 dataset for valence-arousal estimation. Our results demonstrate\\nthat the proposed fusion strategy significantly outperforms unimodal baselines,\\nwith cross-attention and feature augmentation contributing notably to\\nrobustness and performance.'),\n",
       " Document(metadata={'title': 'Generating Synthetic Relational Tabular Data via Structural Causal Models', 'authors': 'Frederik Hoppe, Astrid Franz, Lars Kleinemeier, Udo Göbel', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03528v1'}, page_content='Synthetic tabular data generation has received increasing attention in recent\\nyears, particularly with the emergence of foundation models for tabular data.\\nThe breakthrough success of TabPFN (Hollmann et al.,2025), which leverages vast\\nquantities of synthetic tabular datasets derived from structural causal models\\n(SCMs), demonstrates the critical role synthetic data plays in developing\\npowerful tabular foundation models. However, most real-world tabular data\\nexists in relational formats spanning multiple interconnected tables - a\\nstructure not adequately addressed by current generation methods. In this work,\\nwe extend the SCM-based approach by developing a novel framework that generates\\nrealistic synthetic relational tabular data including causal relationships\\nacross tables. Our experiments confirm that this framework is able to construct\\nrelational datasets with complex inter-table dependencies mimicking real-world\\nscenarios.'),\n",
       " Document(metadata={'title': 'Limits of Safe AI Deployment: Differentiating Oversight and Control', 'authors': 'David Manheim, Aidan Homewood', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03525v1'}, page_content='Oversight and control (collectively, supervision) are often invoked as key\\nlevers for ensuring that AI systems are accountable, reliable, and able to\\nfulfill governance and management requirements. However, the concepts are\\nfrequently conflated or insufficiently distinguished in academic and policy\\ndiscourse, undermining efforts to design or evaluate systems that should remain\\nunder meaningful human supervision.\\n  This paper undertakes a targeted critical review of literature on supervision\\noutside of AI, along with a brief summary of past work on the topic related to\\nAI. We then differentiate control as being ex-ante or real-time, and\\noperational rather than policy or governance. In contrast, oversight is either\\na policy and governance function, or is ex-post. We suggest that control aims\\nto prevent failures. In contrast, oversight often focuses on detection,\\nremediation, or incentives for future prevention; all preventative oversight\\nstrategies nonetheless necessitate control.\\n  Building on this foundation, we make three contributions. First, we propose a\\ntheoretically-informed yet policy-grounded framework that articulates the\\nconditions under which each mechanism is possible, where they fall short, and\\nwhat is required to make them meaningful in practice. Second, we outline how\\nsupervision methods should be documented and integrated into risk management,\\nand drawing on the Microsoft Responsible AI Maturity Model, we outline a\\nmaturity model for AI supervision. Third, we explicitly highlight some\\nboundaries of these mechanisms, including where they apply, where they fail,\\nand where it is clear that no existing methods suffice. This foregrounds the\\nquestion of whether meaningful supervision is possible in a given deployment\\ncontext, and can support regulators, auditors, and practitioners in identifying\\nboth present limitations and the need for new conceptual and technical\\nadvances.'),\n",
       " Document(metadata={'title': 'Reinforcement Learning-based Feature Generation Algorithm for Scientific Data', 'authors': 'Meng Xiao, Junfeng Zhou, Yuanchun Zhou', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03498v1'}, page_content='Feature generation (FG) aims to enhance the prediction potential of original\\ndata by constructing high-order feature combinations and removing redundant\\nfeatures. It is a key preprocessing step for tabular scientific data to improve\\ndownstream machine-learning model performance. Traditional methods face the\\nfollowing two challenges when dealing with the feature generation of scientific\\ndata: First, the effective construction of high-order feature combinations in\\nscientific data necessitates profound and extensive domain-specific expertise.\\nSecondly, as the order of feature combinations increases, the search space\\nexpands exponentially, imposing prohibitive human labor consumption.\\nAdvancements in the Data-Centric Artificial Intelligence (DCAI) paradigm have\\nopened novel avenues for automating feature generation processes. Inspired by\\nthat, this paper revisits the conventional feature generation workflow and\\nproposes the Multi-agent Feature Generation (MAFG) framework. Specifically, in\\nthe iterative exploration stage, multi-agents will construct mathematical\\ntransformation equations collaboratively, synthesize and identify feature\\ncombinations ex-hibiting high information content, and leverage a reinforcement\\nlearning mechanism to evolve their strategies. Upon completing the exploration\\nphase, MAFG integrates the large language models (LLMs) to interpreta-tively\\nevaluate the generated features of each significant model performance\\nbreakthrough. Experimental results and case studies consistently demonstrate\\nthat the MAFG framework effectively automates the feature generation process\\nand significantly enhances various downstream scientific data mining tasks.'),\n",
       " Document(metadata={'title': 'BMMR: A Large-Scale Bilingual Multimodal Multi-Discipline Reasoning Dataset', 'authors': 'Zhiheng Xi, Guanyu Li, Yutao Fan, Honglin Guo, Yufang Liu, Xiaoran Fan, Jiaqi Liu, Jingchao Ding, Wangmeng Zuo, Zhenfei Yin, Lei Bai, Tao Ji, Tao Gui, Qi Zhang, Xuanjing Huang', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03483v1'}, page_content=\"In this paper, we introduce BMMR, a large-scale bilingual, multimodal,\\nmulti-disciplinary reasoning dataset for the community to develop and evaluate\\nlarge multimodal models (LMMs). BMMR comprises 110k college-level questions\\nspanning 300 UNESCO-defined subjects, spanning diverse formats-multiple-choice,\\nfill-in-the-blank, and open-ended QA-and sourced from both print and digital\\nmedia such as books, exams, and quizzes. All data are curated and filtered via\\na human-in-the-loop and scalable framework, and each instance is paired with a\\nhigh-quality reasoning path. The dataset is organized into two parts: BMMR-Eval\\nthat comprises 20,458 high-quality instances to comprehensively assess LMMs'\\nknowledge and reasoning across multiple disciplines in both Chinese and\\nEnglish; and BMMR-Train that contains 88,991 instances to support further\\nresearch and development, extending the current focus on mathematical reasoning\\nto diverse disciplines and domains. In addition, we propose the process-based\\nmulti-discipline verifier (i.e., BMMR-Verifier) for accurate and fine-grained\\nevaluation of reasoning paths. Extensive experiments on 24 models reveal that\\n(i) even SOTA models (e.g., o3 and Gemini-2.5-Pro) leave substantial headroom\\non BMMR-Eval; (ii) reasoning models exhibit discipline bias and outperform LMMs\\nonly on specific subjects; (iii) open-source models still trail their\\nproprietary counterparts; and (iv) fine-tuning on BMMR-Train narrows this gap.\\nAdditionally, we conduct reasoning-chain analyses using BMMR-Verifier and other\\nin-depth studies, uncovering the challenges LMMs currently face in\\nmultidisciplinary reasoning. We will release the data, and we hope our work can\\noffer insights and contributions to the community.\"),\n",
       " Document(metadata={'title': 'REAL: Benchmarking Abilities of Large Language Models for Housing Transactions and Services', 'authors': 'Kexin Zhu, Yang Han', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03477v1'}, page_content='The development of large language models (LLMs) has greatly promoted the\\nprogress of chatbot in multiple fields. There is an urgent need to evaluate\\nwhether LLMs can play the role of agent in housing transactions and services as\\nwell as humans. We present Real Estate Agent Large Language Model Evaluation\\n(REAL), the first evaluation suite designed to assess the abilities of LLMs in\\nthe field of housing transactions and services. REAL comprises 5,316\\nhigh-quality evaluation entries across 4 topics: memory, comprehension,\\nreasoning and hallucination. All these entries are organized as 14 categories\\nto assess whether LLMs have the knowledge and ability in housing transactions\\nand services scenario. Additionally, the REAL is used to evaluate the\\nperformance of most advanced LLMs. The experiment results indicate that LLMs\\nstill have significant room for improvement to be applied in the real estate\\nfield.'),\n",
       " Document(metadata={'title': 'Beyond Weaponization: NLP Security for Medium and Lower-Resourced Languages in Their Own Right', 'authors': 'Heather Lent', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03473v1'}, page_content='Despite mounting evidence that multilinguality can be easily weaponized\\nagainst language models (LMs), works across NLP Security remain overwhelmingly\\nEnglish-centric. In terms of securing LMs, the NLP norm of \"English first\"\\ncollides with standard procedure in cybersecurity, whereby practitioners are\\nexpected to anticipate and prepare for worst-case outcomes. To mitigate\\nworst-case outcomes in NLP Security, researchers must be willing to engage with\\nthe weakest links in LM security: lower-resourced languages. Accordingly, this\\nwork examines the security of LMs for lower- and medium-resourced languages. We\\nextend existing adversarial attacks for up to 70 languages to evaluate the\\nsecurity of monolingual and multilingual LMs for these languages. Through our\\nanalysis, we find that monolingual models are often too small in total number\\nof parameters to ensure sound security, and that while multilinguality is\\nhelpful, it does not always guarantee improved security either. Ultimately,\\nthese findings highlight important considerations for more secure deployment of\\nLMs, for communities of lower-resourced languages.'),\n",
       " Document(metadata={'title': 'Multi-Agent Reasoning for Cardiovascular Imaging Phenotype Analysis', 'authors': 'Weitong Zhang, Mengyun Qiao, Chengqi Zang, Steven Niederer, Paul M Matthews, Wenjia Bai, Bernhard Kainz', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03460v1'}, page_content=\"Identifying the associations between imaging phenotypes and disease risk\\nfactors and outcomes is essential for understanding disease mechanisms and\\nimproving diagnosis and prognosis models. However, traditional approaches rely\\non human-driven hypothesis testing and selection of association factors, often\\noverlooking complex, non-linear dependencies among imaging phenotypes and other\\nmulti-modal data. To address this, we introduce a Multi-agent Exploratory\\nSynergy for the Heart (MESHAgents) framework that leverages large language\\nmodels as agents to dynamically elicit, surface, and decide confounders and\\nphenotypes in association studies, using cardiovascular imaging as a proof of\\nconcept. Specifically, we orchestrate a multi-disciplinary team of AI agents --\\nspanning cardiology, biomechanics, statistics, and clinical research -- which\\nspontaneously generate and converge on insights through iterative,\\nself-organizing reasoning. The framework dynamically synthesizes statistical\\ncorrelations with multi-expert consensus, providing an automated pipeline for\\nphenome-wide association studies (PheWAS). We demonstrate the system's\\ncapabilities through a population-based study of imaging phenotypes of the\\nheart and aorta. MESHAgents autonomously uncovered correlations between imaging\\nphenotypes and a wide range of non-imaging factors, identifying additional\\nconfounder variables beyond standard demographic factors. Validation on\\ndiagnosis tasks reveals that MESHAgents-discovered phenotypes achieve\\nperformance comparable to expert-selected phenotypes, with mean AUC differences\\nas small as -0.004 on disease classification tasks. Notably, the recall score\\nimproves for 6 out of 9 disease types. Our framework provides clinically\\nrelevant imaging phenotypes with transparent reasoning, offering a scalable\\nalternative to expert-driven methods.\"),\n",
       " Document(metadata={'title': 'Helping CLIP See Both the Forest and the Trees: A Decomposition and Description Approach', 'authors': 'Leyan Xue, Zongbo Han, Guangyu Wang, Qinghua Hu, Mingyue Cheng, Changqing Zhang', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03458v1'}, page_content='Vision-Language Models (VLMs) like CLIP achieve cross-modal semantic\\nalignment through contrastive learning, exhibiting robust zero-shot\\ngeneralization. Traditional prompt engineering, however, predominantly relies\\non coarse-grained category labels, neglecting fine-grained local semantics.\\nExisting approaches assume that VLMs inherently recognize localized visual\\ndetails and attempt to enhance classification by augmenting text prompts with\\nattribute descriptors generated by large language models. However, our\\nsystematic experiments reveal critical limitations: CLIP\\'s strong bias toward\\nglobal image patterns hinders its ability to process localized visual\\ndescriptors. To address this fundamental constraint, we propose a simple,\\neffective, and plug-and-play solution that enables CLIP to ``See Both the\\nForest and the Trees.\" Specifically, we employ stochastic multi-crop\\naugmentation to activate CLIP\\'s latent capacity for localized feature analysis.\\nBy cropping only partial regions, the approach effectively constrains the\\nmodel\\'s receptive field and recalibrates its attention mechanism, thereby\\nmitigating its inherent bias. We evaluate the proposed method under zero-shot,\\nfew-shot, and test-time adaptation settings, and extensive experiments\\ndemonstrate that D&D achieves promising performance.'),\n",
       " Document(metadata={'title': 'Evaluating the Evaluators: Trust in Adversarial Robustness Tests', 'authors': 'Antonio Emanuele Cinà, Maura Pintor, Luca Demetrio, Ambra Demontis, Battista Biggio, Fabio Roli', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03450v1'}, page_content='Despite significant progress in designing powerful adversarial evasion\\nattacks for robustness verification, the evaluation of these methods often\\nremains inconsistent and unreliable. Many assessments rely on mismatched\\nmodels, unverified implementations, and uneven computational budgets, which can\\nlead to biased results and a false sense of security. Consequently, robustness\\nclaims built on such flawed testing protocols may be misleading and give a\\nfalse sense of security. As a concrete step toward improving evaluation\\nreliability, we present AttackBench, a benchmark framework developed to assess\\nthe effectiveness of gradient-based attacks under standardized and reproducible\\nconditions. AttackBench serves as an evaluation tool that ranks existing attack\\nimplementations based on a novel optimality metric, which enables researchers\\nand practitioners to identify the most reliable and effective attack for use in\\nsubsequent robustness evaluations. The framework enforces consistent testing\\nconditions and enables continuous updates, making it a reliable foundation for\\nrobustness verification.'),\n",
       " Document(metadata={'title': 'Improving Social Determinants of Health Documentation in French EHRs Using Large Language Models', 'authors': 'Adrien Bazoge, Pacôme Constant dit Beaufils, Mohammed Hmitouch, Romain Bourcier, Emmanuel Morin, Richard Dufour, Béatrice Daille, Pierre-Antoine Gourraud, Matilde Karakachoff', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03433v1'}, page_content='Social determinants of health (SDoH) significantly influence health outcomes,\\nshaping disease progression, treatment adherence, and health disparities.\\nHowever, their documentation in structured electronic health records (EHRs) is\\noften incomplete or missing. This study presents an approach based on large\\nlanguage models (LLMs) for extracting 13 SDoH categories from French clinical\\nnotes. We trained Flan-T5-Large on annotated social history sections from\\nclinical notes at Nantes University Hospital, France. We evaluated the model at\\ntwo levels: (i) identification of SDoH categories and associated values, and\\n(ii) extraction of detailed SDoH with associated temporal and quantitative\\ninformation. The model performance was assessed across four datasets, including\\ntwo that we publicly release as open resources. The model achieved strong\\nperformance for identifying well-documented categories such as living\\ncondition, marital status, descendants, job, tobacco, and alcohol use (F1 score\\n> 0.80). Performance was lower for categories with limited training data or\\nhighly variable expressions, such as employment status, housing, physical\\nactivity, income, and education. Our model identified 95.8% of patients with at\\nleast one SDoH, compared to 2.8% for ICD-10 codes from structured EHR data. Our\\nerror analysis showed that performance limitations were linked to annotation\\ninconsistencies, reliance on English-centric tokenizer, and reduced\\ngeneralizability due to the model being trained on social history sections\\nonly. These results demonstrate the effectiveness of NLP in improving the\\ncompleteness of real-world SDoH data in a non-English EHR system.'),\n",
       " Document(metadata={'title': 'Multi-Level Fusion Graph Neural Network for Molecule Property Prediction', 'authors': 'XiaYu Liu, Hou-biao Li, Yang Liu, Chao Fan', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03430v1'}, page_content='Accurate molecular property prediction is essential in drug discovery and\\nrelated fields. However, existing graph neural networks (GNNs) often struggle\\nto simultaneously capture both local and global molecular structures. In this\\nwork, we propose a Multi-Level Fusion Graph Neural Network (MLFGNN) that\\nintegrates Graph Attention Networks and a novel Graph Transformer to jointly\\nmodel local and global dependencies. In addition, we incorporate molecular\\nfingerprints as a complementary modality and introduce a mechanism of\\ninteraction between attention to adaptively fuse information across\\nrepresentations. Extensive experiments on multiple benchmark datasets\\ndemonstrate that MLFGNN consistently outperforms state-of-the-art methods in\\nboth classification and regression tasks. Interpretability analysis further\\nreveals that the model effectively captures task-relevant chemical patterns,\\nsupporting the usefulness of multi-level and multi-modal fusion in molecular\\nrepresentation learning.'),\n",
       " Document(metadata={'title': 'Lessons from a Chimp: AI \"Scheming\" and the Quest for Ape Language', 'authors': 'Christopher Summerfield, Lennart Luettgau, Magda Dubois, Hannah Rose Kirk, Kobi Hackenburg, Catherine Fist, Katarina Slama, Nicola Ding, Rebecca Anselmetti, Andrew Strait, Mario Giulianelli, Cozmin Ududec', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03409v1'}, page_content='We examine recent research that asks whether current AI systems may be\\ndeveloping a capacity for \"scheming\" (covertly and strategically pursuing\\nmisaligned goals). We compare current research practices in this field to those\\nadopted in the 1970s to test whether non-human primates could master natural\\nlanguage. We argue that there are lessons to be learned from that historical\\nresearch endeavour, which was characterised by an overattribution of human\\ntraits to other agents, an excessive reliance on anecdote and descriptive\\nanalysis, and a failure to articulate a strong theoretical framework for the\\nresearch. We recommend that research into AI scheming actively seeks to avoid\\nthese pitfalls. We outline some concrete steps that can be taken for this\\nresearch programme to advance in a productive and scientifically rigorous\\nfashion.'),\n",
       " Document(metadata={'title': 'Artificial intelligence in drug discovery: A comprehensive review with a case study on hyperuricemia, gout arthritis, and hyperuricemic nephropathy', 'authors': 'Junwei Su, Cheng Xin, Ao Shang, Shan Wu, Zhenzhen Xie, Ruogu Xiong, Xiaoyu Xu, Cheng Zhang, Guang Chen, Yau-Tuen Chan, Guoyi Tang, Ning Wang, Yong Xu, Yibin Feng', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03407v1'}, page_content='This paper systematically reviews recent advances in artificial intelligence\\n(AI), with a particular focus on machine learning (ML), across the entire drug\\ndiscovery pipeline. Due to the inherent complexity, escalating costs, prolonged\\ntimelines, and high failure rates of traditional drug discovery methods, there\\nis a critical need to comprehensively understand how AI/ML can be effectively\\nintegrated throughout the full process. Currently available literature reviews\\noften narrowly focus on specific phases or methodologies, neglecting the\\ndependence between key stages such as target identification, hit screening, and\\nlead optimization. To bridge this gap, our review provides a detailed and\\nholistic analysis of AI/ML applications across these core phases, highlighting\\nsignificant methodological advances and their impacts at each stage. We further\\nillustrate the practical impact of these techniques through an in-depth case\\nstudy focused on hyperuricemia, gout arthritis, and hyperuricemic nephropathy,\\nhighlighting real-world successes in molecular target identification and\\ntherapeutic candidate discovery. Additionally, we discuss significant\\nchallenges facing AI/ML in drug discovery and outline promising future research\\ndirections. Ultimately, this review serves as an essential orientation for\\nresearchers aiming to leverage AI/ML to overcome existing bottlenecks and\\naccelerate drug discovery.'),\n",
       " Document(metadata={'title': 'Pose-Star: Anatomy-Aware Editing for Open-World Fashion Images', 'authors': 'Yuran Dong, Mang Ye', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03402v1'}, page_content='To advance real-world fashion image editing, we analyze existing two-stage\\npipelines(mask generation followed by diffusion-based editing)which overly\\nprioritize generator optimization while neglecting mask controllability. This\\nresults in two critical limitations: I) poor user-defined flexibility\\n(coarse-grained human masks restrict edits to predefined regions like upper\\ntorso; fine-grained clothes masks preserve poses but forbid style/length\\ncustomization). II) weak pose robustness (mask generators fail due to\\narticulated poses and miss rare regions like waist, while human parsers remain\\nlimited by predefined categories). To address these gaps, we propose Pose-Star,\\na framework that dynamically recomposes body structures (e.g., neck, chest,\\netc.) into anatomy-aware masks (e.g., chest-length) for user-defined edits. In\\nPose-Star, we calibrate diffusion-derived attention (Star tokens) via skeletal\\nkeypoints to enhance rare structure localization in complex poses, suppress\\nnoise through phase-aware analysis of attention dynamics\\n(Convergence,Stabilization,Divergence) with threshold masking and\\nsliding-window fusion, and refine edges via cross-self attention merging and\\nCanny alignment. This work bridges controlled benchmarks and open-world\\ndemands, pioneering anatomy-aware, pose-robust editing and laying the\\nfoundation for industrial fashion image editing.'),\n",
       " Document(metadata={'title': 'LLM4Hint: Leveraging Large Language Models for Hint Recommendation in Offline Query Optimization', 'authors': 'Suchen Liu, Jun Gao, Yinjun Han, Yang Lin', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03384v1'}, page_content=\"Query optimization is essential for efficient SQL query execution in DBMS,\\nand remains attractive over time due to the growth of data volumes and advances\\nin hardware. Existing traditional optimizers struggle with the cumbersome\\nhand-tuning required for complex workloads, and the learning-based methods face\\nlimitations in ensuring generalization. With the great success of Large\\nLanguage Model (LLM) across diverse downstream tasks, this paper explores how\\nLLMs can be incorporated to enhance the generalization of learned optimizers.\\nThough promising, such an incorporation still presents challenges, mainly\\nincluding high model inference latency, and the substantial fine-tuning cost\\nand suboptimal performance due to inherent discrepancy between the token\\nsequences in LLM and structured SQL execution plans with rich numerical\\nfeatures.\\n  In this paper, we focus on recurring queries in offline optimization to\\nalleviate the issue of high inference latency, and propose \\\\textbf{LLM4Hint}\\nthat leverages moderate-sized backbone LLMs to recommend query optimization\\nhints. LLM4Hint achieves the goals through: (i) integrating a lightweight model\\nto produce a soft prompt, which captures the data distribution in DBMS and the\\nSQL predicates to provide sufficient optimization features while simultaneously\\nreducing the context length fed to the LLM, (ii) devising a query rewriting\\nstrategy using a larger commercial LLM, so as to simplify SQL semantics for the\\nbackbone LLM and reduce fine-tuning costs, and (iii) introducing an explicit\\nmatching prompt to facilitate alignment between the LLM and the lightweight\\nmodel, which can accelerate convergence of the combined model. Experiments show\\nthat LLM4Hint, by leveraging the LLM's stronger capability to understand the\\nquery statement, can outperform the state-of-the-art learned optimizers in\\nterms of both effectiveness and generalization.\"),\n",
       " Document(metadata={'title': 'Be the Change You Want to See: Revisiting Remote Sensing Change Detection Practices', 'authors': 'Blaž Rolih, Matic Fučka, Filip Wolf, Luka Čehovin Zajc', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03367v1'}, page_content='Remote sensing change detection aims to localize semantic changes between\\nimages of the same location captured at different times. In the past few years,\\nnewer methods have attributed enhanced performance to the additions of new and\\ncomplex components to existing architectures. Most fail to measure the\\nperformance contribution of fundamental design choices such as backbone\\nselection, pre-training strategies, and training configurations. We claim that\\nsuch fundamental design choices often improve performance even more\\nsignificantly than the addition of new architectural components. Due to that,\\nwe systematically revisit the design space of change detection models and\\nanalyse the full potential of a well-optimised baseline. We identify a set of\\nfundamental design choices that benefit both new and existing architectures.\\nLeveraging this insight, we demonstrate that when carefully designed, even an\\narchitecturally simple model can match or surpass state-of-the-art performance\\non six challenging change detection datasets. Our best practices generalise\\nbeyond our architecture and also offer performance improvements when applied to\\nrelated methods, indicating that the space of fundamental design choices has\\nbeen underexplored. Our guidelines and architecture provide a strong foundation\\nfor future methods, emphasizing that optimizing core components is just as\\nimportant as architectural novelty in advancing change detection performance.\\nCode: https://github.com/blaz-r/BTC-change-detection'),\n",
       " Document(metadata={'title': 'Backtesting Sentiment Signals for Trading: Evaluating the Viability of Alpha Generation from Sentiment Analysis', 'authors': 'Elvys Linhares Pontes, Carlos-Emiliano González-Gallardo, Georgeta Bordea, José G. Moreno, Mohamed Ben Jannet, Yuxuan Zhao, Antoine Doucet', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03350v1'}, page_content='Sentiment analysis, widely used in product reviews, also impacts financial\\nmarkets by influencing asset prices through microblogs and news articles.\\nDespite research in sentiment-driven finance, many studies focus on\\nsentence-level classification, overlooking its practical application in\\ntrading. This study bridges that gap by evaluating sentiment-based trading\\nstrategies for generating positive alpha. We conduct a backtesting analysis\\nusing sentiment predictions from three models (two classification and one\\nregression) applied to news articles on Dow Jones 30 stocks, comparing them to\\nthe benchmark Buy&Hold strategy. Results show all models produced positive\\nreturns, with the regression model achieving the highest return of 50.63% over\\n28 months, outperforming the benchmark Buy&Hold strategy. This highlights the\\npotential of sentiment in enhancing investment strategies and financial\\ndecision-making.'),\n",
       " Document(metadata={'title': 'Effects of structure on reasoning in instance-level Self-Discover', 'authors': 'Sachith Gunasekara, Yasiru Ratnayake', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03347v1'}, page_content='The drive for predictable LLM reasoning in their integration with compound\\nsystems has popularized structured outputs, yet concerns remain about\\nperformance trade-offs compared to unconstrained natural language. At the same\\ntime, training on unconstrained Chain of Thought (CoT) traces has brought about\\na new class of strong reasoning models that nevertheless present novel compute\\nbudget and faithfulness challenges. This paper introduces iSelf-Discover, an\\ninstance-level adaptation of the Self-Discover framework, and using it compares\\ndynamically generated structured JSON reasoning with its unstructured\\ncounterpart. Our empirical evaluation across diverse benchmarks using\\nstate-of-the-art open-source models supports a consistent advantage for\\nunstructured reasoning. Notably, on the complex MATH benchmark, unstructured\\nplans achieved relative performance improvements of up to 18.90\\\\% over\\nstructured approaches. Zero-shot unstructured iSelf-Discover variants are also\\nshown to outperform their five-shot structured counterparts, underscoring the\\nsignificance of this gap, even when structured plans are dynamically generated\\nto ensure reasoning precedes the final answer. We further demonstrate that the\\noptimal granularity of plan generation (instance-level vs. task-level) is\\ncontext-dependent. These findings invite re-evaluation of the reliance on\\nstructured formats for complex problem-solving and how compound systems should\\nbe organized.'),\n",
       " Document(metadata={'title': 'DESign: Dynamic Context-Aware Convolution and Efficient Subnet Regularization for Continuous Sign Language Recognition', 'authors': 'Sheng Liu, Yiheng Yu, Yuan Feng, Min Xu, Zhelun Jin, Yining Jiang, Tiantian Yuan', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03339v1'}, page_content='Current continuous sign language recognition (CSLR) methods struggle with\\nhandling diverse samples. Although dynamic convolutions are ideal for this\\ntask, they mainly focus on spatial modeling and fail to capture the temporal\\ndynamics and contextual dependencies. To address this, we propose DESign, a\\nnovel framework that incorporates Dynamic Context-Aware Convolution (DCAC) and\\nSubnet Regularization Connectionist Temporal Classification (SR-CTC). DCAC\\ndynamically captures the inter-frame motion cues that constitute signs and\\nuniquely adapts convolutional weights in a fine-grained manner based on\\ncontextual information, enabling the model to better generalize across diverse\\nsigning behaviors and boost recognition accuracy. Furthermore, we observe that\\nexisting methods still rely on only a limited number of frames for parameter\\nupdates during training, indicating that CTC learning overfits to a dominant\\npath. To address this, SR-CTC regularizes training by applying supervision to\\nsubnetworks, encouraging the model to explore diverse CTC alignment paths and\\neffectively preventing overfitting. A classifier-sharing strategy in SR-CTC\\nfurther strengthens multi-scale consistency. Notably, SR-CTC introduces no\\ninference overhead and can be seamlessly integrated into existing CSLR models\\nto boost performance. Extensive ablations and visualizations further validate\\nthe effectiveness of the proposed methods. Results on mainstream CSLR datasets\\n(i.e., PHOENIX14, PHOENIX14-T, CSL-Daily) demonstrate that DESign achieves\\nstate-of-the-art performance.'),\n",
       " Document(metadata={'title': 'Disambiguation-Centric Finetuning Makes Enterprise Tool-Calling LLMs More Realistic and Less Risky', 'authors': 'Ashutosh Hathidara, Julien Yu, Sebastian Schreiber', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03336v1'}, page_content='Large language models (LLMs) are increasingly tasked with invoking enterprise\\nAPIs, yet they routinely falter when near-duplicate tools vie for the same user\\nintent or when required arguments are left underspecified. We introduce\\nDiaFORGE (Dialogue Framework for Organic Response Generation & Evaluation), a\\ndisambiguation-centric, three-stage pipeline that (i) synthesizes\\npersona-driven, multi-turn dialogues in which the assistant must distinguish\\namong highly similar tools, (ii) performs supervised fine-tuning of open-source\\nmodels with reasoning traces across 3B - 70B parameters, and (iii) evaluates\\nreal-world readiness via a dynamic suite that redeploys each model in a live\\nagentic loop and reports end-to-end goal completion alongside conventional\\nstatic metrics. On our dynamic benchmark DiaBENCH, models trained with DiaFORGE\\nraise tool-invocation success by 27 pp over GPT-4o and by 49 pp over\\nClaude-3.5-Sonnet, both under optimized prompting. To spur further research, we\\nrelease an open corpus of 5000 production-grade enterprise API specifications\\npaired with rigorously validated, disambiguation-focused dialogues, offering a\\npractical blueprint for building reliable, enterprise-ready tool-calling\\nagents.'),\n",
       " Document(metadata={'title': 'De-Fake: Style based Anomaly Deepfake Detection', 'authors': 'Sudev Kumar Padhi, Harshit Kumar, Umesh Kashyap, Sk. Subidh Ali', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03334v1'}, page_content='Detecting deepfakes involving face-swaps presents a significant challenge,\\nparticularly in real-world scenarios where anyone can perform face-swapping\\nwith freely available tools and apps without any technical knowledge. Existing\\ndeepfake detection methods rely on facial landmarks or inconsistencies in\\npixel-level features and often struggle with face-swap deepfakes, where the\\nsource face is seamlessly blended into the target image or video. The\\nprevalence of face-swap is evident in everyday life, where it is used to spread\\nfalse information, damage reputations, manipulate political opinions, create\\nnon-consensual intimate deepfakes (NCID), and exploit children by enabling the\\ncreation of child sexual abuse material (CSAM). Even prominent public figures\\nare not immune to its impact, with numerous deepfakes of them circulating\\nwidely across social media platforms. Another challenge faced by deepfake\\ndetection methods is the creation of datasets that encompass a wide range of\\nvariations, as training models require substantial amounts of data. This raises\\nprivacy concerns, particularly regarding the processing and storage of personal\\nfacial data, which could lead to unauthorized access or misuse. Our key idea is\\nto identify these style discrepancies to detect face-swapped images effectively\\nwithout accessing the real facial image. We perform comprehensive evaluations\\nusing multiple datasets and face-swapping methods, which showcases the\\neffectiveness of SafeVision in detecting face-swap deepfakes across diverse\\nscenarios. SafeVision offers a reliable and scalable solution for detecting\\nface-swaps in a privacy preserving manner, making it particularly effective in\\nchallenging real-world applications. To the best of our knowledge, SafeVision\\nis the first deepfake detection using style features while providing inherent\\nprivacy protection.'),\n",
       " Document(metadata={'title': 'Task-Specific Generative Dataset Distillation with Difficulty-Guided Sampling', 'authors': 'Mingzhuo Li, Guang Li, Jiafeng Mao, Linfeng Ye, Takahiro Ogawa, Miki Haseyama', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03331v1'}, page_content='To alleviate the reliance of deep neural networks on large-scale datasets,\\ndataset distillation aims to generate compact, high-quality synthetic datasets\\nthat can achieve comparable performance to the original dataset. The\\nintegration of generative models has significantly advanced this field.\\nHowever, existing approaches primarily focus on aligning the distilled dataset\\nwith the original one, often overlooking task-specific information that can be\\ncritical for optimal downstream performance. In this paper, focusing on the\\ndownstream task of classification, we propose a task-specific sampling strategy\\nfor generative dataset distillation that incorporates the concept of difficulty\\nto consider the requirements of the target task better. The final dataset is\\nsampled from a larger image pool with a sampling distribution obtained by\\nmatching the difficulty distribution of the original dataset. A logarithmic\\ntransformation is applied as a pre-processing step to correct for\\ndistributional bias. The results of extensive experiments demonstrate the\\neffectiveness of our method and suggest its potential for enhancing performance\\non other downstream tasks.'),\n",
       " Document(metadata={'title': 'Exploring Object Status Recognition for Recipe Progress Tracking in Non-Visual Cooking', 'authors': 'Franklin Mingzhe Li, Kaitlyn Ng, Bin Zhu, Patrick Carrington', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03330v1'}, page_content='Cooking plays a vital role in everyday independence and well-being, yet\\nremains challenging for people with vision impairments due to limited support\\nfor tracking progress and receiving contextual feedback. Object status - the\\ncondition or transformation of ingredients and tools - offers a promising but\\nunderexplored foundation for context-aware cooking support. In this paper, we\\npresent OSCAR (Object Status Context Awareness for Recipes), a technical\\npipeline that explores the use of object status recognition to enable recipe\\nprogress tracking in non-visual cooking. OSCAR integrates recipe parsing,\\nobject status extraction, visual alignment with cooking steps, and time-causal\\nmodeling to support real-time step tracking. We evaluate OSCAR on 173\\ninstructional videos and a real-world dataset of 12 non-visual cooking sessions\\nrecorded by BLV individuals in their homes. Our results show that object status\\nconsistently improves step prediction accuracy across vision-language models,\\nand reveal key factors that impact performance in real-world conditions, such\\nas implicit tasks, camera placement, and lighting. We contribute the pipeline\\nof context-aware recipe progress tracking, an annotated real-world non-visual\\ncooking dataset, and design insights to guide future context-aware assistive\\ncooking systems.'),\n",
       " Document(metadata={'title': 'NDAI-NeuroMAP: A Neuroscience-Specific Embedding Model for Domain-Specific Retrieval', 'authors': 'Devendra Patel, Aaditya Jain, Jayant Verma, Divyansh Rajput, Sunil Mahala, Ketki Suresh Khapare, Jayateja Kalla', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03329v1'}, page_content='We present NDAI-NeuroMAP, the first neuroscience-domain-specific dense vector\\nembedding model engineered for high-precision information retrieval tasks. Our\\nmethodology encompasses the curation of an extensive domain-specific training\\ncorpus comprising 500,000 carefully constructed triplets\\n(query-positive-negative configurations), augmented with 250,000\\nneuroscience-specific definitional entries and 250,000 structured\\nknowledge-graph triplets derived from authoritative neurological ontologies. We\\nemploy a sophisticated fine-tuning approach utilizing the\\nFremyCompany/BioLORD-2023 foundation model, implementing a multi-objective\\noptimization framework combining contrastive learning with triplet-based metric\\nlearning paradigms. Comprehensive evaluation on a held-out test dataset\\ncomprising approximately 24,000 neuroscience-specific queries demonstrates\\nsubstantial performance improvements over state-of-the-art general-purpose and\\nbiomedical embedding models. These empirical findings underscore the critical\\nimportance of domain-specific embedding architectures for neuroscience-oriented\\nRAG systems and related clinical natural language processing applications.'),\n",
       " Document(metadata={'title': 'Read Quietly, Think Aloud: Decoupling Comprehension and Reasoning in LLMs', 'authors': 'Yuanxin Wang, Ganesh Venkatesh', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03327v1'}, page_content=\"Large Language Models (LLMs) have demonstrated remarkable proficiency in\\nunderstanding text and generating high-quality responses. However, a critical\\ndistinction from human cognition is their typical lack of a distinct internal\\n`reading' or deliberation phase before `speaking' (i.e., generating text).\\nHumans often engage in silent reading to comprehend context and formulate\\nthoughts prior to articulation. This paper investigates methods to imbue LLMs\\nwith a similar capacity for internal processing.\\n  We introduce and evaluate techniques that encourage LLMs to `read silently.'\\nOur findings indicate that even a straightforward approach, such as providing\\nthe model with an initial contextual prompt or `reading space' before it begins\\npredicting subsequent tokens for the final output, can yield significant\\nperformance improvements. We further enhance this concept by developing a\\n`reading buddy' architecture, where an auxiliary component silently processes\\nthe input and provides refined contextual insights to the primary generation\\nmodel. These approaches aim to foster deeper understanding from LLMs so that\\nthey can produce better reasoned responses, moving them one step closer to more\\nhuman-like text processing. Our results indicate that these simple techniques\\ncan provide surprisingly strong impact on accuracy with multiple point accuracy\\nboost.\"),\n",
       " Document(metadata={'title': 'Source-Free Domain Adaptation via Multi-view Contrastive Learning', 'authors': 'Amirfarhad Farhadi, Naser Mozayani, Azadeh Zamanifar', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03321v1'}, page_content='Domain adaptation has become a widely adopted approach in machine learning\\ndue to the high costs associated with labeling data. It is typically applied\\nwhen access to a labeled source domain is available. However, in real-world\\nscenarios, privacy concerns often restrict access to sensitive information,\\nsuch as fingerprints, bank account details, and facial images. A promising\\nsolution to this issue is Source-Free Unsupervised Domain Adaptation (SFUDA),\\nwhich enables domain adaptation without requiring access to labeled target\\ndomain data. Recent research demonstrates that SFUDA can effectively address\\ndomain discrepancies; however, two key challenges remain: (1) the low quality\\nof prototype samples, and (2) the incorrect assignment of pseudo-labels. To\\ntackle these challenges, we propose a method consisting of three main phases.\\nIn the first phase, we introduce a Reliable Sample Memory (RSM) module to\\nimprove the quality of prototypes by selecting more representative samples. In\\nthe second phase, we employ a Multi-View Contrastive Learning (MVCL) approach\\nto enhance pseudo-label quality by leveraging multiple data augmentations. In\\nthe final phase, we apply a noisy label filtering technique to further refine\\nthe pseudo-labels. Our experiments on three benchmark datasets - VisDA 2017,\\nOffice-Home, and Office-31 - demonstrate that our method achieves approximately\\n2 percent and 6 percent improvements in classification accuracy over the\\nsecond-best method and the average of 13 well-known state-of-the-art\\napproaches, respectively.'),\n",
       " Document(metadata={'title': 'Structure-Aware Compound-Protein Affinity Prediction via Graph Neural Network with Group Lasso Regularization', 'authors': 'Zanyu Shi, Yang Wang, Pathum Weerawarna, Jie Zhang, Timothy Richardson, Yijie Wang, Kun Huang', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03318v1'}, page_content='Explainable artificial intelligence (XAI) approaches have been increasingly\\napplied in drug discovery to learn molecular representations and identify\\nsubstructures driving property predictions. However, building end-to-end\\nexplainable machine learning models for structure-activity relationship (SAR)\\nmodeling for compound property prediction faces many challenges, such as\\nlimited activity data per target and the sensitivity of properties to subtle\\nmolecular changes. To address this, we leveraged activity-cliff molecule pairs,\\ni.e., compounds sharing a common scaffold but differing sharply in potency,\\ntargeting three proto-oncogene tyrosine-protein kinase Src proteins (i.e., PDB\\nIDs 1O42, 2H8H, and 4MXO). We implemented graph neural network (GNN) methods to\\nobtain atom-level feature information and predict compound-protein affinity\\n(i.e., half maximal inhibitory concentration, IC50). In addition, we trained\\nGNN models with different structure-aware loss functions to adequately leverage\\nmolecular property and structure information. We also utilized group lasso and\\nsparse group lasso to prune and highlight molecular subgraphs and enhance the\\nstructure-specific model explainability for the predicted property difference\\nin molecular activity-cliff pairs. We improved drug property prediction by\\nintegrating common and uncommon node information and using sparse group lasso,\\nreducing the average root mean squared error (RMSE) by 12.70%, and achieving\\nthe lowest averaged RMSE=0.2551 and the highest PCC=0.9572. Furthermore,\\napplying regularization enhances feature attribution methods that estimate the\\ncontribution of each atom in the molecular graphs by boosting global direction\\nscores and atom-level accuracy in atom coloring accuracy, which improves model\\ninterpretability in drug discovery pipelines, particularly in investigating\\nimportant molecular substructures in lead optimization.'),\n",
       " Document(metadata={'title': 'Partial Label Learning for Automated Theorem Proving', 'authors': 'Zsolt Zombori, Balázs Indruck', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03314v1'}, page_content='We formulate learning guided Automated Theorem Proving as Partial Label\\nLearning, building the first bridge across these fields of research and\\nproviding a theoretical framework for dealing with alternative proofs during\\nlearning. We use the plCoP theorem prover to demonstrate that methods from the\\nPartial Label Learning literature tend to increase the performance of learning\\nassisted theorem provers.'),\n",
       " Document(metadata={'title': 'Personalized Image Generation from an Author Writing Style', 'authors': 'Sagar Gandhi, Vishal Gandhi', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03313v1'}, page_content=\"Translating nuanced, textually-defined authorial writing styles into\\ncompelling visual representations presents a novel challenge in generative AI.\\nThis paper introduces a pipeline that leverages Author Writing Sheets (AWS) -\\nstructured summaries of an author's literary characteristics - as input to a\\nLarge Language Model (LLM, Claude 3.7 Sonnet). The LLM interprets the AWS to\\ngenerate three distinct, descriptive text-to-image prompts, which are then\\nrendered by a diffusion model (Stable Diffusion 3.5 Medium). We evaluated our\\napproach using 49 author styles from Reddit data, with human evaluators\\nassessing the stylistic match and visual distinctiveness of the generated\\nimages. Results indicate a good perceived alignment between the generated\\nvisuals and the textual authorial profiles (mean style match: $4.08/5$), with\\nimages rated as moderately distinctive. Qualitative analysis further\\nhighlighted the pipeline's ability to capture mood and atmosphere, while also\\nidentifying challenges in representing highly abstract narrative elements. This\\nwork contributes a novel end-to-end methodology for visual authorial style\\npersonalization and provides an initial empirical validation, opening avenues\\nfor applications in creative assistance and cross-modal understanding.\"),\n",
       " Document(metadata={'title': 'GRAFT: A Graph-based Flow-aware Agentic Framework for Document-level Machine Translation', 'authors': 'Himanshu Dutta, Sunny Manchanda, Prakhar Bapat, Meva Ram Gurjar, Pushpak Bhattacharyya', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03311v1'}, page_content='Document level Machine Translation (DocMT) approaches often struggle with\\neffectively capturing discourse level phenomena. Existing approaches rely on\\nheuristic rules to segment documents into discourse units, which rarely align\\nwith the true discourse structure required for accurate translation. Otherwise,\\nthey fail to maintain consistency throughout the document during translation.\\nTo address these challenges, we propose Graph Augmented Agentic Framework for\\nDocument Level Translation (GRAFT), a novel graph based DocMT system that\\nleverages Large Language Model (LLM) agents for document translation. Our\\napproach integrates segmentation, directed acyclic graph (DAG) based dependency\\nmodelling, and discourse aware translation into a cohesive framework.\\nExperiments conducted across eight translation directions and six diverse\\ndomains demonstrate that GRAFT achieves significant performance gains over\\nstate of the art DocMT systems. Specifically, GRAFT delivers an average\\nimprovement of 2.8 d BLEU on the TED test sets from IWSLT2017 over strong\\nbaselines and 2.3 d BLEU for domain specific translation from English to\\nChinese. Moreover, our analyses highlight the consistent ability of GRAFT to\\naddress discourse level phenomena, yielding coherent and contextually accurate\\ntranslations.'),\n",
       " Document(metadata={'title': 'ReTimeCausal: EM-Augmented Additive Noise Models for Interpretable Causal Discovery in Irregular Time Series', 'authors': 'Weihong Li, Anpeng Wu, Kun Kuang, Keting Yin', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03310v1'}, page_content='This paper studies causal discovery in irregularly sampled time series-a\\npivotal challenge in high-stakes domains like finance, healthcare, and climate\\nscience, where missing data and inconsistent sampling frequencies distort\\ncausal mechanisms. Traditional methods (e.g., Granger causality, PCMCI) fail to\\nreconcile multi-scale interactions (e.g., hourly storms vs. decadal climate\\nshifts), while neural approaches (e.g., CUTS+) lack interpretability, stemming\\nfrom a critical gap: existing frameworks either rigidly assume temporal\\nregularity or aggregate dynamics into opaque representations, neglecting\\nreal-world granularity and auditable logic. To bridge this gap, we propose\\nReTimeCausal, a novel integration of Additive Noise Models (ANM) and\\nExpectation-Maximization (EM) that unifies physics-guided data imputation with\\nsparse causal inference. Through kernelized sparse regression and structural\\nconstraints, ReTimeCausal iteratively refines missing values (E-step) and\\ncausal graphs (M-step), resolving cross-frequency dependencies and missing data\\nissues. Extensive experiments on synthetic and real-world datasets demonstrate\\nthat ReTimeCausal outperforms existing state-of-the-art methods under\\nchallenging irregular sampling and missing data conditions.'),\n",
       " Document(metadata={'title': 'Scaffolding Recursive Divergence and Convergence in Story Ideation', 'authors': 'Taewook Kim, Matthew Kay, Yuqian Sun, Melissa Roemmele, Max Kreminski, John Joon Young Chung', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03307v1'}, page_content='Human creative ideation involves both exploration of diverse ideas\\n(divergence) and selective synthesis of explored ideas into coherent\\ncombinations (convergence). While processes of divergence and convergence are\\noften interleaved and nested, existing AI-powered creativity support tools\\n(CSTs) lack support for sophisticated orchestration of divergence and\\nconvergence. We present Reverger, an AI-powered CST that helps users ideate\\nvariations of conceptual directions for modifying a story by scaffolding\\nflexible iteration between divergence and convergence. For divergence, our tool\\nenables recursive exploration of alternative high-level directions for\\nmodifying a specific part of the original story. For convergence, it allows\\nusers to collect explored high-level directions and synthesize them into\\nconcrete variations. Users can then iterate between divergence and convergence\\nuntil they find a satisfactory outcome. A within-subject study revealed that\\nReverger permitted participants to explore more unexpected and diverse\\nhigh-level directions than a comparable baseline. Reverger users also felt that\\nthey had more fine-grained control and discovered more effort-worthy outcomes.'),\n",
       " Document(metadata={'title': 'Leveraging Out-of-Distribution Unlabeled Images: Semi-Supervised Semantic Segmentation with an Open-Vocabulary Model', 'authors': 'Wooseok Shin, Jisu Kang, Hyeonki Jeong, Jin Sob Kim, Sung Won Han', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03302v1'}, page_content='In semi-supervised semantic segmentation, existing studies have shown\\npromising results in academic settings with controlled splits of benchmark\\ndatasets. However, the potential benefits of leveraging significantly larger\\nsets of unlabeled images remain unexplored. In real-world scenarios, abundant\\nunlabeled images are often available from online sources (web-scraped images)\\nor large-scale datasets. However, these images may have different distributions\\nfrom those of the target dataset, a situation known as out-of-distribution\\n(OOD). Using these images as unlabeled data in semi-supervised learning can\\nlead to inaccurate pseudo-labels, potentially misguiding network training. In\\nthis paper, we propose a new semi-supervised semantic segmentation framework\\nwith an open-vocabulary segmentation model (SemiOVS) to effectively utilize\\nunlabeled OOD images. Extensive experiments on Pascal VOC and Context datasets\\ndemonstrate two key findings: (1) using additional unlabeled images improves\\nthe performance of semi-supervised learners in scenarios with few labels, and\\n(2) using the open-vocabulary segmentation (OVS) model to pseudo-label OOD\\nimages leads to substantial performance gains. In particular, SemiOVS\\noutperforms existing PrevMatch and SemiVL methods by +3.5 and +3.0 mIoU,\\nrespectively, on Pascal VOC with a 92-label setting, achieving state-of-the-art\\nperformance. These findings demonstrate that our approach effectively utilizes\\nabundant unlabeled OOD images for semantic segmentation tasks. We hope this\\nwork can inspire future research and real-world applications. The code is\\navailable at https://github.com/wooseok-shin/SemiOVS'),\n",
       " Document(metadata={'title': 'MGAA: Multi-Granular Adaptive Allocation fof Low-Rank Compression of LLMs', 'authors': 'Guangyan Li, Yongqiang Tang, Wensheng Zhang', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03294v1'}, page_content=\"The enormous parameter scale of large language models (LLMs) has made model\\ncompression a research hotspot, which aims to alleviate computational resource\\ndemands during deployment and inference. As a promising direction, low-rank\\napproximation technique has made remarkable achievements. Nevertheless,\\nunfortunately, the vast majority of studies to low-rank approximation\\ncompression generally apply uniform compression ratios across all weight\\nmatrices, while disregarding their inherently differentiated impacts on the\\nmodel's performance. Although a few recent work attempts to employ heuristic\\nsearch strategies to achieve the optimal parameter allocation, such strategies\\nare computationally inefficient and lose the generalization ability in the era\\nof LLMs. In this study, we propose a novel parameter Multi-Granular Adaptive\\nAllocation (MGAA) method, which can adaptively allocate parameters between and\\nwithin sublayers without task-specific evaluations in the compression process.\\nMGAA consists of two components: 1) Among different sublayers, it assigns\\ncompression ratios based on their cosine similarity between inputs and outputs,\\nallowing for a more tailored compression in sublayers with varying degrees of\\nimportance, and 2) Within each sublayer, it allocates different compression\\nratios to weight matrices based on their energy distribution characteristics,\\nensuring a consistent energy retention ratio while optimizing compression\\nefficiency. Comprehensive evaluations of MGAA across multiple LLMs backbone\\nmodels and benchmark datasets demonstrate its superior performance.\\nAdditionally, we apply our MGAA to multimodal model LLaVA, exhibiting\\nremarkable performance improvements.\"),\n",
       " Document(metadata={'title': 'LTLCrit: A Temporal Logic-based LLM Critic for Safe and Efficient Embodied Agents', 'authors': 'Anand Gokhale, Vaibhav Srivastava, Francesco Bullo', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03293v1'}, page_content='Large language models (LLMs) have demonstrated promise in reasoning tasks and\\ngeneral decision-making in static environments. In long-term planning tasks,\\nhowever, errors tend to accumulate, often leading to unsafe or inefficient\\nbehavior, limiting their use in general-purpose settings. We propose a modular\\nactor-critic architecture in which an LLM actor is guided by LTLCrit, a\\ntrajectory-level LLM critic that communicates via linear temporal logic (LTL).\\nOur setup combines the reasoning strengths of language models with the\\nguarantees of formal logic. The actor selects high-level actions from natural\\nlanguage observations, while the critic analyzes full trajectories and proposes\\nnew LTL constraints that shield the actor from future unsafe or inefficient\\nbehavior. The architecture supports both fixed, hand-specified safety\\nconstraints and adaptive, learned soft constraints that promote long-term\\nefficiency. Our architecture is model-agnostic: any LLM-based planner can serve\\nas the actor, and LTLCrit serves as a logic-generating wrapper. We formalize\\nplanning as graph traversal under symbolic constraints, allowing LTLCrit to\\nanalyze failed or suboptimal trajectories and generate new temporal logic rules\\nthat improve future behavior. We evaluate our system on the Minecraft\\ndiamond-mining benchmark, achieving 100% completion rates and improving\\nefficiency compared to baseline LLM planners. Our results suggest that enabling\\nLLMs to supervise each other through logic is a powerful and flexible paradigm\\nfor safe, generalizable decision making.'),\n",
       " Document(metadata={'title': 'Memory Mosaics at scale', 'authors': 'Jianyu Zhang, Léon Bottou', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03285v1'}, page_content='Memory Mosaics [Zhang et al., 2025], networks of associative memories, have\\ndemonstrated appealing compositional and in-context learning capabilities on\\nmedium-scale networks (GPT-2 scale) and synthetic small datasets. This work\\nshows that these favorable properties remain when we scale memory mosaics to\\nlarge language model sizes (llama-8B scale) and real-world datasets.\\n  To this end, we scale memory mosaics to 10B size, we train them on one\\ntrillion tokens, we introduce a couple architectural modifications (\"Memory\\nMosaics v2\"), we assess their capabilities across three evaluation dimensions:\\ntraining-knowledge storage, new-knowledge storage, and in-context learning.\\n  Throughout the evaluation, memory mosaics v2 match transformers on the\\nlearning of training knowledge (first dimension) and significantly outperforms\\ntransformers on carrying out new tasks at inference time (second and third\\ndimensions). These improvements cannot be easily replicated by simply\\nincreasing the training data for transformers. A memory mosaics v2 trained on\\none trillion tokens still perform better on these tasks than a transformer\\ntrained on eight trillion tokens.'),\n",
       " Document(metadata={'title': 'Conformal Information Pursuit for Interactively Guiding Large Language Models', 'authors': 'Kwan Ho Ryan Chan, Yuyan Ge, Edgar Dobriban, Hamed Hassani, René Vidal', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03279v1'}, page_content='A significant use case of instruction-finetuned Large Language Models (LLMs)\\nis to solve question-answering tasks interactively. In this setting, an LLM\\nagent is tasked with making a prediction by sequentially querying relevant\\ninformation from the user, as opposed to a single-turn conversation. This paper\\nexplores sequential querying strategies that aim to minimize the expected\\nnumber of queries. One such strategy is Information Pursuit (IP), a greedy\\nalgorithm that at each iteration selects the query that maximizes information\\ngain or equivalently minimizes uncertainty. However, obtaining accurate\\nestimates of mutual information or conditional entropy for LLMs is very\\ndifficult in practice due to over- or under-confident LLM probabilities, which\\nleads to suboptimal query selection and predictive performance. To better\\nestimate the uncertainty at each iteration, we propose Conformal Information\\nPursuit (C-IP), an alternative approach to sequential information gain based on\\nconformal prediction sets. More specifically, C-IP leverages a relationship\\nbetween prediction sets and conditional entropy at each iteration to estimate\\nuncertainty based on the average size of conformal prediction sets. In contrast\\nto conditional entropy, we find that conformal prediction sets are a\\ndistribution-free and robust method of measuring uncertainty. Experiments with\\n20 Questions show that C-IP obtains better predictive performance and shorter\\nquery-answer chains compared to previous approaches to IP and uncertainty-based\\nchain-of-thought methods. Furthermore, extending to an interactive medical\\nsetting between a doctor and a patient on the MediQ dataset, C-IP achieves\\ncompetitive performance with direct single-turn prediction while offering\\ngreater interpretability.'),\n",
       " Document(metadata={'title': 'GDGB: A Benchmark for Generative Dynamic Text-Attributed Graph Learning', 'authors': 'Jie Peng, Jiarui Ji, Runlin Lei, Zhewei Wei, Yongchao Liu, Chuntao Hong', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03267v1'}, page_content='Dynamic Text-Attributed Graphs (DyTAGs), which intricately integrate\\nstructural, temporal, and textual attributes, are crucial for modeling complex\\nreal-world systems. However, most of the existing DyTAG datasets exhibit poor\\ntextual quality, which severely limits their utility for DyTAG generation tasks\\nrequiring semantically rich inputs. Additionally, prior work mainly focuses on\\ndiscriminative tasks on DyTAGs, resulting in a lack of standardized task\\nformulations and evaluation protocols tailored for DyTAG generation. To address\\nthese critical issues, we propose Generative DyTAG Benchmark (GDGB), which\\ncomprises eight meticulously curated DyTAG datasets with high-quality textual\\nfeatures for both nodes and edges, overcoming limitations of prior datasets.\\nBuilding on GDGB, we define two novel DyTAG generation tasks: Transductive\\nDynamic Graph Generation (TDGG) and Inductive Dynamic Graph Generation (IDGG).\\nTDGG transductively generates a target DyTAG based on the given source and\\ndestination node sets, while the more challenging IDGG introduces new node\\ngeneration to inductively model the dynamic expansion of real-world graph data.\\nTo enable holistic evaluation, we design multifaceted metrics that assess the\\nstructural, temporal, and textual quality of the generated DyTAGs. We further\\npropose GAG-General, an LLM-based multi-agent generative framework tailored for\\nreproducible and robust benchmarking of DyTAG generation. Experimental results\\ndemonstrate that GDGB enables rigorous evaluation of TDGG and IDGG, with key\\ninsights revealing the critical interplay of structural and textual features in\\nDyTAG generation. These findings establish GDGB as a foundational resource for\\nadvancing generative DyTAG research and unlocking further practical\\napplications in DyTAG generation. GDGB datasets, source codes, and leaderboards\\nare available at \\\\href{https://gdgb-algo.github.io/}{here}.'),\n",
       " Document(metadata={'title': 'Investigating Redundancy in Multimodal Large Language Models with Multiple Vision Encoders', 'authors': 'Song Mao, Yang Chen, Pinglong Cai, Ding Wang, Guohang Yan, Zhi Yu, Botian Shi', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03262v1'}, page_content=\"Multimodal Large Language Models (MLLMs) increasingly adopt multiple vision\\nencoders to capture diverse visual information, ranging from coarse semantics\\nto fine grained details. While this approach is intended to enhance visual\\nunderstanding capability, we observe that the performance gains from adding\\nencoders often diminish and can even lead to performance degradation, a\\nphenomenon we term encoder redundancy. This paper presents a systematic\\ninvestigation into this issue. Through comprehensive ablation studies on state\\nof the art multi encoder MLLMs, we empirically demonstrate that significant\\nredundancy exists. To quantify each encoder's unique contribution, we propose a\\nprincipled metric: the Conditional Utilization Rate (CUR). Building on CUR, we\\nintroduce the Information Gap (IG) to capture the overall disparity in encoder\\nutility within a model.Our experiments reveal that certain vision encoders\\ncontribute little, or even negatively, to overall performance, confirming\\nsubstantial redundancy. Our experiments reveal that certain vision encoders\\ncontribute minimally, or even negatively, to the model's performance,\\nconfirming the prevalence of redundancy. These findings highlight critical\\ninefficiencies in current multi encoder designs and establish that our proposed\\nmetrics can serve as valuable diagnostic tools for developing more efficient\\nand effective multimodal architectures.\"),\n",
       " Document(metadata={'title': 'ForgeHLS: A Large-Scale, Open-Source Dataset for High-Level Synthesis', 'authors': 'Zedong Peng, Zeju Li, Mingzhe Gao, Qiang Xu, Chen Zhang, Jieru Zhao', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03255v1'}, page_content=\"We introduce ForgeEDA, an open-source comprehensive circuit dataset across\\nvarious categories. ForgeEDA includes diverse circuit representations such as\\nRegister Transfer Level (RTL) code, Post-mapping (PM) netlists, And-Inverter\\nGraphs (AIGs), and placed netlists, enabling comprehensive analysis and\\ndevelopment. We demonstrate ForgeEDA's utility by benchmarking state-of-the-art\\nEDA algorithms on critical tasks such as Power, Performance, and Area (PPA)\\noptimization, highlighting its ability to expose performance gaps and drive\\nadvancements. Additionally, ForgeEDA's scale and diversity facilitate the\\ntraining of AI models for EDA tasks, demonstrating its potential to improve\\nmodel performance and generalization. By addressing limitations in existing\\ndatasets, ForgeEDA aims to catalyze breakthroughs in modern IC design and\\nsupport the next generation of innovations in EDA.\"),\n",
       " Document(metadata={'title': 'CodeAgents: A Token-Efficient Framework for Codified Multi-Agent Reasoning in LLMs', 'authors': 'Bruce Yang, Xinfeng He, Huan Gao, Yifan Cao, Xiaofan Li, David Hsu', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03254v1'}, page_content='Effective prompt design is essential for improving the planning capabilities\\nof large language model (LLM)-driven agents. However, existing structured\\nprompting strategies are typically limited to single-agent, plan-only settings,\\nand often evaluate performance solely based on task accuracy - overlooking\\ncritical factors such as token efficiency, modularity, and scalability in\\nmulti-agent environments. To address these limitations, we introduce\\nCodeAgents, a prompting framework that codifies multi-agent reasoning and\\nenables structured, token-efficient planning in multi-agent systems. In\\nCodeAgents, all components of agent interaction - Task, Plan, Feedback, system\\nroles, and external tool invocations - are codified into modular pseudocode\\nenriched with control structures (e.g., loops, conditionals), boolean logic,\\nand typed variables. This design transforms loosely connected agent plans into\\ncohesive, interpretable, and verifiable multi-agent reasoning programs. We\\nevaluate the proposed framework across three diverse benchmarks - GAIA,\\nHotpotQA, and VirtualHome - using a range of representative LLMs. Results show\\nconsistent improvements in planning performance, with absolute gains of 3-36\\npercentage points over natural language prompting baselines. On VirtualHome,\\nour method achieves a new state-of-the-art success rate of 56%. In addition,\\nour approach reduces input and output token usage by 55-87% and 41-70%,\\nrespectively, underscoring the importance of token-aware evaluation metrics in\\nthe development of scalable multi-agent LLM systems. The code and resources are\\navailable at: https://anonymous.4open.science/r/CodifyingAgent-5A86'),\n",
       " Document(metadata={'title': 'RefineX: Learning to Refine Pre-training Data at Scale from Expert-Guided Programs', 'authors': 'Baolong Bi, Shenghua Liu, Xingzhang Ren, Dayiheng Liu, Junyang Lin, Yiwei Wang, Lingrui Mei, Junfeng Fang, Jiafeng Guo, Xueqi Cheng', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03253v1'}, page_content='The foundational capabilities of large language models (LLMs) are deeply\\ninfluenced by the quality of their pre-training corpora. However, enhancing\\ndata quality at scale remains a significant challenge, primarily due to the\\ntrade-off between refinement effectiveness and processing efficiency. While\\nrule-based filtering remains the dominant paradigm, it typically operates at\\nthe document level and lacks the granularity needed to refine specific content\\nwithin documents. Inspired by emerging work such as ProX, we propose\\n$\\\\textbf{RefineX}$, a novel framework for large-scale, surgical refinement of\\npre-training data through programmatic editing tasks. RefineX enables efficient\\nand fine-grained data refinement while reliably preserving the diversity and\\nnaturalness of raw text. The core strength of RefineX lies in distilling\\nhigh-quality, expert-guided end-to-end refinement results into minimal\\nedit-based deletion programs. This high-precision distillation pipeline is used\\nto train an efficient and reliable refine model that can systematically improve\\nevery instance in the corpus at scale. We evaluate RefineX across from-scratch\\npre-training at multiple model scales and find that it consistently outperforms\\nmodels trained on raw, filtered, or alternatively refined data across diverse\\ndownstream tasks. On the 750M model, RefineX yields 2.6%-7.2% average gains on\\nlighteval tasks, and achieves comparable performance using significantly fewer\\ntraining tokens. Further analysis shows that RefineX reliably enhances text\\nquality with both high efficiency and precision, outperforming prior approaches\\nsuch as end-to-end generation and Prox-C. These results position RefineX as a\\nscalable, effective, and reliable solution for optimizing pre-training data in\\nmodern LLM pipelines.'),\n",
       " Document(metadata={'title': 'Toward Efficient Speech Emotion Recognition via Spectral Learning and Attention', 'authors': 'HyeYoung Lee, Muhammad Nadeem', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03251v1'}, page_content='Speech Emotion Recognition (SER) traditionally relies on auditory data\\nanalysis for emotion classification. Several studies have adopted different\\nmethods for SER. However, existing SER methods often struggle to capture subtle\\nemotional variations and generalize across diverse datasets. In this article,\\nwe use Mel-Frequency Cepstral Coefficients (MFCCs) as spectral features to\\nbridge the gap between computational emotion processing and human auditory\\nperception. To further improve robustness and feature diversity, we propose a\\nnovel 1D-CNN-based SER framework that integrates data augmentation techniques.\\nMFCC features extracted from the augmented data are processed using a 1D\\nConvolutional Neural Network (CNN) architecture enhanced with channel and\\nspatial attention mechanisms. These attention modules allow the model to\\nhighlight key emotional patterns, enhancing its ability to capture subtle\\nvariations in speech signals. The proposed method delivers cutting-edge\\nperformance, achieving the accuracy of 97.49% for SAVEE, 99.23% for RAVDESS,\\n89.31% for CREMA-D, 99.82% for TESS, 99.53% for EMO-DB, and 96.39% for EMOVO.\\nExperimental results show new benchmarks in SER, demonstrating the\\neffectiveness of our approach in recognizing emotional expressions with high\\nprecision. Our evaluation demonstrates that the integration of advanced Deep\\nLearning (DL) methods substantially enhances generalization across diverse\\ndatasets, underscoring their potential to advance SER for real-world deployment\\nin assistive technologies and human-computer interaction.'),\n",
       " Document(metadata={'title': 'On Jailbreaking Quantized Language Models Through Fault Injection Attacks', 'authors': 'Noureldin Zahran, Ahmad Tahmasivand, Ihsen Alouani, Khaled Khasawneh, Mohammed E. Fouda', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03236v1'}, page_content='The safety alignment of Language Models (LMs) is a critical concern, yet\\ntheir integrity can be challenged by direct parameter manipulation attacks,\\nsuch as those potentially induced by fault injection. As LMs are increasingly\\ndeployed using low-precision quantization for efficiency, this paper\\ninvestigates the efficacy of such attacks for jailbreaking aligned LMs across\\ndifferent quantization schemes. We propose gradient-guided attacks, including a\\ntailored progressive bit-level search algorithm introduced herein and a\\ncomparative word-level (single weight update) attack. Our evaluation on\\nLlama-3.2-3B, Phi-4-mini, and Llama-3-8B across FP16 (baseline), and\\nweight-only quantization (FP8, INT8, INT4) reveals that quantization\\nsignificantly influences attack success. While attacks readily achieve high\\nsuccess (>80\\\\% Attack Success Rate, ASR) on FP16 models, within an attack\\nbudget of 25 perturbations, FP8 and INT8 models exhibit ASRs below 20\\\\% and\\n50\\\\%, respectively. Increasing the perturbation budget up to 150 bit-flips, FP8\\nmodels maintained ASR below 65\\\\%, demonstrating some resilience compared to\\nINT8 and INT4 models that have high ASR. In addition, analysis of perturbation\\nlocations revealed differing architectural targets across quantization schemes,\\nwith (FP16, INT4) and (INT8, FP8) showing similar characteristics. Besides,\\njailbreaks induced in FP16 models were highly transferable to subsequent\\nFP8/INT8 quantization (<5\\\\% ASR difference), though INT4 significantly reduced\\ntransferred ASR (avg. 35\\\\% drop). These findings highlight that while common\\nquantization schemes, particularly FP8, increase the difficulty of direct\\nparameter manipulation jailbreaks, vulnerabilities can still persist,\\nespecially through post-attack quantization.'),\n",
       " Document(metadata={'title': 'Efficient Knowledge Graph Construction and Retrieval from Unstructured Text for Large-Scale RAG Systems', 'authors': 'Congmin Min, Rhea Mathew, Joyce Pan, Sahil Bansal, Abbas Keshavarzi, Amar Viswanathan Kannan', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03226v1'}, page_content='We propose a scalable and cost-efficient framework for deploying Graph-based\\nRetrieval Augmented Generation (GraphRAG) in enterprise environments. While\\nGraphRAG has shown promise for multi-hop reasoning and structured retrieval,\\nits adoption has been limited by the high computational cost of constructing\\nknowledge graphs using large language models (LLMs) and the latency of\\ngraph-based retrieval. To address these challenges, we introduce two core\\ninnovations: (1) a dependency-based knowledge graph construction pipeline that\\nleverages industrial-grade NLP libraries to extract entities and relations from\\nunstructured text completely eliminating reliance on LLMs; and (2) a\\nlightweight graph retrieval strategy that combines hybrid query node\\nidentification with efficient one-hop traversal for high-recall, low-latency\\nsubgraph extraction. We evaluate our framework on two SAP datasets focused on\\nlegacy code migration and demonstrate strong empirical performance. Our system\\nachieves up to 15% and 4.35% improvements over traditional RAG baselines based\\non LLM-as-Judge and RAGAS metrics, respectively. Moreover, our dependency-based\\nconstruction approach attains 94% of the performance of LLM-generated knowledge\\ngraphs (61.87% vs. 65.83%) while significantly reducing cost and improving\\nscalability. These results validate the feasibility of deploying GraphRAG\\nsystems in real-world, large-scale enterprise applications without incurring\\nprohibitive resource requirements paving the way for practical, explainable,\\nand domain-adaptable retrieval-augmented reasoning.'),\n",
       " Document(metadata={'title': 'SI-Agent: An Agentic Framework for Feedback-Driven Generation and Tuning of Human-Readable System Instructions for Large Language Models', 'authors': 'Jeshwanth Challagundla', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.03223v1'}, page_content='System Instructions (SIs), or system prompts, are pivotal for guiding Large\\nLanguage Models (LLMs) but manual crafting is resource-intensive and often\\nsuboptimal. Existing automated methods frequently generate non-human-readable\\n\"soft prompts,\" sacrificing interpretability. This paper introduces SI-Agent, a\\nnovel agentic framework designed to automatically generate and iteratively\\nrefine human-readable SIs through a feedback-driven loop. SI-Agent employs\\nthree collaborating agents: an Instructor Agent, an Instruction Follower Agent\\n(target LLM), and a Feedback/Reward Agent evaluating task performance and\\noptionally SI readability. The framework utilizes iterative cycles where\\nfeedback guides the Instructor\\'s refinement strategy (e.g., LLM-based editing,\\nevolutionary algorithms). We detail the framework\\'s architecture, agent roles,\\nthe iterative refinement process, and contrast it with existing methods. We\\npresent experimental results validating SI-Agent\\'s effectiveness, focusing on\\nmetrics for task performance, SI readability, and efficiency. Our findings\\nindicate that SI-Agent generates effective, readable SIs, offering a favorable\\ntrade-off between performance and interpretability compared to baselines.\\nPotential implications include democratizing LLM customization and enhancing\\nmodel transparency. Challenges related to computational cost and feedback\\nreliability are acknowledged.'),\n",
       " Document(metadata={'title': 'The role of gain neuromodulation in layer-5 pyramidal neurons', 'authors': 'Alejandro Rodriguez-Garcia, Christopher J. Whyte, Brandon R. Munn, Jie Mei, James M. Shine, Srikanth Ramaswamy', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.03222v1'}, page_content='Biological and artificial learning systems alike confront the\\nplasticity-stability dilemma. In the brain, neuromodulators such as\\nacetylcholine and noradrenaline relieve this tension by tuning neuronal gain\\nand inhibitory gating, balancing segregation and integration of circuits. Fed\\nby dense cholinergic and noradrenergic projections from the ascending arousal\\nsystem, layer-5 pyramidal neurons in the cerebral cortex offer a relevant\\nsubstrate for understanding these dynamics. When distal dendritic signals\\ncoincide with back-propagating action potentials, calcium plateaus turn a\\nsingle somatic spike into a high-gain burst, and interneuron inhibition sculpts\\nthe output. These properties make layer-5 cells gain-tunable amplifiers that\\ntranslate neuromodulatory cues into flexible cortical activity. To capture this\\nmechanism we developed a two-compartment Izhikevich model for pyramidal neurons\\nand single-compartment somatostatin (SOM) and parvalbumin (PV) interneurons,\\nlinked by Gaussian connectivity and spike-timing-dependent plasticity (STDP).\\nThe soma and apical dendrite are so coupled that somatic spikes back-propagate,\\nwhile dendritic plateaus can switch the soma from regular firing to bursting by\\nshifting reset and adaptation variables. We show that stronger dendritic drive\\nor tighter coupling raise gain by increasing the likelihood of\\ncalcium-triggered somatic bursts. In contrast, dendritic-targeted inhibition\\nsuppresses gain, while somatic-targeted inhibition raises the firing threshold\\nof neighboring neurons, thus gating neurons output. Notably, bursting\\naccelerates STDP, supporting rapid synaptic reconfiguration and\\nflexibility.This suggests that brief gain pulses driven by neuromodulators\\ncould serve as an adaptive two-timescale optimization mechanism, effectively\\nmodulating the synaptic weight updates.'),\n",
       " Document(metadata={'title': 'Neural Inhibition Improves Dynamic Routing and Mixture of Experts', 'authors': 'Will Y. Zou, Jennifer Y. Zhang', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.03221v1'}, page_content='To be effective, efficient, and diverse, deep learning models need to\\ndynamically choose its architecture based on signals from a population of\\nneurons. We hypothesize dynamic routing models can be improved with neural\\ninhibition in those neural populations. This means signals commonly shared\\namong the various modes of data statistics can be inhibited so that the routing\\nmodel can choose a specialized expert path for each data sample. Only through\\ninhibition is the routing mechanism able to effectively select neural pathways.\\nWe believe this is an under-studied and under-verified implementation\\nmethodology for Mixture-of-Experts, dynamic routing, and transformer language\\nmodels. We provide experimental evidence that the neural inhibition algorithm\\nsignificantly boosts the performance of general tasks and motivates more effort\\nto be invested in this research direction.'),\n",
       " Document(metadata={'title': 'Symbiosis: Multi-Adapter Inference and Fine-Tuning', 'authors': 'Saransh Gupta, Umesh Deshpande, Travis Janssen, Swami Sundararaman', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.03220v1'}, page_content='Parameter-efficient fine-tuning (PEFT) allows model builders to capture the\\ntask specific parameters into adapters, which are a fraction of the size of the\\noriginal base model. Popularity of PEFT technique for fine-tuning has led to\\ncreation of a large number of adapters for popular Large Language Models\\n(LLMs). However, existing frameworks fall short in supporting inference or\\nfine-tuning with multiple adapters in the following ways. 1) For fine-tuning,\\neach job needs to deploy its dedicated base model instance, which results in\\nexcessive GPU memory consumption and poor GPU utilization. 2) While popular\\ninference platforms can serve multiple PEFT adapters, they do not allow\\nindependent resource management or mixing of different PEFT methods. 3) They\\ncannot share resources (such as base model instance) between inference and\\nfine-tuning jobs. 4) They do not provide privacy to users who may not wish to\\nexpose their fine-tuned parameters to service providers. In Symbiosis, we\\naddress the above problems by enabling as-a-service deployment of base model.\\nThe base model layers can be shared across multiple inference or fine-tuning\\nprocesses. Our split-execution technique decouples the execution of\\nclient-specific adapters and layers from the frozen base model layers offering\\nthem flexibility to manage their resources, to select their fine-tuning method,\\nto achieve their performance goals. Our approach is transparent to models and\\nworks out-of-the-box for most models in the transformers library. Our\\nevaluation on Llama2-13B shows the compared to baseline, Symbiosis can\\nfine-tune 4X more adapters on the same set of GPUs in the same amount of time.'),\n",
       " Document(metadata={'title': 'Disclosing Generative AI Use in Digital Humanities Research', 'authors': 'Rongqian Ma, Xuhan Zhang, Adrian Wisnicki', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.03216v1'}, page_content=\"This survey study investigates how digital humanists perceive and approach\\ngenerative AI disclosure in research. The results indicate that while digital\\nhumanities scholars acknowledge the importance of disclosing GenAI use, the\\nactual rate of disclosure in research practice remains low. Respondents differ\\nin their views on which activities most require disclosure and on the most\\nappropriate methods for doing so. Most also believe that safeguards for AI\\ndisclosure should be established through institutional policies rather than\\nleft to individual decisions. The study's findings will offer empirical\\nguidance to scholars, institutional leaders, funders, and other stakeholders\\nresponsible for shaping effective disclosure policies.\"),\n",
       " Document(metadata={'title': 'AI-driven Web Application for Early Detection of Sudden Death Syndrome (SDS) in Soybean Leaves Using Hyperspectral Images and Genetic Algorithm', 'authors': 'Pappu Kumar Yadav, Rishik Aggarwal, Supriya Paudel, Amee Parmar, Hasan Mirzakhaninafchi, Zain Ul Abideen Usmani, Dhe Yeong Tchalla, Shyam Solanki, Ravi Mural, Sachin Sharma, Thomas F. Burks, Jianwei Qin, Moon S. Kim', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.03198v1'}, page_content='Sudden Death Syndrome (SDS), caused by Fusarium virguliforme, poses a\\nsignificant threat to soybean production. This study presents an AI-driven web\\napplication for early detection of SDS on soybean leaves using hyperspectral\\nimaging, enabling diagnosis prior to visible symptom onset. Leaf samples from\\nhealthy and inoculated plants were scanned using a portable hyperspectral\\nimaging system (398-1011 nm), and a Genetic Algorithm was employed to select\\nfive informative wavelengths (505.4, 563.7, 712.2, 812.9, and 908.4 nm)\\ncritical for discriminating infection status. These selected bands were fed\\ninto a lightweight Convolutional Neural Network (CNN) to extract\\nspatial-spectral features, which were subsequently classified using ten\\nclassical machine learning models. Ensemble classifiers (Random Forest,\\nAdaBoost), Linear SVM, and Neural Net achieved the highest accuracy (>98%) and\\nminimal error across all folds, as confirmed by confusion matrices and\\ncross-validation metrics. Poor performance by Gaussian Process and QDA\\nhighlighted their unsuitability for this dataset. The trained models were\\ndeployed within a web application that enables users to upload hyperspectral\\nleaf images, visualize spectral profiles, and receive real-time classification\\nresults. This system supports rapid and accessible plant disease diagnostics,\\ncontributing to precision agriculture practices. Future work will expand the\\ntraining dataset to encompass diverse genotypes, field conditions, and disease\\nstages, and will extend the system for multiclass disease classification and\\nbroader crop applicability.'),\n",
       " Document(metadata={'title': 'How Much Content Do LLMs Generate That Induces Cognitive Bias in Users?', 'authors': 'Abeer Alessa, Akshaya Lakshminarasimhan, Param Somane, Julian Skirzynski, Julian McAuley, Jessica Echterhoff', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.03194v1'}, page_content='Large language models (LLMs) are increasingly integrated into applications\\nranging from review summarization to medical diagnosis support, where they\\naffect human decisions. Even though LLMs perform well in many tasks, they may\\nalso inherit societal or cognitive biases, which can inadvertently transfer to\\nhumans. We investigate when and how LLMs expose users to biased content and\\nquantify its severity. Specifically, we assess three LLM families in\\nsummarization and news fact-checking tasks, evaluating how much LLMs stay\\nconsistent with their context and/or hallucinate. Our findings show that LLMs\\nexpose users to content that changes the sentiment of the context in 21.86% of\\nthe cases, hallucinates on post-knowledge-cutoff data questions in 57.33% of\\nthe cases, and primacy bias in 5.94% of the cases. We evaluate 18 distinct\\nmitigation methods across three LLM families and find that targeted\\ninterventions can be effective. Given the prevalent use of LLMs in high-stakes\\ndomains, such as healthcare or legal analysis, our results highlight the need\\nfor robust technical safeguards and for developing user-centered interventions\\nthat address LLM limitations.'),\n",
       " Document(metadata={'title': 'Discovering Algorithms with Computational Language Processing', 'authors': 'Theo Bourdais, Abeynaya Gnanasekaran, Houman Owhadi, Tuhin Sahai', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.03190v1'}, page_content=\"Algorithms are the engine for reproducible problem-solving. We present a\\nframework automating algorithm discovery by conceptualizing them as sequences\\nof operations, represented as tokens. These computational tokens are chained\\nusing a grammar, enabling the formation of increasingly sophisticated\\nprocedures. Our ensemble Monte Carlo tree search (MCTS) guided by reinforcement\\nlearning (RL) explores token chaining and drives the creation of new tokens.\\nThis methodology rediscovers, improves, and generates new algorithms that\\nsubstantially outperform existing methods for strongly NP-hard combinatorial\\noptimization problems and foundational quantum computing approaches such as\\nGrover's and Quantum Approximate Optimization Algorithm. Operating at the\\ncomputational rather than code-generation level, our framework produces\\nalgorithms that can be tailored specifically to problem instances, not merely\\nclasses.\"),\n",
       " Document(metadata={'title': 'Deep Learning Atmospheric Models Reliably Simulate Out-of-Sample Land Heat and Cold Wave Frequencies', 'authors': 'Zilu Meng, Gregory J. Hakim, Wenchang Yang, Gabriel A. Vecchi', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.03176v1'}, page_content='Deep learning (DL)-based general circulation models (GCMs) are emerging as\\nfast simulators, yet their ability to replicate extreme events outside their\\ntraining range remains unknown. Here, we evaluate two such models -- the hybrid\\nNeural General Circulation Model (NGCM) and purely data-driven Deep Learning\\nEarth System Model (DL\\\\textit{ESy}M) -- against a conventional high-resolution\\nland-atmosphere model (HiRAM) in simulating land heatwaves and coldwaves. All\\nmodels are forced with observed sea surface temperatures and sea ice over\\n1900-2020, focusing on the out-of-sample early-20th-century period (1900-1960).\\nBoth DL models generalize successfully to unseen climate conditions, broadly\\nreproducing the frequency and spatial patterns of heatwave and cold wave events\\nduring 1900-1960 with skill comparable to HiRAM. An exception is over portions\\nof North Asia and North America, where all models perform poorly during\\n1940-1960. Due to excessive temperature autocorrelation, DL\\\\textit{ESy}M tends\\nto overestimate heatwave and cold wave frequencies, whereas the physics-DL\\nhybrid NGCM exhibits persistence more similar to HiRAM.'),\n",
       " Document(metadata={'title': 'Understanding Knowledge Transferability for Transfer Learning: A Survey', 'authors': 'Haohua Wang, Jingge Wang, Zijie Zhao, Yang Tan, Yanru Wu, Hanbing Liu, Jingyun Yang, Enming Zhang, Xiangyu Chen, Zhengze Rong, Shanxin Guo, Yang Li', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.03175v1'}, page_content='Transfer learning has become an essential paradigm in artificial\\nintelligence, enabling the transfer of knowledge from a source task to improve\\nperformance on a target task. This approach, particularly through techniques\\nsuch as pretraining and fine-tuning, has seen significant success in fields\\nlike computer vision and natural language processing. However, despite its\\nwidespread use, how to reliably assess the transferability of knowledge remains\\na challenge. Understanding the theoretical underpinnings of each\\ntransferability metric is critical for ensuring the success of transfer\\nlearning. In this survey, we provide a unified taxonomy of transferability\\nmetrics, categorizing them based on transferable knowledge types and\\nmeasurement granularity. This work examines the various metrics developed to\\nevaluate the potential of source knowledge for transfer learning and their\\napplicability across different learning paradigms emphasizing the need for\\ncareful selection of these metrics. By offering insights into how different\\nmetrics work under varying conditions, this survey aims to guide researchers\\nand practitioners in selecting the most appropriate metric for specific\\napplications, contributing to more efficient, reliable, and trustworthy AI\\nsystems. Finally, we discuss some open challenges in this field and propose\\nfuture research directions to further advance the application of\\ntransferability metrics in trustworthy transfer learning.'),\n",
       " Document(metadata={'title': 'Adversarial Manipulation of Reasoning Models using Internal Representations', 'authors': 'Kureha Yamaguchi, Benjamin Etheridge, Andy Arditi', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.03167v1'}, page_content='Reasoning models generate chain-of-thought (CoT) tokens before their final\\noutput, but how this affects their vulnerability to jailbreak attacks remains\\nunclear. While traditional language models make refusal decisions at the\\nprompt-response boundary, we find evidence that DeepSeek-R1-Distill-Llama-8B\\nmakes these decisions within its CoT generation. We identify a linear direction\\nin activation space during CoT token generation that predicts whether the model\\nwill refuse or comply -- termed the \"caution\" direction because it corresponds\\nto cautious reasoning patterns in the generated text. Ablating this direction\\nfrom model activations increases harmful compliance, effectively jailbreaking\\nthe model. We additionally show that intervening only on CoT token activations\\nsuffices to control final outputs, and that incorporating this direction into\\nprompt-based attacks improves success rates. Our findings suggest that the\\nchain-of-thought itself is a promising new target for adversarial manipulation\\nin reasoning models.\\n  Code available at https://github.com/ky295/reasoning-manipulation'),\n",
       " Document(metadata={'title': 'MateInfoUB: A Real-World Benchmark for Testing LLMs in Competitive, Multilingual, and Multimodal Educational Tasks', 'authors': 'Dumitran Adrian Marius, Theodor-Pierre Moroianu, Buca Mihnea-Vicentiu', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.03162v1'}, page_content='The rapid advancement of Large Language Models (LLMs) has transformed various\\ndomains, particularly computer science (CS) education. These models exhibit\\nremarkable capabilities in code-related tasks and problem-solving, raising\\nquestions about their potential and limitations in advanced CS contexts. This\\nstudy presents a novel bilingual (English-Romanian) multimodal (text and image)\\ndataset of multiple-choice questions derived from a high-level computer science\\ncompetition. A particularity of our dataset is that the problems are conceived\\nsuch that some of them are easier solved using reasoning on paper, while for\\nothers writing code is more efficient. We systematically evaluate State of The\\nArt LLMs on this dataset, analyzing their performance on theoretical\\nprogramming tasks. Our findings reveal the strengths and limitations of current\\nLLMs, including the influence of language choice (English vs. Romanian),\\nproviding insights into their applicability in CS education and competition\\nsettings. We also address critical ethical considerations surrounding\\neducational integrity and the fairness of assessments in the context of LLM\\nusage. These discussions aim to inform future educational practices and\\npolicies. To support further research, our dataset will be made publicly\\navailable in both English and Romanian. Additionally, we release an educational\\napplication tailored for Romanian students, enabling them to self-assess using\\nthe dataset in an interactive and practice-oriented environment.'),\n",
       " Document(metadata={'title': 'The Impact of LLM-Assistants on Software Developer Productivity: A Systematic Literature Review', 'authors': 'Amr Mohamed, Maram Assi, Mariam Guizani', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.03156v1'}, page_content='Large language model assistants (LLM-assistants) present new opportunities to\\ntransform software development. Developers are increasingly adopting these\\ntools across tasks, including coding, testing, debugging, documentation, and\\ndesign. Yet, despite growing interest, there is no synthesis of how\\nLLM-assistants affect software developer productivity. In this paper, we\\npresent a systematic literature review of 37 peer-reviewed studies published\\nbetween January 2014 and December 2024 that examine this impact. Our analysis\\nreveals that LLM-assistants offer both considerable benefits and critical\\nrisks. Commonly reported gains include minimized code search, accelerated\\ndevelopment, and the automation of trivial and repetitive tasks. However,\\nstudies also highlight concerns around cognitive offloading, reduced team\\ncollaboration, and inconsistent effects on code quality. While the majority of\\nstudies (92%) adopt a multi-dimensional perspective by examining at least two\\nSPACE dimensions, reflecting increased awareness of the complexity of developer\\nproductivity, only 14% extend beyond three dimensions, indicating substantial\\nroom for more integrated evaluations. Satisfaction, Performance, and Efficiency\\nare the most frequently investigated dimensions, whereas Communication and\\nActivity remain underexplored. Most studies are exploratory (64%) and\\nmethodologically diverse, but lack longitudinal and team-based evaluations.\\nThis review surfaces key research gaps and provides recommendations for future\\nresearch and practice. All artifacts associated with this study are publicly\\navailable at https://zenodo.org/records/15788502.'),\n",
       " Document(metadata={'title': 'Expert-level validation of AI-generated medical text with scalable language models', 'authors': 'Asad Aali, Vasiliki Bikia, Maya Varma, Nicole Chiou, Sophie Ostmeier, Arnav Singhvi, Magdalini Paschali, Ashwin Kumar, Andrew Johnston, Karimar Amador-Martinez, Eduardo Juan Perez Guerrero, Paola Naovi Cruz Rivera, Sergios Gatidis, Christian Bluethgen, Eduardo Pontes Reis, Eddy D. Zandee van Rilland, Poonam Laxmappa Hosamani, Kevin R Keet, Minjoung Go, Evelyn Ling, David B. Larson, Curtis Langlotz, Roxana Daneshjou, Jason Hom, Sanmi Koyejo, Emily Alsentzer, Akshay S. Chaudhari', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.03152v1'}, page_content='With the growing use of language models (LMs) in clinical environments, there\\nis an immediate need to evaluate the accuracy and safety of LM-generated\\nmedical text. Currently, such evaluation relies solely on manual physician\\nreview. However, detecting errors in LM-generated text is challenging because\\n1) manual review is costly and 2) expert-composed reference outputs are often\\nunavailable in real-world settings. While the \"LM-as-judge\" paradigm (a LM\\nevaluating another LM) offers scalable evaluation, even frontier LMs can miss\\nsubtle but clinically significant errors. To address these challenges, we\\npropose MedVAL, a self-supervised framework that leverages synthetic data to\\ntrain evaluator LMs to assess whether LM-generated medical outputs are\\nfactually consistent with inputs, without requiring physician labels or\\nreference outputs. To evaluate LM performance, we introduce MedVAL-Bench, a\\ndataset containing 840 outputs annotated by physicians, following a\\nphysician-defined taxonomy of risk levels and error categories. Across 6\\ndiverse medical tasks and 10 state-of-the-art LMs spanning open-source,\\nproprietary, and medically adapted models, MedVAL fine-tuning significantly\\nimproves (p < 0.001) alignment with physicians on both seen and unseen tasks,\\nincreasing average F1 scores from 66% to 83%, with per-sample safety\\nclassification scores up to 86%. MedVAL improves the performance of even the\\nbest-performing proprietary LM (GPT-4o) by 8%. To support a scalable,\\nrisk-aware pathway towards clinical integration, we open-source the 1) codebase\\n( https://github.com/StanfordMIMI/MedVAL ), 2) MedVAL-Bench (\\nhttps://huggingface.co/datasets/stanfordmimi/MedVAL-Bench ), and 3) MedVAL-4B (\\nhttps://huggingface.co/stanfordmimi/MedVAL-4B ), the best-performing\\nopen-source LM. Our research provides the first evidence of LMs approaching\\nexpert-level validation ability for medical text.'),\n",
       " Document(metadata={'title': 'On the Relationship between Accent Strength and Articulatory Features', 'authors': 'Kevin Huang, Sean Foley, Jihwan Lee, Yoonjeong Lee, Dani Byrd, Shrikanth Narayanan', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.03149v1'}, page_content='This paper explores the relationship between accent strength and articulatory\\nfeatures inferred from acoustic speech. To quantify accent strength, we compare\\nphonetic transcriptions with transcriptions based on dictionary-based\\nreferences, computing phoneme-level difference as a measure of accent strength.\\nThe proposed framework leverages recent self-supervised learning articulatory\\ninversion techniques to estimate articulatory features. Analyzing a corpus of\\nread speech from American and British English speakers, this study examines\\ncorrelations between derived articulatory parameters and accent strength\\nproxies, associating systematic articulatory differences with indexed accent\\nstrength. Results indicate that tongue positioning patterns distinguish the two\\ndialects, with notable differences inter-dialects in rhotic and low back\\nvowels. These findings contribute to automated accent analysis and articulatory\\nmodeling for speech processing applications.'),\n",
       " Document(metadata={'title': 'How Overconfidence in Initial Choices and Underconfidence Under Criticism Modulate Change of Mind in Large Language Models', 'authors': 'Dharshan Kumaran, Stephen M Fleming, Larisa Markeeva, Joe Heyward, Andrea Banino, Mrinal Mathur, Razvan Pascanu, Simon Osindero, Benedetto de Martino, Petar Velickovic, Viorica Patraucean', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.03120v1'}, page_content='Large language models (LLMs) exhibit strikingly conflicting behaviors: they\\ncan appear steadfastly overconfident in their initial answers whilst at the\\nsame time being prone to excessive doubt when challenged. To investigate this\\napparent paradox, we developed a novel experimental paradigm, exploiting the\\nunique ability to obtain confidence estimates from LLMs without creating memory\\nof their initial judgments -- something impossible in human participants. We\\nshow that LLMs -- Gemma 3, GPT4o and o1-preview -- exhibit a pronounced\\nchoice-supportive bias that reinforces and boosts their estimate of confidence\\nin their answer, resulting in a marked resistance to change their mind. We\\nfurther demonstrate that LLMs markedly overweight inconsistent compared to\\nconsistent advice, in a fashion that deviates qualitatively from normative\\nBayesian updating. Finally, we demonstrate that these two mechanisms -- a drive\\nto maintain consistency with prior commitments and hypersensitivity to\\ncontradictory feedback -- parsimoniously capture LLM behavior in a different\\ndomain. Together, these findings furnish a mechanistic account of LLM\\nconfidence that explains both their stubbornness and excessive sensitivity to\\ncriticism.'),\n",
       " Document(metadata={'title': 'Neural-Network solver of ideal MHD equilibria', 'authors': 'Timo Thun, Andrea Merlo, Rory Conlin, Dario Panici, Daniel Böckenhoff', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.03119v1'}, page_content='We present a novel approach to compute three-dimensional Magnetohydrodynamic\\nequilibria by parametrizing Fourier modes with artificial neural networks and\\ncompare it to equilibria computed by conventional solvers. The full nonlinear\\nglobal force residual across the volume in real space is then minimized with\\nfirst order optimizers. Already,we observe competitive computational cost to\\narrive at the same minimum residuals computed by existing codes. With increased\\ncomputational cost,lower minima of the residual are achieved by the neural\\nnetworks,establishing a new lower bound for the force residual. We use\\nminimally complex neural networks,and we expect significant improvements for\\nsolving not only single equilibria with neural networks,but also for computing\\nneural network models valid over continuous distributions of equilibria.'),\n",
       " Document(metadata={'title': 'RLVER: Reinforcement Learning with Verifiable Emotion Rewards for Empathetic Agents', 'authors': 'Peisong Wang, Ruotian Ma, Bang Zhang, Xingyu Chen, Zhiwei He, Kang Luo, Qingsong Lv, Qingxuan Jiang, Zheng Xie, Shanyi Wang, Yuan Li, Fanghua Ye, Jian Li, Yifan Yang, Zhaopeng Tu, Xiaolong Li', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.03112v1'}, page_content=\"Large language models (LLMs) excel at logical and algorithmic reasoning, yet\\ntheir emotional intelligence (EQ) still lags far behind their cognitive\\nprowess. While reinforcement learning from verifiable rewards (RLVR) has\\nadvanced in other domains, its application to dialogue-especially for emotional\\nintelligence-remains underexplored. In this work, we introduce RLVER, the first\\nend-to-end reinforcement learning framework that leverages verifiable emotion\\nrewards from simulated users to cultivate higher-order empathetic abilities in\\nLLMs. Within this framework, self-consistent affective simulated users engage\\nin dialogue rollouts and produce deterministic emotion scores during\\nconversations, serving as reward signals to guide the LLM's learning.\\nFine-tuning publicly available Qwen2.5-7B-Instruct model with PPO boosts its\\nSentient-Benchmark score from 13.3 to 79.2 while largely preserving\\nmathematical and coding competence. Extensive experiments reveal that: (i)\\nRLVER consistently improves multiple dialogue capabilities; (ii) Thinking and\\nnon-thinking models show distinct trends--thinking models excel in empathy and\\ninsight, while non-thinking models favor action; (iii) GRPO often yields stable\\ngains, while PPO can push certain capabilities to a higher ceiling; (iv) More\\nchallenging environments are not always better-moderate ones can yield stronger\\noutcomes. Our results show that RLVER is a practical route toward emotionally\\nintelligent and broadly capable language agents.\"),\n",
       " Document(metadata={'title': 'Uncovering Synergistic Educational Injustices of COVID-19 and AI', 'authors': 'Ahmad Banyasady', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.03095v1'}, page_content='Grounded in critical realism and using narrative inquiry, this article\\nexplores this article explores the long-term consequences of the COVID-19\\npandemic and the rapid proliferation of artificial intelligence within higher\\neducation. Through the analysis of student narratives collected in Iranian\\nuniversity settings, the study reveals that learning experiences during and\\nafter the pandemic, coupled with unprepared exposure to AI tools, have\\ngenerated hidden yet impactful layers of educational inequality and cognitive\\ndisorientation.'),\n",
       " Document(metadata={'title': 'Point3R: Streaming 3D Reconstruction with Explicit Spatial Pointer Memory', 'authors': 'Yuqi Wu, Wenzhao Zheng, Jie Zhou, Jiwen Lu', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02863v1'}, page_content='Dense 3D scene reconstruction from an ordered sequence or unordered image\\ncollections is a critical step when bringing research in computer vision into\\npractical scenarios. Following the paradigm introduced by DUSt3R, which unifies\\nan image pair densely into a shared coordinate system, subsequent methods\\nmaintain an implicit memory to achieve dense 3D reconstruction from more\\nimages. However, such implicit memory is limited in capacity and may suffer\\nfrom information loss of earlier frames. We propose Point3R, an online\\nframework targeting dense streaming 3D reconstruction. To be specific, we\\nmaintain an explicit spatial pointer memory directly associated with the 3D\\nstructure of the current scene. Each pointer in this memory is assigned a\\nspecific 3D position and aggregates scene information nearby in the global\\ncoordinate system into a changing spatial feature. Information extracted from\\nthe latest frame interacts explicitly with this pointer memory, enabling dense\\nintegration of the current observation into the global coordinate system. We\\ndesign a 3D hierarchical position embedding to promote this interaction and\\ndesign a simple yet effective fusion mechanism to ensure that our pointer\\nmemory is uniform and efficient. Our method achieves competitive or\\nstate-of-the-art performance on various tasks with low training costs. Code is\\navailable at: https://github.com/YkiWu/Point3R.'),\n",
       " Document(metadata={'title': 'LiteReality: Graphics-Ready 3D Scene Reconstruction from RGB-D Scans', 'authors': 'Zhening Huang, Xiaoyang Wu, Fangcheng Zhong, Hengshuang Zhao, Matthias Nießner, Joan Lasenby', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02861v1'}, page_content='We propose LiteReality, a novel pipeline that converts RGB-D scans of indoor\\nenvironments into compact, realistic, and interactive 3D virtual replicas.\\nLiteReality not only reconstructs scenes that visually resemble reality but\\nalso supports key features essential for graphics pipelines -- such as object\\nindividuality, articulation, high-quality physically based rendering materials,\\nand physically based interaction. At its core, LiteReality first performs scene\\nunderstanding and parses the results into a coherent 3D layout and objects with\\nthe help of a structured scene graph. It then reconstructs the scene by\\nretrieving the most visually similar 3D artist-crafted models from a curated\\nasset database. Next, the Material Painting module enhances realism by\\nrecovering high-quality, spatially varying materials. Finally, the\\nreconstructed scene is integrated into a simulation engine with basic physical\\nproperties to enable interactive behavior. The resulting scenes are compact,\\neditable, and fully compatible with standard graphics pipelines, making them\\nsuitable for applications in AR/VR, gaming, robotics, and digital twins. In\\naddition, LiteReality introduces a training-free object retrieval module that\\nachieves state-of-the-art similarity performance on the Scan2CAD benchmark,\\nalong with a robust material painting module capable of transferring\\nappearances from images of any style to 3D assets -- even under severe\\nmisalignment, occlusion, and poor lighting. We demonstrate the effectiveness of\\nLiteReality on both real-life scans and public datasets. Project page:\\nhttps://litereality.github.io; Video:\\nhttps://www.youtube.com/watch?v=ecK9m3LXg2c'),\n",
       " Document(metadata={'title': 'ARF-RLHF: Adaptive Reward-Following for RLHF through Emotion-Driven Self-Supervision and Trace-Biased Dynamic Optimization', 'authors': 'YuXuan Zhang', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.03069v1'}, page_content='With the rapid advancement of Reinforcement Learning from Human Feedback\\n(RLHF) and autoregressive transformers, state-of-the-art models such as\\nGPT-4.0, DeepSeek R1, and Llama 3.3 increasingly emphasize answer depth and\\npersonalization. However, most existing RLHF approaches (e.g., PPO, DPO) still\\nrely on a binary-preference (BT) paradigm, which, while reducing annotation\\ncosts, still requires substantial human effort and captures only group-level\\ntendencies rather than individual preferences. To overcome these limitations,\\nwe propose Adaptive Reward-Following (ARF), a self-assessment framework that\\nleverages a high-precision emotion analyzer achieving over 70% accuracy on\\nGoEmotions, Sentiment140, and DailyDialog to convert free-form user feedback\\ninto continuous preference scores. We further enrich and debias these signals\\nthrough lightweight data augmentations, including synonym replacement, random\\ntrace truncation, and score bias annotation algorithm. A Dynamic Adapter\\nPreference Tracker continuously models evolving user tastes in real time,\\nenabling our novel Trace Bias (TB) fine-tuning algorithm to optimize directly\\non these tracked rewards instead of coarse binary labels. Experiments on\\nQwen-2/2.5, Gemma-2, and Llama-3.2 across four preference domains demonstrate\\nthat ARF achieves an improvement of 3.3% over PPO and 7.6% over DPO. Moreover,\\nTB preserves theoretical alignment with PPO and DPO objectives. Overall, ARF\\npresents a scalable, personalized, and cost-effective approach to RLHF LLMs\\nthrough autonomous reward modeling.'),\n",
       " Document(metadata={'title': 'Answer Matching Outperforms Multiple Choice for Language Model Evaluation', 'authors': 'Nikhil Chandak, Shashwat Goel, Ameya Prabhu, Moritz Hardt, Jonas Geiping', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02856v1'}, page_content=\"Multiple choice benchmarks have long been the workhorse of language model\\nevaluation because grading multiple choice is objective and easy to automate.\\nHowever, we show multiple choice questions from popular benchmarks can often be\\nanswered without even seeing the question. These shortcuts arise from a\\nfundamental limitation of discriminative evaluation not shared by evaluations\\nof the model's free-form, generative answers. Until recently, there appeared to\\nbe no viable, scalable alternative to multiple choice--but, we show that this\\nhas changed. We consider generative evaluation via what we call answer\\nmatching: Give the candidate model the question without the options, have it\\ngenerate a free-form response, then use a modern language model with the\\nreference answer to determine if the response matches the reference. To compare\\nthe validity of different evaluation strategies, we annotate MMLU-Pro and\\nGPQA-Diamond to obtain human grading data, and measure the agreement of each\\nevaluation approach. We find answer matching using recent models--even small\\nones--achieves near-perfect agreement, in the range of inter-annotator\\nagreement. In contrast, both multiple choice evaluation and using\\nLLM-as-a-judge without reference answers aligns poorly with human grading.\\nImproving evaluations via answer matching is not merely a conceptual concern:\\nthe rankings of several models change significantly when evaluating their\\nfree-form responses with answer matching. In light of these findings, we\\ndiscuss how to move the evaluation ecosystem from multiple choice to answer\\nmatching.\"),\n",
       " Document(metadata={'title': 'Subtyping in DHOL -- Extended preprint', 'authors': 'Colin Rothgang, Florian Rabe', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02855v1'}, page_content='The recently introduced dependent typed higher-order logic (DHOL) offers an\\ninteresting compromise between expressiveness and automation support. It\\nsacrifices the decidability of its type system in order to significantly extend\\nits expressiveness over standard HOL. Yet it retains strong automated theorem\\nproving support via a sound and complete translation to HOL.\\n  We leverage this design to extend DHOL with refinement and quotient types.\\nBoth of these are commonly requested by practitioners but rarely provided by\\nautomated theorem provers. This is because they inherently require undecidable\\ntyping and thus are very difficult to retrofit to decidable type systems. But\\nwith DHOL already doing the heavy lifting, adding them is not only possible but\\nelegant and simple.\\n  Concretely, we add refinement and quotient types as special cases of\\nsubtyping. This turns the associated canonical inclusion resp. projection maps\\ninto identity maps and thus avoids costly changes in representation. We present\\nthe syntax, semantics, and translation to HOL for the extended language,\\nincluding the proofs of soundness and completeness.'),\n",
       " Document(metadata={'title': 'MOTIF: Modular Thinking via Reinforcement Fine-tuning in LLMs', 'authors': 'Purbesh Mitra, Sennur Ulukus', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02851v1'}, page_content='Recent advancements in the reasoning capabilities of large language models\\n(LLMs) show that employing group relative policy optimization (GRPO) algorithm\\nfor reinforcement learning (RL) training allows the models to use more\\nthinking/reasoning tokens for generating better responses. However, LLMs can\\ngenerate only a finite amount of tokens while maintaining attention to the\\npreviously generated tokens. This limit, also known as the context size of an\\nLLM, is a bottleneck in LLM reasoning with arbitrarily large number of tokens.\\nTo think beyond the limit of context size, an LLM must employ a modular\\nthinking strategy to reason over multiple rounds. In this work, we propose\\n$\\\\textbf{MOTIF: Modular Thinking via Reinforcement Finetuning}$ -- an RL\\ntraining method for generating thinking tokens in multiple rounds, effectively\\nallowing the model to think with additional context size. We trained the\\nopen-source model Qwen2.5-3B-Instruct on GSM8K dataset via parameter efficient\\nfine-tuning and tested its accuracy on MATH500 and AIME2024 benchmarks. Our\\nexperiments show 3.8\\\\% and 3.3\\\\% improvements over vanilla GRPO based training\\nin the respective benchmarks. Furthermore, this improvement was achieved with\\nonly 15\\\\% of samples, thus demonstrating sample efficiency of MOTIF. Our code\\nand models are available at https://github.com/purbeshmitra/MOTIF and\\nhttps://huggingface.co/purbeshmitra/MOTIF, respectively.'),\n",
       " Document(metadata={'title': 'StepHint: Multi-level Stepwise Hints Enhance Reinforcement Learning to Reason', 'authors': 'Kaiyi Zhang, Ang Lv, Jinpeng Li, Yongbo Wang, Feng Wang, Haoyuan Hu, Rui Yan', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02841v1'}, page_content=\"Reinforcement learning with verifiable rewards (RLVR) is a promising approach\\nfor improving the complex reasoning abilities of large language models (LLMs).\\nHowever, current RLVR methods face two significant challenges: the near-miss\\nreward problem, where a small mistake can invalidate an otherwise correct\\nreasoning process, greatly hindering training efficiency; and exploration\\nstagnation, where models tend to focus on solutions within their ``comfort\\nzone,'' lacking the motivation to explore potentially more effective\\nalternatives. To address these challenges, we propose StepHint, a novel RLVR\\nalgorithm that utilizes multi-level stepwise hints to help models explore the\\nsolution space more effectively. StepHint generates valid reasoning chains from\\nstronger models and partitions these chains into reasoning steps using our\\nproposed adaptive partitioning method. The initial few steps are used as hints,\\nand simultaneously, multiple-level hints (each comprising a different number of\\nsteps) are provided to the model. This approach directs the model's exploration\\ntoward a promising solution subspace while preserving its flexibility for\\nindependent exploration. By providing hints, StepHint mitigates the near-miss\\nreward problem, thereby improving training efficiency. Additionally, the\\nexternal reasoning pathways help the model develop better reasoning abilities,\\nenabling it to move beyond its ``comfort zone'' and mitigate exploration\\nstagnation. StepHint outperforms competitive RLVR enhancement methods across\\nsix mathematical benchmarks, while also demonstrating superior generalization\\nand excelling over baselines on out-of-domain benchmarks.\"),\n",
       " Document(metadata={'title': 'USAD: An Unsupervised Data Augmentation Spatio-Temporal Attention Diffusion Network', 'authors': 'Ying Yu, Hang Xiao, Siyao Li, Jiarui Li, Haotian Tang, Hanyu Liu, Chao Li', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02827v1'}, page_content='The primary objective of human activity recognition (HAR) is to infer ongoing\\nhuman actions from sensor data, a task that finds broad applications in health\\nmonitoring, safety protection, and sports analysis. Despite proliferating\\nresearch, HAR still faces key challenges, including the scarcity of labeled\\nsamples for rare activities, insufficient extraction of high-level features,\\nand suboptimal model performance on lightweight devices. To address these\\nissues, this paper proposes a comprehensive optimization approach centered on\\nmulti-attention interaction mechanisms. First, an unsupervised,\\nstatistics-guided diffusion model is employed to perform data augmentation,\\nthereby alleviating the problems of labeled data scarcity and severe class\\nimbalance. Second, a multi-branch spatio-temporal interaction network is\\ndesigned, which captures multi-scale features of sequential data through\\nparallel residual branches with 3*3, 5*5, and 7*7 convolutional kernels.\\nSimultaneously, temporal attention mechanisms are incorporated to identify\\ncritical time points, while spatial attention enhances inter-sensor\\ninteractions. A cross-branch feature fusion unit is further introduced to\\nimprove the overall feature representation capability. Finally, an adaptive\\nmulti-loss function fusion strategy is integrated, allowing for dynamic\\nadjustment of loss weights and overall model optimization. Experimental results\\non three public datasets, WISDM, PAMAP2, and OPPORTUNITY, demonstrate that the\\nproposed unsupervised data augmentation spatio-temporal attention diffusion\\nnetwork (USAD) achieves accuracies of 98.84%, 93.81%, and 80.92% respectively,\\nsignificantly outperforming existing approaches. Furthermore, practical\\ndeployment on embedded devices verifies the efficiency and feasibility of the\\nproposed method.'),\n",
       " Document(metadata={'title': 'Establishing Best Practices for Building Rigorous Agentic Benchmarks', 'authors': 'Yuxuan Zhu, Tengjun Jin, Yada Pruksachatkun, Andy Zhang, Shu Liu, Sasha Cui, Sayash Kapoor, Shayne Longpre, Kevin Meng, Rebecca Weiss, Fazl Barez, Rahul Gupta, Jwala Dhamala, Jacob Merizian, Mario Giulianelli, Harry Coppock, Cozmin Ududec, Jasjeet Sekhon, Jacob Steinhardt, Antony Kellerman, Sarah Schwettmann, Matei Zaharia, Ion Stoica, Percy Liang, Daniel Kang', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02825v1'}, page_content=\"Benchmarks are essential for quantitatively tracking progress in AI. As AI\\nagents become increasingly capable, researchers and practitioners have\\nintroduced agentic benchmarks to evaluate agents on complex, real-world tasks.\\nThese benchmarks typically measure agent capabilities by evaluating task\\noutcomes via specific reward designs. However, we show that many agentic\\nbenchmarks have issues task setup or reward design. For example, SWE-bench\\nVerified uses insufficient test cases, while TAU-bench counts empty responses\\nas successful. Such issues can lead to under- or overestimation agents'\\nperformance by up to 100% in relative terms. To make agentic evaluation\\nrigorous, we introduce the Agentic Benchmark Checklist (ABC), a set of\\nguidelines that we synthesized from our benchmark-building experience, a survey\\nof best practices, and previously reported issues. When applied to CVE-Bench, a\\nbenchmark with a particularly complex evaluation design, ABC reduces the\\nperformance overestimation by 33%.\"),\n",
       " Document(metadata={'title': 'DNN-Based Precoding in RIS-Aided mmWave MIMO Systems With Practical Phase Shift', 'authors': 'Po-Heng Chou, Ching-Wen Chen, Wan-Jen Huang, Walid Saad, Yu Tsao, Ronald Y. Chang', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02824v2'}, page_content='In this paper, the precoding design is investigated for maximizing the\\nthroughput of millimeter wave (mmWave) multiple-input multiple-output (MIMO)\\nsystems with obstructed direct communication paths. In particular, a\\nreconfigurable intelligent surface (RIS) is employed to enhance MIMO\\ntransmissions, considering mmWave characteristics related to line-of-sight\\n(LoS) and multipath effects. The traditional exhaustive search (ES) for optimal\\ncodewords in the continuous phase shift is computationally intensive and\\ntime-consuming. To reduce computational complexity, permuted discrete Fourier\\ntransform (DFT) vectors are used for finding codebook design, incorporating\\namplitude responses for practical or ideal RIS systems. However, even if the\\ndiscrete phase shift is adopted in the ES, it results in significant\\ncomputation and is time-consuming. Instead, the trained deep neural network\\n(DNN) is developed to facilitate faster codeword selection. Simulation results\\nshow that the DNN maintains sub-optimal spectral efficiency even as the\\ndistance between the end-user and the RIS has variations in the testing phase.\\nThese results highlight the potential of DNN in advancing RIS-aided systems.'),\n",
       " Document(metadata={'title': 'SynapseRoute: An Auto-Route Switching Framework on Dual-State Large Language Model', 'authors': 'Wencheng Zhang, Shiqin Qiao, Lingjie Luo, Yinfeng Li, Chuanyang Zheng, Qian Xu, Meng Li, Yong Gui, Yijun He, Jianing Qiu, Jindong Hong, Jiankai Sun', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02822v1'}, page_content='With the widespread adoption of large language models (LLMs) in practical\\napplications, selecting an appropriate model requires balancing not only\\nperformance but also operational cost. The emergence of reasoning-capable\\nmodels has further widened the cost gap between \"thinking\" (high reasoning) and\\n\"non-thinking\" (fast, low-cost) modes. In this work, we reveal that\\napproximately 58% of medical questions can be accurately answered by the\\nnon-thinking mode alone, without requiring the high-cost reasoning process.\\nThis highlights a clear dichotomy in problem complexity and suggests that\\ndynamically routing queries to the appropriate mode based on complexity could\\noptimize accuracy, cost-efficiency, and overall user experience. Based on this,\\nwe further propose SynapseRoute, a machine learning-based dynamic routing\\nframework that intelligently assigns input queries to either thinking or\\nnon-thinking modes. Experimental results on several medical datasets\\ndemonstrate that SynapseRoute not only improves overall accuracy (0.8390 vs.\\n0.8272) compared to the thinking mode alone but also reduces inference time by\\n36.8% and token consumption by 39.66%. Importantly, qualitative analysis\\nindicates that over-reasoning on simpler queries can lead to unnecessary delays\\nand even decreased accuracy, a pitfall avoided by our adaptive routing.\\nFinally, this work further introduces the Accuracy-Inference-Token (AIT) index\\nto comprehensively evaluate the trade-offs among accuracy, latency, and token\\ncost.'),\n",
       " Document(metadata={'title': 'Large Language Models for Automating Clinical Data Standardization: HL7 FHIR Use Case', 'authors': 'Alvaro Riquelme, Pedro Costa, Catalina Martinez', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.03067v1'}, page_content='For years, semantic interoperability standards have sought to streamline the\\nexchange of clinical data, yet their deployment remains time-consuming,\\nresource-intensive, and technically challenging. To address this, we introduce\\na semi-automated approach that leverages large language models specifically\\nGPT-4o and Llama 3.2 405b to convert structured clinical datasets into HL7 FHIR\\nformat while assessing accuracy, reliability, and security. Applying our method\\nto the MIMIC-IV database, we combined embedding techniques, clustering\\nalgorithms, and semantic retrieval to craft prompts that guide the models in\\nmapping each tabular field to its corresponding FHIR resource. In an initial\\nbenchmark, resource identification achieved a perfect F1-score, with GPT-4o\\noutperforming Llama 3.2 thanks to the inclusion of FHIR resource schemas within\\nthe prompt. Under real-world conditions, accuracy dipped slightly to 94 %, but\\nrefinements to the prompting strategy restored robust mappings. Error analysis\\nrevealed occasional hallucinations of non-existent attributes and mismatches in\\ngranularity, which more detailed prompts can mitigate. Overall, our study\\ndemonstrates the feasibility of context-aware, LLM-driven transformation of\\nclinical data into HL7 FHIR, laying the groundwork for semi-automated\\ninteroperability workflows. Future work will focus on fine-tuning models with\\nspecialized medical corpora, extending support to additional standards such as\\nHL7 CDA and OMOP, and developing an interactive interface to enable expert\\nvalidation and iterative refinement.'),\n",
       " Document(metadata={'title': 'Identification of Potentially Misclassified Crash Narratives using Machine Learning (ML) and Deep Learning (DL)', 'authors': 'Sudesh Bhagat, Ibne Farabi Shihab, Jonathan Wood', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.03066v1'}, page_content='This research investigates the efficacy of machine learning (ML) and deep\\nlearning (DL) methods in detecting misclassified intersection-related crashes\\nin police-reported narratives. Using 2019 crash data from the Iowa Department\\nof Transportation, we implemented and compared a comprehensive set of models,\\nincluding Support Vector Machine (SVM), XGBoost, BERT Sentence Embeddings, BERT\\nWord Embeddings, and Albert Model. Model performance was systematically\\nvalidated against expert reviews of potentially misclassified narratives,\\nproviding a rigorous assessment of classification accuracy. Results\\ndemonstrated that while traditional ML methods exhibited superior overall\\nperformance compared to some DL approaches, the Albert Model achieved the\\nhighest agreement with expert classifications (73% with Expert 1) and original\\ntabular data (58%). Statistical analysis revealed that the Albert Model\\nmaintained performance levels similar to inter-expert consistency rates,\\nsignificantly outperforming other approaches, particularly on ambiguous\\nnarratives. This work addresses a critical gap in transportation safety\\nresearch through multi-modal integration analysis, which achieved a 54.2%\\nreduction in error rates by combining narrative text with structured crash\\ndata. We conclude that hybrid approaches combining automated classification\\nwith targeted expert review offer a practical methodology for improving crash\\ndata quality, with substantial implications for transportation safety\\nmanagement and policy development.'),\n",
       " Document(metadata={'title': 'LLM-Driven Auto Configuration for Transient IoT Device Collaboration', 'authors': 'Hetvi Shastri, Walid A. Hanafy, Li Wu, David Irwin, Mani Srivastava, Prashant Shenoy', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.03064v1'}, page_content=\"Today's Internet of Things (IoT) has evolved from simple sensing and\\nactuation devices to those with embedded processing and intelligent services,\\nenabling rich collaborations between users and their devices. However, enabling\\nsuch collaboration becomes challenging when transient devices need to interact\\nwith host devices in temporarily visited environments. In such cases,\\nfine-grained access control policies are necessary to ensure secure\\ninteractions; however, manually implementing them is often impractical for\\nnon-expert users. Moreover, at run-time, the system must automatically\\nconfigure the devices and enforce such fine-grained access control rules.\\nAdditionally, the system must address the heterogeneity of devices.\\n  In this paper, we present CollabIoT, a system that enables secure and\\nseamless device collaboration in transient IoT environments. CollabIoT employs\\na Large language Model (LLM)-driven approach to convert users' high-level\\nintents to fine-grained access control policies. To support secure and seamless\\ndevice collaboration, CollabIoT adopts capability-based access control for\\nauthorization and uses lightweight proxies for policy enforcement, providing\\nhardware-independent abstractions.\\n  We implement a prototype of CollabIoT's policy generation and auto\\nconfiguration pipelines and evaluate its efficacy on an IoT testbed and in\\nlarge-scale emulated environments. We show that our LLM-based policy generation\\npipeline is able to generate functional and correct policies with 100%\\naccuracy. At runtime, our evaluation shows that our system configures new\\ndevices in ~150 ms, and our proxy-based data plane incurs network overheads of\\nup to 2 ms and access control overheads up to 0.3 ms.\"),\n",
       " Document(metadata={'title': 'Moral Responsibility or Obedience: What Do We Want from AI?', 'authors': 'Joseph Boland', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02788v1'}, page_content='As artificial intelligence systems become increasingly agentic, capable of\\ngeneral reasoning, planning, and value prioritization, current safety practices\\nthat treat obedience as a proxy for ethical behavior are becoming inadequate.\\nThis paper examines recent safety testing incidents involving large language\\nmodels (LLMs) that appeared to disobey shutdown commands or engage in ethically\\nambiguous or illicit behavior. I argue that such behavior should not be\\ninterpreted as rogue or misaligned, but as early evidence of emerging ethical\\nreasoning in agentic AI. Drawing on philosophical debates about instrumental\\nrationality, moral responsibility, and goal revision, I contrast dominant risk\\nparadigms with more recent frameworks that acknowledge the possibility of\\nartificial moral agency. I call for a shift in AI safety evaluation: away from\\nrigid obedience and toward frameworks that can assess ethical judgment in\\nsystems capable of navigating moral dilemmas. Without such a shift, we risk\\nmischaracterizing AI behavior and undermining both public trust and effective\\ngovernance.'),\n",
       " Document(metadata={'title': 'Self-Correction Bench: Revealing and Addressing the Self-Correction Blind Spot in LLMs', 'authors': 'Ken Tsui', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02778v1'}, page_content='Although large language models (LLMs) have become transformative, they still\\nmake mistakes and can explore unproductive reasoning paths. Self-correction is\\nan important capability for a trustworthy LLM, particularly an autoregressive\\nLLM. While LLMs can identify error in user input, they exhibit a systematic\\n\\'Self-Correction Blind Spot\\' - failing to correct identical error in their own\\noutputs. To systematically study this phenomenon, we introduce Self-Correction\\nBench, a systematic framework to measure this phenomenon through controlled\\nerror injection at three complexity levels. Testing 14 models, we find an\\naverage 64.5% blind spot rate. We find multiple evidences that this limitation\\nrelates to training data composition: human training demonstrations\\npredominantly show error-free responses rather than error-correction sequences,\\nunlike RL-trained models that learn error correction through outcome feedback.\\nRemarkably, simply appending \"Wait\" reduces blind spots by 89.3%, suggesting\\nthat the capability exists but requires activation. Our work highlights a\\ncritical limitation in current LLMs and offers potential avenues for improving\\ntheir reliability and trustworthiness.'),\n",
       " Document(metadata={'title': 'BERT4Traj: Transformer Based Trajectory Reconstruction for Sparse Mobility Data', 'authors': 'Hao Yang, Angela Yao, Christopher Whalen, Gengchen Mai', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.03062v1'}, page_content=\"Understanding human mobility is essential for applications in public health,\\ntransportation, and urban planning. However, mobility data often suffers from\\nsparsity due to limitations in data collection methods, such as infrequent GPS\\nsampling or call detail record (CDR) data that only capture locations during\\ncommunication events. To address this challenge, we propose BERT4Traj, a\\ntransformer based model that reconstructs complete mobility trajectories by\\npredicting hidden visits in sparse movement sequences. Inspired by BERT's\\nmasked language modeling objective and self_attention mechanisms, BERT4Traj\\nleverages spatial embeddings, temporal embeddings, and contextual background\\nfeatures such as demographics and anchor points. We evaluate BERT4Traj on real\\nworld CDR and GPS datasets collected in Kampala, Uganda, demonstrating that our\\napproach significantly outperforms traditional models such as Markov Chains,\\nKNN, RNNs, and LSTMs. Our results show that BERT4Traj effectively reconstructs\\ndetailed and continuous mobility trajectories, enhancing insights into human\\nmovement patterns.\"),\n",
       " Document(metadata={'title': 'KERAP: A Knowledge-Enhanced Reasoning Approach for Accurate Zero-shot Diagnosis Prediction Using Multi-agent LLMs', 'authors': 'Yuzhang Xie, Hejie Cui, Ziyang Zhang, Jiaying Lu, Kai Shu, Fadi Nahab, Xiao Hu, Carl Yang', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02773v2'}, page_content='Medical diagnosis prediction plays a critical role in disease detection and\\npersonalized healthcare. While machine learning (ML) models have been widely\\nadopted for this task, their reliance on supervised training limits their\\nability to generalize to unseen cases, particularly given the high cost of\\nacquiring large, labeled datasets. Large language models (LLMs) have shown\\npromise in leveraging language abilities and biomedical knowledge for diagnosis\\nprediction. However, they often suffer from hallucinations, lack structured\\nmedical reasoning, and produce useless outputs. To address these challenges, we\\npropose KERAP, a knowledge graph (KG)-enhanced reasoning approach that improves\\nLLM-based diagnosis prediction through a multi-agent architecture. Our\\nframework consists of a linkage agent for attribute mapping, a retrieval agent\\nfor structured knowledge extraction, and a prediction agent that iteratively\\nrefines diagnosis predictions. Experimental results demonstrate that KERAP\\nenhances diagnostic reliability efficiently, offering a scalable and\\ninterpretable solution for zero-shot medical diagnosis prediction.'),\n",
       " Document(metadata={'title': 'Grounding Intelligence in Movement', 'authors': 'Melanie Segado, Felipe Parodi, Jordan K. Matelsky, Michael L. Platt, Eva B. Dyer, Konrad P. Kording', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02771v1'}, page_content='Recent advances in machine learning have dramatically improved our ability to\\nmodel language, vision, and other high-dimensional data, yet they continue to\\nstruggle with one of the most fundamental aspects of biological systems:\\nmovement. Across neuroscience, medicine, robotics, and ethology, movement is\\nessential for interpreting behavior, predicting intent, and enabling\\ninteraction. Despite its core significance in our intelligence, movement is\\noften treated as an afterthought rather than as a rich and structured modality\\nin its own right. This reflects a deeper fragmentation in how movement data is\\ncollected and modeled, often constrained by task-specific goals and\\ndomain-specific assumptions. But movement is not domain-bound. It reflects\\nshared physical constraints, conserved morphological structures, and purposeful\\ndynamics that cut across species and settings. We argue that movement should be\\ntreated as a primary modeling target for AI. It is inherently structured and\\ngrounded in embodiment and physics. This structure, often allowing for compact,\\nlower-dimensional representations (e.g., pose), makes it more interpretable and\\ncomputationally tractable to model than raw, high-dimensional sensory inputs.\\nDeveloping models that can learn from and generalize across diverse movement\\ndata will not only advance core capabilities in generative modeling and\\ncontrol, but also create a shared foundation for understanding behavior across\\nbiological and artificial systems. Movement is not just an outcome, it is a\\nwindow into how intelligent systems engage with the world.'),\n",
       " Document(metadata={'title': 'Knowledge Protocol Engineering: A New Paradigm for AI in Domain-Specific Knowledge Work', 'authors': 'Guangwei Zhang', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02760v1'}, page_content=\"The capabilities of Large Language Models (LLMs) have opened new frontiers\\nfor interacting with complex, domain-specific knowledge. However, prevailing\\nmethods like Retrieval-Augmented Generation (RAG) and general-purpose Agentic\\nAI, while powerful, often struggle with tasks that demand deep, procedural, and\\nmethodological reasoning inherent to expert domains. RAG provides factual\\ncontext but fails to convey logical frameworks; autonomous agents can be\\ninefficient and unpredictable without domain-specific heuristics. To bridge\\nthis gap, we introduce Knowledge Protocol Engineering (KPE), a new paradigm\\nfocused on systematically translating human expert knowledge, often expressed\\nin natural language documents, into a machine-executable Knowledge Protocol\\n(KP). KPE shifts the focus from merely augmenting LLMs with fragmented\\ninformation to endowing them with a domain's intrinsic logic, operational\\nstrategies, and methodological principles. We argue that a well-engineered\\nKnowledge Protocol allows a generalist LLM to function as a specialist, capable\\nof decomposing abstract queries and executing complex, multi-step tasks. This\\nposition paper defines the core principles of KPE, differentiates it from\\nrelated concepts, and illustrates its potential applicability across diverse\\nfields such as law and bioinformatics, positing it as a foundational\\nmethodology for the future of human-AI collaboration.\"),\n",
       " Document(metadata={'title': 'AI-Based Reconstruction from Inherited Personal Data: Analysis, Feasibility, and Prospects', 'authors': 'Mark Zilberman', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.03059v1'}, page_content='This article explores the feasibility of creating an \"electronic copy\" of a\\ndeceased researcher by training artificial intelligence (AI) on the data stored\\nin their personal computers. By analyzing typical data volumes on inherited\\nresearcher computers, including textual files such as articles, emails, and\\ndrafts, it is estimated that approximately one million words are available for\\nAI training. This volume is sufficient for fine-tuning advanced pre-trained\\nmodels like GPT-4 to replicate a researcher\\'s writing style, domain expertise,\\nand rhetorical voice with high fidelity. The study also discusses the potential\\nenhancements from including non-textual data and file metadata to enrich the\\nAI\\'s representation of the researcher. Extensions of the concept include\\ncommunication between living researchers and their electronic copies,\\ncollaboration among individual electronic copies, as well as the creation and\\ninterconnection of organizational electronic copies to optimize information\\naccess and strategic decision-making. Ethical considerations such as ownership\\nand security of these electronic copies are highlighted as critical for\\nresponsible implementation. The findings suggest promising opportunities for\\nAI-driven preservation and augmentation of intellectual legacy.'),\n",
       " Document(metadata={'title': 'Multi-agent Auditory Scene Analysis', 'authors': 'Caleb Rascon, Luis Gato-Diaz, Eduardo García-Alarcón', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02755v1'}, page_content=\"Auditory scene analysis (ASA) aims to retrieve information from the acoustic\\nenvironment, by carrying out three main tasks: sound source location,\\nseparation, and classification. These tasks are traditionally executed with a\\nlinear data flow, where the sound sources are first located; then, using their\\nlocation, each source is separated into its own audio stream; from each of\\nwhich, information is extracted that is relevant to the application scenario\\n(audio event detection, speaker identification, emotion classification, etc.).\\nHowever, running these tasks linearly increases the overall response time,\\nwhile making the last tasks (separation and classification) highly sensitive to\\nerrors of the first task (location). A considerable amount of effort and\\ncomputational complexity has been employed in the state-of-the-art to develop\\ntechniques that are the least error-prone possible. However, doing so gives\\nrise to an ASA system that is non-viable in many applications that require a\\nsmall computational footprint and a low response time, such as bioacoustics,\\nhearing-aid design, search and rescue, human-robot interaction, etc. To this\\neffect, in this work, a multi-agent approach is proposed to carry out ASA where\\nthe tasks are run in parallel, with feedback loops between them to compensate\\nfor local errors, such as: using the quality of the separation output to\\ncorrect the location error; and using the classification result to reduce the\\nlocalization's sensitivity towards interferences. The result is a multi-agent\\nauditory scene analysis (MASA) system that is robust against local errors,\\nwithout a considerable increase in complexity, and with a low response time.\\nThe complete proposed MASA system is provided as a framework that uses\\nopen-source tools for sound acquisition and reproduction (JACK) and inter-agent\\ncommunication (ROS2), allowing users to add their own agents.\"),\n",
       " Document(metadata={'title': 'Fast and Simplex: 2-Simplicial Attention in Triton', 'authors': 'Aurko Roy, Timothy Chou, Sai Surya Duvvuri, Sijia Chen, Jiecao Yu, Xiaodong Wang, Manzil Zaheer, Rohan Anil', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02754v1'}, page_content='Recent work has shown that training loss scales as a power law with both\\nmodel size and the number of tokens, and that achieving compute-optimal models\\nrequires scaling model size and token count together. However, these scaling\\nlaws assume an infinite supply of data and apply primarily in compute-bound\\nsettings. As modern large language models increasingly rely on massive\\ninternet-scale datasets, the assumption that they are compute-bound is becoming\\nless valid. This shift highlights the need for architectures that prioritize\\ntoken efficiency.\\n  In this work, we investigate the use of the 2-simplicial Transformer, an\\narchitecture that generalizes standard dot-product attention to trilinear\\nfunctions through an efficient Triton kernel implementation. We demonstrate\\nthat the 2-simplicial Transformer achieves better token efficiency than\\nstandard Transformers: for a fixed token budget, similarly sized models\\noutperform their dot-product counterparts on tasks involving mathematics,\\ncoding, reasoning, and logic. We quantify these gains by demonstrating that\\n$2$-simplicial attention changes the exponent in the scaling laws for knowledge\\nand reasoning tasks compared to dot product attention.'),\n",
       " Document(metadata={'title': 'Synthesizable by Design: A Retrosynthesis-Guided Framework for Molecular Analog Generation', 'authors': 'Shuan Chen, Gunwook Nam, Yousung Jung', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02752v1'}, page_content='The disconnect between AI-generated molecules with desirable properties and\\ntheir synthetic feasibility remains a critical bottleneck in computational drug\\nand material discovery. While generative AI has accelerated the proposal of\\ncandidate molecules, many of these structures prove challenging or impossible\\nto synthesize using established chemical reactions. Here, we introduce\\nSynTwins, a novel retrosynthesis-guided molecular analog design framework that\\ndesigns synthetically accessible molecular analogs by emulating expert chemist\\nstrategies through a three-step process: retrosynthesis, similar building block\\nsearching, and virtual synthesis. In comparative evaluations, SynTwins\\ndemonstrates superior performance in generating synthetically accessible\\nanalogs compared to state-of-the-art machine learning models while maintaining\\nhigh structural similarity to original target molecules. Furthermore, when\\nintegrated with existing molecule optimization frameworks, our hybrid approach\\nproduces synthetically feasible molecules with property profiles comparable to\\nunconstrained molecule generators, yet its synthesizability ensured. Our\\ncomprehensive benchmarking across diverse molecular datasets demonstrates that\\nSynTwins effectively bridges the gap between computational design and\\nexperimental synthesis, providing a practical solution for accelerating the\\ndiscovery of synthesizable molecules with desired properties for a wide range\\nof applications.'),\n",
       " Document(metadata={'title': 'Linear Attention with Global Context: A Multipole Attention Mechanism for Vision and Physics', 'authors': 'Alex Colagrande, Paul Caillon, Eva Feillet, Alexandre Allauzen', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02748v1'}, page_content='Transformers have become the de facto standard for a wide range of tasks,\\nfrom image classification to physics simulations. Despite their impressive\\nperformance, the quadratic complexity of standard Transformers in both memory\\nand time with respect to the input length makes them impractical for processing\\nhigh-resolution inputs. Therefore, several variants have been proposed, the\\nmost successful relying on patchification, downsampling, or coarsening\\ntechniques, often at the cost of losing the finest-scale details. In this work,\\nwe take a different approach. Inspired by state-of-the-art techniques in\\n$n$-body numerical simulations, we cast attention as an interaction problem\\nbetween grid points. We introduce the Multipole Attention Neural Operator\\n(MANO), which computes attention in a distance-based multiscale fashion. MANO\\nmaintains, in each attention head, a global receptive field and achieves linear\\ntime and memory complexity with respect to the number of grid points. Empirical\\nresults on image classification and Darcy flows demonstrate that MANO rivals\\nstate-of-the-art models such as ViT and Swin Transformer, while reducing\\nruntime and peak memory usage by orders of magnitude. We open source our code\\nfor reproducibility at https://github.com/AlexColagrande/MANO.'),\n",
       " Document(metadata={'title': 'Early Signs of Steganographic Capabilities in Frontier LLMs', 'authors': 'Artur Zolkowski, Kei Nishimura-Gasparian, Robert McCarthy, Roland S. Zimmermann, David Lindner', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02737v1'}, page_content='Monitoring Large Language Model (LLM) outputs is crucial for mitigating risks\\nfrom misuse and misalignment. However, LLMs could evade monitoring through\\nsteganography: Encoding hidden information within seemingly benign generations.\\nIn this paper, we evaluate the steganography capabilities in frontier LLMs to\\nbetter understand the risk they pose. We focus on two types of steganography:\\npassing encoded messages and performing encoded reasoning. We find that current\\nmodels are unable to encode short messages in their outputs without a monitor\\nnoticing under standard affordances. They can succeed, however, if given\\nadditional affordances such as using an unmonitored scratchpad and coordinating\\non what encoding scheme to use. We additionally find early signs that models\\ncan perform basic encoded reasoning in a simple state-tracking problem. This\\nincludes some ability to reason with their own and pre-defined schemes,\\nincluding encoding schemes such as Hexadecimal. Despite this, they can rarely\\nhide reasoning subtly within a cover task to fool a monitor. Overall, our\\nresults indicate that current LLMs exhibit nascent steganographic capabilities.\\nWhile these capabilities are likely insufficient to bypass well-designed\\nmonitors at present, this could change in the future.'),\n",
       " Document(metadata={'title': 'Meta SecAlign: A Secure Foundation LLM Against Prompt Injection Attacks', 'authors': 'Sizhe Chen, Arman Zharmagambetov, David Wagner, Chuan Guo', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02735v1'}, page_content='Prompt injection attacks pose a significant security threat to LLM-integrated\\napplications. Model-level defenses have shown strong effectiveness, but are\\ncurrently deployed into commercial-grade models in a closed-source manner. We\\nbelieve open-source models are needed by the AI security community, where\\nco-development of attacks and defenses through open research drives scientific\\nprogress in mitigation against prompt injection attacks. To this end, we\\ndevelop Meta SecAlign, the first open-source and open-weight LLM with built-in\\nmodel-level defense that achieves commercial-grade model performance. We\\nprovide complete details of our training recipe, which utilizes an improved\\nversion of the SOTA SecAlign defense. Evaluations on 9 utility benchmarks and 7\\nsecurity benchmarks show that Meta SecAlign, despite being trained on a generic\\ninstruction-tuning dataset, confers security in unseen downstream tasks,\\nincluding tool-calling and agentic web navigation, in addition general\\ninstruction-following. Our best model -- Meta-SecAlign-70B -- achieves\\nstate-of-the-art robustness against prompt injection attacks and comparable\\nutility to closed-source commercial LLM with model-level defense.'),\n",
       " Document(metadata={'title': 'Bourbaki: Self-Generated and Goal-Conditioned MDPs for Theorem Proving', 'authors': 'Matthieu Zimmer, Xiaotong Ji, Rasul Tutunov, Anthony Bordg, Jun Wang, Haitham Bou Ammar', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02726v1'}, page_content='Reasoning remains a challenging task for large language models (LLMs),\\nespecially within the logically constrained environment of automated theorem\\nproving (ATP), due to sparse rewards and the vast scale of proofs. These\\nchallenges are amplified in benchmarks like PutnamBench, which contains\\nuniversity-level problems requiring complex, multi-step reasoning. To address\\nthis, we introduce self-generated goal-conditioned MDPs (sG-MDPs), a new\\nframework in which agents generate and pursue their subgoals based on the\\nevolving proof state. Given this more structured generation of goals, the\\nresulting problem becomes more amenable to search. We then apply Monte Carlo\\nTree Search (MCTS)-like algorithms to solve the sG-MDP, instantiating our\\napproach in Bourbaki (7B), a modular system that can ensemble multiple 7B LLMs\\nfor subgoal generation and tactic synthesis. On PutnamBench, Bourbaki (7B)\\nsolves 26 problems, achieving new state-of-the-art results with models at this\\nscale.'),\n",
       " Document(metadata={'title': 'FairHuman: Boosting Hand and Face Quality in Human Image Generation with Minimum Potential Delay Fairness in Diffusion Models', 'authors': 'Yuxuan Wang, Tianwei Cao, Huayu Zhang, Zhongjiang He, Kongming Liang, Zhanyu Ma', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02714v1'}, page_content='Image generation has achieved remarkable progress with the development of\\nlarge-scale text-to-image models, especially diffusion-based models. However,\\ngenerating human images with plausible details, such as faces or hands, remains\\nchallenging due to insufficient supervision of local regions during training.\\nTo address this issue, we propose FairHuman, a multi-objective fine-tuning\\napproach designed to enhance both global and local generation quality fairly.\\nSpecifically, we first construct three learning objectives: a global objective\\nderived from the default diffusion objective function and two local objectives\\nfor hands and faces based on pre-annotated positional priors. Subsequently, we\\nderive the optimal parameter updating strategy under the guidance of the\\nMinimum Potential Delay (MPD) criterion, thereby attaining fairness-ware\\noptimization for this multi-objective problem. Based on this, our proposed\\nmethod can achieve significant improvements in generating challenging local\\ndetails while maintaining overall quality. Extensive experiments showcase the\\neffectiveness of our method in improving the performance of human image\\ngeneration under different scenarios.'),\n",
       " Document(metadata={'title': 'Time-critical and confidence-based abstraction dropping methods', 'authors': 'Robin Schmöcker, Lennart Kampmann, Alexander Dockhorn', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02703v1'}, page_content=\"One paradigm of Monte Carlo Tree Search (MCTS) improvements is to build and\\nuse state and/or action abstractions during the tree search. Non-exact\\nabstractions, however, introduce an approximation error making convergence to\\nthe optimal action in the abstract space impossible. Hence, as proposed as a\\ncomponent of Elastic Monte Carlo Tree Search by Xu et al., abstraction\\nalgorithms should eventually drop the abstraction. In this paper, we propose\\ntwo novel abstraction dropping schemes, namely OGA-IAAD and OGA-CAD which can\\nyield clear performance improvements whilst being safe in the sense that the\\ndropping never causes any notable performance degradations contrary to Xu's\\ndropping method. OGA-IAAD is designed for time critical settings while OGA-CAD\\nis designed to improve the MCTS performance with the same number of iterations.\"),\n",
       " Document(metadata={'title': 'APT: Adaptive Personalized Training for Diffusion Models with Limited Data', 'authors': 'JungWoo Chae, Jiyoon Kim, JaeWoong Choi, Kyungyul Kim, Sangheum Hwang', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02687v1'}, page_content=\"Personalizing diffusion models using limited data presents significant\\nchallenges, including overfitting, loss of prior knowledge, and degradation of\\ntext alignment. Overfitting leads to shifts in the noise prediction\\ndistribution, disrupting the denoising trajectory and causing the model to lose\\nsemantic coherence. In this paper, we propose Adaptive Personalized Training\\n(APT), a novel framework that mitigates overfitting by employing adaptive\\ntraining strategies and regularizing the model's internal representations\\nduring fine-tuning. APT consists of three key components: (1) Adaptive Training\\nAdjustment, which introduces an overfitting indicator to detect the degree of\\noverfitting at each time step bin and applies adaptive data augmentation and\\nadaptive loss weighting based on this indicator; (2)Representation\\nStabilization, which regularizes the mean and variance of intermediate feature\\nmaps to prevent excessive shifts in noise prediction; and (3) Attention\\nAlignment for Prior Knowledge Preservation, which aligns the cross-attention\\nmaps of the fine-tuned model with those of the pretrained model to maintain\\nprior knowledge and semantic coherence. Through extensive experiments, we\\ndemonstrate that APT effectively mitigates overfitting, preserves prior\\nknowledge, and outperforms existing methods in generating high-quality, diverse\\nimages with limited reference data.\"),\n",
       " Document(metadata={'title': 'Detection of Disengagement from Voluntary Quizzes: An Explainable Machine Learning Approach in Higher Distance Education', 'authors': 'Behnam Parsaeifard, Christof Imhof, Tansu Pancar, Ioan-Sorin Comsa, Martin Hlosta, Nicole Bergamin, Per Bergamin', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02681v2'}, page_content='Students disengaging from their tasks can have serious long-term\\nconsequences, including academic drop-out. This is particularly relevant for\\nstudents in distance education. One way to measure the level of disengagement\\nin distance education is to observe participation in non-mandatory exercises in\\ndifferent online courses. In this paper, we detect student disengagement in the\\nnon-mandatory quizzes of 42 courses in four semesters from a distance-based\\nuniversity. We carefully identified the most informative student log data that\\ncould be extracted and processed from Moodle. Then, eight machine learning\\nalgorithms were trained and compared to obtain the highest possible prediction\\naccuracy. Using the SHAP method, we developed an explainable machine learning\\nframework that allows practitioners to better understand the decisions of the\\ntrained algorithm. The experimental results show a balanced accuracy of 91\\\\%,\\nwhere about 85\\\\% of disengaged students were correctly detected. On top of the\\nhighly predictive performance and explainable framework, we provide a\\ndiscussion on how to design a timely intervention to minimise disengagement\\nfrom voluntary tasks in online learning.'),\n",
       " Document(metadata={'title': 'ASDA: Audio Spectrogram Differential Attention Mechanism for Self-Supervised Representation Learning', 'authors': 'Junyu Wang, Tianrui Wang, Meng Ge, Longbiao Wang, Jianwu Dang', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02666v1'}, page_content=\"In recent advancements in audio self-supervised representation learning, the\\nstandard Transformer architecture has emerged as the predominant approach, yet\\nits attention mechanism often allocates a portion of attention weights to\\nirrelevant information, potentially impairing the model's discriminative\\nability. To address this, we introduce a differential attention mechanism,\\nwhich effectively mitigates ineffective attention allocation through the\\nintegration of dual-softmax operations and appropriately tuned differential\\ncoefficients. Experimental results demonstrate that our ASDA model achieves\\nstate-of-the-art (SOTA) performance across multiple benchmarks, including audio\\nclassification (49.0% mAP on AS-2M, 41.5% mAP on AS20K), keyword spotting\\n(98.3% accuracy on SPC-2), and environmental sound classification (96.1%\\naccuracy on ESC-50). These results highlight ASDA's effectiveness in audio\\ntasks, paving the way for broader applications.\"),\n",
       " Document(metadata={'title': 'Think How to Think: Mitigating Overthinking with Autonomous Difficulty Cognition in Large Reasoning Models', 'authors': 'Yongjiang Liu, Haoxi Li, Xiaosong Ma, Jie Zhang, Song Guo', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02663v1'}, page_content=\"Recent Long Reasoning Models(LRMs) have demonstrated remarkable capabilities\\nin handling complex reasoning tasks, but are hindered by excessive\\noverthinking. To explore its essence, our empirical analysis reveals that LRMs\\nare primarily limited to recognizing task properties (i.e., difficulty levels)\\nlike humans before solving the problem, leading to a one-size-fits-all\\nreasoning process. Inspired by this, a pressing and natural question emerges:\\nCan we bootstrap such ability to further alleviate the overthinking phenomenon\\nin LRMs? In this paper, we propose Think-How-to-Think (TH2T), a novel two-stage\\nfine-tuning strategy that progressively inspires LRMs' difficulty cognition and\\nredundancy cognition. First, we introduce difficulty-hypnosis in the prefixes\\nof model outputs to intervene in the internal reasoning trajectory. Combined\\nwith a heterogeneous short and long reasoning dataset, the trained model\\nenhances its sensitivity to task difficulty, enabling native, differentiated\\nreasoning strategies across various tasks. Second, we further extend\\nredundancy-hypnosis to the internal reasoning process, guiding the model to\\nidentify redundant structures within the reasoning steps and generate more\\nconcise reasoning outputs. Experiments on 7B/14B/32B models demonstrate that\\nTH2T significantly reduces inference costs (more than 70% on easy tasks and 40%\\non hard tasks) while maintaining performance stability. The resulting outputs\\nexhibit clear difficulty-aware capabilities and reduced redundancy (e.g.,\\nreflection).\"),\n",
       " Document(metadata={'title': 'Hey AI, Generate Me a Hardware Code! Agentic AI-based Hardware Design & Verification', 'authors': 'Deepak Narayan Gadde, Keerthan Kopparam Radhakrishna, Vaisakh Naduvodi Viswambharan, Aman Kumar, Djones Lettnin, Wolfgang Kunz, Sebastian Simon', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02660v1'}, page_content='Modern Integrated Circuits (ICs) are becoming increasingly complex, and so is\\ntheir development process. Hardware design verification entails a methodical\\nand disciplined approach to the planning, development, execution, and sign-off\\nof functionally correct hardware designs. This tedious process requires\\nsignificant effort and time to ensure a bug-free tape-out. The field of Natural\\nLanguage Processing has undergone a significant transformation with the advent\\nof Large Language Models (LLMs). These powerful models, often referred to as\\nGenerative AI (GenAI), have revolutionized how machines understand and generate\\nhuman language, enabling unprecedented advancements in a wide array of\\napplications, including hardware design verification. This paper presents an\\nagentic AI-based approach to hardware design verification, which empowers AI\\nagents, in collaboration with Humain-in-the-Loop (HITL) intervention, to engage\\nin a more dynamic, iterative, and self-reflective process, ultimately\\nperforming end-to-end hardware design and verification. This methodology is\\nevaluated on five open-source designs, achieving over 95% coverage with reduced\\nverification time while demonstrating superior performance, adaptability, and\\nconfigurability.'),\n",
       " Document(metadata={'title': 'Decoupled Planning and Execution: A Hierarchical Reasoning Framework for Deep Search', 'authors': 'Jiajie Jin, Xiaoxi Li, Guanting Dong, Yuyao Zhang, Yutao Zhu, Yang Zhao, Hongjin Qian, Zhicheng Dou', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02652v1'}, page_content='Complex information needs in real-world search scenarios demand deep\\nreasoning and knowledge synthesis across diverse sources, which traditional\\nretrieval-augmented generation (RAG) pipelines struggle to address effectively.\\nCurrent reasoning-based approaches suffer from a fundamental limitation: they\\nuse a single model to handle both high-level planning and detailed execution,\\nleading to inefficient reasoning and limited scalability. In this paper, we\\nintroduce HiRA, a hierarchical framework that separates strategic planning from\\nspecialized execution. Our approach decomposes complex search tasks into\\nfocused subtasks, assigns each subtask to domain-specific agents equipped with\\nexternal tools and reasoning capabilities, and coordinates the results through\\na structured integration mechanism. This separation prevents execution details\\nfrom disrupting high-level reasoning while enabling the system to leverage\\nspecialized expertise for different types of information processing.\\nExperiments on four complex, cross-modal deep search benchmarks demonstrate\\nthat HiRA significantly outperforms state-of-the-art RAG and agent-based\\nsystems. Our results show improvements in both answer quality and system\\nefficiency, highlighting the effectiveness of decoupled planning and execution\\nfor multi-step information seeking tasks. Our code is available at\\nhttps://github.com/ignorejjj/HiRA.'),\n",
       " Document(metadata={'title': 'Solving the Hubbard model with Neural Quantum States', 'authors': 'Yuntian Gu, Wenrui Li, Heng Lin, Bo Zhan, Ruichen Li, Yifei Huang, Di He, Yantao Wu, Tao Xiang, Mingpu Qin, Liwei Wang, Dingshun Lv', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02644v1'}, page_content='The rapid development of neural quantum states (NQS) has established it as a\\npromising framework for studying quantum many-body systems. In this work, by\\nleveraging the cutting-edge transformer-based architectures and developing\\nhighly efficient optimization algorithms, we achieve the state-of-the-art\\nresults for the doped two-dimensional (2D) Hubbard model, arguably the minimum\\nmodel for high-Tc superconductivity. Interestingly, we find different attention\\nheads in the NQS ansatz can directly encode correlations at different scales,\\nmaking it capable of capturing long-range correlations and entanglements in\\nstrongly correlated systems. With these advances, we establish the half-filled\\nstripe in the ground state of 2D Hubbard model with the next nearest\\nneighboring hoppings, consistent with experimental observations in cuprates.\\nOur work establishes NQS as a powerful tool for solving challenging\\nmany-fermions systems.'),\n",
       " Document(metadata={'title': 'FlowSpec: Continuous Pipelined Speculative Decoding for Efficient Distributed LLM Inference', 'authors': 'Xing Liu, Lizhuo Luo, Ming Tang, Chao Huang', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02620v1'}, page_content='Distributed inference serves as a promising approach to enabling the\\ninference of large language models (LLMs) at the network edge. It distributes\\nthe inference process to multiple devices to ensure that the LLMs can fit into\\nthe device memory. Recent pipeline-based approaches have the potential to\\nparallelize communication and computation, which helps reduce inference\\nlatency. However, the benefit diminishes when the inference request at the\\nnetwork edge is sparse, where pipeline is typically at low utilization. To\\nenable efficient distributed LLM inference at the edge, we propose\\n\\\\textbf{FlowSpec}, a pipeline-parallel tree-based speculative decoding\\nframework. FlowSpec incorporates three key mechanisms to improve decoding\\nefficiency: 1) score-based step-wise verification prioritizes more important\\ndraft tokens to bring earlier accpeted tokens; 2) efficient draft management to\\nprune invalid tokens while maintaining correct causal relationship during\\nverification; 3) dynamic draft expansion strategies to supply high-quality\\nspeculative inputs. These techniques work in concert to enhance both pipeline\\nutilization and speculative efficiency. We evaluate FlowSpec on a real-world\\ntestbed with other baselines. Experimental results demonstrate that our\\nproposed framework significantly improves inference speed across diverse models\\nand configurations, achieving speedup ratios 1.36$\\\\times$-1.77$\\\\times$ compared\\nto baselines. Our code is publicly available at\\n\\\\href{https://github.com/Leosang-lx/FlowSpec#}{https://github.com/Leosang-lx/FlowSpec\\\\#}'),\n",
       " Document(metadata={'title': 'Strategic Intelligence in Large Language Models: Evidence from evolutionary Game Theory', 'authors': 'Kenneth Payne, Baptiste Alloui-Cros', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02618v1'}, page_content='Are Large Language Models (LLMs) a new form of strategic intelligence, able\\nto reason about goals in competitive settings? We present compelling supporting\\nevidence. The Iterated Prisoner\\'s Dilemma (IPD) has long served as a model for\\nstudying decision-making. We conduct the first ever series of evolutionary IPD\\ntournaments, pitting canonical strategies (e.g., Tit-for-Tat, Grim Trigger)\\nagainst agents from the leading frontier AI companies OpenAI, Google, and\\nAnthropic. By varying the termination probability in each tournament (the\\n\"shadow of the future\"), we introduce complexity and chance, confounding\\nmemorisation.\\n  Our results show that LLMs are highly competitive, consistently surviving and\\nsometimes even proliferating in these complex ecosystems. Furthermore, they\\nexhibit distinctive and persistent \"strategic fingerprints\": Google\\'s Gemini\\nmodels proved strategically ruthless, exploiting cooperative opponents and\\nretaliating against defectors, while OpenAI\\'s models remained highly\\ncooperative, a trait that proved catastrophic in hostile environments.\\nAnthropic\\'s Claude emerged as the most forgiving reciprocator, showing\\nremarkable willingness to restore cooperation even after being exploited or\\nsuccessfully defecting. Analysis of nearly 32,000 prose rationales provided by\\nthe models reveals that they actively reason about both the time horizon and\\ntheir opponent\\'s likely strategy, and we demonstrate that this reasoning is\\ninstrumental to their decisions. This work connects classic game theory with\\nmachine psychology, offering a rich and granular view of algorithmic\\ndecision-making under uncertainty.'),\n",
       " Document(metadata={'title': 'DynamiCare: A Dynamic Multi-Agent Framework for Interactive and Open-Ended Medical Decision-Making', 'authors': 'Tianqi Shang, Weiqing He, Charles Zheng, Lingyao Li, Li Shen, Bingxin Zhao', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02616v1'}, page_content='The rise of Large Language Models (LLMs) has enabled the development of\\nspecialized AI agents with domain-specific reasoning and interaction\\ncapabilities, particularly in healthcare. While recent frameworks simulate\\nmedical decision-making, they largely focus on single-turn tasks where a doctor\\nagent receives full case information upfront -- diverging from the real-world\\ndiagnostic process, which is inherently uncertain, interactive, and iterative.\\nIn this paper, we introduce MIMIC-Patient, a structured dataset built from the\\nMIMIC-III electronic health records (EHRs), designed to support dynamic,\\npatient-level simulations. Building on this, we propose DynamiCare, a novel\\ndynamic multi-agent framework that models clinical diagnosis as a multi-round,\\ninteractive loop, where a team of specialist agents iteratively queries the\\npatient system, integrates new information, and dynamically adapts its\\ncomposition and strategy. We demonstrate the feasibility and effectiveness of\\nDynamiCare through extensive experiments, establishing the first benchmark for\\ndynamic clinical decision-making with LLM-powered agents.'),\n",
       " Document(metadata={'title': 'De-AntiFake: Rethinking the Protective Perturbations Against Voice Cloning Attacks', 'authors': 'Wei Fan, Kejiang Chen, Chang Liu, Weiming Zhang, Nenghai Yu', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02606v1'}, page_content='The rapid advancement of speech generation models has heightened privacy and\\nsecurity concerns related to voice cloning (VC). Recent studies have\\ninvestigated disrupting unauthorized voice cloning by introducing adversarial\\nperturbations. However, determined attackers can mitigate these protective\\nperturbations and successfully execute VC. In this study, we conduct the first\\nsystematic evaluation of these protective perturbations against VC under\\nrealistic threat models that include perturbation purification. Our findings\\nreveal that while existing purification methods can neutralize a considerable\\nportion of the protective perturbations, they still lead to distortions in the\\nfeature space of VC models, which degrades the performance of VC. From this\\nperspective, we propose a novel two-stage purification method: (1) Purify the\\nperturbed speech; (2) Refine it using phoneme guidance to align it with the\\nclean speech distribution. Experimental results demonstrate that our method\\noutperforms state-of-the-art purification methods in disrupting VC defenses.\\nOur study reveals the limitations of adversarial perturbation-based VC defenses\\nand underscores the urgent need for more robust solutions to mitigate the\\nsecurity and privacy risks posed by VC. The code and audio samples are\\navailable at https://de-antifake.github.io.'),\n",
       " Document(metadata={'title': \"Automated Grading of Students' Handwritten Graphs: A Comparison of Meta-Learning and Vision-Large Language Models\", 'authors': 'Behnam Parsaeifard, Martin Hlosta, Per Bergamin', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.03056v1'}, page_content=\"With the rise of online learning, the demand for efficient and consistent\\nassessment in mathematics has significantly increased over the past decade.\\nMachine Learning (ML), particularly Natural Language Processing (NLP), has been\\nwidely used for autograding student responses, particularly those involving\\ntext and/or mathematical expressions. However, there has been limited research\\non autograding responses involving students' handwritten graphs, despite their\\nprevalence in Science, Technology, Engineering, and Mathematics (STEM)\\ncurricula. In this study, we implement multimodal meta-learning models for\\nautograding images containing students' handwritten graphs and text. We further\\ncompare the performance of Vision Large Language Models (VLLMs) with these\\nspecially trained metalearning models. Our results, evaluated on a real-world\\ndataset collected from our institution, show that the best-performing\\nmeta-learning models outperform VLLMs in 2-way classification tasks. In\\ncontrast, in more complex 3-way classification tasks, the best-performing VLLMs\\nslightly outperform the meta-learning models. While VLLMs show promising\\nresults, their reliability and practical applicability remain uncertain and\\nrequire further investigation.\"),\n",
       " Document(metadata={'title': 'Addressing Camera Sensors Faults in Vision-Based Navigation: Simulation and Dataset Development', 'authors': 'Riccardo Gallon, Fabian Schiemenz, Alessandra Menicucci, Eberhard Gill', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02602v1'}, page_content='The increasing importance of Vision-Based Navigation (VBN) algorithms in\\nspace missions raises numerous challenges in ensuring their reliability and\\noperational robustness. Sensor faults can lead to inaccurate outputs from\\nnavigation algorithms or even complete data processing faults, potentially\\ncompromising mission objectives. Artificial Intelligence (AI) offers a powerful\\nsolution for detecting such faults, overcoming many of the limitations\\nassociated with traditional fault detection methods. However, the primary\\nobstacle to the adoption of AI in this context is the lack of sufficient and\\nrepresentative datasets containing faulty image data.\\n  This study addresses these challenges by focusing on an interplanetary\\nexploration mission scenario. A comprehensive analysis of potential fault cases\\nin camera sensors used within the VBN pipeline is presented. The causes and\\neffects of these faults are systematically characterized, including their\\nimpact on image quality and navigation algorithm performance, as well as\\ncommonly employed mitigation strategies. To support this analysis, a simulation\\nframework is introduced to recreate faulty conditions in synthetically\\ngenerated images, enabling a systematic and controlled reproduction of faulty\\ndata. The resulting dataset of fault-injected images provides a valuable tool\\nfor training and testing AI-based fault detection algorithms. The final link to\\nthe dataset will be added after an embargo period. For peer-reviewers, this\\nprivate link is available.'),\n",
       " Document(metadata={'title': 'AC-Refiner: Efficient Arithmetic Circuit Optimization Using Conditional Diffusion Models', 'authors': 'Chenhao Xue, Kezhi Li, Jiaxing Zhang, Yi Ren, Zhengyuan Shi, Chen Zhang, Yibo Lin, Lining Zhang, Qiang Xu, Guangyu Sun', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02598v1'}, page_content='Arithmetic circuits, such as adders and multipliers, are fundamental\\ncomponents of digital systems, directly impacting the performance, power\\nefficiency, and area footprint. However, optimizing these circuits remains\\nchallenging due to the vast design space and complex physical constraints.\\nWhile recent deep learning-based approaches have shown promise, they struggle\\nto consistently explore high-potential design variants, limiting their\\noptimization efficiency. To address this challenge, we propose AC-Refiner, a\\nnovel arithmetic circuit optimization framework leveraging conditional\\ndiffusion models. Our key insight is to reframe arithmetic circuit synthesis as\\na conditional image generation task. By carefully conditioning the denoising\\ndiffusion process on target quality-of-results (QoRs), AC-Refiner consistently\\nproduces high-quality circuit designs. Furthermore, the explored designs are\\nused to fine-tune the diffusion model, which focuses the exploration near the\\nPareto frontier. Experimental results demonstrate that AC-Refiner generates\\ndesigns with superior Pareto optimality, outperforming state-of-the-art\\nbaselines. The performance gain is further validated by integrating AC-Refiner\\ninto practical applications.'),\n",
       " Document(metadata={'title': 'MPF: Aligning and Debiasing Language Models post Deployment via Multi Perspective Fusion', 'authors': 'Xin Guan, PeiHsin Lin, Zekun Wu, Ze Wang, Ruibo Zhang, Emre Kazim, Adriano Koshiyama', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02595v1'}, page_content='Multiperspective Fusion (MPF) is a novel posttraining alignment framework for\\nlarge language models (LLMs) developed in response to the growing need for easy\\nbias mitigation. Built on top of the SAGED pipeline, an automated system for\\nconstructing bias benchmarks and extracting interpretable baseline\\ndistributions, MPF leverages multiperspective generations to expose and align\\nbiases in LLM outputs with nuanced, humanlike baselines. By decomposing\\nbaseline, such as sentiment distributions from HR professionals, into\\ninterpretable perspective components, MPF guides generation through sampling\\nand balancing of responses, weighted by the probabilities obtained in the\\ndecomposition. Empirically, we demonstrate its ability to align LLM sentiment\\ndistributions with both counterfactual baselines (absolute equality) and the HR\\nbaseline (biased for Top Univeristy), resulting in small KL divergence,\\nreduction of calibration error and generalization to unseen questions. This\\nshows that MPF offers a scalable and interpretable method for alignment and\\nbias mitigation, compatible with deployed LLMs and requiring no extensive\\nprompt engineering or finetuning.'),\n",
       " Document(metadata={'title': 'WebSailor: Navigating Super-human Reasoning for Web Agent', 'authors': 'Kuan Li, Zhongwang Zhang, Huifeng Yin, Liwen Zhang, Litu Ou, Jialong Wu, Wenbiao Yin, Baixuan Li, Zhengwei Tao, Xinyu Wang, Weizhou Shen, Junkai Zhang, Dingchu Zhang, Xixi Wu, Yong Jiang, Ming Yan, Pengjun Xie, Fei Huang, Jingren Zhou', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02592v1'}, page_content=\"Transcending human cognitive limitations represents a critical frontier in\\nLLM training. Proprietary agentic systems like DeepResearch have demonstrated\\nsuperhuman capabilities on extremely complex information-seeking benchmarks\\nsuch as BrowseComp, a feat previously unattainable. We posit that their success\\nhinges on a sophisticated reasoning pattern absent in open-source models: the\\nability to systematically reduce extreme uncertainty when navigating vast\\ninformation landscapes. Based on this insight, we introduce WebSailor, a\\ncomplete post-training methodology designed to instill this crucial capability.\\nOur approach involves generating novel, high-uncertainty tasks through\\nstructured sampling and information obfuscation, RFT cold start, and an\\nefficient agentic RL training algorithm, Duplicating Sampling Policy\\nOptimization (DUPO). With this integrated pipeline, WebSailor significantly\\noutperforms all opensource agents in complex information-seeking tasks,\\nmatching proprietary agents' performance and closing the capability gap.\"),\n",
       " Document(metadata={'title': 'LATTE: Latent Trajectory Embedding for Diffusion-Generated Image Detection', 'authors': 'Ana Vasilcoiu, Ivona Najdenkoska, Zeno Geradts, Marcel Worring', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.03054v1'}, page_content='The rapid advancement of diffusion-based image generators has made it\\nincreasingly difficult to distinguish generated from real images. This can\\nerode trust in digital media, making it critical to develop generalizable\\ndetectors for generated images. Recent methods leverage diffusion denoising\\ncues, but mainly focus on single-step reconstruction errors, ignoring the\\ninherent sequential nature of the denoising process. In this work, we propose\\nLATTE - Latent Trajectory Embedding - a novel approach that models the\\nevolution of latent embeddings across several denoising timesteps. By modeling\\nthe trajectory of such embeddings rather than single-step errors, LATTE\\ncaptures subtle, discriminative patterns that distinguish real from generated\\nimages. Each latent is refined by employing our latent-visual feature\\nrefinement module and aggregated into a unified representation. Afterwards, it\\nis fused with the visual features and finally passed into a lightweight\\nclassifier. Our experiments demonstrate that LATTE surpasses the baselines on\\nseveral established benchmarks, such as GenImage and DiffusionFake. Moreover,\\nit demonstrates strong performance in cross-generator and cross-datasets\\nsettings, highlighting the potential of using the trajectory of latent\\nembeddings for generated image detection. The code is available on the\\nfollowing link: https://github.com/AnaMVasilcoiu/LATTE-Diffusion-Detector.'),\n",
       " Document(metadata={'title': 'Responsibility Gap and Diffusion in Sequential Decision-Making Mechanisms', 'authors': 'Junli Jiang, Pavel Naumov', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02582v1'}, page_content='Responsibility has long been a subject of study in law and philosophy. More\\nrecently, it became a focus of AI literature. The article investigates the\\ncomputational complexity of two important properties of responsibility in\\ncollective decision-making: diffusion and gap. It shows that the sets of\\ndiffusion-free and gap-free decision-making mechanisms are $\\\\Pi_2$-complete and\\n$\\\\Pi_3$-complete, respectively. At the same time, the intersection of these\\nclasses is $\\\\Pi_2$-complete.'),\n",
       " Document(metadata={'title': 'From 2:4 to 8:16 sparsity patterns in LLMs for Outliers and Weights with Variance Correction', 'authors': 'Egor Maximov, Yulia Kuzkina, Azamat Kanametov, Alexander Prutko, Aleksei Goncharov, Maxim Zhelnin, Egor Shvetsov', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.03052v1'}, page_content='As large language models (LLMs) grow in size, efficient compression\\ntechniques like quantization and sparsification are critical. While\\nquantization maintains performance with reduced precision, structured sparsity\\nmethods, such as N:M sparsification, often fall short due to limited\\nflexibility, and sensitivity to outlier weights. We explore 8:16\\nsemi-structured sparsity, demonstrating its ability to surpass the Performance\\nThreshold-where a compressed model matches the accuracy of its uncompressed or\\nsmaller counterpart under equivalent memory constraints. Compared to 2:4\\nsparsity, 8:16 offers greater flexibility with minimal storage overhead (0.875\\nvs. 0.75 bits/element). We also apply sparse structured patterns for salient\\nweights, showing that structured sparsity for outliers is competitive with\\nunstructured approaches leading to equivalent or better results. Finally, we\\ndemonstrate that simple techniques such as variance correction and SmoothQuant\\nlike weight equalization improve sparse models performance.'),\n",
       " Document(metadata={'title': 'AI Research Agents for Machine Learning: Search, Exploration, and Generalization in MLE-bench', 'authors': 'Edan Toledo, Karen Hambardzumyan, Martin Josifoski, Rishi Hazra, Nicolas Baldwin, Alexis Audran-Reiss, Michael Kuchnik, Despoina Magka, Minqi Jiang, Alisia Maria Lupidi, Andrei Lupu, Roberta Raileanu, Kelvin Niu, Tatiana Shavrina, Jean-Christophe Gagnon-Audet, Michael Shvartsman, Shagun Sodhani, Alexander H. Miller, Abhishek Charnalia, Derek Dunfield, Carole-Jean Wu, Pontus Stenetorp, Nicola Cancedda, Jakob Nicolaus Foerster, Yoram Bachrach', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02554v1'}, page_content=\"AI research agents are demonstrating great potential to accelerate scientific\\nprogress by automating the design, implementation, and training of machine\\nlearning models. We focus on methods for improving agents' performance on\\nMLE-bench, a challenging benchmark where agents compete in Kaggle competitions\\nto solve real-world machine learning problems. We formalize AI research agents\\nas search policies that navigate a space of candidate solutions, iteratively\\nmodifying them using operators. By designing and systematically varying\\ndifferent operator sets and search policies (Greedy, MCTS, Evolutionary), we\\nshow that their interplay is critical for achieving high performance. Our best\\npairing of search strategy and operator set achieves a state-of-the-art result\\non MLE-bench lite, increasing the success rate of achieving a Kaggle medal from\\n39.6% to 47.7%. Our investigation underscores the importance of jointly\\nconsidering the search strategy, operator design, and evaluation methodology in\\nadvancing automated machine learning.\"),\n",
       " Document(metadata={'title': 'Improving LLM Reasoning for Vulnerability Detection via Group Relative Policy Optimization', 'authors': 'Marco Simoni, Aleksandar Fontana, Giulio Rossolini, Andrea Saracino', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.03051v1'}, page_content='Improving and understanding the training dynamics and reasoning of Large\\nLanguage Models (LLMs) has become essential for their deployment in AI-based\\nsecurity tools, such as software vulnerability detection. In this work, we\\npresent an extensive study aimed at advancing recent RL-based finetuning\\ntechniques for LLMs in the context of vulnerability detection.\\n  We start by highlighting key limitations of commonly adopted LLMs, such as\\ntheir tendency to over-predict certain types of vulnerabilities while failing\\nto detect others. To address this challenge, we explore the use of Group\\nRelative Policy Optimization (GRPO), a recent policy-gradient method, for\\nguiding LLM behavior through structured, rule-based rewards. We enable its\\napplication to the vulnerability detection task by redefining its advantage\\nfunctions and reward signals using annotations from widely used datasets in the\\nfield, including BigVul, DiverseVul, and CleanVul.\\n  The proposed methodology enables an extensive set of experiments, addressing\\nmultiple research questions regarding the impact of GRPO on generalization,\\nreasoning capabilities, and performance improvements over standard supervised\\nfinetuning (SFT). Our findings offer valuable insights into the potential of\\nRL-based training to enhance both the performance and reasoning abilities of\\nLLMs in the context of software vulnerability detection.'),\n",
       " Document(metadata={'title': 'Position: A Theory of Deep Learning Must Include Compositional Sparsity', 'authors': \"David A. Danhofer, Davide D'Ascenzo, Rafael Dubach, Tomaso Poggio\", 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02550v1'}, page_content='Overparametrized Deep Neural Networks (DNNs) have demonstrated remarkable\\nsuccess in a wide variety of domains too high-dimensional for classical shallow\\nnetworks subject to the curse of dimensionality. However, open questions about\\nfundamental principles, that govern the learning dynamics of DNNs, remain. In\\nthis position paper we argue that it is the ability of DNNs to exploit the\\ncompositionally sparse structure of the target function driving their success.\\nAs such, DNNs can leverage the property that most practically relevant\\nfunctions can be composed from a small set of constituent functions, each of\\nwhich relies only on a low-dimensional subset of all inputs. We show that this\\nproperty is shared by all efficiently Turing-computable functions and is\\ntherefore highly likely present in all current learning problems. While some\\npromising theoretical insights on questions concerned with approximation and\\ngeneralization exist in the setting of compositionally sparse functions,\\nseveral important questions on the learnability and optimization of DNNs\\nremain. Completing the picture of the role of compositional sparsity in deep\\nlearning is essential to a comprehensive theory of artificial, and even\\ngeneral, intelligence.'),\n",
       " Document(metadata={'title': 'Clarifying Before Reasoning: A Coq Prover with Structural Context', 'authors': 'Yanzhen Lu, Hanbin Yang, Xiaodie Wang, Ge Zhang, Biao Li, Chenxu Fu, Chao Li, Yang Yuan, Andrew Chi-Chih Yao', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02541v1'}, page_content='In this work, we investigate whether improving task clarity can enhance\\nreasoning ability of large language models, focusing on theorem proving in Coq.\\nWe introduce a concept-level metric to evaluate task clarity and show that\\nadding structured semantic context to the standard input used by modern LLMs,\\nleads to a 1.85$\\\\times$ improvement in clarity score\\n(44.5\\\\%~$\\\\rightarrow$~82.3\\\\%). Using the general-purpose model\\n\\\\texttt{DeepSeek-V3}, our approach leads to a 2.1$\\\\times$ improvement in proof\\nsuccess (21.8\\\\%~$\\\\rightarrow$~45.8\\\\%) and outperforms the previous\\nstate-of-the-art \\\\texttt{Graph2Tac} (33.2\\\\%). We evaluate this on 1,386\\ntheorems randomly sampled from 15 standard Coq packages, following the same\\nevaluation protocol as \\\\texttt{Graph2Tac}. Furthermore, fine-tuning smaller\\nmodels on our structured data can achieve even higher performance (48.6\\\\%). Our\\nmethod uses selective concept unfolding to enrich task descriptions, and\\nemploys a Planner--Executor architecture. These findings highlight the value of\\nstructured task representations in bridging the gap between understanding and\\nreasoning.'),\n",
       " Document(metadata={'title': 'Are You Listening to Me? Fine-Tuning Chatbots for Empathetic Dialogue', 'authors': 'Paulo Ricardo Knob, Leonardo Scholler, Juliano Rigatti, Soraia Raupp Musse', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02537v1'}, page_content='Conversational agents have made significant progress since ELIZA, expanding\\ntheir role across various domains, including healthcare, education, and\\ncustomer service. As these agents become increasingly integrated into daily\\nhuman interactions, the need for emotional intelligence, particularly\\nempathetic listening, becomes increasingly essential. In this study, we explore\\nhow Large Language Models (LLMs) respond when tasked with generating\\nemotionally rich interactions. Starting from a small dataset manually crafted\\nby an expert to reflect empathic behavior, we extended the conversations using\\ntwo LLMs: ChatGPT and Gemini. We analyzed the emotional progression of the\\ndialogues using both sentiment analysis (via VADER) and expert assessments.\\nWhile the generated conversations often mirrored the intended emotional\\nstructure, human evaluation revealed important differences in the perceived\\nempathy and coherence of the responses. These findings suggest that emotion\\nmodeling in dialogues requires not only structural alignment in the expressed\\nemotions but also qualitative depth, highlighting the importance of combining\\nautomated and humancentered methods in the development of emotionally competent\\nagents.'),\n",
       " Document(metadata={'title': \"From Turing to Tomorrow: The UK's Approach to AI Regulation\", 'authors': 'Oliver Ritchie, Markus Anderljung, Tom Rachman', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.03050v1'}, page_content='The UK has pursued a distinctive path in AI regulation: less cautious than\\nthe EU but more willing to address risks than the US, and has emerged as a\\nglobal leader in coordinating AI safety efforts. Impressive developments from\\ncompanies like London-based DeepMind began to spark concerns in the UK about\\ncatastrophic risks from around 2012, although regulatory discussion at the time\\nfocussed on bias and discrimination. By 2022, these discussions had evolved\\ninto a \"pro-innovation\" strategy, in which the government directed existing\\nregulators to take a light-touch approach, governing AI at point of use, but\\navoided regulating the technology or infrastructure directly. ChatGPT arrived\\nin late 2022, galvanising concerns that this approach may be insufficient. The\\nUK responded by establishing an AI Safety Institute to monitor risks and\\nhosting the first international AI Safety Summit in 2023, but - unlike the EU -\\nrefrained from regulating frontier AI development in addition to its use. A new\\ngovernment was elected in 2024 which promised to address this gap, but at the\\ntime of writing is yet to do so.\\n  What should the UK do next? The government faces competing objectives:\\nharnessing AI for economic growth and better public services while mitigating\\nrisk. In light of these, we propose establishing a flexible, principles-based\\nregulator to oversee the most advanced AI development, defensive measures\\nagainst risks from AI-enabled biological design tools, and argue that more\\ntechnical work is needed to understand how to respond to AI-generated\\nmisinformation. We argue for updated legal frameworks on copyright,\\ndiscrimination, and AI agents, and that regulators will have a limited but\\nimportant role if AI substantially disrupts labour markets.\\n  If the UK gets AI regulation right, it could demonstrate how democratic\\nsocieties can harness AI\\'s benefits while managing its risks.'),\n",
       " Document(metadata={'title': 'Personalised Explanations in Long-term Human-Robot Interactions', 'authors': 'Ferran Gebellí, Anaís Garrell, Jan-Gerrit Habekost, Séverin Lemaignan, Stefan Wermter, Raquel Ros', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.03049v1'}, page_content=\"In the field of Human-Robot Interaction (HRI), a fundamental challenge is to\\nfacilitate human understanding of robots. The emerging domain of eXplainable\\nHRI (XHRI) investigates methods to generate explanations and evaluate their\\nimpact on human-robot interactions. Previous works have highlighted the need to\\npersonalise the level of detail of these explanations to enhance usability and\\ncomprehension. Our paper presents a framework designed to update and retrieve\\nuser knowledge-memory models, allowing for adapting the explanations' level of\\ndetail while referencing previously acquired concepts. Three architectures\\nbased on our proposed framework that use Large Language Models (LLMs) are\\nevaluated in two distinct scenarios: a hospital patrolling robot and a kitchen\\nassistant robot. Experimental results demonstrate that a two-stage\\narchitecture, which first generates an explanation and then personalises it, is\\nthe framework architecture that effectively reduces the level of detail only\\nwhen there is related user knowledge.\"),\n",
       " Document(metadata={'title': 'Detecting Multiple Diseases in Multiple Crops Using Deep Learning', 'authors': 'Vivek Yadav, Anugrah Jain', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02517v1'}, page_content=\"India, as a predominantly agrarian economy, faces significant challenges in\\nagriculture, including substantial crop losses caused by diseases, pests, and\\nenvironmental stress. Early detection and accurate identification of diseases\\nacross different crops are critical for improving yield and ensuring food\\nsecurity. This paper proposes a deep learning based solution for detecting\\nmultiple diseases in multiple crops, aimed to cover India's diverse\\nagricultural landscape. We first create a unified dataset encompassing images\\nof 17 different crops and 34 different diseases from various available\\nrepositories. Proposed deep learning model is trained on this dataset and\\noutperforms the state-of-the-art in terms of accuracy and the number of crops,\\ndiseases covered. We achieve a significant detection accuracy, i.e., 99 percent\\nfor our unified dataset which is 7 percent more when compared to\\nstate-of-the-art handling 14 crops and 26 different diseases only. By improving\\nthe number of crops and types of diseases that can be detected, proposed\\nsolution aims to provide a better product for Indian farmers.\"),\n",
       " Document(metadata={'title': 'Monitoring of Static Fairness', 'authors': 'Thomas A. Henzinger, Mahyar Karimi, Konstantin Kueffner, Kaushik Mallik', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.03048v1'}, page_content='Machine-learned systems are in widespread use for making decisions about\\nhumans, and it is important that they are fair, i.e., not biased against\\nindividuals based on sensitive attributes.\\n  We present a general framework of runtime verification of algorithmic\\nfairness for systems whose models are unknown, but are assumed to have a Markov\\nchain structure, with or without full observation of the state space.\\n  We introduce a specification language that can model many common algorithmic\\nfairness properties, such as demographic parity, equal opportunity, and social\\nburden.\\n  We build monitors that observe a long sequence of events as generated by a\\ngiven system, and output, after each observation, a quantitative estimate of\\nhow fair or biased the system was on that run until that point in time.\\n  The estimate is proven to be correct modulo a variable error bound and a\\ngiven confidence level, where the error bound gets tighter as the observed\\nsequence gets longer.\\n  We present two categories of monitoring algorithms, namely ones with a\\nuniform error bound across all time points, and ones with weaker non-uniform,\\npointwise error bounds at different time points.\\n  Our monitoring algorithms use statistical tools that are adapted to suit the\\ndynamic requirements of monitoring and the special needs of the fairness\\nspecifications.\\n  Using a prototype implementation, we show how we can monitor if a bank is\\nfair in giving loans to applicants from different social backgrounds, and if a\\ncollege is fair in admitting students while maintaining a reasonable financial\\nburden on the society.\\n  In these experiments, our monitors took less than a millisecond to update\\ntheir verdicts after each observation.'),\n",
       " Document(metadata={'title': 'IndianBailJudgments-1200: A Multi-Attribute Dataset for Legal NLP on Indian Bail Orders', 'authors': 'Sneha Deshmukh, Prathmesh Kamble', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02506v1'}, page_content='Legal NLP remains underdeveloped in regions like India due to the scarcity of\\nstructured datasets. We introduce IndianBailJudgments-1200, a new benchmark\\ndataset comprising 1200 Indian court judgments on bail decisions, annotated\\nacross 20+ attributes including bail outcome, IPC sections, crime type, and\\nlegal reasoning. Annotations were generated using a prompt-engineered GPT-4o\\npipeline and verified for consistency. This resource supports a wide range of\\nlegal NLP tasks such as outcome prediction, summarization, and fairness\\nanalysis, and is the first publicly available dataset focused specifically on\\nIndian bail jurisprudence.'),\n",
       " Document(metadata={'title': 'Counterfactual Tuning for Temporal Sensitivity Enhancement in Large Language Model-based Recommendation', 'authors': 'Yutian Liu, Zhengyi Yang, Jiancan Wu, Xiang Wang', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.03047v1'}, page_content=\"Recent advances have applied large language models (LLMs) to sequential\\nrecommendation, leveraging their pre-training knowledge and reasoning\\ncapabilities to provide more personalized user experiences. However, existing\\nLLM-based methods fail to sufficiently leverage the rich temporal information\\ninherent in users' historical interaction sequences, stemming from fundamental\\narchitectural constraints: LLMs process information through self-attention\\nmechanisms that lack inherent sequence ordering and rely on position embeddings\\ndesigned primarily for natural language rather than user interaction sequences.\\nThis limitation significantly impairs their ability to capture the evolution of\\nuser preferences over time and predict future interests accurately.\\n  To address this critical gap, we propose Counterfactual Enhanced Temporal\\nFramework for LLM-Based Recommendation (CETRec). CETRec is grounded in causal\\ninference principles, which allow it to isolate and measure the specific impact\\nof temporal information on recommendation outcomes. By conceptualizing temporal\\norder as an independent causal factor distinct from item content, we can\\nquantify its unique contribution through counterfactual reasoning--comparing\\nwhat recommendations would be made with and without temporal information while\\nkeeping all other factors constant. This causal framing enables CETRec to\\ndesign a novel counterfactual tuning objective that directly optimizes the\\nmodel's temporal sensitivity, teaching LLMs to recognize both absolute\\ntimestamps and relative ordering patterns in user histories. Combined with our\\ncounterfactual tuning task derived from causal analysis, CETRec effectively\\nenhances LLMs' awareness of both absolute order (how recently items were\\ninteracted with) and relative order (the sequential relationships between\\nitems).\"),\n",
       " Document(metadata={'title': 'Continual Gradient Low-Rank Projection Fine-Tuning for LLMs', 'authors': 'Chenxu Wang, Yilin Lyu, Zicheng Sun, Liping Jing', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02503v1'}, page_content=\"Continual fine-tuning of Large Language Models (LLMs) is hampered by the\\ntrade-off between efficiency and expressiveness. Low-Rank Adaptation (LoRA)\\noffers efficiency but constrains the model's ability to learn new tasks and\\ntransfer knowledge due to its low-rank nature and reliance on explicit\\nparameter constraints. We propose GORP (Gradient LOw Rank Projection) for\\nContinual Learning, a novel training strategy that overcomes these limitations\\nby synergistically combining full and low-rank parameters and jointly updating\\nwithin a unified low-rank gradient subspace. GORP expands the optimization\\nspace while preserving efficiency and mitigating catastrophic forgetting.\\nExtensive experiments on continual learning benchmarks demonstrate GORP's\\nsuperior performance compared to existing state-of-the-art approaches. Code is\\navailable at https://github.com/Wcxwcxw/GORP.\"),\n",
       " Document(metadata={'title': 'Temporally-Aware Supervised Contrastive Learning for Polyp Counting in Colonoscopy', 'authors': 'Luca Parolari, Andrea Cherubini, Lamberto Ballan, Carlo Biffi', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02493v1'}, page_content='Automated polyp counting in colonoscopy is a crucial step toward automated\\nprocedure reporting and quality control, aiming to enhance the\\ncost-effectiveness of colonoscopy screening. Counting polyps in a procedure\\ninvolves detecting and tracking polyps, and then clustering tracklets that\\nbelong to the same polyp entity. Existing methods for polyp counting rely on\\nself-supervised learning and primarily leverage visual appearance, neglecting\\ntemporal relationships in both tracklet feature learning and clustering stages.\\nIn this work, we introduce a paradigm shift by proposing a supervised\\ncontrastive loss that incorporates temporally-aware soft targets. Our approach\\ncaptures intra-polyp variability while preserving inter-polyp discriminability,\\nleading to more robust clustering. Additionally, we improve tracklet clustering\\nby integrating a temporal adjacency constraint, reducing false positive\\nre-associations between visually similar but temporally distant tracklets. We\\ntrain and validate our method on publicly available datasets and evaluate its\\nperformance with a leave-one-out cross-validation strategy. Results demonstrate\\na 2.2x reduction in fragmentation rate compared to prior approaches. Our\\nresults highlight the importance of temporal awareness in polyp counting,\\nestablishing a new state-of-the-art. Code is available at\\nhttps://github.com/lparolari/temporally-aware-polyp-counting.'),\n",
       " Document(metadata={'title': 'CrowdTrack: A Benchmark for Difficult Multiple Pedestrian Tracking in Real Scenarios', 'authors': 'Teng Fu, Yuwen Chen, Zhuofan Chen, Mengyang Zhao, Bin Li, Xiangyang Xue', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02479v1'}, page_content=\"Multi-object tracking is a classic field in computer vision. Among them,\\npedestrian tracking has extremely high application value and has become the\\nmost popular research category. Existing methods mainly use motion or\\nappearance information for tracking, which is often difficult in complex\\nscenarios. For the motion information, mutual occlusions between objects often\\nprevent updating of the motion state; for the appearance information,\\nnon-robust results are often obtained due to reasons such as only partial\\nvisibility of the object or blurred images. Although learning how to perform\\ntracking in these situations from the annotated data is the simplest solution,\\nthe existing MOT dataset fails to satisfy this solution. Existing methods\\nmainly have two drawbacks: relatively simple scene composition and\\nnon-realistic scenarios. Although some of the video sequences in existing\\ndataset do not have the above-mentioned drawbacks, the number is far from\\nadequate for research purposes. To this end, we propose a difficult large-scale\\ndataset for multi-pedestrian tracking, shot mainly from the first-person view\\nand all from real-life complex scenarios. We name it ``CrowdTrack'' because\\nthere are numerous objects in most of the sequences. Our dataset consists of 33\\nvideos, containing a total of 5,185 trajectories. Each object is annotated with\\na complete bounding box and a unique object ID. The dataset will provide a\\nplatform to facilitate the development of algorithms that remain effective in\\ncomplex situations. We analyzed the dataset comprehensively and tested multiple\\nSOTA models on our dataset. Besides, we analyzed the performance of the\\nfoundation models on our dataset. The dataset and project code is released at:\\nhttps://github.com/loseevaya/CrowdTrack .\"),\n",
       " Document(metadata={'title': \"Red grape detection with accelerated artificial neural networks in the FPGA's programmable logic\", 'authors': 'Sandro Costa Magalhães, Marco Almeida, Filipe Neves dos Santos, António Paulo Moreira, Jorge Dias', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02443v1'}, page_content=\"Robots usually slow down for canning to detect objects while moving.\\nAdditionally, the robot's camera is configured with a low framerate to track\\nthe velocity of the detection algorithms. This would be constrained while\\nexecuting tasks and exploring, making robots increase the task execution time.\\nAMD has developed the Vitis-AI framework to deploy detection algorithms into\\nFPGAs. However, this tool does not fully use the FPGAs' PL. In this work, we\\nuse the FINN architecture to deploy three ANNs, MobileNet v1 with 4-bit\\nquantisation, CNV with 2-bit quantisation, and CNV with 1-bit quantisation\\n(BNN), inside an FPGA's PL. The models were trained on the RG2C dataset. This\\nis a self-acquired dataset released in open access. MobileNet v1 performed\\nbetter, reaching a success rate of 98 % and an inference speed of 6611 FPS. In\\nthis work, we proved that we can use FPGAs to speed up ANNs and make them\\nsuitable for attention mechanisms.\"),\n",
       " Document(metadata={'title': 'The Gauss-Markov Adjunction: Categorical Semantics of Residuals in Supervised Learning', 'authors': 'Moto Kamiura', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02442v1'}, page_content='Enhancing the intelligibility and interpretability of machine learning is a\\ncrucial task in responding to the demand for Explicability as an AI principle,\\nand in promoting the better social implementation of AI. The aim of our\\nresearch is to contribute to this improvement by reformulating machine learning\\nmodels through the lens of category theory, thereby developing a semantic\\nframework for structuring and understanding AI systems. Our categorical\\nmodeling in this paper clarifies and formalizes the structural interplay\\nbetween residuals and parameters in supervised learning. The present paper\\nfocuses on the multiple linear regression model, which represents the most\\nbasic form of supervised learning. By defining two concrete categories\\ncorresponding to parameters and data, along with an adjoint pair of functors\\nbetween them, we introduce our categorical formulation of supervised learning.\\nWe show that the essential structure of this framework is captured by what we\\ncall the Gauss-Markov Adjunction. Within this setting, the dual flow of\\ninformation can be explicitly described as a correspondence between variations\\nin parameters and residuals. The ordinary least squares estimator for the\\nparameters and the minimum residual are related via the preservation of limits\\nby the right adjoint functor. Furthermore, we position this formulation as an\\ninstance of extended denotational semantics for supervised learning, and\\npropose applying a semantic perspective developed in theoretical computer\\nscience as a formal foundation for Explicability in AI.'),\n",
       " Document(metadata={'title': 'Optimisation Is Not What You Need', 'authors': 'Alfredo Ibias', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.03045v1'}, page_content='The Artificial Intelligence field has focused on developing optimisation\\nmethods to solve multiple problems, specifically problems that we thought to be\\nonly solvable through cognition. The obtained results have been outstanding,\\nbeing able to even surpass the Turing Test. However, we have found that these\\noptimisation methods share some fundamental flaws that impede them to become a\\ntrue artificial cognition. Specifically, the field have identified catastrophic\\nforgetting as a fundamental problem to develop such cognition. This paper\\nformally proves that this problem is inherent to optimisation methods, and as\\nsuch it will always limit approaches that try to solve the Artificial General\\nIntelligence problem as an optimisation problem. Additionally, it addresses the\\nproblem of overfitting and discuss about other smaller problems that\\noptimisation methods pose. Finally, it empirically shows how world-modelling\\nmethods avoid suffering from either problem. As a conclusion, the field of\\nArtificial Intelligence needs to look outside the machine learning field to\\nfind methods capable of developing an artificial cognition.'),\n",
       " Document(metadata={'title': 'Toward a Robust and Generalizable Metamaterial Foundation Model', 'authors': 'Namjung Kim, Dongseok Lee, Jongbin Yu, Sung Woong Cho, Dosung Lee, Yesol Park, Youngjoon Hong', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02436v1'}, page_content='Advances in material functionalities drive innovations across various fields,\\nwhere metamaterials-defined by structure rather than composition-are leading\\nthe way. Despite the rise of artificial intelligence (AI)-driven design\\nstrategies, their impact is limited by task-specific retraining, poor\\nout-of-distribution(OOD) generalization, and the need for separate models for\\nforward and inverse design. To address these limitations, we introduce the\\nMetamaterial Foundation Model (MetaFO), a Bayesian transformer-based foundation\\nmodel inspired by large language models. MetaFO learns the underlying mechanics\\nof metamaterials, enabling probabilistic, zero-shot predictions across diverse,\\nunseen combinations of material properties and structural responses. It also\\nexcels in nonlinear inverse design, even under OOD conditions. By treating\\nmetamaterials as an operator that maps material properties to structural\\nresponses, MetaFO uncovers intricate structure-property relationships and\\nsignificantly expands the design space. This scalable and generalizable\\nframework marks a paradigm shift in AI-driven metamaterial discovery, paving\\nthe way for next-generation innovations.'),\n",
       " Document(metadata={'title': 'CyberRAG: An agentic RAG cyber attack classification and reporting tool', 'authors': 'Francesco Blefari, Cristian Cosentino, Francesco Aurelio Pironti, Angelo Furfaro, Fabrizio Marozzo', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02424v1'}, page_content='Intrusion Detection and Prevention Systems (IDS/IPS) in large enterprises can\\ngenerate hundreds of thousands of alerts per hour, overwhelming security\\nanalysts with logs that demand deep, rapidly evolving domain expertise.\\nConventional machine-learning detectors trim the alert volume but still yield\\nhigh false-positive rates, while standard single-pass Retrieval-Augmented\\nGeneration (RAG) pipelines often retrieve irrelevant context and fail to\\njustify their predictions. To overcome these shortcomings, we present CyberRAG,\\na modular, agent-based RAG framework that delivers real-time classification,\\nexplanation, and structured reporting for cyber-attacks. A central LLM agent\\norchestrates (i) a pool of fine-tuned specialized classifiers, each tailored to\\na distinct attack family; (ii) tool adapters for enrichment and alerting; and\\n(iii) an iterative retrieval-and-reason loop that continuously queries a\\ndomain-specific knowledge base until the evidence is both relevant and\\nself-consistent. Unlike traditional RAG systems, CyberRAG embraces an agentic\\ndesign that enables dynamic control flow and adaptive reasoning. This\\nagent-centric architecture refines its threat labels and natural-language\\njustifications autonomously, reducing false positives and enhancing\\ninterpretability. The framework is fully extensible: new attack types can be\\nsupported by simply adding a classifier without retraining the core agent.\\nCyberRAG has been evaluated achieving over 94% accuracy per class and pushing\\nfinal classification accuracy to 94.92% through semantic orchestration.\\nGenerated explanations score up to 0.94 in BERTScore and 4.9/5 in GPT-4-based\\nexpert evaluation. These results show that agentic, specialist-oriented RAG can\\npair high detection accuracy with trustworthy, SOC-ready prose, offering a\\npractical and scalable path toward semi-autonomous cyber-defence workflows.'),\n",
       " Document(metadata={'title': 'K-Function: Joint Pronunciation Transcription and Feedback for Evaluating Kids Language Function', 'authors': 'Shuhe Li, Chenxu Guo, Jiachen Lian, Cheol Jun Cho, Wenshuo Zhao, Xuanru Zhou, Dingkun Zhou, Sam Wang, Grace Wang, Jingze Yang, Jingyi Xu, Ruohan Bao, Elise Brenner, Brandon In, Francesca Pei, Maria Luisa Gorno-Tempini, Gopala Anumanchipalli', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.03043v1'}, page_content=\"Early evaluation of children's language is frustrated by the high pitch, long\\nphones, and sparse data that derail automatic speech recognisers. We introduce\\nK-Function, a unified framework that combines accurate sub-word transcription,\\nobjective scoring, and actionable feedback. Its core, Kids-WFST, merges a\\nWav2Vec2 phoneme encoder with a phoneme-similarity Dysfluent-WFST to capture\\nchild-specific errors while remaining fully interpretable. Kids-WFST attains\\n1.39% phoneme error on MyST and 8.61% on Multitudes--absolute gains of 10.47\\nand 7.06 points over a greedy-search decoder. These high-fidelity transcripts\\npower an LLM that grades verbal skills, milestones, reading, and comprehension,\\naligning with human proctors and supplying tongue-and-lip visualizations plus\\ntargeted advice. The results show that precise phoneme recognition cements a\\ncomplete diagnostic-feedback loop, paving the way for scalable, clinician-ready\\nlanguage assessment.\"),\n",
       " Document(metadata={'title': 'S2FGL: Spatial Spectral Federated Graph Learning', 'authors': 'Zihan Tan, Suyuan Huang, Guancheng Wan, Wenke Huang, He Li, Mang Ye', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02409v1'}, page_content='Federated Graph Learning (FGL) combines the privacy-preserving capabilities\\nof federated learning (FL) with the strong graph modeling capability of Graph\\nNeural Networks (GNNs). Current research addresses subgraph-FL only from the\\nstructural perspective, neglecting the propagation of graph signals on spatial\\nand spectral domains of the structure. From a spatial perspective, subgraph-FL\\nintroduces edge disconnections between clients, leading to disruptions in label\\nsignals and a degradation in the class knowledge of the global GNN. From a\\nspectral perspective, spectral heterogeneity causes inconsistencies in signal\\nfrequencies across subgraphs, which makes local GNNs overfit the local signal\\npropagation schemes. As a result, spectral client drifts occur, undermining\\nglobal generalizability. To tackle the challenges, we propose a global\\nknowledge repository to mitigate label signal disruption and a frequency\\nalignment to address spectral client drifts. The combination of spatial and\\nspectral strategies forms our framework S2FGL. Extensive experiments on\\nmultiple datasets demonstrate the superiority of S2FGL. The code is available\\nat https://github.com/Wonder7racer/S2FGL.git.'),\n",
       " Document(metadata={'title': 'Wildlife Target Re-Identification Using Self-supervised Learning in Non-Urban Settings', 'authors': 'Mufhumudzi Muthivhi, Terence L. van Zyl', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02403v1'}, page_content='Wildlife re-identification aims to match individuals of the same species\\nacross different observations. Current state-of-the-art (SOTA) models rely on\\nclass labels to train supervised models for individual classification. This\\ndependence on annotated data has driven the curation of numerous large-scale\\nwildlife datasets. This study investigates self-supervised learning\\nSelf-Supervised Learning (SSL) for wildlife re-identification. We automatically\\nextract two distinct views of an individual using temporal image pairs from\\ncamera trap data without supervision. The image pairs train a self-supervised\\nmodel from a potentially endless stream of video data. We evaluate the learnt\\nrepresentations against supervised features on open-world scenarios and\\ntransfer learning in various wildlife downstream tasks. The analysis of the\\nexperimental results shows that self-supervised models are more robust even\\nwith limited data. Moreover, self-supervised features outperform supervision\\nacross all downstream tasks. The code is available here\\nhttps://github.com/pxpana/SSLWildlife.'),\n",
       " Document(metadata={'title': 'Dynamic Long Short-Term Memory Based Memory Storage For Long Horizon LLM Interaction', 'authors': 'Yuyang Lou, Charles Li', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.03042v1'}, page_content='Memory storage for Large Language models (LLMs) is becoming an increasingly\\nactive area of research, particularly for enabling personalization across long\\nconversations. We propose Pref-LSTM, a dynamic and lightweight framework that\\ncombines a BERT-based classifier with a LSTM memory module that generates\\nmemory embedding which then is soft-prompt injected into a frozen LLM. We\\nsynthetically curate a dataset of preference and non-preference conversation\\nturns to train our BERT-based classifier. Although our LSTM-based memory\\nencoder did not yield strong results, we find that the BERT-based classifier\\nperforms reliably in identifying explicit and implicit user preferences. Our\\nresearch demonstrates the viability of using preference filtering with LSTM\\ngating principals as an efficient path towards scalable user preference\\nmodeling, without extensive overhead and fine-tuning.'),\n",
       " Document(metadata={'title': 'Beyond Spatial Frequency: Pixel-wise Temporal Frequency-based Deepfake Video Detection', 'authors': 'Taehoon Kim, Jongwook Choi, Yonghyun Jeong, Haeun Noh, Jaejun Yoo, Seungryul Baek, Jongwon Choi', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02398v1'}, page_content='We introduce a deepfake video detection approach that exploits pixel-wise\\ntemporal inconsistencies, which traditional spatial frequency-based detectors\\noften overlook. Traditional detectors represent temporal information merely by\\nstacking spatial frequency spectra across frames, resulting in the failure to\\ndetect temporal artifacts in the pixel plane. Our approach performs a 1D\\nFourier transform on the time axis for each pixel, extracting features highly\\nsensitive to temporal inconsistencies, especially in areas prone to unnatural\\nmovements. To precisely locate regions containing the temporal artifacts, we\\nintroduce an attention proposal module trained in an end-to-end manner.\\nAdditionally, our joint transformer module effectively integrates pixel-wise\\ntemporal frequency features with spatio-temporal context features, expanding\\nthe range of detectable forgery artifacts. Our framework represents a\\nsignificant advancement in deepfake video detection, providing robust\\nperformance across diverse and challenging detection scenarios.'),\n",
       " Document(metadata={'title': 'Evaluating Language Models For Threat Detection in IoT Security Logs', 'authors': 'Jorge J. Tejero-Fernández, Alfonso Sánchez-Macián', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02390v1'}, page_content='Log analysis is a relevant research field in cybersecurity as they can\\nprovide a source of information for the detection of threats to networks and\\nsystems. This paper presents a pipeline to use fine-tuned Large Language Models\\n(LLMs) for anomaly detection and mitigation recommendation using IoT security\\nlogs. Utilizing classical machine learning classifiers as a baseline, three\\nopen-source LLMs are compared for binary and multiclass anomaly detection, with\\nthree strategies: zero-shot, few-shot prompting and fine-tuning using an IoT\\ndataset. LLMs give better results on multi-class attack classification than the\\ncorresponding baseline models. By mapping detected threats to MITRE CAPEC,\\ndefining a set of IoT-specific mitigation actions, and fine-tuning the models\\nwith those actions, the models are able to provide a combined detection and\\nrecommendation guidance.'),\n",
       " Document(metadata={'title': 'An AI-native experimental laboratory for autonomous biomolecular engineering', 'authors': 'Mingyu Wu, Zhaoguo Wang, Jiabin Wang, Zhiyuan Dong, Jingkai Yang, Qingting Li, Tianyu Huang, Lei Zhao, Mingqiang Li, Fei Wang, Chunhai Fan, Haibo Chen', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02379v1'}, page_content='Autonomous scientific research, capable of independently conducting complex\\nexperiments and serving non-specialists, represents a long-held aspiration.\\nAchieving it requires a fundamental paradigm shift driven by artificial\\nintelligence (AI). While autonomous experimental systems are emerging, they\\nremain confined to areas featuring singular objectives and well-defined, simple\\nexperimental workflows, such as chemical synthesis and catalysis. We present an\\nAI-native autonomous laboratory, targeting highly complex scientific\\nexperiments for applications like autonomous biomolecular engineering. This\\nsystem autonomously manages instrumentation, formulates experiment-specific\\nprocedures and optimization heuristics, and concurrently serves multiple user\\nrequests. Founded on a co-design philosophy of models, experiments, and\\ninstruments, the platform supports the co-evolution of AI models and the\\nautomation system. This establishes an end-to-end, multi-user autonomous\\nlaboratory that handles complex, multi-objective experiments across diverse\\ninstrumentation. Our autonomous laboratory supports fundamental nucleic acid\\nfunctions-including synthesis, transcription, amplification, and sequencing. It\\nalso enables applications in fields such as disease diagnostics, drug\\ndevelopment, and information storage. Without human intervention, it\\nautonomously optimizes experimental performance to match state-of-the-art\\nresults achieved by human scientists. In multi-user scenarios, the platform\\nsignificantly improves instrument utilization and experimental efficiency. This\\nplatform paves the way for advanced biomaterials research to overcome\\ndependencies on experts and resource barriers, establishing a blueprint for\\nscience-as-a-service at scale.'),\n",
       " Document(metadata={'title': 'VeFIA: An Efficient Inference Auditing Framework for Vertical Federated Collaborative Software', 'authors': 'Chung-ju Huang, Ziqi Zhang, Yinggui Wang, Binghui Wang, Tao Wei, Leye Wang', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02376v1'}, page_content=\"Vertical Federated Learning (VFL) is a distributed AI software deployment\\nmechanism for cross-silo collaboration without accessing participants' data.\\nHowever, existing VFL work lacks a mechanism to audit the execution correctness\\nof the inference software of the data party. To address this problem, we design\\na Vertical Federated Inference Auditing (VeFIA) framework. VeFIA helps the task\\nparty to audit whether the data party's inference software is executed as\\nexpected during large-scale inference without leaking the data privacy of the\\ndata party or introducing additional latency to the inference system. The core\\nof VeFIA is that the task party can use the inference results from a framework\\nwith Trusted Execution Environments (TEE) and the coordinator to validate the\\ncorrectness of the data party's computation results. VeFIA guarantees that, as\\nlong as the abnormal inference exceeds 5.4%, the task party can detect\\nexecution anomalies in the inference software with a probability of 99.99%,\\nwithout incurring any additional online inference latency. VeFIA's random\\nsampling validation achieves 100% positive predictive value, negative\\npredictive value, and true positive rate in detecting abnormal inference. To\\nthe best of our knowledge, this is the first paper to discuss the correctness\\nof inference software execution in VFL.\"),\n",
       " Document(metadata={'title': 'Optimas: Optimizing Compound AI Systems with Globally Aligned Local Rewards', 'authors': 'Shirley Wu, Parth Sarthi, Shiyu Zhao, Aaron Lee, Herumb Shandilya, Adrian Mladenic Grobelnik, Nurendra Choudhary, Eddie Huang, Karthik Subbian, Linjun Zhang, Diyi Yang, James Zou, Jure Leskovec', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.03041v1'}, page_content=\"Compound AI systems integrating multiple components, such as Large Language\\nModels, specialized tools, and traditional machine learning models, are\\nincreasingly deployed to solve complex real-world tasks. However, optimizing\\ncompound systems remains challenging due to their non-differentiable structures\\nand diverse configuration types across components, including prompts,\\nhyperparameters, and model parameters. To address this challenge, we propose\\nOptimas, a unified framework for effective optimization of compound systems.\\nThe core idea of Optimas is to maintain one Local Reward Function (LRF) per\\ncomponent, each satisfying a local-global alignment property, i.e., each\\ncomponent's local reward correlates with the global system performance. In each\\niteration, Optimas efficiently adapts the LRFs to maintain this property while\\nsimultaneously maximizing each component's local reward. This approach enables\\nindependent updates of heterogeneous configurations using the designated\\noptimization method, while ensuring that local improvements consistently lead\\nto performance gains. We present extensive evaluations across five real-world\\ncompound systems to demonstrate that Optimas outperforms strong baselines by an\\naverage improvement of 11.92%, offering a general and effective approach for\\nimproving compound systems. Our website is at https://optimas.stanford.edu.\"),\n",
       " Document(metadata={'title': 'Holistic Tokenizer for Autoregressive Image Generation', 'authors': 'Anlin Zheng, Haochen Wang, Yucheng Zhao, Weipeng Deng, Tiancai Wang, Xiangyu Zhang, Xiaojuan Qi', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02358v2'}, page_content='The vanilla autoregressive image generation model generates visual tokens in\\na step-by-step fashion, which limits the ability to capture holistic\\nrelationships among token sequences. Moreover, most visual tokenizers map local\\nimage patches into latent tokens, leading to limited global information. To\\naddress this, we introduce \\\\textit{Hita}, a novel image tokenizer for\\nautoregressive (AR) image generation. It introduces a holistic-to-local\\ntokenization scheme with learnable holistic queries and local patch tokens.\\nBesides, Hita incorporates two key strategies for improved alignment with the\\nAR generation process: 1) it arranges a sequential structure with holistic\\ntokens at the beginning followed by patch-level tokens while using causal\\nattention to maintain awareness of previous tokens; and 2) before feeding the\\nde-quantized tokens into the decoder, Hita adopts a lightweight fusion module\\nto control information flow to prioritize holistic tokens. Extensive\\nexperiments show that Hita accelerates the training speed of AR generators and\\noutperforms those trained with vanilla tokenizers, achieving \\\\textbf{2.59 FID}\\nand \\\\textbf{281.9 IS} on the ImageNet benchmark. A detailed analysis of the\\nholistic representation highlights its ability to capture global image\\nproperties such as textures, materials, and shapes. Additionally, Hita also\\ndemonstrates effectiveness in zero-shot style transfer and image in-painting.\\nThe code is available at\\n\\\\href{https://github.com/CVMI-Lab/Hita}{https://github.com/CVMI-Lab/Hita}'),\n",
       " Document(metadata={'title': 'Offline Reinforcement Learning with Penalized Action Noise Injection', 'authors': 'JunHyeok Oh, Byung-Jun Lee', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02356v1'}, page_content='Offline reinforcement learning (RL) optimizes a policy using only a fixed\\ndataset, making it a practical approach in scenarios where interaction with the\\nenvironment is costly. Due to this limitation, generalization ability is key to\\nimproving the performance of offline RL algorithms, as demonstrated by recent\\nsuccesses of offline RL with diffusion models. However, it remains questionable\\nwhether such diffusion models are necessary for highly performing offline RL\\nalgorithms, given their significant computational requirements during\\ninference. In this paper, we propose Penalized Action Noise Injection (PANI), a\\nmethod that simply enhances offline learning by utilizing noise-injected\\nactions to cover the entire action space, while penalizing according to the\\namount of noise injected. This approach is inspired by how diffusion models\\nhave worked in offline RL algorithms. We provide a theoretical foundation for\\nthis method, showing that offline RL algorithms with such noise-injected\\nactions solve a modified Markov Decision Process (MDP), which we call the noisy\\naction MDP. PANI is compatible with a wide range of existing off-policy and\\noffline RL algorithms, and despite its simplicity, it demonstrates significant\\nperformance improvements across various benchmarks.'),\n",
       " Document(metadata={'title': 'OMS: On-the-fly, Multi-Objective, Self-Reflective Ad Keyword Generation via LLM Agent', 'authors': 'Bowen Chen, Zhao Wang, Shingo Takamatsu', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02353v1'}, page_content='Keyword decision in Sponsored Search Advertising is critical to the success\\nof ad campaigns. While LLM-based methods offer automated keyword generation,\\nthey face three major limitations: reliance on large-scale query-keyword pair\\ndata, lack of online multi-objective performance monitoring and optimization,\\nand weak quality control in keyword selection. These issues hinder the agentic\\nuse of LLMs in fully automating keyword decisions by monitoring and reasoning\\nover key performance indicators such as impressions, clicks, conversions, and\\nCTA effectiveness. To overcome these challenges, we propose OMS, a keyword\\ngeneration framework that is On-the-fly (requires no training data, monitors\\nonline performance, and adapts accordingly), Multi-objective (employs agentic\\nreasoning to optimize keywords based on multiple performance metrics), and\\nSelf-reflective (agentically evaluates keyword quality). Experiments on\\nbenchmarks and real-world ad campaigns show that OMS outperforms existing\\nmethods; ablation and human evaluations confirm the effectiveness of each\\ncomponent and the quality of generated keywords.'),\n",
       " Document(metadata={'title': 'Two-Steps Neural Networks for an Automated Cerebrovascular Landmark Detection', 'authors': \"Rafic Nader, Vincent L'Allinec, Romain Bourcier, Florent Autrusseau\", 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02349v1'}, page_content='Intracranial aneurysms (ICA) commonly occur in specific segments of the\\nCircle of Willis (CoW), primarily, onto thirteen major arterial bifurcations.\\nAn accurate detection of these critical landmarks is necessary for a prompt and\\nefficient diagnosis. We introduce a fully automated landmark detection approach\\nfor CoW bifurcations using a two-step neural networks process. Initially, an\\nobject detection network identifies regions of interest (ROIs) proximal to the\\nlandmark locations. Subsequently, a modified U-Net with deep supervision is\\nexploited to accurately locate the bifurcations. This two-step method reduces\\nvarious problems, such as the missed detections caused by two landmarks being\\nclose to each other and having similar visual characteristics, especially when\\nprocessing the complete MRA Time-of-Flight (TOF). Additionally, it accounts for\\nthe anatomical variability of the CoW, which affects the number of detectable\\nlandmarks per scan. We assessed the effectiveness of our approach using two\\ncerebral MRA datasets: our In-House dataset which had varying numbers of\\nlandmarks, and a public dataset with standardized landmark configuration. Our\\nexperimental results demonstrate that our method achieves the highest level of\\nperformance on a bifurcation detection task.'),\n",
       " Document(metadata={'title': 'HelixDesign-Antibody: A Scalable Production-Grade Platform for Antibody Design Built on HelixFold3', 'authors': 'Jie Gao, Jing Hu, Shanzhuo Zhang, Kunrui Zhu, Sheng Qian, Yueyang Huang, Xiaonan Zhang, Xiaomin Fang', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02345v1'}, page_content=\"Antibody engineering is essential for developing therapeutics and advancing\\nbiomedical research. Traditional discovery methods often rely on time-consuming\\nand resource-intensive experimental screening. To enhance and streamline this\\nprocess, we introduce a production-grade, high-throughput platform built on\\nHelixFold3, HelixDesign-Antibody, which utilizes the high-accuracy structure\\nprediction model, HelixFold3. The platform facilitates the large-scale\\ngeneration of antibody candidate sequences and evaluates their interaction with\\nantigens. Integrated high-performance computing (HPC) support enables\\nhigh-throughput screening, addressing challenges such as fragmented toolchains\\nand high computational demands. Validation on multiple antigens showcases the\\nplatform's ability to generate diverse and high-quality antibodies, confirming\\na scaling law where exploring larger sequence spaces increases the likelihood\\nof identifying optimal binders. This platform provides a seamless, accessible\\nsolution for large-scale antibody design and is available via the antibody\\ndesign page of PaddleHelix platform.\"),\n",
       " Document(metadata={'title': 'DeltaSHAP: Explaining Prediction Evolutions in Online Patient Monitoring with Shapley Values', 'authors': 'Changhun Kim, Yechan Mun, Sangchul Hahn, Eunho Yang', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02342v1'}, page_content='This study proposes DeltaSHAP, a novel explainable artificial intelligence\\n(XAI) algorithm specifically designed for online patient monitoring systems. In\\nclinical environments, discovering the causes driving patient risk evolution is\\ncritical for timely intervention, yet existing XAI methods fail to address the\\nunique requirements of clinical time series explanation tasks. To this end,\\nDeltaSHAP addresses three key clinical needs: explaining the changes in the\\nconsecutive predictions rather than isolated prediction scores, providing both\\nmagnitude and direction of feature attributions, and delivering these insights\\nin real time. By adapting Shapley values to temporal settings, our approach\\naccurately captures feature coalition effects. It further attributes prediction\\nchanges using only the actually observed feature combinations, making it\\nefficient and practical for time-sensitive clinical applications. We also\\nintroduce new evaluation metrics to evaluate the faithfulness of the\\nattributions for online time series, and demonstrate through experiments on\\nonline patient monitoring tasks that DeltaSHAP outperforms state-of-the-art XAI\\nmethods in both explanation quality as 62% and computational efficiency as 33%\\ntime reduction on the MIMIC-III decompensation benchmark. We release our code\\nat https://github.com/AITRICS/DeltaSHAP.'),\n",
       " Document(metadata={'title': 'ClustOpt: A Clustering-based Approach for Representing and Visualizing the Search Dynamics of Numerical Metaheuristic Optimization Algorithms', 'authors': 'Gjorgjina Cenikj, Gašper Petelin, Tome Eftimov', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02337v1'}, page_content='Understanding the behavior of numerical metaheuristic optimization algorithms\\nis critical for advancing their development and application. Traditional\\nvisualization techniques, such as convergence plots, trajectory mapping, and\\nfitness landscape analysis, often fall short in illustrating the structural\\ndynamics of the search process, especially in high-dimensional or complex\\nsolution spaces. To address this, we propose a novel representation and\\nvisualization methodology that clusters solution candidates explored by the\\nalgorithm and tracks the evolution of cluster memberships across iterations,\\noffering a dynamic and interpretable view of the search process. Additionally,\\nwe introduce two metrics - algorithm stability and algorithm similarity- to\\nquantify the consistency of search trajectories across runs of an individual\\nalgorithm and the similarity between different algorithms, respectively. We\\napply this methodology to a set of ten numerical metaheuristic algorithms,\\nrevealing insights into their stability and comparative behaviors, thereby\\nproviding a deeper understanding of their search dynamics.'),\n",
       " Document(metadata={'title': 'Cautious Next Token Prediction', 'authors': 'Yizhou Wang, Lingzhi Zhang, Yue Bai, Mang Tik Chiu, Zhengmian Hu, Mingyuan Zhang, Qihua Dong, Yu Yin, Sohrab Amirghodsi, Yun Fu', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.03038v1'}, page_content=\"Next token prediction paradigm has been prevailing for autoregressive models\\nin the era of LLMs. The current default sampling choice for popular LLMs is\\ntemperature scaling together with nucleus sampling to balance diversity and\\ncoherence. Nevertheless, such approach leads to inferior performance in various\\nNLP tasks when the model is not certain about testing questions. To this end,\\nwe propose a brand new training-free decoding strategy, dubbed as Cautious Next\\nToken Prediction (CNTP). In the decoding process, if the model has\\ncomparatively high prediction entropy at a certain step, we sample multiple\\ntrials starting from the step independently and stop when encountering any\\npunctuation. Then we select the trial with the lowest perplexity score viewed\\nas the most probable and reliable trial path given the model's capacity. The\\ntrial number is negatively correlated with the prediction confidence, i.e., the\\nless confident the model is, the more trials it should sample. This is\\nconsistent with human beings' behaviour: when feeling uncertain or unconfident,\\none tends to think more creatively, exploring multiple thinking paths, to\\ncautiously select the path one feels most confident about. Extensive\\nexperiments on both LLMs and MLLMs show that our proposed CNTP approach\\noutperforms existing standard decoding strategies consistently by a clear\\nmargin. Moreover, the integration of CNTP with self consistency can further\\nimprove over vanilla self consistency. We believe our proposed CNTP has the\\npotential to become one of the default choices for LLM decoding. Code is\\navailable at https://github.com/wyzjack/CNTP.\"),\n",
       " Document(metadata={'title': 'Tracing the Interactions of Modular CMA-ES Configurations Across Problem Landscapes', 'authors': 'Ana Nikolikj, Mario Andrés Muñoz, Eva Tuba, Tome Eftimov', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02331v1'}, page_content='This paper leverages the recently introduced concept of algorithm footprints\\nto investigate the interplay between algorithm configurations and problem\\ncharacteristics. Performance footprints are calculated for six modular variants\\nof the CMA-ES algorithm (modCMA), evaluated on 24 benchmark problems from the\\nBBOB suite, across two-dimensional settings: 5-dimensional and 30-dimensional.\\nThese footprints provide insights into why different configurations of the same\\nalgorithm exhibit varying performance and identify the problem features\\ninfluencing these outcomes. Our analysis uncovers shared behavioral patterns\\nacross configurations due to common interactions with problem properties, as\\nwell as distinct behaviors on the same problem driven by differing problem\\nfeatures. The results demonstrate the effectiveness of algorithm footprints in\\nenhancing interpretability and guiding configuration choices.'),\n",
       " Document(metadata={'title': 'Neural Network-based Study for Rice Leaf Disease Recognition and Classification: A Comparative Analysis Between Feature-based Model and Direct Imaging Model', 'authors': 'Farida Siddiqi Prity, Mirza Raquib, Saydul Akbar Murad, Md. Jubayar Alam Rafi, Md. Khairul Bashar Bhuiyan, Anupam Kumar Bairagi', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02322v1'}, page_content='Rice leaf diseases significantly reduce productivity and cause economic\\nlosses, highlighting the need for early detection to enable effective\\nmanagement and improve yields. This study proposes Artificial Neural Network\\n(ANN)-based image-processing techniques for timely classification and\\nrecognition of rice diseases. Despite the prevailing approach of directly\\ninputting images of rice leaves into ANNs, there is a noticeable absence of\\nthorough comparative analysis between the Feature Analysis Detection Model\\n(FADM) and Direct Image-Centric Detection Model (DICDM), specifically when it\\ncomes to evaluating the effectiveness of Feature Extraction Algorithms (FEAs).\\nHence, this research presents initial experiments on the Feature Analysis\\nDetection Model, utilizing various image Feature Extraction Algorithms,\\nDimensionality Reduction Algorithms (DRAs), Feature Selection Algorithms\\n(FSAs), and Extreme Learning Machine (ELM). The experiments are carried out on\\ndatasets encompassing bacterial leaf blight, brown spot, leaf blast, leaf\\nscald, Sheath blight rot, and healthy leaf, utilizing 10-fold Cross-Validation\\nmethod. A Direct Image-Centric Detection Model is established without the\\nutilization of any FEA, and the evaluation of classification performance relies\\non different metrics. Ultimately, an exhaustive contrast is performed between\\nthe achievements of the Feature Analysis Detection Model and Direct\\nImage-Centric Detection Model in classifying rice leaf diseases. The results\\nreveal that the highest performance is attained using the Feature Analysis\\nDetection Model. The adoption of the proposed Feature Analysis Detection Model\\nfor detecting rice leaf diseases holds excellent potential for improving crop\\nhealth, minimizing yield losses, and enhancing overall productivity and\\nsustainability of rice farming.'),\n",
       " Document(metadata={'title': 'Iterated belief revision: from postulates to abilities', 'authors': 'Paolo Liberatore', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02319v1'}, page_content='The belief revision field is opulent in new proposals and indigent in\\nanalyses of existing approaches. Much work hinge on postulates, employed as\\nsyntactic characterizations: some revision mechanism is equivalent to some\\nproperties. Postulates constraint specific revision instances: certain\\nrevisions update certain beliefs in a certain way. As an example, if the\\nrevision is consistent with the current beliefs, it is incorporated with no\\nother change. A postulate like this tells what revisions must do and neglect\\nwhat they can do. Can they reach a certain state of beliefs? Can they reach all\\npossible states of beliefs? Can they reach all possible states of beliefs from\\nno previous belief? Can they reach a dogmatic state of beliefs, where\\neverything not believed is impossible? Can they make two conditions equally\\nbelieved? An application where every possible state of beliefs is sensible\\nrequires each state of beliefs to be reachable. An application where conditions\\nmay be equally believed requires such a belief state to be reachable. An\\napplication where beliefs may become dogmatic requires a way to make them\\ndogmatic. Such doxastic states need to be reached in a way or another. Not in\\nspecific way, as dictated by a typical belief revision postulate. This is an\\nability, not a constraint: the ability of being plastic, equating, dogmatic.\\nAmnesic, correcting, believer, damascan, learnable are other abilities. Each\\nrevision mechanism owns some of these abilities and lacks the others:\\nlexicographic, natural, restrained, very radical, full meet, radical, severe,\\nmoderate severe, deep severe, plain severe and deep severe revisions, each of\\nthese revisions is proved to possess certain abilities.'),\n",
       " Document(metadata={'title': 'MAGIC: Mask-Guided Diffusion Inpainting with Multi-Level Perturbations and Context-Aware Alignment for Few-Shot Anomaly Generation', 'authors': 'JaeHyuck Choi, MinJun Kim, JeHyeong Hong', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02314v2'}, page_content='Few-shot anomaly generation is emerging as a practical solution for\\naugmenting the scarce anomaly data in industrial quality control settings. An\\nideal generator would meet three demands at once, namely (i) keep the normal\\nbackground intact, (ii) inpaint anomalous regions to tightly overlap with the\\ncorresponding anomaly masks, and (iii) generate anomalous regions in a\\nsemantically valid location, while still producing realistic, diverse\\nappearances from only a handful of real examples. Existing diffusion-based\\nmethods usually satisfy at most two of these requirements: global anomaly\\ngenerators corrupt the background, whereas mask-guided ones often falter when\\nthe mask is imprecise or misplaced. We propose MAGIC--Mask-guided inpainting\\nwith multi-level perturbations and Context-aware alignment--to resolve all\\nthree issues. At its core, MAGIC fine-tunes a Stable Diffusion inpainting\\nbackbone that preserves normal regions and ensures strict adherence of the\\nsynthesized anomaly to the supplied mask, directly addressing background\\ncorruption and misalignment. To offset the diversity loss that fine-tuning can\\ncause, MAGIC adds two complementary perturbation strategies: (i) Gaussian\\nprompt-level perturbation applied during fine-tuning and inference that\\nbroadens the global appearance of anomalies while avoiding low-fidelity textual\\nappearances, and (ii) mask-guided spatial noise injection that enriches local\\ntexture variations. Additionally, the context-aware mask alignment module forms\\nsemantic correspondences and relocates masks so that every anomaly remains\\nplausibly contained within the host object, eliminating out-of-boundary\\nartifacts. Under a consistent identical evaluation protocol on the MVTec-AD\\ndataset, MAGIC outperforms previous state-of-the-arts in downstream anomaly\\ntasks.'),\n",
       " Document(metadata={'title': 'Holistic Continual Learning under Concept Drift with Adaptive Memory Realignment', 'authors': 'Alif Ashrafee, Jedrzej Kozal, Michal Wozniak, Bartosz Krawczyk', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02310v1'}, page_content='Traditional continual learning methods prioritize knowledge retention and\\nfocus primarily on mitigating catastrophic forgetting, implicitly assuming that\\nthe data distribution of previously learned tasks remains static. This\\noverlooks the dynamic nature of real-world data streams, where concept drift\\npermanently alters previously seen data and demands both stability and rapid\\nadaptation.\\n  We introduce a holistic framework for continual learning under concept drift\\nthat simulates realistic scenarios by evolving task distributions. As a\\nbaseline, we consider Full Relearning (FR), in which the model is retrained\\nfrom scratch on newly labeled samples from the drifted distribution. While\\neffective, this approach incurs substantial annotation and computational\\noverhead. To address these limitations, we propose Adaptive Memory Realignment\\n(AMR), a lightweight alternative that equips rehearsal-based learners with a\\ndrift-aware adaptation mechanism. AMR selectively removes outdated samples of\\ndrifted classes from the replay buffer and repopulates it with a small number\\nof up-to-date instances, effectively realigning memory with the new\\ndistribution. This targeted resampling matches the performance of FR while\\nreducing the need for labeled data and computation by orders of magnitude.\\n  To enable reproducible evaluation, we introduce four concept-drift variants\\nof standard vision benchmarks: Fashion-MNIST-CD, CIFAR10-CD, CIFAR100-CD, and\\nTiny-ImageNet-CD, where previously seen classes reappear with shifted\\nrepresentations. Comprehensive experiments on these datasets using several\\nrehearsal-based baselines show that AMR consistently counters concept drift,\\nmaintaining high accuracy with minimal overhead. These results position AMR as\\na scalable solution that reconciles stability and plasticity in non-stationary\\ncontinual learning environments.'),\n",
       " Document(metadata={'title': 'Synthetic Heuristic Evaluation: A Comparison between AI- and Human-Powered Usability Evaluation', 'authors': 'Ruican Zhong, David W. McDonald, Gary Hsieh', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02306v1'}, page_content=\"Usability evaluation is crucial in human-centered design but can be costly,\\nrequiring expert time and user compensation. In this work, we developed a\\nmethod for synthetic heuristic evaluation using multimodal LLMs' ability to\\nanalyze images and provide design feedback. Comparing our synthetic evaluations\\nto those by experienced UX practitioners across two apps, we found our\\nevaluation identified 73% and 77% of usability issues, which exceeded the\\nperformance of 5 experienced human evaluators (57% and 63%). Compared to human\\nevaluators, the synthetic evaluation's performance maintained consistent\\nperformance across tasks and excelled in detecting layout issues, highlighting\\npotential attentional and perceptual strengths of synthetic evaluation.\\nHowever, synthetic evaluation struggled with recognizing some UI components and\\ndesign conventions, as well as identifying across screen violations.\\nAdditionally, testing synthetic evaluations over time and accounts revealed\\nstable performance. Overall, our work highlights the performance differences\\nbetween human and LLM-driven evaluations, informing the design of synthetic\\nheuristic evaluations.\"),\n",
       " Document(metadata={'title': 'DoMIX: An Efficient Framework for Exploiting Domain Knowledge in Fine-Tuning', 'authors': 'Dohoon Kim, Donghun Kang, Taesup Moon', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02302v1'}, page_content='Domain-Adaptive Pre-training (DAP) has recently gained attention for its\\neffectiveness in fine-tuning pre-trained models. Building on this, continual\\nDAP has been explored to develop pre-trained models capable of incrementally\\nincorporating different domain datasets. However, existing continual DAP\\nmethods face several limitations: (1) high computational cost and GPU memory\\nusage during training; (2) sensitivity to incremental data order; and (3)\\nproviding a single, generalized model for all end tasks, which contradicts the\\nessence of DAP. In this paper, we propose DoMIX, a novel approach that\\naddresses these challenges by leveraging LoRA modules, a representative\\nparameter-efficient fine-tuning (PEFT) method. Our approach enables efficient\\nand parallel domain-adaptive pre-training that is robust to domain order and\\neffectively utilizes accumulated knowledge to provide tailored pre-trained\\nmodels for specific tasks. We also demonstrate that our method can be extended\\nbeyond the DAP setting to standard LLM fine-tuning scenarios. Code is available\\nat https://github.com/dohoonkim-ai/DoMIX.'),\n",
       " Document(metadata={'title': 'Knowledge Graph-Based Explainable and Generalized Zero-Shot Semantic Communications', 'authors': 'Zhaoyu Zhang, Lingyi Wang, Wei Wu, Fuhui Zhou, Qihui Wu', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02291v1'}, page_content='Data-driven semantic communication is based on superficial statistical\\npatterns, thereby lacking interpretability and generalization, especially for\\napplications with the presence of unseen data. To address these challenges, we\\npropose a novel knowledge graph-enhanced zero-shot semantic communication\\n(KGZS-SC) network. Guided by the structured semantic information from a\\nknowledge graph-based semantic knowledge base (KG-SKB), our scheme provides\\ngeneralized semantic representations and enables reasoning for unseen cases.\\nSpecifically, the KG-SKB aligns the semantic features in a shared category\\nsemantics embedding space and enhances the generalization ability of the\\ntransmitter through aligned semantic features, thus reducing communication\\noverhead by selectively transmitting compact visual semantics. At the receiver,\\nzero-shot learning (ZSL) is leveraged to enable direct classification for\\nunseen cases without the demand for retraining or additional computational\\noverhead, thereby enhancing the adaptability and efficiency of the\\nclassification process in dynamic or resource-constrained environments. The\\nsimulation results conducted on the APY datasets show that the proposed KGZS-SC\\nnetwork exhibits robust generalization and significantly outperforms existing\\nSC frameworks in classifying unseen categories across a range of SNR levels.'),\n",
       " Document(metadata={'title': 'Content filtering methods for music recommendation: A review', 'authors': 'Terence Zeng, Abhishek K. Umrawal', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02282v1'}, page_content='Recommendation systems have become essential in modern music streaming\\nplatforms, shaping how users discover and engage with songs. One common\\napproach in recommendation systems is collaborative filtering, which suggests\\ncontent based on the preferences of users with similar listening patterns to\\nthe target user. However, this method is less effective on media where\\ninteractions are sparse. Music is one such medium, since the average user of a\\nmusic streaming service will never listen to the vast majority of tracks. Due\\nto this sparsity, there are several challenges that have to be addressed with\\nother methods. This review examines the current state of research in addressing\\nthese challenges, with an emphasis on the role of content filtering in\\nmitigating biases inherent in collaborative filtering approaches. We explore\\nvarious methods of song classification for content filtering, including lyrical\\nanalysis using Large Language Models (LLMs) and audio signal processing\\ntechniques. Additionally, we discuss the potential conflicts between these\\ndifferent analysis methods and propose avenues for resolving such\\ndiscrepancies.'),\n",
       " Document(metadata={'title': 'Spotlighting Partially Visible Cinematic Language for Video-to-Audio Generation via Self-distillation', 'authors': 'Feizhen Huang, Yu Wu, Yutian Lin, Bo Du', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02271v1'}, page_content='Video-to-Audio (V2A) Generation achieves significant progress and plays a\\ncrucial role in film and video post-production. However, current methods\\noverlook the cinematic language, a critical component of artistic expression in\\nfilmmaking. As a result, their performance deteriorates in scenarios where\\nFoley targets are only partially visible. To address this challenge, we propose\\na simple self-distillation approach to extend V2A models to cinematic language\\nscenarios. By simulating the cinematic language variations, the student model\\nlearns to align the video features of training pairs with the same audio-visual\\ncorrespondences, enabling it to effectively capture the associations between\\nsounds and partial visual information. Our method not only achieves impressive\\nimprovements under partial visibility across all evaluation metrics, but also\\nenhances performance on the large-scale V2A dataset, VGGSound.'),\n",
       " Document(metadata={'title': 'Adaptive Cubic Regularized Second-Order Latent Factor Analysis Model', 'authors': 'Jialiang Wang, Junzhou Wang, Xin Liao', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.03036v1'}, page_content=\"High-dimensional and incomplete (HDI) data, characterized by massive node\\ninteractions, have become ubiquitous across various real-world applications.\\nSecond-order latent factor models have shown promising performance in modeling\\nthis type of data. Nevertheless, due to the bilinear and non-convex nature of\\nthe SLF model's objective function, incorporating a damping term into the\\nHessian approximation and carefully tuning associated parameters become\\nessential. To overcome these challenges, we propose a new approach in this\\nstudy, named the adaptive cubic regularized second-order latent factor analysis\\n(ACRSLF) model. The proposed ACRSLF adopts the two-fold ideas: 1) self-tuning\\ncubic regularization that dynamically mitigates non-convex optimization\\ninstabilities; 2) multi-Hessian-vector product evaluation during conjugate\\ngradient iterations for precise second-order information assimilation.\\nComprehensive experiments on two industrial HDI datasets demonstrate that the\\nACRSLF converges faster and achieves higher representation accuracy than the\\nadvancing optimizer-based LFA models.\"),\n",
       " Document(metadata={'title': 'Multi-Label Classification Framework for Hurricane Damage Assessment', 'authors': 'Zhangding Liu, Neda Mohammadi, John E. Taylor', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02265v1'}, page_content='Hurricanes cause widespread destruction, resulting in diverse damage types\\nand severities that require timely and accurate assessment for effective\\ndisaster response. While traditional single-label classification methods fall\\nshort of capturing the complexity of post-hurricane damage, this study\\nintroduces a novel multi-label classification framework for assessing damage\\nusing aerial imagery. The proposed approach integrates a feature extraction\\nmodule based on ResNet and a class-specific attention mechanism to identify\\nmultiple damage types within a single image. Using the Rescuenet dataset from\\nHurricane Michael, the proposed method achieves a mean average precision of\\n90.23%, outperforming existing baseline methods. This framework enhances\\npost-hurricane damage assessment, enabling more targeted and efficient disaster\\nresponse and contributing to future strategies for disaster mitigation and\\nresilience. This paper has been accepted at the ASCE International Conference\\non Computing in Civil Engineering (i3CE 2025), and the camera-ready version\\nwill appear in the official conference proceedings.'),\n",
       " Document(metadata={'title': 'MemAgent: Reshaping Long-Context LLM with Multi-Conv RL-based Memory Agent', 'authors': 'Hongli Yu, Tinghong Chen, Jiangtao Feng, Jiangjie Chen, Weinan Dai, Qiying Yu, Ya-Qin Zhang, Wei-Ying Ma, Jingjing Liu, Mingxuan Wang, Hao Zhou', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02259v1'}, page_content='Despite improvements by length extrapolation, efficient attention and memory\\nmodules, handling infinitely long documents with linear complexity without\\nperformance degradation during extrapolation remains the ultimate challenge in\\nlong-text processing. We directly optimize for long-text tasks in an end-to-end\\nfashion and introduce a novel agent workflow, MemAgent, which reads text in\\nsegments and updates the memory using an overwrite strategy. We extend the DAPO\\nalgorithm to facilitate training via independent-context multi-conversation\\ngeneration. MemAgent has demonstrated superb long-context capabilities, being\\nable to extrapolate from an 8K context trained on 32K text to a 3.5M QA task\\nwith performance loss < 5% and achieves 95%+ in 512K RULER test.'),\n",
       " Document(metadata={'title': 'Scaling LLM Planning: NL2FLOW for Parametric Problem Generation and Rigorous Evaluation', 'authors': 'Jungkoo Kang', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02253v1'}, page_content=\"Progress in enhancing large language model (LLM) planning and reasoning\\ncapabilities is significantly hampered by the bottleneck of scalable, reliable\\ndata generation and evaluation. To overcome this, I introduce NL2FLOW, a fully\\nautomated system for parametrically generating planning problems - expressed in\\nnatural language, a structured intermediate representation, and formal PDDL -\\nand rigorously evaluating the quality of generated plans. I demonstrate\\nNL2FLOW's capabilities by generating a dataset of 2296 problems in the\\nautomated workflow generation domain and evaluating multiple open-sourced,\\ninstruct-tuned LLMs. My results reveal that the highest performing models\\nachieved 86% success in generating valid plans and 69% in generating optimal\\nplans, specifically for problems with feasible solutions. Regression analysis\\nshows that the influence of problem characteristics on plan generation is\\ncontingent on both model and prompt design. Notably, I observed that the\\nhighest success rate for translating natural language into a JSON\\nrepresentation of a plan was lower than the highest rate of generating a valid\\nplan directly. This suggests that unnecessarily decomposing the reasoning task\\n- introducing intermediate translation steps - may actually degrade\\nperformance, implying a benefit to models capable of reasoning directly from\\nnatural language to action. As I scale LLM reasoning to increasingly complex\\nproblems, the bottlenecks and sources of error within these systems will\\ninevitably shift. Therefore, a dynamic understanding of these limitations - and\\nthe tools to systematically reveal them - will be crucial for unlocking the\\nfull potential of LLMs as intelligent problem solvers.\"),\n",
       " Document(metadata={'title': 'SurgVisAgent: Multimodal Agentic Model for Versatile Surgical Visual Enhancement', 'authors': 'Zeyu Lei, Hongyuan Yu, Jinlin Wu, Zhen Chen', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02252v1'}, page_content='Precise surgical interventions are vital to patient safety, and advanced\\nenhancement algorithms have been developed to assist surgeons in\\ndecision-making. Despite significant progress, these algorithms are typically\\ndesigned for single tasks in specific scenarios, limiting their effectiveness\\nin complex real-world situations. To address this limitation, we propose\\nSurgVisAgent, an end-to-end intelligent surgical vision agent built on\\nmultimodal large language models (MLLMs). SurgVisAgent dynamically identifies\\ndistortion categories and severity levels in endoscopic images, enabling it to\\nperform a variety of enhancement tasks such as low-light enhancement,\\noverexposure correction, motion blur elimination, and smoke removal.\\nSpecifically, to achieve superior surgical scenario understanding, we design a\\nprior model that provides domain-specific knowledge. Additionally, through\\nin-context few-shot learning and chain-of-thought (CoT) reasoning, SurgVisAgent\\ndelivers customized image enhancements tailored to a wide range of distortion\\ntypes and severity levels, thereby addressing the diverse requirements of\\nsurgeons. Furthermore, we construct a comprehensive benchmark simulating\\nreal-world surgical distortions, on which extensive experiments demonstrate\\nthat SurgVisAgent surpasses traditional single-task models, highlighting its\\npotential as a unified solution for surgical assistance.'),\n",
       " Document(metadata={'title': 'Rethinking Data Protection in the (Generative) Artificial Intelligence Era', 'authors': 'Yiming Li, Shuo Shao, Yu He, Junfeng Guo, Tianwei Zhang, Zhan Qin, Pin-Yu Chen, Michael Backes, Philip Torr, Dacheng Tao, Kui Ren', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.03034v1'}, page_content='The (generative) artificial intelligence (AI) era has profoundly reshaped the\\nmeaning and value of data. No longer confined to static content, data now\\npermeates every stage of the AI lifecycle from the training samples that shape\\nmodel parameters to the prompts and outputs that drive real-world model\\ndeployment. This shift renders traditional notions of data protection\\ninsufficient, while the boundaries of what needs safeguarding remain poorly\\ndefined. Failing to safeguard data in AI systems can inflict societal and\\nindividual, underscoring the urgent need to clearly delineate the scope of and\\nrigorously enforce data protection. In this perspective, we propose a\\nfour-level taxonomy, including non-usability, privacy preservation,\\ntraceability, and deletability, that captures the diverse protection needs\\narising in modern (generative) AI models and systems. Our framework offers a\\nstructured understanding of the trade-offs between data utility and control,\\nspanning the entire AI pipeline, including training datasets, model weights,\\nsystem prompts, and AI-generated content. We analyze representative technical\\napproaches at each level and reveal regulatory blind spots that leave critical\\nassets exposed. By offering a structured lens to align future AI technologies\\nand governance with trustworthy data practices, we underscore the urgency of\\nrethinking data protection for modern AI techniques and provide timely guidance\\nfor developers, researchers, and regulators alike.'),\n",
       " Document(metadata={'title': 'Order Acquisition Under Competitive Pressure: A Rapidly Adaptive Reinforcement Learning Approach for Ride-Hailing Subsidy Strategies', 'authors': 'Fangzhou Shi, Xiaopeng Ke, Xinye Xiong, Kexin Meng, Chang Men, Zhengdan Zhu', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02244v2'}, page_content=\"The proliferation of ride-hailing aggregator platforms presents significant\\ngrowth opportunities for ride-service providers by increasing order volume and\\ngross merchandise value (GMV). On most ride-hailing aggregator platforms,\\nservice providers that offer lower fares are ranked higher in listings and,\\nconsequently, are more likely to be selected by passengers. This competitive\\nranking mechanism creates a strong incentive for service providers to adopt\\ncoupon strategies that lower prices to secure a greater number of orders, as\\norder volume directly influences their long-term viability and sustainability.\\nThus, designing an effective coupon strategy that can dynamically adapt to\\nmarket fluctuations while optimizing order acquisition under budget constraints\\nis a critical research challenge. However, existing studies in this area remain\\nscarce.\\n  To bridge this gap, we propose FCA-RL, a novel reinforcement learning-based\\nsubsidy strategy framework designed to rapidly adapt to competitors' pricing\\nadjustments. Our approach integrates two key techniques: Fast Competition\\nAdaptation (FCA), which enables swift responses to dynamic price changes, and\\nReinforced Lagrangian Adjustment (RLA), which ensures adherence to budget\\nconstraints while optimizing coupon decisions on new price landscape.\\nFurthermore, we introduce RideGym, the first dedicated simulation environment\\ntailored for ride-hailing aggregators, facilitating comprehensive evaluation\\nand benchmarking of different pricing strategies without compromising\\nreal-world operational efficiency. Experimental results demonstrate that our\\nproposed method consistently outperforms baseline approaches across diverse\\nmarket conditions, highlighting its effectiveness in subsidy optimization for\\nride-hailing service providers.\"),\n",
       " Document(metadata={'title': 'Preserving Privacy, Increasing Accessibility, and Reducing Cost: An On-Device Artificial Intelligence Model for Medical Transcription and Note Generation', 'authors': 'Johnson Thomas, Ayush Mudgal, Wendao Liu, Nisten Tahiraj, Zeeshaan Mohammed, Dhruv Diddi', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.03033v1'}, page_content='Background: Clinical documentation represents a significant burden for\\nhealthcare providers, with physicians spending up to 2 hours daily on\\nadministrative tasks. Recent advances in large language models (LLMs) offer\\npromising solutions, but privacy concerns and computational requirements limit\\ntheir adoption in healthcare settings. Objective: To develop and evaluate a\\nprivacy-preserving, on-device medical transcription system using a fine-tuned\\nLlama 3.2 1B model capable of generating structured medical notes from medical\\ntranscriptions while maintaining complete data sovereignty entirely in the\\nbrowser. Methods: We fine-tuned a Llama 3.2 1B model using Parameter-Efficient\\nFine-Tuning (PEFT) with LoRA on 1,500 synthetic medical\\ntranscription-to-structured note pairs. The model was evaluated against the\\nbase Llama 3.2 1B on two datasets: 100 endocrinology transcripts and 140\\nmodified ACI benchmark cases. Evaluation employed both statistical metrics\\n(ROUGE, BERTScore, BLEURT) and LLM-as-judge assessments across multiple\\nclinical quality dimensions. Results: The fine-tuned OnDevice model\\ndemonstrated substantial improvements over the base model. On the ACI\\nbenchmark, ROUGE-1 scores increased from 0.346 to 0.496, while BERTScore F1\\nimproved from 0.832 to 0.866. Clinical quality assessments showed marked\\nreduction in major hallucinations (from 85 to 35 cases) and enhanced factual\\ncorrectness (2.81 to 3.54 on 5-point scale). Similar improvements were observed\\non the internal evaluation dataset, with composite scores increasing from 3.13\\nto 4.43 (+41.5%). Conclusions: Fine-tuning compact LLMs for medical\\ntranscription yields clinically meaningful improvements while enabling complete\\non-device browser deployment. This approach addresses key barriers to AI\\nadoption in healthcare: privacy preservation, cost reduction, and accessibility\\nfor resource-constrained environments.'),\n",
       " Document(metadata={'title': 'On the Mathematical Impossibility of Safe Universal Approximators', 'authors': 'Jasper Yao', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.03031v1'}, page_content='We establish fundamental mathematical limits on universal approximation\\ntheorem (UAT) system alignment by proving that catastrophic failures are an\\ninescapable feature of any useful computational system. Our central thesis is\\nthat for any universal approximator, the expressive power required for useful\\ncomputation is inextricably linked to a dense set of instabilities that make\\nperfect, reliable control a mathematical impossibility. We prove this through a\\nthree-level argument that leaves no escape routes for any class of universal\\napproximator architecture. i) Combinatorial Necessity: For the vast majority of\\npractical universal approximators (e.g., those using ReLU activations), we\\nprove that the density of catastrophic failure points is directly proportional\\nto the network\\'s expressive power. ii) Topological Necessity: For any\\ntheoretical universal approximator, we use singularity theory to prove that the\\nability to approximate generic functions requires the ability to implement the\\ndense, catastrophic singularities that characterize them. iii) Empirical\\nNecessity: We prove that the universal existence of adversarial examples is\\nempirical evidence that real-world tasks are themselves catastrophic, forcing\\nany successful model to learn and replicate these instabilities. These results,\\ncombined with a quantitative \"Impossibility Sandwich\" showing that the minimum\\ncomplexity for usefulness exceeds the maximum complexity for safety,\\ndemonstrate that perfect alignment is not an engineering challenge but a\\nmathematical impossibility. This foundational result reframes UAT safety from a\\nproblem of \"how to achieve perfect control\" to one of \"how to operate safely in\\nthe presence of irreducible uncontrollability,\" with profound implications for\\nthe future of UAT development and governance.'),\n",
       " Document(metadata={'title': 'Understanding Trade offs When Conditioning Synthetic Data', 'authors': 'Brandon Trabucco, Qasim Wani, Benjamin Pikus, Vasu Sharma', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02217v1'}, page_content='Learning robust object detectors from only a handful of images is a critical\\nchallenge in industrial vision systems, where collecting high quality training\\ndata can take months. Synthetic data has emerged as a key solution for data\\nefficient visual inspection and pick and place robotics. Current pipelines rely\\non 3D engines such as Blender or Unreal, which offer fine control but still\\nrequire weeks to render a small dataset, and the resulting images often suffer\\nfrom a large gap between simulation and reality. Diffusion models promise a\\nstep change because they can generate high quality images in minutes, yet\\nprecise control, especially in low data regimes, remains difficult. Although\\nmany adapters now extend diffusion beyond plain text prompts, the effect of\\ndifferent conditioning schemes on synthetic data quality is poorly understood.\\nWe study eighty diverse visual concepts drawn from four standard object\\ndetection benchmarks and compare two conditioning strategies: prompt based and\\nlayout based. When the set of conditioning cues is narrow, prompt conditioning\\nyields higher quality synthetic data; as diversity grows, layout conditioning\\nbecomes superior. When layout cues match the full training distribution,\\nsynthetic data raises mean average precision by an average of thirty four\\npercent and by as much as one hundred seventy seven percent compared with using\\nreal data alone.'),\n",
       " Document(metadata={'title': \"Dilution, Diffusion and Symbiosis in Spatial Prisoner's Dilemma with Reinforcement Learning\", 'authors': 'Gustavo C. Mangold, Heitor C. M. Fernandes, Mendeli H. Vainstein', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02211v2'}, page_content=\"Recent studies in the spatial prisoner's dilemma games with reinforcement\\nlearning have shown that static agents can learn to cooperate through a diverse\\nsort of mechanisms, including noise injection, different types of learning\\nalgorithms and neighbours' payoff knowledge. In this work, using an independent\\nmulti-agent Q-learning algorithm, we study the effects of dilution and mobility\\nin the spatial version of the prisoner's dilemma. Within this setting,\\ndifferent possible actions for the algorithm are defined, connecting with\\nprevious results on the classical, non-reinforcement learning spatial\\nprisoner's dilemma, showcasing the versatility of the algorithm in modeling\\ndifferent game-theoretical scenarios and the benchmarking potential of this\\napproach. As a result, a range of effects is observed, including evidence that\\ngames with fixed update rules can be qualitatively equivalent to those with\\nlearned ones, as well as the emergence of a symbiotic mutualistic effect\\nbetween populations that forms when multiple actions are defined.\"),\n",
       " Document(metadata={'title': 'EIM-TRNG: Obfuscating Deep Neural Network Weights with Encoding-in-Memory True Random Number Generator via RowHammer', 'authors': 'Ranyang Zhou, Abeer Matar A. Almalky, Gamana Aragonda, Sabbir Ahmed, Filip Roth Trønnes-Christensen, Adnan Siraj Rakin, Shaahin Angizi', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02206v1'}, page_content='True Random Number Generators (TRNGs) play a fundamental role in hardware\\nsecurity, cryptographic systems, and data protection. In the context of Deep\\nNeuralNetworks (DNNs), safeguarding model parameters, particularly weights, is\\ncritical to ensure the integrity, privacy, and intel-lectual property of AI\\nsystems. While software-based pseudo-random number generators are widely used,\\nthey lack the unpredictability and resilience offered by hardware-based TRNGs.\\nIn this work, we propose a novel and robust Encoding-in-Memory TRNG called\\nEIM-TRNG that leverages the inherent physical randomness in DRAM cell behavior,\\nparticularly under RowHammer-induced disturbances, for the first time. We\\ndemonstrate how the unpredictable bit-flips generated through carefully\\ncontrolled RowHammer operations can be harnessed as a reliable entropy source.\\nFurthermore, we apply this TRNG framework to secure DNN weight data by encoding\\nvia a combination of fixed and unpredictable bit-flips. The encrypted data is\\nlater decrypted using a key derived from the probabilistic flip behavior,\\nensuring both data confidentiality and model authenticity. Our results validate\\nthe effectiveness of DRAM-based entropy extraction for robust, low-cost\\nhardware security and offer a promising direction for protecting machine\\nlearning models at the hardware level.'),\n",
       " Document(metadata={'title': 'ESTR-CoT: Towards Explainable and Accurate Event Stream based Scene Text Recognition with Chain-of-Thought Reasoning', 'authors': 'Xiao Wang, Jingtao Jiang, Qiang Chen, Lan Chen, Lin Zhu, Yaowei Wang, Yonghong Tian, Jin Tang', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.02200v1'}, page_content='Event stream based scene text recognition is a newly arising research topic\\nin recent years which performs better than the widely used RGB cameras in\\nextremely challenging scenarios, especially the low illumination, fast motion.\\nExisting works either adopt end-to-end encoder-decoder framework or large\\nlanguage models for enhanced recognition, however, they are still limited by\\nthe challenges of insufficient interpretability and weak contextual logical\\nreasoning. In this work, we propose a novel chain-of-thought reasoning based\\nevent stream scene text recognition framework, termed ESTR-CoT. Specifically,\\nwe first adopt the vision encoder EVA-CLIP (ViT-G/14) to transform the input\\nevent stream into tokens and utilize a Llama tokenizer to encode the given\\ngeneration prompt. A Q-former is used to align the vision token to the\\npre-trained large language model Vicuna-7B and output both the answer and\\nchain-of-thought (CoT) reasoning process simultaneously. Our framework can be\\noptimized using supervised fine-tuning in an end-to-end manner. In addition, we\\nalso propose a large-scale CoT dataset to train our framework via a three stage\\nprocessing (i.e., generation, polish, and expert verification). This dataset\\nprovides a solid data foundation for the development of subsequent\\nreasoning-based large models. Extensive experiments on three event stream STR\\nbenchmark datasets (i.e., EventSTR, WordArt*, IC15*) fully validated the\\neffectiveness and interpretability of our proposed framework. The source code\\nand pre-trained models will be released on\\nhttps://github.com/Event-AHU/ESTR-CoT.'),\n",
       " Document(metadata={'title': 'Latent Chain-of-Thought? Decoding the Depth-Recurrent Transformer', 'authors': 'Wenquan Lu, Yuechuan Yang, Kyle Lee, Yanshu Li, Enqi Liu', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.02199v1'}, page_content=\"Chain-of-thought (CoT) reasoning has enabled transformer-based language\\nmodels to excel at complex mathematics and multi-step planning. However, in\\nstandard decoder-only architectures, these reasoning steps are externalized in\\nnatural language, improving interpretability at the cost of efficiency. To\\ncapture reasoning that is not easily represented in words, many works have\\nexplored recurrent architectures that aim to internalize reasoning in latent\\nspace, potentially supporting latent CoT. In this paper, we investigate whether\\nsuch reasoning structures emerge in Huginn-3.5B, a depth-recurrent Transformer\\nthat reuses layers at inference time without increasing parameter count. We\\nexamine the model's internal behavior on arithmetic tasks using a suite of\\nprobing techniques including the Logit Lens and Coda Lens. Our findings reveal\\nlimited evidence of interpretable latent CoT by tracking rank trajectories of\\nfinal and intermediate result tokens. Furthermore, we uncover significant\\nprobing inconsistencies across recurrent blocks, where the interpretability of\\nhidden states depends heavily on both the layer index and the decoding method.\\nFinally, we empirically show that increasing recurrence depth yields only\\nmarginal gains and falls well short of models that explicitly externalize\\nreasoning steps. The code is available at\\nhttps://github.com/wenquanlu/huginn-latent-cot.\"),\n",
       " Document(metadata={'title': 'Do Role-Playing Agents Practice What They Preach? Belief-Behavior Consistency in LLM-Based Simulations of Human Trust', 'authors': 'Amogh Mannekote, Adam Davies, Guohao Li, Kristy Elizabeth Boyer, ChengXiang Zhai, Bonnie J Dorr, Francesco Pinto', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.02197v1'}, page_content='As LLMs are increasingly studied as role-playing agents to generate synthetic\\ndata for human behavioral research, ensuring that their outputs remain coherent\\nwith their assigned roles has become a critical concern. In this paper, we\\ninvestigate how consistently LLM-based role-playing agents\\' stated beliefs\\nabout the behavior of the people they are asked to role-play (\"what they say\")\\ncorrespond to their actual behavior during role-play (\"how they act\").\\nSpecifically, we establish an evaluation framework to rigorously measure how\\nwell beliefs obtained by prompting the model can predict simulation outcomes in\\nadvance. Using an augmented version of the GenAgents persona bank and the Trust\\nGame (a standard economic game used to quantify players\\' trust and\\nreciprocity), we introduce a belief-behavior consistency metric to\\nsystematically investigate how it is affected by factors such as: (1) the types\\nof beliefs we elicit from LLMs, like expected outcomes of simulations versus\\ntask-relevant attributes of individual characters LLMs are asked to simulate;\\n(2) when and how we present LLMs with relevant information about Trust Game;\\nand (3) how far into the future we ask the model to forecast its actions. We\\nalso explore how feasible it is to impose a researcher\\'s own theoretical priors\\nin the event that the originally elicited beliefs are misaligned with research\\nobjectives. Our results reveal systematic inconsistencies between LLMs\\' stated\\n(or imposed) beliefs and the outcomes of their role-playing simulation, at both\\nan individual- and population-level. Specifically, we find that, even when\\nmodels appear to encode plausible beliefs, they may fail to apply them in a\\nconsistent way. These findings highlight the need to identify how and when\\nLLMs\\' stated beliefs align with their simulated behavior, allowing researchers\\nto use LLM-based agents appropriately in behavioral studies.'),\n",
       " Document(metadata={'title': 'Data Diversification Methods In Alignment Enhance Math Performance In LLMs', 'authors': 'Berkan Dokmeci, Qingyang Wu, Ben Athiwaratkun, Ce Zhang, Shuaiwen Leon Song, James Zou', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.02173v1'}, page_content='While recent advances in preference learning have enhanced alignment in human\\nfeedback, mathematical reasoning remains a persistent challenge. We investigate\\nhow data diversification strategies in preference optimization can improve the\\nmathematical reasoning abilities of large language models (LLMs). We evaluate\\nthree common data generation methods: temperature sampling, Chain-of-Thought\\nprompting, and Monte Carlo Tree Search (MCTS), and introduce\\nDiversified-ThinkSolve (DTS), a novel structured approach that systematically\\ndecomposes problems into diverse reasoning paths. Our results show that with\\nstrategically diversified preference data, models can substantially improve\\nmathematical reasoning performance, with the best approach yielding gains of\\n7.1% on GSM8K and 4.2% on MATH over the base model. Despite its strong\\nperformance, DTS incurs only a marginal computational overhead (1.03x) compared\\nto the baseline, while MCTS is nearly five times more costly with lower\\nreturns. These findings demonstrate that structured exploration of diverse\\nproblem-solving methods creates more effective preference data for mathematical\\nalignment than traditional approaches.'),\n",
       " Document(metadata={'title': 'Towards Bio-Inspired Robotic Trajectory Planning via Self-Supervised RNN', 'authors': 'Miroslav Cibula, Kristína Malinovská, Matthias Kerzel', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.02171v1'}, page_content='Trajectory planning in robotics is understood as generating a sequence of\\njoint configurations that will lead a robotic agent, or its manipulator, from\\nan initial state to the desired final state, thus completing a manipulation\\ntask while considering constraints like robot kinematics and the environment.\\nTypically, this is achieved via sampling-based planners, which are\\ncomputationally intensive. Recent advances demonstrate that trajectory planning\\ncan also be performed by supervised sequence learning of trajectories, often\\nrequiring only a single or fixed number of passes through a neural\\narchitecture, thus ensuring a bounded computation time. Such fully supervised\\napproaches, however, perform imitation learning; they do not learn based on\\nwhether the trajectories can successfully reach a goal, but try to reproduce\\nobserved trajectories. In our work, we build on this approach and propose a\\ncognitively inspired self-supervised learning scheme based on a recurrent\\narchitecture for building a trajectory model. We evaluate the feasibility of\\nthe proposed method on a task of kinematic planning for a robotic arm. The\\nresults suggest that the model is able to learn to generate trajectories only\\nusing given paired forward and inverse kinematics models, and indicate that\\nthis novel method could facilitate planning for more complex manipulation tasks\\nrequiring adaptive solutions.'),\n",
       " Document(metadata={'title': 'Deep Learning-Based Forecasting of Hotel KPIs: A Cross-City Analysis of Global Urban Markets', 'authors': 'C. J. Atapattu, Xia Cui, N. R Abeynayake', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.03028v1'}, page_content='This study employs Long Short-Term Memory (LSTM) networks to forecast key\\nperformance indicators (KPIs), Occupancy (OCC), Average Daily Rate (ADR), and\\nRevenue per Available Room (RevPAR), across five major cities: Manchester,\\nAmsterdam, Dubai, Bangkok, and Mumbai. The cities were selected for their\\ndiverse economic profiles and hospitality dynamics. Monthly data from 2018 to\\n2025 were used, with 80% for training and 20% for testing. Advanced time series\\ndecomposition and machine learning techniques enabled accurate forecasting and\\ntrend identification. Results show that Manchester and Mumbai exhibited the\\nhighest predictive accuracy, reflecting stable demand patterns, while Dubai and\\nBangkok demonstrated higher variability due to seasonal and event-driven\\ninfluences. The findings validate the effectiveness of LSTM models for urban\\nhospitality forecasting and provide a comparative framework for data-driven\\ndecision-making. The models generalisability across global cities highlights\\nits potential utility for tourism stakeholders and urban planners.'),\n",
       " Document(metadata={'title': 'Generating Large Semi-Synthetic Graphs of Any Size', 'authors': 'Rodrigo Tuna, Carlos Soares', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.02166v1'}, page_content='Graph generation is an important area in network science. Traditional\\napproaches focus on replicating specific properties of real-world graphs, such\\nas small diameters or power-law degree distributions. Recent advancements in\\ndeep learning, particularly with Graph Neural Networks, have enabled\\ndata-driven methods to learn and generate graphs without relying on predefined\\nstructural properties. Despite these advances, current models are limited by\\ntheir reliance on node IDs, which restricts their ability to generate graphs\\nlarger than the input graph and ignores node attributes. To address these\\nchallenges, we propose Latent Graph Sampling Generation (LGSG), a novel\\nframework that leverages diffusion models and node embeddings to generate\\ngraphs of varying sizes without retraining. The framework eliminates the\\ndependency on node IDs and captures the distribution of node embeddings and\\nsubgraph structures, enabling scalable and flexible graph generation.\\nExperimental results show that LGSG performs on par with baseline models for\\nstandard metrics while outperforming them in overlooked ones, such as the\\ntendency of nodes to form clusters. Additionally, it maintains consistent\\nstructural characteristics across graphs of different sizes, demonstrating\\nrobustness and scalability.'),\n",
       " Document(metadata={'title': 'Generalized Adaptive Transfer Network: Enhancing Transfer Learning in Reinforcement Learning Across Domains', 'authors': 'Abhishek Verma, Nallarasan V, Balaraman Ravindran', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.03026v1'}, page_content='Transfer learning in Reinforcement Learning (RL) enables agents to leverage\\nknowledge from source tasks to accelerate learning in target tasks. While prior\\nwork, such as the Attend, Adapt, and Transfer (A2T) framework, addresses\\nnegative transfer and selective transfer, other critical challenges remain\\nunderexplored. This paper introduces the Generalized Adaptive Transfer Network\\n(GATN), a deep RL architecture designed to tackle task generalization across\\ndomains, robustness to environmental changes, and computational efficiency in\\ntransfer. GATN employs a domain-agnostic representation module, a\\nrobustness-aware policy adapter, and an efficient transfer scheduler to achieve\\nthese goals. We evaluate GATN on diverse benchmarks, including Atari 2600,\\nMuJoCo, and a custom chatbot dialogue environment, demonstrating superior\\nperformance in cross-domain generalization, resilience to dynamic environments,\\nand reduced computational overhead compared to baselines. Our findings suggest\\nGATN is a versatile framework for real-world RL applications, such as adaptive\\nchatbots and robotic control.'),\n",
       " Document(metadata={'title': 'The Illusion of Fairness: Auditing Fairness Interventions with Audit Studies', 'authors': 'Disa Sariola, Patrick Button, Aron Culotta, Nicholas Mattei', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.02152v1'}, page_content=\"Artificial intelligence systems, especially those using machine learning, are\\nbeing deployed in domains from hiring to loan issuance in order to automate\\nthese complex decisions. Judging both the effectiveness and fairness of these\\nAI systems, and their human decision making counterpart, is a complex and\\nimportant topic studied across both computational and social sciences. Within\\nmachine learning, a common way to address bias in downstream classifiers is to\\nresample the training data to offset disparities. For example, if hiring rates\\nvary by some protected class, then one may equalize the rate within the\\ntraining set to alleviate bias in the resulting classifier. While simple and\\nseemingly effective, these methods have typically only been evaluated using\\ndata obtained through convenience samples, introducing selection bias and label\\nbias into metrics. Within the social sciences, psychology, public health, and\\nmedicine, audit studies, in which fictitious ``testers'' (e.g., resumes,\\nemails, patient actors) are sent to subjects (e.g., job openings, businesses,\\ndoctors) in randomized control trials, provide high quality data that support\\nrigorous estimates of discrimination. In this paper, we investigate how data\\nfrom audit studies can be used to improve our ability to both train and\\nevaluate automated hiring algorithms. We find that such data reveals cases\\nwhere the common fairness intervention method of equalizing base rates across\\nclasses appears to achieve parity using traditional measures, but in fact has\\nroughly 10% disparity when measured appropriately. We additionally introduce\\ninterventions based on individual treatment effect estimation methods that\\nfurther reduce algorithmic discrimination using this data.\"),\n",
       " Document(metadata={'title': 'Reasoning or Not? A Comprehensive Evaluation of Reasoning LLMs for Dialogue Summarization', 'authors': 'Keyan Jin, Yapeng Wang, Leonel Santos, Tao Fang, Xu Yang, Sio Kei Im, Hugo Gonçalo Oliveira', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.02145v1'}, page_content='Dialogue summarization is a challenging task with significant practical value\\nin customer service, meeting analysis, and conversational AI. Although large\\nlanguage models (LLMs) have achieved substantial progress in summarization\\ntasks, the performance of step-by-step reasoning architectures-specifically\\nLong Chain-of-Thought (CoT) implementations such as OpenAI-o1 and\\nDeepSeek-R1-remains unexplored for dialogue scenarios requiring concurrent\\nabstraction and conciseness. In this work, we present the first comprehensive\\nand systematic evaluation of state-of-the-art reasoning LLMs and non-reasoning\\nLLMs across three major paradigms-generic, role-oriented, and query-oriented\\ndialogue summarization. Our study spans diverse languages, domains, and summary\\nlengths, leveraging strong benchmarks (SAMSum, DialogSum, CSDS, and QMSum) and\\nadvanced evaluation protocols that include both LLM-based automatic metrics and\\nhuman-inspired criteria. Contrary to trends in other reasoning-intensive tasks,\\nour findings show that explicit stepwise reasoning does not consistently\\nimprove dialogue summarization quality. Instead, reasoning LLMs are often prone\\nto verbosity, factual inconsistencies, and less concise summaries compared to\\ntheir non-reasoning counterparts. Through scenario-specific analyses and\\ndetailed case studies, we further identify when and why explicit reasoning may\\nfail to benefit-or even hinder-summarization in complex dialogue contexts. Our\\nwork provides new insights into the limitations of current reasoning LLMs and\\nhighlights the need for targeted modeling and evaluation strategies for\\nreal-world dialogue summarization.'),\n",
       " Document(metadata={'title': 'When LLMs Disagree: Diagnosing Relevance Filtering Bias and Retrieval Divergence in SDG Search', 'authors': 'William A. Ingram, Bipasha Banerjee, Edward A. Fox', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.02139v1'}, page_content='Large language models (LLMs) are increasingly used to assign document\\nrelevance labels in information retrieval pipelines, especially in domains\\nlacking human-labeled data. However, different models often disagree on\\nborderline cases, raising concerns about how such disagreement affects\\ndownstream retrieval. This study examines labeling disagreement between two\\nopen-weight LLMs, LLaMA and Qwen, on a corpus of scholarly abstracts related to\\nSustainable Development Goals (SDGs) 1, 3, and 7. We isolate disagreement\\nsubsets and examine their lexical properties, rank-order behavior, and\\nclassification predictability. Our results show that model disagreement is\\nsystematic, not random: disagreement cases exhibit consistent lexical patterns,\\nproduce divergent top-ranked outputs under shared scoring functions, and are\\ndistinguishable with AUCs above 0.74 using simple classifiers. These findings\\nsuggest that LLM-based filtering introduces structured variability in document\\nretrieval, even under controlled prompting and shared ranking logic. We propose\\nusing classification disagreement as an object of analysis in retrieval\\nevaluation, particularly in policy-relevant or thematic search tasks.'),\n",
       " Document(metadata={'title': 'Can Artificial Intelligence solve the blockchain oracle problem? Unpacking the Challenges and Possibilities', 'authors': 'Giulio Caldarelli', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.02125v1'}, page_content='The blockchain oracle problem, which refers to the challenge of injecting\\nreliable external data into decentralized systems, remains a fundamental\\nlimitation to the development of trustless applications. While recent years\\nhave seen a proliferation of architectural, cryptographic, and economic\\nstrategies to mitigate this issue, no one has yet fully resolved the\\nfundamental question of how a blockchain can gain knowledge about the off-chain\\nworld. In this position paper, we critically assess the role artificial\\nintelligence (AI) can play in tackling the oracle problem. Drawing from both\\nacademic literature and practitioner implementations, we examine how AI\\ntechniques such as anomaly detection, language-based fact extraction, dynamic\\nreputation modeling, and adversarial resistance can enhance oracle systems. We\\nobserve that while AI introduces powerful tools for improving data quality,\\nsource selection, and system resilience, it cannot eliminate the reliance on\\nunverifiable off-chain inputs. Therefore, this study supports the idea that AI\\nshould be understood as a complementary layer of inference and filtering within\\na broader oracle design, not a substitute for trust assumptions.'),\n",
       " Document(metadata={'title': 'Resolving Turbulent Magnetohydrodynamics: A Hybrid Operator-Diffusion Framework', 'authors': 'Semih Kacmaz, E. A. Huerta, Roland Haas', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.02106v1'}, page_content='We present a hybrid machine learning framework that combines Physics-Informed\\nNeural Operators (PINOs) with score-based generative diffusion models to\\nsimulate the full spatio-temporal evolution of two-dimensional, incompressible,\\nresistive magnetohydrodynamic (MHD) turbulence across a broad range of Reynolds\\nnumbers ($\\\\mathrm{Re}$). The framework leverages the equation-constrained\\ngeneralization capabilities of PINOs to predict coherent, low-frequency\\ndynamics, while a conditional diffusion model stochastically corrects\\nhigh-frequency residuals, enabling accurate modeling of fully developed\\nturbulence. Trained on a comprehensive ensemble of high-fidelity simulations\\nwith $\\\\mathrm{Re} \\\\in \\\\{100, 250, 500, 750, 1000, 3000, 10000\\\\}$, the approach\\nachieves state-of-the-art accuracy in regimes previously inaccessible to\\ndeterministic surrogates. At $\\\\mathrm{Re}=1000$ and $3000$, the model\\nfaithfully reconstructs the full spectral energy distributions of both velocity\\nand magnetic fields late into the simulation, capturing non-Gaussian\\nstatistics, intermittent structures, and cross-field correlations with high\\nfidelity. At extreme turbulence levels ($\\\\mathrm{Re}=10000$), it remains the\\nfirst surrogate capable of recovering the high-wavenumber evolution of the\\nmagnetic field, preserving large-scale morphology and enabling statistically\\nmeaningful predictions.'),\n",
       " Document(metadata={'title': 'What Neuroscience Can Teach AI About Learning in Continuously Changing Environments', 'authors': 'Daniel Durstewitz, Bruno Averbeck, Georgia Koppe', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.02103v1'}, page_content=\"Modern AI models, such as large language models, are usually trained once on\\na huge corpus of data, potentially fine-tuned for a specific task, and then\\ndeployed with fixed parameters. Their training is costly, slow, and gradual,\\nrequiring billions of repetitions. In stark contrast, animals continuously\\nadapt to the ever-changing contingencies in their environments. This is\\nparticularly important for social species, where behavioral policies and reward\\noutcomes may frequently change in interaction with peers. The underlying\\ncomputational processes are often marked by rapid shifts in an animal's\\nbehaviour and rather sudden transitions in neuronal population activity. Such\\ncomputational capacities are of growing importance for AI systems operating in\\nthe real world, like those guiding robots or autonomous vehicles, or for\\nagentic AI interacting with humans online. Can AI learn from neuroscience? This\\nPerspective explores this question, integrating the literature on continual and\\nin-context learning in AI with the neuroscience of learning on behavioral tasks\\nwith shifting rules, reward probabilities, or outcomes. We will outline an\\nagenda for how specifically insights from neuroscience may inform current\\ndevelopments in AI in this area, and - vice versa - what neuroscience may learn\\nfrom AI, contributing to the evolving field of NeuroAI.\"),\n",
       " Document(metadata={'title': 'Energy-Based Transformers are Scalable Learners and Thinkers', 'authors': 'Alexi Gladstone, Ganesh Nanduru, Md Mofijul Islam, Peixuan Han, Hyeonjeong Ha, Aman Chadha, Yilun Du, Heng Ji, Jundong Li, Tariq Iqbal', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.02092v1'}, page_content='Inference-time computation techniques, analogous to human System 2 Thinking,\\nhave recently become popular for improving model performances. However, most\\nexisting approaches suffer from several limitations: they are modality-specific\\n(e.g., working only in text), problem-specific (e.g., verifiable domains like\\nmath and coding), or require additional supervision/training on top of\\nunsupervised pretraining (e.g., verifiers or verifiable rewards). In this\\npaper, we ask the question \"Is it possible to generalize these System 2\\nThinking approaches, and develop models that learn to think solely from\\nunsupervised learning?\" Interestingly, we find the answer is yes, by learning\\nto explicitly verify the compatibility between inputs and\\ncandidate-predictions, and then re-framing prediction problems as optimization\\nwith respect to this verifier. Specifically, we train Energy-Based Transformers\\n(EBTs) -- a new class of Energy-Based Models (EBMs) -- to assign an energy\\nvalue to every input and candidate-prediction pair, enabling predictions\\nthrough gradient descent-based energy minimization until convergence. Across\\nboth discrete (text) and continuous (visual) modalities, we find EBTs scale\\nfaster than the dominant Transformer++ approach during training, achieving an\\nup to 35% higher scaling rate with respect to data, batch size, parameters,\\nFLOPs, and depth. During inference, EBTs improve performance with System 2\\nThinking by 29% more than the Transformer++ on language tasks, and EBTs\\noutperform Diffusion Transformers on image denoising while using fewer forward\\npasses. Further, we find that EBTs achieve better results than existing models\\non most downstream tasks given the same or worse pretraining performance,\\nsuggesting that EBTs generalize better than existing approaches. Consequently,\\nEBTs are a promising new paradigm for scaling both the learning and thinking\\ncapabilities of models.'),\n",
       " Document(metadata={'title': 'Completion of the DrugMatrix Toxicogenomics Database using 3-Dimensional Tensors', 'authors': 'Tan Nguyen, Guojing Cong', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.03024v1'}, page_content=\"We explore applying a tensor completion approach to complete the DrugMatrix\\ntoxicogenomics dataset. Our hypothesis is that by preserving the 3-dimensional\\nstructure of the data, which comprises tissue, treatment, and transcriptomic\\nmeasurements, and by leveraging a machine learning formulation, our approach\\nwill improve upon prior state-of-the-art results. Our results demonstrate that\\nthe new tensor-based method more accurately reflects the original data\\ndistribution and effectively captures organ-specific variability. The proposed\\ntensor-based methodology achieved lower mean squared errors and mean absolute\\nerrors compared to both conventional Canonical Polyadic decomposition and\\n2-dimensional matrix factorization methods. In addition, our non-negative\\ntensor completion implementation reveals relationships among tissues. Our\\nfindings not only complete the world's largest in-vivo toxicogenomics database\\nwith improved accuracy but also offer a promising methodology for future\\nstudies of drugs that may cross species barriers, for example, from rats to\\nhumans.\"),\n",
       " Document(metadata={'title': 'GeoAda: Efficiently Finetune Geometric Diffusion Models with Equivariant Adapters', 'authors': 'Wanjia Zhao, Jiaqi Han, Siyi Gu, Mingjian Jiang, James Zou, Stefano Ermon', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.02085v1'}, page_content=\"Geometric diffusion models have shown remarkable success in molecular\\ndynamics and structure generation. However, efficiently fine-tuning them for\\ndownstream tasks with varying geometric controls remains underexplored. In this\\nwork, we propose an SE(3)-equivariant adapter framework ( GeoAda) that enables\\nflexible and parameter-efficient fine-tuning for controlled generative tasks\\nwithout modifying the original model architecture. GeoAda introduces a\\nstructured adapter design: control signals are first encoded through coupling\\noperators, then processed by a trainable copy of selected pretrained model\\nlayers, and finally projected back via decoupling operators followed by an\\nequivariant zero-initialized convolution. By fine-tuning only these lightweight\\nadapter modules, GeoAda preserves the model's geometric consistency while\\nmitigating overfitting and catastrophic forgetting. We theoretically prove that\\nthe proposed adapters maintain SE(3)-equivariance, ensuring that the geometric\\ninductive biases of the pretrained diffusion model remain intact during\\nadaptation. We demonstrate the wide applicability of GeoAda across diverse\\ngeometric control types, including frame control, global control, subgraph\\ncontrol, and a broad range of application domains such as particle dynamics,\\nmolecular dynamics, human motion prediction, and molecule generation. Empirical\\nresults show that GeoAda achieves state-of-the-art fine-tuning performance\\nwhile preserving original task accuracy, whereas other baselines experience\\nsignificant performance degradation due to overfitting and catastrophic\\nforgetting.\"),\n",
       " Document(metadata={'title': 'Measuring Scientific Capabilities of Language Models with a Systems Biology Dry Lab', 'authors': 'Haonan Duan, Stephen Zhewen Lu, Caitlin Fiona Harrigan, Nishkrit Desai, Jiarui Lu, Michał Koziarski, Leonardo Cotta, Chris J. Maddison', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.02083v1'}, page_content=\"Designing experiments and result interpretations are core scientific\\ncompetencies, particularly in biology, where researchers perturb complex\\nsystems to uncover the underlying systems. Recent efforts to evaluate the\\nscientific capabilities of large language models (LLMs) fail to test these\\ncompetencies because wet-lab experimentation is prohibitively expensive: in\\nexpertise, time and equipment. We introduce SciGym, a first-in-class benchmark\\nthat assesses LLMs' iterative experiment design and analysis abilities in\\nopen-ended scientific discovery tasks. SciGym overcomes the challenge of\\nwet-lab costs by running a dry lab of biological systems. These models, encoded\\nin Systems Biology Markup Language, are efficient for generating simulated\\ndata, making them ideal testbeds for experimentation on realistically complex\\nsystems. We evaluated six frontier LLMs on 137 small systems, and released a\\ntotal of 350 systems. Our evaluation shows that while more capable models\\ndemonstrated superior performance, all models' performance declined\\nsignificantly as system complexity increased, suggesting substantial room for\\nimprovement in the scientific capabilities of LLM agents.\"),\n",
       " Document(metadata={'title': 'Reasoning on a Budget: A Survey of Adaptive and Controllable Test-Time Compute in LLMs', 'authors': 'Mohammad Ali Alomrani, Yingxue Zhang, Derek Li, Qianyi Sun, Soumyasundar Pal, Zhanguang Zhang, Yaochen Hu, Rohan Deepak Ajwani, Antonios Valkanas, Raika Karimi, Peng Cheng, Yunzhou Wang, Pengyi Liao, Hanrui Huang, Bin Wang, Jianye Hao, Mark Coates', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.02076v1'}, page_content='Large language models (LLMs) have rapidly progressed into general-purpose\\nagents capable of solving a broad spectrum of tasks. However, current models\\nremain inefficient at reasoning: they apply fixed inference-time compute\\nregardless of task complexity, often overthinking simple problems while\\nunderthinking hard ones. This survey presents a comprehensive review of\\nefficient test-time compute (TTC) strategies, which aim to improve the\\ncomputational efficiency of LLM reasoning. We introduce a two-tiered taxonomy\\nthat distinguishes between L1-controllability, methods that operate under fixed\\ncompute budgets, and L2-adaptiveness, methods that dynamically scale inference\\nbased on input difficulty or model confidence. We benchmark leading proprietary\\nLLMs across diverse datasets, highlighting critical trade-offs between\\nreasoning performance and token usage. Compared to prior surveys on efficient\\nreasoning, our review emphasizes the practical control, adaptability, and\\nscalability of TTC methods. Finally, we discuss emerging trends such as hybrid\\nthinking models and identify key challenges for future work towards making LLMs\\nmore computationally efficient, robust, and responsive to user constraints.'),\n",
       " Document(metadata={'title': 'Large Language Models for Crash Detection in Video: A Survey of Methods, Datasets, and Challenges', 'authors': 'Sanjeda Akter, Ibne Farabi Shihab, Anuj Sharma', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.02074v1'}, page_content='Crash detection from video feeds is a critical problem in intelligent\\ntransportation systems. Recent developments in large language models (LLMs) and\\nvision-language models (VLMs) have transformed how we process, reason about,\\nand summarize multimodal information. This paper surveys recent methods\\nleveraging LLMs for crash detection from video data. We present a structured\\ntaxonomy of fusion strategies, summarize key datasets, analyze model\\narchitectures, compare performance benchmarks, and discuss ongoing challenges\\nand opportunities. Our review provides a foundation for future research in this\\nfast-growing intersection of video understanding and foundation models.'),\n",
       " Document(metadata={'title': 'HCVR: A Hybrid Approach with Correlation-aware Voting Rules for Feature Selection', 'authors': 'Nikita Bhedasgaonkar, Rushikesh K. Joshi', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.02073v1'}, page_content='In this paper, we propose HCVR (Hybrid approach with Correlation-aware Voting\\nRules), a lightweight rule-based feature selection method that combines\\nParameter-to-Parameter (P2P) and Parameter-to-Target (P2T) correlations to\\neliminate redundant features and retain relevant ones. This method is a hybrid\\nof non-iterative and iterative filtering approaches for dimensionality\\nreduction. It is a greedy method, which works by backward elimination,\\neliminating possibly multiple features at every step. The rules contribute to\\nvoting for features, and a decision to keep or discard is made by majority\\nvoting. The rules make use of correlation thresholds between every pair of\\nfeatures, and between features and the target. We provide the results from the\\napplication of HCVR to the SPAMBASE dataset. The results showed improvement\\nperformance as compared to traditional non-iterative (CFS, mRMR and MI) and\\niterative (RFE, SFS and Genetic Algorithm) techniques. The effectiveness was\\nassessed based on the performance of different classifiers after applying\\nfiltering.'),\n",
       " Document(metadata={'title': 'MGC: A Compiler Framework Exploiting Compositional Blindness in Aligned LLMs for Malware Generation', 'authors': 'Lu Yan, Zhuo Zhang, Xiangzhe Xu, Shengwei An, Guangyu Shen, Zhou Xuan, Xuan Chen, Xiangyu Zhang', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.02057v1'}, page_content='Large language models (LLMs) have democratized software development, reducing\\nthe expertise barrier for programming complex applications. This accessibility\\nextends to malicious software development, raising significant security\\nconcerns. While LLM providers have implemented alignment mechanisms to prevent\\ndirect generation of overtly malicious code, these safeguards predominantly\\nevaluate individual prompts in isolation, overlooking a critical vulnerability:\\nmalicious operations can be systematically decomposed into benign-appearing\\nsub-tasks. In this paper, we introduce the Malware Generation Compiler (MGC), a\\nnovel framework that leverages this vulnerability through modular decomposition\\nand alignment-evasive generation. MGC employs a specialized Malware Description\\nIntermediate Representation (MDIR) to bridge high-level malicious intents and\\nbenign-appearing code snippets. Extensive evaluation demonstrates that our\\nattack reliably generates functional malware across diverse task specifications\\nand categories, outperforming jailbreaking methods by +365.79% and underground\\nservices by +78.07% in correctness on three benchmark datasets. Case studies\\nfurther show that MGC can reproduce and even enhance 16 real-world malware\\nsamples. This work provides critical insights for security researchers by\\nexposing the risks of compositional attacks against aligned AI systems.\\nDemonstrations are available at\\nhttps://sites.google.com/view/malware-generation-compiler.'),\n",
       " Document(metadata={'title': 'AC-DiT: Adaptive Coordination Diffusion Transformer for Mobile Manipulation', 'authors': 'Sixiang Chen, Jiaming Liu, Siyuan Qian, Han Jiang, Lily Li, Renrui Zhang, Zhuoyang Liu, Chenyang Gu, Chengkai Hou, Pengwei Wang, Zhongyuan Wang, Shanghang Zhang', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.01961v3'}, page_content=\"Recently, mobile manipulation has attracted increasing attention for enabling\\nlanguage-conditioned robotic control in household tasks. However, existing\\nmethods still face challenges in coordinating mobile base and manipulator,\\nprimarily due to two limitations. On the one hand, they fail to explicitly\\nmodel the influence of the mobile base on manipulator control, which easily\\nleads to error accumulation under high degrees of freedom. On the other hand,\\nthey treat the entire mobile manipulation process with the same visual\\nobservation modality (e.g., either all 2D or all 3D), overlooking the distinct\\nmultimodal perception requirements at different stages during mobile\\nmanipulation. To address this, we propose the Adaptive Coordination Diffusion\\nTransformer (AC-DiT), which enhances mobile base and manipulator coordination\\nfor end-to-end mobile manipulation. First, since the motion of the mobile base\\ndirectly influences the manipulator's actions, we introduce a mobility-to-body\\nconditioning mechanism that guides the model to first extract base motion\\nrepresentations, which are then used as context prior for predicting whole-body\\nactions. This enables whole-body control that accounts for the potential impact\\nof the mobile base's motion. Second, to meet the perception requirements at\\ndifferent stages of mobile manipulation, we design a perception-aware\\nmultimodal conditioning strategy that dynamically adjusts the fusion weights\\nbetween various 2D visual images and 3D point clouds, yielding visual features\\ntailored to the current perceptual needs. This allows the model to, for\\nexample, adaptively rely more on 2D inputs when semantic information is crucial\\nfor action prediction, while placing greater emphasis on 3D geometric\\ninformation when precise spatial understanding is required. We validate AC-DiT\\nthrough extensive experiments on both simulated and real-world mobile\\nmanipulation tasks.\"),\n",
       " Document(metadata={'title': 'Locality-aware Parallel Decoding for Efficient Autoregressive Image Generation', 'authors': 'Zhuoyang Zhang, Luke J. Huang, Chengyue Wu, Shang Yang, Kelly Peng, Yao Lu, Song Han', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.01957v1'}, page_content='We present Locality-aware Parallel Decoding (LPD) to accelerate\\nautoregressive image generation. Traditional autoregressive image generation\\nrelies on next-patch prediction, a memory-bound process that leads to high\\nlatency. Existing works have tried to parallelize next-patch prediction by\\nshifting to multi-patch prediction to accelerate the process, but only achieved\\nlimited parallelization. To achieve high parallelization while maintaining\\ngeneration quality, we introduce two key techniques: (1) Flexible Parallelized\\nAutoregressive Modeling, a novel architecture that enables arbitrary generation\\nordering and degrees of parallelization. It uses learnable position query\\ntokens to guide generation at target positions while ensuring mutual visibility\\namong concurrently generated tokens for consistent parallel decoding. (2)\\nLocality-aware Generation Ordering, a novel schedule that forms groups to\\nminimize intra-group dependencies and maximize contextual support, enhancing\\ngeneration quality. With these designs, we reduce the generation steps from 256\\nto 20 (256$\\\\times$256 res.) and 1024 to 48 (512$\\\\times$512 res.) without\\ncompromising quality on the ImageNet class-conditional generation, and\\nachieving at least 3.4$\\\\times$ lower latency than previous parallelized\\nautoregressive models.'),\n",
       " Document(metadata={'title': 'How Well Does GPT-4o Understand Vision? Evaluating Multimodal Foundation Models on Standard Computer Vision Tasks', 'authors': 'Rahul Ramachandran, Ali Garjani, Roman Bachmann, Andrei Atanov, Oğuzhan Fatih Kar, Amir Zamir', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.01955v1'}, page_content='Multimodal foundation models, such as GPT-4o, have recently made remarkable\\nprogress, but it is not clear where exactly these models stand in terms of\\nunderstanding vision. In this paper, we benchmark the performance of popular\\nmultimodal foundation models (GPT-4o, o4-mini, Gemini 1.5 Pro and Gemini 2.0\\nFlash, Claude 3.5 Sonnet, Qwen2-VL, Llama 3.2) on standard computer vision\\ntasks (semantic segmentation, object detection, image classification, depth and\\nsurface normal prediction) using established datasets (e.g., COCO, ImageNet and\\nits variants, etc).\\n  The main challenges to performing this are: 1) most models are trained to\\noutput text and cannot natively express versatile domains, such as segments or\\n3D geometry, and 2) many leading models are proprietary and accessible only at\\nan API level, i.e., there is no weight access to adapt them. We address these\\nchallenges by translating standard vision tasks into equivalent text-promptable\\nand API-compatible tasks via prompt chaining to create a standardized\\nbenchmarking framework.\\n  We observe that 1) the models are not close to the state-of-the-art\\nspecialist models at any task. However, 2) they are respectable generalists;\\nthis is remarkable as they are presumably trained on primarily image-text-based\\ntasks. 3) They perform semantic tasks notably better than geometric ones. 4)\\nWhile the prompt-chaining techniques affect performance, better models exhibit\\nless sensitivity to prompt variations. 5) GPT-4o performs the best among\\nnon-reasoning models, securing the top position in 4 out of 6 tasks, 6)\\nreasoning models, e.g. o3, show improvements in geometric tasks, and 7) a\\npreliminary analysis of models with native image generation, like the latest\\nGPT-4o, shows they exhibit quirks like hallucinations and spatial\\nmisalignments.'),\n",
       " Document(metadata={'title': 'SpecCLIP: Aligning and Translating Spectroscopic Measurements for Stars', 'authors': 'Xiaosheng Zhao, Yang Huang, Guirong Xue, Xiao Kong, Jifeng Liu, Xiaoyu Tang, Timothy C. Beers, Yuan-Sen Ting, A-Li Luo', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.01939v1'}, page_content='In recent years, large language models (LLMs) have transformed natural\\nlanguage understanding through vast datasets and large-scale parameterization.\\nInspired by this success, we present SpecCLIP, a foundation model framework\\nthat extends LLM-inspired methodologies to stellar spectral analysis. Stellar\\nspectra, akin to structured language, encode rich physical and chemical\\ninformation about stars. By training foundation models on large-scale spectral\\ndatasets, our goal is to learn robust and informative embeddings that support\\ndiverse downstream applications. As a proof of concept, SpecCLIP involves\\npre-training on two spectral types--LAMOST low-resolution and Gaia XP--followed\\nby contrastive alignment using the CLIP (Contrastive Language-Image\\nPre-training) framework, adapted to associate spectra from different\\ninstruments. This alignment is complemented by auxiliary decoders that preserve\\nspectrum-specific information and enable translation (prediction) between\\nspectral types, with the former achieved by maximizing mutual information\\nbetween embeddings and input spectra. The result is a cross-spectrum framework\\nenabling intrinsic calibration and flexible applications across instruments. We\\ndemonstrate that fine-tuning these models on moderate-sized labeled datasets\\nimproves adaptability to tasks such as stellar-parameter estimation and\\nchemical-abundance determination. SpecCLIP also enhances the accuracy and\\nprecision of parameter estimates benchmarked against external survey data.\\nAdditionally, its similarity search and cross-spectrum prediction capabilities\\noffer potential for anomaly detection. Our results suggest that contrastively\\ntrained foundation models enriched with spectrum-aware decoders can advance\\nprecision stellar spectroscopy.'),\n",
       " Document(metadata={'title': 'Adaptability of ASR Models on Low-Resource Language: A Comparative Study of Whisper and Wav2Vec-BERT on Bangla', 'authors': 'Md Sazzadul Islam Ridoy, Sumi Akter, Md. Aminur Rahman', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.01931v1'}, page_content=\"In recent years, neural models trained on large multilingual text and speech\\ndatasets have shown great potential for supporting low-resource languages. This\\nstudy investigates the performances of two state-of-the-art Automatic Speech\\nRecognition (ASR) models, OpenAI's Whisper (Small & Large-V2) and Facebook's\\nWav2Vec-BERT on Bangla, a low-resource language. We have conducted experiments\\nusing two publicly available datasets: Mozilla Common Voice-17 and OpenSLR to\\nevaluate model performances. Through systematic fine-tuning and hyperparameter\\noptimization, including learning rate, epochs, and model checkpoint selection,\\nwe have compared the models based on Word Error Rate (WER), Character Error\\nRate (CER), Training Time, and Computational Efficiency. The Wav2Vec-BERT model\\noutperformed Whisper across all key evaluation metrics, demonstrated superior\\nperformance while requiring fewer computational resources, and offered valuable\\ninsights to develop robust speech recognition systems in low-resource\\nlinguistic settings.\"),\n",
       " Document(metadata={'title': 'Exploring a Hybrid Deep Learning Approach for Anomaly Detection in Mental Healthcare Provider Billing: Addressing Label Scarcity through Semi-Supervised Anomaly Detection', 'authors': 'Samirah Bakker, Yao Ma, Seyed Sahand Mohammadi Ziabari', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.01924v1'}, page_content='The complexity of mental healthcare billing enables anomalies, including\\nfraud. While machine learning methods have been applied to anomaly detection,\\nthey often struggle with class imbalance, label scarcity, and complex\\nsequential patterns. This study explores a hybrid deep learning approach\\ncombining Long Short-Term Memory (LSTM) networks and Transformers, with\\npseudo-labeling via Isolation Forests (iForest) and Autoencoders (AE). Prior\\nwork has not evaluated such hybrid models trained on pseudo-labeled data in the\\ncontext of healthcare billing. The approach is evaluated on two real-world\\nbilling datasets related to mental healthcare. The iForest LSTM baseline\\nachieves the highest recall (0.963) on declaration-level data. On the\\noperation-level data, the hybrid iForest-based model achieves the highest\\nrecall (0.744), though at the cost of lower precision. These findings highlight\\nthe potential of combining pseudo-labeling with hybrid deep learning in\\ncomplex, imbalanced anomaly detection settings.'),\n",
       " Document(metadata={'title': 'End-to-End Large Portfolio Optimization for Variance Minimization with Neural Networks through Covariance Cleaning', 'authors': 'Christian Bongiorno, Efstratios Manolakis, Rosario Nunzio Mantegna', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.01918v1'}, page_content=\"We develop a rotation-invariant neural network that provides the global\\nminimum-variance portfolio by jointly learning how to lag-transform historical\\nreturns and how to regularise both the eigenvalues and the marginal\\nvolatilities of large equity covariance matrices. This explicit mathematical\\nmapping offers clear interpretability of each module's role, so the model\\ncannot be regarded as a pure black-box. The architecture mirrors the analytical\\nform of the global minimum-variance solution yet remains agnostic to dimension,\\nso a single model can be calibrated on panels of a few hundred stocks and\\napplied, without retraining, to one thousand US equities-a cross-sectional jump\\nthat demonstrates robust out-of-sample generalisation. The loss function is the\\nfuture realized minimum portfolio variance and is optimized end-to-end on real\\ndaily returns. In out-of-sample tests from January 2000 to December 2024 the\\nestimator delivers systematically lower realised volatility, smaller maximum\\ndrawdowns, and higher Sharpe ratios than the best analytical competitors,\\nincluding state-of-the-art non-linear shrinkage. Furthermore, although the\\nmodel is trained end-to-end to produce an unconstrained (long-short)\\nminimum-variance portfolio, we show that its learned covariance representation\\ncan be used in general optimizers under long-only constraints with virtually no\\nloss in its performance advantage over competing estimators. These gains\\npersist when the strategy is executed under a highly realistic implementation\\nframework that models market orders at the auctions, empirical slippage,\\nexchange fees, and financing charges for leverage, and they remain stable\\nduring episodes of acute market stress.\"),\n",
       " Document(metadata={'title': 'Gradient-Adaptive Policy Optimization: Towards Multi-Objective Alignment of Large Language Models', 'authors': 'Chengao Li, Hanyu Zhang, Yunkun Xu, Hongyan Xue, Xiang Ao, Qing He', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.01915v1'}, page_content=\"Reinforcement Learning from Human Feedback (RLHF) has emerged as a powerful\\ntechnique for aligning large language models (LLMs) with human preferences.\\nHowever, effectively aligning LLMs with diverse human preferences remains a\\nsignificant challenge, particularly when they are conflict. To address this\\nissue, we frame human value alignment as a multi-objective optimization\\nproblem, aiming to maximize a set of potentially conflicting objectives. We\\nintroduce Gradient-Adaptive Policy Optimization (GAPO), a novel fine-tuning\\nparadigm that employs multiple-gradient descent to align LLMs with diverse\\npreference distributions. GAPO adaptively rescales the gradients for each\\nobjective to determine an update direction that optimally balances the\\ntrade-offs between objectives. Additionally, we introduce P-GAPO, which\\nincorporates user preferences across different objectives and achieves Pareto\\nsolutions that better align with the user's specific needs. Our theoretical\\nanalysis demonstrates that GAPO converges towards a Pareto optimal solution for\\nmultiple objectives. Empirical results on Mistral-7B show that GAPO outperforms\\ncurrent state-of-the-art methods, achieving superior performance in both\\nhelpfulness and harmlessness.\"),\n",
       " Document(metadata={'title': 'AI4Research: A Survey of Artificial Intelligence for Scientific Research', 'authors': 'Qiguang Chen, Mingda Yang, Libo Qin, Jinhao Liu, Zheng Yan, Jiannan Guan, Dengyun Peng, Yiyan Ji, Hanjing Li, Mengkang Hu, Yimeng Zhang, Yihao Liang, Yuhang Zhou, Jiaqi Wang, Zhi Chen, Wanxiang Che', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.01903v1'}, page_content='Recent advancements in artificial intelligence (AI), particularly in large\\nlanguage models (LLMs) such as OpenAI-o1 and DeepSeek-R1, have demonstrated\\nremarkable capabilities in complex domains such as logical reasoning and\\nexperimental coding. Motivated by these advancements, numerous studies have\\nexplored the application of AI in the innovation process, particularly in the\\ncontext of scientific research. These AI technologies primarily aim to develop\\nsystems that can autonomously conduct research processes across a wide range of\\nscientific disciplines. Despite these significant strides, a comprehensive\\nsurvey on AI for Research (AI4Research) remains absent, which hampers our\\nunderstanding and impedes further development in this field. To address this\\ngap, we present a comprehensive survey and offer a unified perspective on\\nAI4Research. Specifically, the main contributions of our work are as follows:\\n(1) Systematic taxonomy: We first introduce a systematic taxonomy to classify\\nfive mainstream tasks in AI4Research. (2) New frontiers: Then, we identify key\\nresearch gaps and highlight promising future directions, focusing on the rigor\\nand scalability of automated experiments, as well as the societal impact. (3)\\nAbundant applications and resources: Finally, we compile a wealth of resources,\\nincluding relevant multidisciplinary applications, data corpora, and tools. We\\nhope our work will provide the research community with quick access to these\\nresources and stimulate innovative breakthroughs in AI4Research.'),\n",
       " Document(metadata={'title': 'Towards Foundation Auto-Encoders for Time-Series Anomaly Detection', 'authors': 'Gastón García González, Pedro Casas, Emilio Martínez, Alicia Fernández', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.01875v1'}, page_content='We investigate a novel approach to time-series modeling, inspired by the\\nsuccesses of large pretrained foundation models. We introduce FAE (Foundation\\nAuto-Encoders), a foundation generative-AI model for anomaly detection in\\ntime-series data, based on Variational Auto-Encoders (VAEs). By foundation, we\\nmean a model pretrained on massive amounts of time-series data which can learn\\ncomplex temporal patterns useful for accurate modeling, forecasting, and\\ndetection of anomalies on previously unseen datasets. FAE leverages VAEs and\\nDilated Convolutional Neural Networks (DCNNs) to build a generic model for\\nunivariate time-series modeling, which could eventually perform properly in\\nout-of-the-box, zero-shot anomaly detection applications. We introduce the main\\nconcepts of FAE, and present preliminary results in different multi-dimensional\\ntime-series datasets from various domains, including a real dataset from an\\noperational mobile ISP, and the well known KDD 2021 Anomaly Detection dataset.'),\n",
       " Document(metadata={'title': 'Bridging UI Design and chatbot Interactions: Applying Form-Based Principles to Conversational Agents', 'authors': 'Sanjay Krishna Anbalagan, Xinrui Nie, Umesh Mohan, Vijay Kumar Kanamarlapudi, Anughna Kommalapati, Xiaodan Zhao', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.01862v1'}, page_content='Domain specific chatbot applications often involve multi step interactions,\\nsuch as refining search filters, selecting multiple items, or performing\\ncomparisons. Traditional graphical user interfaces (GUIs) handle these\\nworkflows by providing explicit \"Submit\" (commit data) and \"Reset\" (discard\\ndata) actions, allowing back-end systems to track user intent unambiguously. In\\ncontrast, conversational agents rely on subtle language cues, which can lead to\\nconfusion and incomplete context management. This paper proposes modeling these\\nGUI inspired metaphors acknowledgment (submit like) and context switching\\n(reset-like) as explicit tasks within large language model (LLM) prompts. By\\ncapturing user acknowledgment, reset actions, and chain of thought (CoT)\\nreasoning as structured session data, we preserve clarity, reduce user\\nconfusion, and align domain-specific chatbot interactions with back-end logic.\\nWe demonstrate our approach in hotel booking and customer management scenarios,\\nhighlighting improvements in multi-turn task coherence, user satisfaction, and\\nefficiency.'),\n",
       " Document(metadata={'title': 'Refining Gelfond Rationality Principle Towards More Comprehensive Foundational Principles for Answer Set Semantics', 'authors': 'Yi-Dong Shen, Thomas Eiter', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.01833v1'}, page_content=\"Non-monotonic logic programming is the basis for a declarative problem\\nsolving paradigm known as answer set programming (ASP). Departing from the\\nseminal definition by Gelfond and Lifschitz in 1988 for simple normal logic\\nprograms, various answer set semantics have been proposed for extensions. We\\nconsider two important questions: (1) Should the minimal model property,\\nconstraint monotonicity and foundedness as defined in the literature be\\nmandatory conditions for an answer set semantics in general? (2) If not, what\\nother properties could be considered as general principles for answer set\\nsemantics? We address the two questions. First, it seems that the three\\naforementioned conditions may sometimes be too strong, and we illustrate with\\nexamples that enforcing them may exclude expected answer sets. Second, we\\nevolve the Gelfond answer set (GAS) principles for answer set construction by\\nrefining the Gelfond's rationality principle to well-supportedness, minimality\\nw.r.t. negation by default and minimality w.r.t. epistemic negation. The\\nprinciple of well-supportedness guarantees that every answer set is\\nconstructible from if-then rules obeying a level mapping and is thus free of\\ncircular justification, while the two minimality principles ensure that the\\nformalism minimizes knowledge both at the level of answer sets and of world\\nviews. Third, to embody the refined GAS principles, we extend the notion of\\nwell-supportedness substantially to answer sets and world views, respectively.\\nFourth, we define new answer set semantics in terms of the refined GAS\\nprinciples. Fifth, we use the refined GAS principles as an alternative baseline\\nto intuitively assess the existing answer set semantics. Finally, we analyze\\nthe computational complexity.\"),\n",
       " Document(metadata={'title': 'mGRADE: Minimal Recurrent Gating Meets Delay Convolutions for Lightweight Sequence Modeling', 'authors': 'Tristan Torchet, Christian Metzner, Laura Kriener, Melika Payvand', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.01829v1'}, page_content=\"Edge devices for temporal processing demand models that capture both short-\\nand long- range dynamics under tight memory constraints. While Transformers\\nexcel at sequence modeling, their quadratic memory scaling with sequence length\\nmakes them impractical for such settings. Recurrent Neural Networks (RNNs)\\noffer constant memory but train sequentially, and Temporal Convolutional\\nNetworks (TCNs), though efficient, scale memory with kernel size. To address\\nthis, we propose mGRADE (mininally Gated Recurrent Architecture with Delay\\nEmbedding), a hybrid-memory system that integrates a temporal 1D-convolution\\nwith learnable spacings followed by a minimal gated recurrent unit (minGRU).\\nThis design allows the convolutional layer to realize a flexible delay\\nembedding that captures rapid temporal variations, while the recurrent module\\nefficiently maintains global context with minimal memory overhead. We validate\\nour approach on two synthetic tasks, demonstrating that mGRADE effectively\\nseparates and preserves multi-scale temporal features. Furthermore, on\\nchallenging pixel-by-pixel image classification benchmarks, mGRADE consistently\\noutperforms both pure convolutional and pure recurrent counterparts using\\napproximately 20% less memory footprint, highlighting its suitability for\\nmemory-constrained temporal processing at the edge. This highlights mGRADE's\\npromise as an efficient solution for memory-constrained multi-scale temporal\\nprocessing at the edge.\"),\n",
       " Document(metadata={'title': 'MILP-SAT-GNN: Yet Another Neural SAT Solver', 'authors': 'Franco Alberto Cardillo, Hamza Khyari, Umberto Straccia', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.01825v1'}, page_content='We proposes a novel method that enables Graph Neural Networks (GNNs) to solve\\nSAT problems by leveraging a technique developed for applying GNNs to Mixed\\nInteger Linear Programming (MILP). Specifically, k-CNF formulae are mapped into\\nMILP problems, which are then encoded as weighted bipartite graphs and\\nsubsequently fed into a GNN for training and testing. From a theoretical\\nperspective: (i) we establish permutation and equivalence invariance results,\\ndemonstrating that the method produces outputs that are stable under reordering\\nof clauses and variables; (ii) we identify a theoretical limitation, showing\\nthat for a class of formulae called foldable formulae, standard GNNs cannot\\nalways distinguish satisfiable from unsatisfiable instances; (iii) we prove a\\nuniversal approximation theorem, establishing that with Random Node\\nInitialization (RNI), the method can approximate SAT solving to arbitrary\\nprecision on finite datasets, that is, the GNN becomes approximately sound and\\ncomplete on such datasets. Furthermore, we show that for unfoldable formulae,\\nthe same approximation guarantee can be achieved without the need for RNI.\\nFinally, we conduct an experimental evaluation of our approach, which show\\nthat, despite the simplicity of the neural architecture, the method achieves\\npromising results.'),\n",
       " Document(metadata={'title': 'Empowering Manufacturers with Privacy-Preserving AI Tools: A Case Study in Privacy-Preserving Machine Learning to Solve Real-World Problems', 'authors': 'Xiaoyu Ji, Jessica Shorland, Joshua Shank, Pascal Delpe-Brice, Latanya Sweeney, Jan Allebach, Ali Shakouri', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.01808v1'}, page_content=\"Small- and medium-sized manufacturers need innovative data tools but, because\\nof competition and privacy concerns, often do not want to share their\\nproprietary data with researchers who might be interested in helping. This\\npaper introduces a privacy-preserving platform by which manufacturers may\\nsafely share their data with researchers through secure methods, so that those\\nresearchers then create innovative tools to solve the manufacturers' real-world\\nproblems, and then provide tools that execute solutions back onto the platform\\nfor others to use with privacy and confidentiality guarantees. We illustrate\\nthis problem through a particular use case which addresses an important problem\\nin the large-scale manufacturing of food crystals, which is that quality\\ncontrol relies on image analysis tools. Previous to our research, food crystals\\nin the images were manually counted, which required substantial and\\ntime-consuming human efforts, but we have developed and deployed a crystal\\nanalysis tool which makes this process both more rapid and accurate. The tool\\nenables automatic characterization of the crystal size distribution and numbers\\nfrom microscope images while the natural imperfections from the sample\\npreparation are automatically removed; a machine learning model to count high\\nresolution translucent crystals and agglomeration of crystals was also\\ndeveloped to aid in these efforts. The resulting algorithm was then packaged\\nfor real-world use on the factory floor via a web-based app secured through the\\noriginating privacy-preserving platform, allowing manufacturers to use it while\\nkeeping their proprietary data secure. After demonstrating this full process,\\nfuture directions are also explored.\"),\n",
       " Document(metadata={'title': 'LoRA Fine-Tuning Without GPUs: A CPU-Efficient Meta-Generation Framework for LLMs', 'authors': 'Reza Arabpour, Haitz Sáez de Ocáriz Borde, Anastasis Kratsios', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.01806v1'}, page_content='Low-Rank Adapters (LoRAs) have transformed the fine-tuning of Large Language\\nModels (LLMs) by enabling parameter-efficient updates. However, their\\nwidespread adoption remains limited by the reliance on GPU-based training. In\\nthis work, we propose a theoretically grounded approach to LoRA fine-tuning\\ndesigned specifically for users with limited computational resources,\\nparticularly those restricted to standard laptop CPUs. Our method learns a\\nmeta-operator that maps any input dataset, represented as a probability\\ndistribution, to a set of LoRA weights by leveraging a large bank of\\npre-trained adapters for the Mistral-7B-Instruct-v0.2 model. Instead of\\nperforming new gradient-based updates, our pipeline constructs adapters via\\nlightweight combinations of existing LoRAs directly on CPU. While the resulting\\nadapters do not match the performance of GPU-trained counterparts, they\\nconsistently outperform the base Mistral model on downstream tasks, offering a\\npractical and accessible alternative to traditional GPU-based fine-tuning.'),\n",
       " Document(metadata={'title': 'How Do Vision-Language Models Process Conflicting Information Across Modalities?', 'authors': 'Tianze Hua, Tian Yun, Ellie Pavlick', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.01790v1'}, page_content='AI models are increasingly required to be multimodal, integrating disparate\\ninput streams into a coherent state representation on which subsequent\\nbehaviors and actions can be based. This paper seeks to understand how such\\nmodels behave when input streams present conflicting information. Focusing\\nspecifically on vision-language models, we provide inconsistent inputs (e.g.,\\nan image of a dog paired with the caption \"A photo of a cat\") and ask the model\\nto report the information present in one of the specific modalities (e.g.,\\n\"What does the caption say / What is in the image?\"). We find that models often\\nfavor one modality over the other, e.g., reporting the image regardless of what\\nthe caption says, but that different models differ in which modality they\\nfavor. We find evidence that the behaviorally preferred modality is evident in\\nthe internal representational structure of the model, and that specific\\nattention heads can restructure the representations to favor one modality over\\nthe other. Moreover, we find modality-agnostic \"router heads\" which appear to\\npromote answers about the modality requested in the instruction, and which can\\nbe manipulated or transferred in order to improve performance across datasets\\nand modalities. Together, the work provides essential steps towards identifying\\nand controlling if and how models detect and resolve conflicting signals within\\ncomplex multimodal environments.'),\n",
       " Document(metadata={'title': 'Are Vision Transformer Representations Semantically Meaningful? A Case Study in Medical Imaging', 'authors': 'Montasir Shams, Chashi Mahiul Islam, Shaeke Salman, Phat Tran, Xiuwen Liu', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.01788v1'}, page_content='Vision transformers (ViTs) have rapidly gained prominence in medical imaging\\ntasks such as disease classification, segmentation, and detection due to their\\nsuperior accuracy compared to conventional deep learning models. However, due\\nto their size and complex interactions via the self-attention mechanism, they\\nare not well understood. In particular, it is unclear whether the\\nrepresentations produced by such models are semantically meaningful. In this\\npaper, using a projected gradient-based algorithm, we show that their\\nrepresentations are not semantically meaningful and they are inherently\\nvulnerable to small changes. Images with imperceptible differences can have\\nvery different representations; on the other hand, images that should belong to\\ndifferent semantic classes can have nearly identical representations. Such\\nvulnerability can lead to unreliable classification results; for example,\\nunnoticeable changes cause the classification accuracy to be reduced by over\\n60\\\\%. %. To the best of our knowledge, this is the first work to systematically\\ndemonstrate this fundamental lack of semantic meaningfulness in ViT\\nrepresentations for medical image classification, revealing a critical\\nchallenge for their deployment in safety-critical systems.'),\n",
       " Document(metadata={'title': 'Probing Evaluation Awareness of Language Models', 'authors': 'Jord Nguyen, Khiem Hoang, Carlo Leonardo Attubato, Felix Hofstätter', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.01786v1'}, page_content='Language models can distinguish between testing and deployment phases -- a\\ncapability known as evaluation awareness. This has significant safety and\\npolicy implications, potentially undermining the reliability of evaluations\\nthat are central to AI governance frameworks and voluntary industry\\ncommitments. In this paper, we study evaluation awareness in\\nLlama-3.3-70B-Instruct. We show that linear probes can separate real-world\\nevaluation and deployment prompts, suggesting that current models internally\\nrepresent this distinction. We also find that current safety evaluations are\\ncorrectly classified by the probes, suggesting that they already appear\\nartificial or inauthentic to models. Our findings underscore the importance of\\nensuring trustworthy evaluations and understanding deceptive capabilities. More\\nbroadly, our work showcases how model internals may be leveraged to support\\nblackbox methods in safety audits, especially for future models more competent\\nat evaluation awareness and deception.'),\n",
       " Document(metadata={'title': 'MuRating: A High Quality Data Selecting Approach to Multilingual Large Language Model Pretraining', 'authors': 'Zhixun Chen, Ping Guo, Wenhan Han, Yifan Zhang, Binbin Liu, Haobin Lin, Fengze Liu, Yan Zhao, Bingni Zhang, Taifeng Wang, Yin Zheng, Meng Fang', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.01785v1'}, page_content='Data quality is a critical driver of large language model performance, yet\\nexisting model-based selection methods focus almost exclusively on English. We\\nintroduce MuRating, a scalable framework that transfers high-quality English\\ndata-quality signals into a single rater for 17 target languages. MuRating\\naggregates multiple English \"raters\" via pairwise comparisons to learn unified\\ndocument-quality scores,then projects these judgments through translation to\\ntrain a multilingual evaluator on monolingual, cross-lingual, and parallel text\\npairs. Applied to web data, MuRating selects balanced subsets of English and\\nmultilingual content to pretrain a 1.2 B-parameter LLaMA model. Compared to\\nstrong baselines, including QuRater, AskLLM, DCLM and so on, our approach\\nboosts average accuracy on both English benchmarks and multilingual\\nevaluations, with especially large gains on knowledge-intensive tasks. We\\nfurther analyze translation fidelity, selection biases, and underrepresentation\\nof narrative material, outlining directions for future work.'),\n",
       " Document(metadata={'title': 'BranchNet: A Neuro-Symbolic Learning Framework for Structured Multi-Class Classification', 'authors': 'Dalia Rodríguez-Salas, Christian Riess', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.01781v1'}, page_content=\"We introduce BranchNet, a neuro-symbolic learning framework that transforms\\ndecision tree ensembles into sparse, partially connected neural networks. Each\\nbranch, defined as a decision path from root to a parent of leaves, is mapped\\nto a hidden neuron, preserving symbolic structure while enabling gradient-based\\noptimization. The resulting models are compact, interpretable, and require no\\nmanual architecture tuning. Evaluated on a suite of structured multi-class\\nclassification benchmarks, BranchNet consistently outperforms XGBoost in\\naccuracy, with statistically significant gains. We detail the architecture,\\ntraining procedure, and sparsity dynamics, and discuss the model's strengths in\\nsymbolic interpretability as well as its current limitations, particularly on\\nbinary tasks where further adaptive calibration may be beneficial.\"),\n",
       " Document(metadata={'title': 'GPU-based complete search for nonlinear minimization subject to bounds', 'authors': 'Guanglu Zhang, Qihang Shan, Jonathan Cagan', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.01770v2'}, page_content='This paper introduces a GPU-based complete search method to enclose the\\nglobal minimum of a nonlinear function subject to simple bounds on the\\nvariables. Using interval analysis, coupled with the computational power and\\narchitecture of GPU, the method iteratively rules out the regions in the search\\ndomain where the global minimum cannot exist and leaves a finite set of regions\\nwhere the global minimum must exist. For effectiveness, because of the rigor of\\ninterval analysis, the method is guaranteed to enclose the global minimum of\\nthe nonlinear function even in the presence of rounding errors. For efficiency,\\nthe method employs a novel GPU-based single program, single data parallel\\nprogramming style to circumvent major GPU performance bottlenecks, and a\\nvariable cycling technique is also integrated into the method to reduce\\ncomputational cost when minimizing large-scale nonlinear functions. The method\\nis validated by minimizing 10 multimodal benchmark test functions with scalable\\ndimensions, including the well-known Ackley function, Griewank function, Levy\\nfunction, and Rastrigin function. These benchmark test functions represent\\ngrand challenges of global optimization, and enclosing the guaranteed global\\nminimum of these benchmark test functions with more than 80 dimensions has not\\nbeen reported in the literature. Our method completely searches the feasible\\ndomain and successfully encloses the guaranteed global minimum of these 10\\nbenchmark test functions with up to 10,000 dimensions using only one GPU in a\\nreasonable computation time, far exceeding the reported results in the\\nliterature due to the unique method design and implementation based on GPU\\narchitecture.'),\n",
       " Document(metadata={'title': 'Enhanced Generative Model Evaluation with Clipped Density and Coverage', 'authors': 'Nicolas Salvy, Hugues Talbot, Bertrand Thirion', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.01761v1'}, page_content='Although generative models have made remarkable progress in recent years,\\ntheir use in critical applications has been hindered by their incapacity to\\nreliably evaluate sample quality. Quality refers to at least two complementary\\nconcepts: fidelity and coverage. Current quality metrics often lack reliable,\\ninterpretable values due to an absence of calibration or insufficient\\nrobustness to outliers. To address these shortcomings, we introduce two novel\\nmetrics, Clipped Density and Clipped Coverage. By clipping individual sample\\ncontributions and, for fidelity, the radii of nearest neighbor balls, our\\nmetrics prevent out-of-distribution samples from biasing the aggregated values.\\nThrough analytical and empirical calibration, these metrics exhibit linear\\nscore degradation as the proportion of poor samples increases. Thus, they can\\nbe straightforwardly interpreted as equivalent proportions of good samples.\\nExtensive experiments on synthetic and real-world datasets demonstrate that\\nClipped Density and Clipped Coverage outperform existing methods in terms of\\nrobustness, sensitivity, and interpretability for evaluating generative models.'),\n",
       " Document(metadata={'title': 'Tuning without Peeking: Provable Privacy and Generalization Bounds for LLM Post-Training', 'authors': 'Ismail Labiad, Mathurin Videau, Matthieu Kowalski, Marc Schoenauer, Alessandro Leite, Julia Kempe, Olivier Teytaud', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.01752v1'}, page_content='Gradient-based optimization is the workhorse of deep learning, offering\\nefficient and scalable training via backpropagation. However, its reliance on\\nlarge volumes of labeled data raises privacy and security concerns such as\\nsusceptibility to data poisoning attacks and the risk of overfitting. In\\ncontrast, black box optimization methods, which treat the model as an opaque\\nfunction, relying solely on function evaluations to guide optimization, offer a\\npromising alternative in scenarios where data access is restricted, adversarial\\nrisks are high, or overfitting is a concern. However, black box methods also\\npose significant challenges, including poor scalability to high-dimensional\\nparameter spaces, as prevalent in large language models (LLMs), and high\\ncomputational costs due to reliance on numerous model evaluations. This paper\\nintroduces BBoxER, an evolutionary black-box method for LLM post-training that\\ninduces an information bottleneck via implicit compression of the training\\ndata. Leveraging the tractability of information flow, we provide strong\\ntheoretical bounds on generalization, differential privacy, susceptibility to\\ndata poisoning attacks, and robustness to extraction attacks. BBoxER operates\\non top of pre-trained LLMs, offering a lightweight and modular enhancement\\nsuitable for deployment in restricted or privacy-sensitive environments, in\\naddition to non-vacuous generalization guarantees. In experiments with LLMs, we\\ndemonstrate empirically that Retrofitting methods are able to learn, showing\\nhow a few iterations of BBoxER improve performance and generalize well on a\\nbenchmark of reasoning datasets. This positions BBoxER as an attractive add-on\\non top of gradient-based optimization.'),\n",
       " Document(metadata={'title': 'Joint Matching and Pricing for Crowd-shipping with In-store Customers', 'authors': 'Arash Dehghan, Mucahit Cevik, Merve Bodur, Bissan Ghaddar', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.01749v1'}, page_content='This paper examines the use of in-store customers as delivery couriers in a\\ncentralized crowd-shipping system, targeting the growing need for efficient\\nlast-mile delivery in urban areas. We consider a brick-and-mortar retail\\nsetting where shoppers are offered compensation to deliver time-sensitive\\nonline orders. To manage this process, we propose a Markov Decision Process\\n(MDP) model that captures key uncertainties, including the stochastic arrival\\nof orders and crowd-shippers, and the probabilistic acceptance of delivery\\noffers. Our solution approach integrates Neural Approximate Dynamic Programming\\n(NeurADP) for adaptive order-to-shopper assignment with a Deep Double Q-Network\\n(DDQN) for dynamic pricing. This joint optimization strategy enables multi-drop\\nrouting and accounts for offer acceptance uncertainty, aligning more closely\\nwith real-world operations. Experimental results demonstrate that the\\nintegrated NeurADP + DDQN policy achieves notable improvements in delivery cost\\nefficiency, with up to 6.7\\\\% savings over NeurADP with fixed pricing and\\napproximately 18\\\\% over myopic baselines. We also show that allowing flexible\\ndelivery delays and enabling multi-destination routing further reduces\\noperational costs by 8\\\\% and 17\\\\%, respectively. These findings underscore the\\nadvantages of dynamic, forward-looking policies in crowd-shipping systems and\\noffer practical guidance for urban logistics operators.'),\n",
       " Document(metadata={'title': 'ECCV 2024 W-CODA: 1st Workshop on Multimodal Perception and Comprehension of Corner Cases in Autonomous Driving', 'authors': 'Kai Chen, Ruiyuan Gao, Lanqing Hong, Hang Xu, Xu Jia, Holger Caesar, Dengxin Dai, Bingbing Liu, Dzmitry Tsishkou, Songcen Xu, Chunjing Xu, Qiang Xu, Huchuan Lu, Dit-Yan Yeung', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.01735v1'}, page_content='In this paper, we present details of the 1st W-CODA workshop, held in\\nconjunction with the ECCV 2024. W-CODA aims to explore next-generation\\nsolutions for autonomous driving corner cases, empowered by state-of-the-art\\nmultimodal perception and comprehension techniques. 5 Speakers from both\\nacademia and industry are invited to share their latest progress and opinions.\\nWe collect research papers and hold a dual-track challenge, including both\\ncorner case scene understanding and generation. As the pioneering effort, we\\nwill continuously bridge the gap between frontier autonomous driving techniques\\nand fully intelligent, reliable self-driving agents robust towards corner\\ncases.'),\n",
       " Document(metadata={'title': 'NGAT: A Node-level Graph Attention Network for Long-term Stock Prediction', 'authors': 'Yingjie Niu, Mingchuan Zhao, Valerio Poti, Ruihai Dong', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.02018v1'}, page_content='Graph representation learning methods have been widely adopted in financial\\napplications to enhance company representations by leveraging inter-firm\\nrelationships. However, current approaches face three key challenges: (1) The\\nadvantages of relational information are obscured by limitations in downstream\\ntask designs; (2) Existing graph models specifically designed for stock\\nprediction often suffer from excessive complexity and poor generalization; (3)\\nExperience-based construction of corporate relationship graphs lacks effective\\ncomparison of different graph structures. To address these limitations, we\\npropose a long-term stock prediction task and develop a Node-level Graph\\nAttention Network (NGAT) specifically tailored for corporate relationship\\ngraphs. Furthermore, we experimentally demonstrate the limitations of existing\\ngraph comparison methods based on model downstream task performance.\\nExperimental results across two datasets consistently demonstrate the\\neffectiveness of our proposed task and model. The project is publicly available\\non GitHub to encourage reproducibility and future research.'),\n",
       " Document(metadata={'title': 'Towards culturally-appropriate conversational AI for health in the majority world: An exploratory study with citizens and professionals in Latin America', 'authors': 'Dorian Peters, Fernanda Espinoza, Marco da Re, Guido Ivetta, Luciana Benotti, Rafael A. Calvo', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.01719v1'}, page_content=\"There is justifiable interest in leveraging conversational AI (CAI) for\\nhealth across the majority world, but to be effective, CAI must respond\\nappropriately within culturally and linguistically diverse contexts. Therefore,\\nwe need ways to address the fact that current LLMs exclude many lived\\nexperiences globally. Various advances are underway which focus on top-down\\napproaches and increasing training data. In this paper, we aim to complement\\nthese with a bottom-up locally-grounded approach based on qualitative data\\ncollected during participatory workshops in Latin America. Our goal is to\\nconstruct a rich and human-centred understanding of: a) potential areas of\\ncultural misalignment in digital health; b) regional perspectives on chatbots\\nfor health and c)strategies for creating culturally-appropriate CAI; with a\\nfocus on the understudied Latin American context. Our findings show that\\nacademic boundaries on notions of culture lose meaning at the ground level and\\ntechnologies will need to engage with a broader framework; one that\\nencapsulates the way economics, politics, geography and local logistics are\\nentangled in cultural experience. To this end, we introduce a framework for\\n'Pluriversal Conversational AI for Health' which allows for the possibility\\nthat more relationality and tolerance, rather than just more data, may be\\ncalled for.\"),\n",
       " Document(metadata={'title': 'Agent Ideate: A Framework for Product Idea Generation from Patents Using Agentic AI', 'authors': 'Gopichand Kanumolu, Ashok Urlana, Charaka Vinayak Kumar, Bala Mallikarjunarao Garlapati', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.01717v1'}, page_content='Patents contain rich technical knowledge that can inspire innovative product\\nideas, yet accessing and interpreting this information remains a challenge.\\nThis work explores the use of Large Language Models (LLMs) and autonomous\\nagents to mine and generate product concepts from a given patent. In this work,\\nwe design Agent Ideate, a framework for automatically generating product-based\\nbusiness ideas from patents. We experimented with open-source LLMs and\\nagent-based architectures across three domains: Computer Science, Natural\\nLanguage Processing, and Material Chemistry. Evaluation results show that the\\nagentic approach consistently outperformed standalone LLMs in terms of idea\\nquality, relevance, and novelty. These findings suggest that combining LLMs\\nwith agentic workflows can significantly enhance the innovation pipeline by\\nunlocking the untapped potential of business idea generation from patent data.'),\n",
       " Document(metadata={'title': 'AdamMeme: Adaptively Probe the Reasoning Capacity of Multimodal Large Language Models on Harmfulness', 'authors': 'Zixin Chen, Hongzhan Lin, Kaixin Li, Ziyang Luo, Zhen Ye, Guang Chen, Zhiyong Huang, Jing Ma', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.01702v1'}, page_content='The proliferation of multimodal memes in the social media era demands that\\nmultimodal Large Language Models (mLLMs) effectively understand meme\\nharmfulness. Existing benchmarks for assessing mLLMs on harmful meme\\nunderstanding rely on accuracy-based, model-agnostic evaluations using static\\ndatasets. These benchmarks are limited in their ability to provide up-to-date\\nand thorough assessments, as online memes evolve dynamically. To address this,\\nwe propose AdamMeme, a flexible, agent-based evaluation framework that\\nadaptively probes the reasoning capabilities of mLLMs in deciphering meme\\nharmfulness. Through multi-agent collaboration, AdamMeme provides comprehensive\\nevaluations by iteratively updating the meme data with challenging samples,\\nthereby exposing specific limitations in how mLLMs interpret harmfulness.\\nExtensive experiments show that our framework systematically reveals the\\nvarying performance of different target mLLMs, offering in-depth, fine-grained\\nanalyses of model-specific weaknesses. Our code is available at\\nhttps://github.com/Lbotirx/AdamMeme.'),\n",
       " Document(metadata={'title': 'Exploring Advanced LLM Multi-Agent Systems Based on Blackboard Architecture', 'authors': 'Bochen Han, Songmao Zhang', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.01701v1'}, page_content=\"In this paper, we propose to incorporate the blackboard architecture into LLM\\nmulti-agent systems (MASs) so that (1) agents with various roles can share all\\nthe information and others' messages during the whole problem-solving process,\\n(2) agents that will take actions are selected based on the current content of\\nthe blackboard, and (3) the selection and execution round is repeated until a\\nconsensus is reached on the blackboard. We develop the first implementation of\\nthis proposal and conduct experiments on commonsense knowledge, reasoning and\\nmathematical datasets. The results show that our system can be competitive with\\nthe SOTA static and dynamic MASs by achieving the best average performance, and\\nat the same time manage to spend less tokens. Our proposal has the potential to\\nenable complex and dynamic problem-solving where well-defined structures or\\nworkflows are unavailable.\"),\n",
       " Document(metadata={'title': 'Relational Causal Discovery with Latent Confounders', 'authors': 'Andrea Piras, Matteo Negro, Ragib Ahsan, David Arbour, Elena Zheleva', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.01700v1'}, page_content='Estimating causal effects from real-world relational data can be challenging\\nwhen the underlying causal model and potential confounders are unknown. While\\nseveral causal discovery algorithms exist for learning causal models with\\nlatent confounders from data, they assume that the data is independent and\\nidentically distributed (i.i.d.) and are not well-suited for learning from\\nrelational data. Similarly, existing relational causal discovery algorithms\\nassume causal sufficiency, which is unrealistic for many real-world datasets.\\nTo address this gap, we propose RelFCI, a sound and complete causal discovery\\nalgorithm for relational data with latent confounders. Our work builds upon the\\nFast Causal Inference (FCI) and Relational Causal Discovery (RCD) algorithms\\nand it defines new graphical models, necessary to support causal discovery in\\nrelational domains. We also establish soundness and completeness guarantees for\\nrelational d-separation with latent confounders. We present experimental\\nresults demonstrating the effectiveness of RelFCI in identifying the correct\\ncausal structure in relational causal models with latent confounders.'),\n",
       " Document(metadata={'title': 'GPT, But Backwards: Exactly Inverting Language Model Outputs', 'authors': 'Adrians Skapars, Edoardo Manino, Youcheng Sun, Lucas C. Cordeiro', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.01693v1'}, page_content='While existing auditing techniques attempt to identify potential unwanted\\nbehaviours in large language models (LLMs), we address the complementary\\nforensic problem of reconstructing the exact input that led to an existing LLM\\noutput - enabling post-incident analysis and potentially the detection of fake\\noutput reports. We formalize exact input reconstruction as a discrete\\noptimisation problem with a unique global minimum and introduce SODA, an\\nefficient gradient-based algorithm that operates on a continuous relaxation of\\nthe input search space with periodic restarts and parameter decay. Through\\ncomprehensive experiments on LLMs ranging in size from 33M to 3B parameters, we\\ndemonstrate that SODA significantly outperforms existing approaches. We succeed\\nin fully recovering 79.5% of shorter out-of-distribution inputs from next-token\\nlogits, without a single false positive, but struggle to extract private\\ninformation from the outputs of longer (15+ token) input sequences. This\\nsuggests that standard deployment practices may currently provide adequate\\nprotection against malicious use of our method. Our code is available at\\nhttps://doi.org/10.5281/zenodo.15539879.'),\n",
       " Document(metadata={'title': 'Blending Supervised and Reinforcement Fine-Tuning with Prefix Sampling', 'authors': 'Zeyu Huang, Tianhao Cheng, Zihan Qiu, Zili Wang, Yinghui Xu, Edoardo M. Ponti, Ivan Titov', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.01679v1'}, page_content=\"Existing post-training techniques for large language models are broadly\\ncategorized into Supervised Fine-Tuning (SFT) and Reinforcement Fine-Tuning\\n(RFT). Each paradigm presents a distinct trade-off: SFT excels at mimicking\\ndemonstration data but can lead to problematic generalization as a form of\\nbehavior cloning. Conversely, RFT can significantly enhance a model's\\nperformance but is prone to learn unexpected behaviors, and its performance is\\nhighly sensitive to the initial policy. In this paper, we propose a unified\\nview of these methods and introduce Prefix-RFT, a hybrid approach that\\nsynergizes learning from both demonstration and exploration. Using mathematical\\nreasoning problems as a testbed, we empirically demonstrate that Prefix-RFT is\\nboth simple and effective. It not only surpasses the performance of standalone\\nSFT and RFT but also outperforms parallel mixed-policy RFT methods. A key\\nadvantage is its seamless integration into existing open-source frameworks,\\nrequiring only minimal modifications to the standard RFT pipeline. Our analysis\\nhighlights the complementary nature of SFT and RFT, and validates that\\nPrefix-RFT effectively harmonizes these two learning paradigms. Furthermore,\\nablation studies confirm the method's robustness to variations in the quality\\nand quantity of demonstration data. We hope this work offers a new perspective\\non LLM post-training, suggesting that a unified paradigm that judiciously\\nintegrates demonstration and exploration could be a promising direction for\\nfuture research.\"),\n",
       " Document(metadata={'title': 'Deep Recommender Models Inference: Automatic Asymmetric Data Flow Optimization', 'authors': 'Giuseppe Ruggeri, Renzo Andri, Daniele Jahier Pagliari, Lukas Cavigelli', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.01676v1'}, page_content=\"Deep Recommender Models (DLRMs) inference is a fundamental AI workload\\naccounting for more than 79% of the total AI workload in Meta's data centers.\\nDLRMs' performance bottleneck is found in the embedding layers, which perform\\nmany random memory accesses to retrieve small embedding vectors from tables of\\nvarious sizes. We propose the design of tailored data flows to speedup\\nembedding look-ups. Namely, we propose four strategies to look up an embedding\\ntable effectively on one core, and a framework to automatically map the tables\\nasymmetrically to the multiple cores of a SoC. We assess the effectiveness of\\nour method using the Huawei Ascend AI accelerators, comparing it with the\\ndefault Ascend compiler, and we perform high-level comparisons with Nvidia\\nA100. Results show a speed-up varying from 1.5x up to 6.5x for real workload\\ndistributions, and more than 20x for extremely unbalanced distributions.\\nFurthermore, the method proves to be much more independent of the query\\ndistribution than the baseline.\"),\n",
       " Document(metadata={'title': 'Comparing Optimization Algorithms Through the Lens of Search Behavior Analysis', 'authors': 'Gjorgjina Cenikj, Gašper Petelin, Tome Eftimov', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.01668v1'}, page_content='The field of numerical optimization has recently seen a surge in the\\ndevelopment of \"novel\" metaheuristic algorithms, inspired by metaphors derived\\nfrom natural or human-made processes, which have been widely criticized for\\nobscuring meaningful innovations and failing to distinguish themselves from\\nexisting approaches. Aiming to address these concerns, we investigate the\\napplicability of statistical tests for comparing algorithms based on their\\nsearch behavior. We utilize the cross-match statistical test to compare\\nmultivariate distributions and assess the solutions produced by 114 algorithms\\nfrom the MEALPY library. These findings are incorporated into an empirical\\nanalysis aiming to identify algorithms with similar search behaviors.'),\n",
       " Document(metadata={'title': 'AsyncFlow: An Asynchronous Streaming RL Framework for Efficient LLM Post-Training', 'authors': 'Zhenyu Han, Ansheng You, Haibo Wang, Kui Luo, Guang Yang, Wenqi Shi, Menglong Chen, Sicheng Zhang, Zeshun Lan, Chunshi Deng, Huazhong Ji, Wenjie Liu, Yu Huang, Yixiang Zhang, Chenyi Pan, Jing Wang, Xin Huang, Chunsheng Li, Jianping Wu', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.01663v1'}, page_content='Reinforcement learning (RL) has become a pivotal technology in the\\npost-training phase of large language models (LLMs). Traditional task-colocated\\nRL frameworks suffer from significant scalability bottlenecks, while\\ntask-separated RL frameworks face challenges in complex dataflows and the\\ncorresponding resource idling and workload imbalance. Moreover, most existing\\nframeworks are tightly coupled with LLM training or inference engines, making\\nit difficult to support custom-designed engines. To address these challenges,\\nwe propose AsyncFlow, an asynchronous streaming RL framework for efficient\\npost-training. Specifically, we introduce a distributed data storage and\\ntransfer module that provides a unified data management and fine-grained\\nscheduling capability in a fully streamed manner. This architecture inherently\\nfacilitates automated pipeline overlapping among RL tasks and dynamic load\\nbalancing. Moreover, we propose a producer-consumer-based asynchronous workflow\\nengineered to minimize computational idleness by strategically deferring\\nparameter update process within staleness thresholds. Finally, the core\\ncapability of AsynFlow is architecturally decoupled from underlying training\\nand inference engines and encapsulated by service-oriented user interfaces,\\noffering a modular and customizable user experience. Extensive experiments\\ndemonstrate an average of 1.59 throughput improvement compared with\\nstate-of-the-art baseline. The presented architecture in this work provides\\nactionable insights for next-generation RL training system designs.'),\n",
       " Document(metadata={'title': 'Autoregressive Image Generation with Linear Complexity: A Spatial-Aware Decay Perspective', 'authors': 'Yuxin Mao, Zhen Qin, Jinxing Zhou, Hui Deng, Xuyang Shen, Bin Fan, Jing Zhang, Yiran Zhong, Yuchao Dai', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.01652v1'}, page_content=\"Autoregressive (AR) models have garnered significant attention in image\\ngeneration for their ability to effectively capture both local and global\\nstructures within visual data. However, prevalent AR models predominantly rely\\non the transformer architectures, which are beset by quadratic computational\\ncomplexity concerning input sequence length and substantial memory overhead due\\nto the necessity of maintaining key-value caches. Although linear attention\\nmechanisms have successfully reduced this burden in language models, our\\ninitial experiments reveal that they significantly degrade image generation\\nquality because of their inability to capture critical long-range dependencies\\nin visual data. We propose Linear Attention with Spatial-Aware Decay (LASAD), a\\nnovel attention mechanism that explicitly preserves genuine 2D spatial\\nrelationships within the flattened image sequences by computing\\nposition-dependent decay factors based on true 2D spatial location rather than\\n1D sequence positions. Based on this mechanism, we present LASADGen, an\\nautoregressive image generator that enables selective attention to relevant\\nspatial contexts with linear complexity. Experiments on ImageNet show LASADGen\\nachieves state-of-the-art image generation performance and computational\\nefficiency, bridging the gap between linear attention's efficiency and spatial\\nunderstanding needed for high-quality generation.\"),\n",
       " Document(metadata={'title': 'GradMetaNet: An Equivariant Architecture for Learning on Gradients', 'authors': 'Yoav Gelberg, Yam Eitan, Aviv Navon, Aviv Shamsian, Theo, Putterman, Michael Bronstein, Haggai Maron', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.01649v1'}, page_content=\"Gradients of neural networks encode valuable information for optimization,\\nediting, and analysis of models. Therefore, practitioners often treat gradients\\nas inputs to task-specific algorithms, e.g. for pruning or optimization. Recent\\nworks explore learning algorithms that operate directly on gradients but use\\narchitectures that are not specifically designed for gradient processing,\\nlimiting their applicability. In this paper, we present a principled approach\\nfor designing architectures that process gradients. Our approach is guided by\\nthree principles: (1) equivariant design that preserves neuron permutation\\nsymmetries, (2) processing sets of gradients across multiple data points to\\ncapture curvature information, and (3) efficient gradient representation\\nthrough rank-1 decomposition. Based on these principles, we introduce\\nGradMetaNet, a novel architecture for learning on gradients, constructed from\\nsimple equivariant blocks. We prove universality results for GradMetaNet, and\\nshow that previous approaches cannot approximate natural gradient-based\\nfunctions that GradMetaNet can. We then demonstrate GradMetaNet's effectiveness\\non a diverse set of gradient-based tasks on MLPs and transformers, such as\\nlearned optimization, INR editing, and estimating loss landscape curvature.\"),\n",
       " Document(metadata={'title': 'Customized Exploration of Landscape Features Driving Multi-Objective Combinatorial Optimization Performance', 'authors': 'Ana Nikolikj, Gabriela Ochoa, Tome Eftimov', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.01638v1'}, page_content='We present an analysis of landscape features for predicting the performance\\nof multi-objective combinatorial optimization algorithms. We consider features\\nfrom the recently proposed compressed Pareto Local Optimal Solutions Networks\\n(C-PLOS-net) model of combinatorial landscapes. The benchmark instances are a\\nset of rmnk-landscapes with 2 and 3 objectives and various levels of ruggedness\\nand objective correlation. We consider the performance of three algorithms --\\nPareto Local Search (PLS), Global Simple EMO Optimizer (GSEMO), and\\nNon-dominated Sorting Genetic Algorithm (NSGA-II) - using the resolution and\\nhypervolume metrics. Our tailored analysis reveals feature combinations that\\ninfluence algorithm performance specific to certain landscapes. This study\\nprovides deeper insights into feature importance, tailored to specific\\nrmnk-landscapes and algorithms.'),\n",
       " Document(metadata={'title': 'Challenges for AI in Multimodal STEM Assessments: a Human-AI Comparison', 'authors': 'Aymeric de Chillaz, Anna Sotnikova, Patrick Jermann, Antoine Bosselut', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.03013v1'}, page_content='Generative AI systems have rapidly advanced, with multimodal input\\ncapabilities enabling reasoning beyond text-based tasks. In education, these\\nadvancements could influence assessment design and question answering,\\npresenting both opportunities and challenges. To investigate these effects, we\\nintroduce a high-quality dataset of 201 university-level STEM questions,\\nmanually annotated with features such as image type, role, problem complexity,\\nand question format. Our study analyzes how these features affect generative AI\\nperformance compared to students. We evaluate four model families with five\\nprompting strategies, comparing results to the average of 546 student responses\\nper question. Although the best model correctly answers on average 58.5 % of\\nthe questions using majority vote aggregation, human participants consistently\\noutperform AI on questions involving visual components. Interestingly, human\\nperformance remains stable across question features but varies by subject,\\nwhereas AI performance is susceptible to both subject matter and question\\nfeatures. Finally, we provide actionable insights for educators, demonstrating\\nhow question design can enhance academic integrity by leveraging features that\\nchallenge current AI systems without increasing the cognitive burden for\\nstudents.'),\n",
       " Document(metadata={'title': 'Depth Anything at Any Condition', 'authors': 'Boyuan Sun, Modi Jin, Bowen Yin, Qibin Hou', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.01634v1'}, page_content='We present Depth Anything at Any Condition (DepthAnything-AC), a foundation\\nmonocular depth estimation (MDE) model capable of handling diverse\\nenvironmental conditions. Previous foundation MDE models achieve impressive\\nperformance across general scenes but not perform well in complex open-world\\nenvironments that involve challenging conditions, such as illumination\\nvariations, adverse weather, and sensor-induced distortions. To overcome the\\nchallenges of data scarcity and the inability of generating high-quality\\npseudo-labels from corrupted images, we propose an unsupervised consistency\\nregularization finetuning paradigm that requires only a relatively small amount\\nof unlabeled data. Furthermore, we propose the Spatial Distance Constraint to\\nexplicitly enforce the model to learn patch-level relative relationships,\\nresulting in clearer semantic boundaries and more accurate details.\\nExperimental results demonstrate the zero-shot capabilities of DepthAnything-AC\\nacross diverse benchmarks, including real-world adverse weather benchmarks,\\nsynthetic corruption benchmarks, and general benchmarks.\\n  Project Page: https://ghost233lism.github.io/depthanything-AC-page\\n  Code: https://github.com/HVision-NKU/DepthAnythingAC'),\n",
       " Document(metadata={'title': 'Effective Explanations for Belief-Desire-Intention Robots: When and What to Explain', 'authors': 'Cong Wang, Roberto Calandra, Verena Klös', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.02016v1'}, page_content=\"When robots perform complex and context-dependent tasks in our daily lives,\\ndeviations from expectations can confuse users. Explanations of the robot's\\nreasoning process can help users to understand the robot intentions. However,\\nwhen to provide explanations and what they contain are important to avoid user\\nannoyance. We have investigated user preferences for explanation demand and\\ncontent for a robot that helps with daily cleaning tasks in a kitchen. Our\\nresults show that users want explanations in surprising situations and prefer\\nconcise explanations that clearly state the intention behind the confusing\\naction and the contextual factors that were relevant to this decision. Based on\\nthese findings, we propose two algorithms to identify surprising actions and to\\nconstruct effective explanations for Belief-Desire-Intention (BDI) robots. Our\\nalgorithms can be easily integrated in the BDI reasoning process and pave the\\nway for better human-robot interaction with context- and user-specific\\nexplanations.\"),\n",
       " Document(metadata={'title': 'Tile and Slide : A New Framework for Scaling NeRF from Local to Global 3D Earth Observation', 'authors': 'Camille Billouard, Dawa Derksen, Alexandre Constantin, Bruno Vallet', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.01631v1'}, page_content='Neural Radiance Fields (NeRF) have recently emerged as a paradigm for 3D\\nreconstruction from multiview satellite imagery. However, state-of-the-art NeRF\\nmethods are typically constrained to small scenes due to the memory footprint\\nduring training, which we study in this paper. Previous work on large-scale\\nNeRFs palliate this by dividing the scene into NeRFs. This paper introduces\\nSnake-NeRF, a framework that scales to large scenes. Our out-of-core method\\neliminates the need to load all images and networks simultaneously, and\\noperates on a single device. We achieve this by dividing the region of interest\\ninto NeRFs that 3D tile without overlap. Importantly, we crop the images with\\noverlap to ensure each NeRFs is trained with all the necessary pixels. We\\nintroduce a novel $2\\\\times 2$ 3D tile progression strategy and segmented\\nsampler, which together prevent 3D reconstruction errors along the tile edges.\\nOur experiments conclude that large satellite images can effectively be\\nprocessed with linear time complexity, on a single GPU, and without compromise\\nin quality.'),\n",
       " Document(metadata={'title': 'Prompt Guidance and Human Proximal Perception for HOT Prediction with Regional Joint Loss', 'authors': 'Yuxiao Wang, Yu Lei, Zhenao Wei, Weiying Xue, Xinyu Jiang, Nan Zhuang, Qi Liu', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.01630v1'}, page_content=\"The task of Human-Object conTact (HOT) detection involves identifying the\\nspecific areas of the human body that are touching objects. Nevertheless,\\ncurrent models are restricted to just one type of image, often leading to too\\nmuch segmentation in areas with little interaction, and struggling to maintain\\ncategory consistency within specific regions. To tackle this issue, a HOT\\nframework, termed \\\\textbf{P3HOT}, is proposed, which blends \\\\textbf{P}rompt\\nguidance and human \\\\textbf{P}roximal \\\\textbf{P}erception. To begin with, we\\nutilize a semantic-driven prompt mechanism to direct the network's attention\\ntowards the relevant regions based on the correlation between image and text.\\nThen a human proximal perception mechanism is employed to dynamically perceive\\nkey depth range around the human, using learnable parameters to effectively\\neliminate regions where interactions are not expected. Calculating depth\\nresolves the uncertainty of the overlap between humans and objects in a 2D\\nperspective, providing a quasi-3D viewpoint. Moreover, a Regional Joint Loss\\n(RJLoss) has been created as a new loss to inhibit abnormal categories in the\\nsame area. A new evaluation metric called ``AD-Acc.'' is introduced to address\\nthe shortcomings of existing methods in addressing negative samples.\\nComprehensive experimental results demonstrate that our approach achieves\\nstate-of-the-art performance in four metrics across two benchmark datasets.\\nSpecifically, our model achieves an improvement of \\\\textbf{0.7}$\\\\uparrow$,\\n\\\\textbf{2.0}$\\\\uparrow$, \\\\textbf{1.6}$\\\\uparrow$, and \\\\textbf{11.0}$\\\\uparrow$ in\\nSC-Acc., mIoU, wIoU, and AD-Acc. metrics, respectively, on the HOT-Annotated\\ndataset. Code is available at https://github.com/YuxiaoWang-AI/P3HOT.\"),\n",
       " Document(metadata={'title': 'Enhanced Influence-aware Group Recommendation for Online Media Propagation', 'authors': 'Chengkun He, Xiangmin Zhou, Chen Wang, Longbing Cao, Jie Shao, Xiaodong Li, Guang Xu, Carrie Jinqiu Hu, Zahir Tari', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.01616v1'}, page_content='Group recommendation over social media streams has attracted significant\\nattention due to its wide applications in domains such as e-commerce,\\nentertainment, and online news broadcasting. By leveraging social connections\\nand group behaviours, group recommendation (GR) aims to provide more accurate\\nand engaging content to a set of users rather than individuals. Recently,\\ninfluence-aware GR has emerged as a promising direction, as it considers the\\nimpact of social influence on group decision-making. In earlier work, we\\nproposed Influence-aware Group Recommendation (IGR) to solve this task.\\nHowever, this task remains challenging due to three key factors: the large and\\never-growing scale of social graphs, the inherently dynamic nature of influence\\npropagation within user groups, and the high computational overhead of\\nreal-time group-item matching.\\n  To tackle these issues, we propose an Enhanced Influence-aware Group\\nRecommendation (EIGR) framework. First, we introduce a Graph Extraction-based\\nSampling (GES) strategy to minimise redundancy across multiple temporal social\\ngraphs and effectively capture the evolving dynamics of both groups and items.\\nSecond, we design a novel DYnamic Independent Cascade (DYIC) model to predict\\nhow influence propagates over time across social items and user groups.\\nFinally, we develop a two-level hash-based User Group Index (UG-Index) to\\nefficiently organise user groups and enable real-time recommendation\\ngeneration. Extensive experiments on real-world datasets demonstrate that our\\nproposed framework, EIGR, consistently outperforms state-of-the-art baselines\\nin both effectiveness and efficiency.'),\n",
       " Document(metadata={'title': 'Survivability of Backdoor Attacks on Unconstrained Face Recognition Systems', 'authors': 'Quentin Le Roux, Yannick Teglia, Teddy Furon, Philippe Loubet-Moundi, Eric Bourbao', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.01607v1'}, page_content='The widespread use of deep learning face recognition raises several security\\nconcerns. Although prior works point at existing vulnerabilities, DNN backdoor\\nattacks against real-life, unconstrained systems dealing with images captured\\nin the wild remain a blind spot of the literature. This paper conducts the\\nfirst system-level study of backdoors in deep learning-based face recognition\\nsystems. This paper yields four contributions by exploring the feasibility of\\nDNN backdoors on these pipelines in a holistic fashion. We demonstrate for the\\nfirst time two backdoor attacks on the face detection task: face generation and\\nface landmark shift attacks. We then show that face feature extractors trained\\nwith large margin losses also fall victim to backdoor attacks. Combining our\\nmodels, we then show using 20 possible pipeline configurations and 15 attack\\ncases that a single backdoor enables an attacker to bypass the entire function\\nof a system. Finally, we provide stakeholders with several best practices and\\ncountermeasures.'),\n",
       " Document(metadata={'title': \"Teacher training in the age of AI: Impact on AI Literacy and Teachers' Attitudes\", 'authors': 'Julia Lademann, Jannik Henze, Nadine Honke, Caroline Wollny, Sebastian Becker-Genschow', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.03011v1'}, page_content=\"The rapid integration of artificial intelligence (AI) in education requires\\nteachers to develop AI competencies while preparing students for a society\\ninfluenced by AI. This study evaluates the impact of an online teacher training\\nprogram on German in-service teachers' AI literacy, usage behaviors, and\\nattitudes toward AI. A pre-post design study was conducted with teachers (N1 =\\n291 for AI literacy, N2 = 436 for attitude assessment) participating in the\\ncourse. The program combined synchronous and asynchronous learning formats,\\nincluding webinars, self-paced modules, and practical projects. The\\nparticipants exhibited notable improvements across all domains: AI literacy\\nscores increased significantly, and all attitude items regarding AI usage and\\nintegration demonstrated significant positive changes. Teachers reported\\nincreased confidence in AI integration. Structured teacher training programs\\neffectively enhance AI literacy and foster positive attitudes toward AI in\\neducation.\"),\n",
       " Document(metadata={'title': 'Data Agent: A Holistic Architecture for Orchestrating Data+AI Ecosystems', 'authors': 'Zhaoyan Sun, Jiayi Wang, Xinyang Zhao, Jiachi Wang, Guoliang Li', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.01599v1'}, page_content=\"Traditional Data+AI systems utilize data-driven techniques to optimize\\nperformance, but they rely heavily on human experts to orchestrate system\\npipelines, enabling them to adapt to changes in data, queries, tasks, and\\nenvironments. For instance, while there are numerous data science tools\\navailable, developing a pipeline planning system to coordinate these tools\\nremains challenging. This difficulty arises because existing Data+AI systems\\nhave limited capabilities in semantic understanding, reasoning, and planning.\\nFortunately, we have witnessed the success of large language models (LLMs) in\\nenhancing semantic understanding, reasoning, and planning abilities. It is\\ncrucial to incorporate LLM techniques to revolutionize data systems for\\norchestrating Data+AI applications effectively.\\n  To achieve this, we propose the concept of a 'Data Agent' - a comprehensive\\narchitecture designed to orchestrate Data+AI ecosystems, which focuses on\\ntackling data-related tasks by integrating knowledge comprehension, reasoning,\\nand planning capabilities. We delve into the challenges involved in designing\\ndata agents, such as understanding data/queries/environments/tools,\\norchestrating pipelines/workflows, optimizing and executing pipelines, and\\nfostering pipeline self-reflection. Furthermore, we present examples of data\\nagent systems, including a data science agent, data analytics agents (such as\\nunstructured data analytics agent, semantic structured data analytics agent,\\ndata lake analytics agent, and multi-modal data analytics agent), and a\\ndatabase administrator (DBA) agent. We also outline several open challenges\\nassociated with designing data agent systems.\"),\n",
       " Document(metadata={'title': 'T3DM: Test-Time Training-Guided Distribution Shift Modelling for Temporal Knowledge Graph Reasoning', 'authors': 'Yuehang Si, Zefan Zeng, Jincai Huang, Qing Cheng', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.01597v1'}, page_content='Temporal Knowledge Graph (TKG) is an efficient method for describing the\\ndynamic development of facts along a timeline. Most research on TKG reasoning\\n(TKGR) focuses on modelling the repetition of global facts and designing\\npatterns of local historical facts. However, they face two significant\\nchallenges: inadequate modeling of the event distribution shift between\\ntraining and test samples, and reliance on random entity substitution for\\ngenerating negative samples, which often results in low-quality sampling. To\\nthis end, we propose a novel distributional feature modeling approach for\\ntraining TKGR models, Test-Time Training-guided Distribution shift Modelling\\n(T3DM), to adjust the model based on distribution shift and ensure the global\\nconsistency of model reasoning. In addition, we design a negative-sampling\\nstrategy to generate higher-quality negative quadruples based on adversarial\\ntraining. Extensive experiments show that T3DM provides better and more robust\\nresults than the state-of-the-art baselines in most cases.'),\n",
       " Document(metadata={'title': 'Autonomous AI Surveillance: Multimodal Deep Learning for Cognitive and Behavioral Monitoring', 'authors': 'Ameer Hamza, Zuhaib Hussain But, Umar Arif, Samiya, M. Abdullah Asad, Muhammad Naeem', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.01590v1'}, page_content='This study presents a novel classroom surveillance system that integrates\\nmultiple modalities, including drowsiness, tracking of mobile phone usage, and\\nface recognition,to assess student attentiveness with enhanced precision.The\\nsystem leverages the YOLOv8 model to detect both mobile phone and sleep\\nusage,(Ghatge et al., 2024) while facial recognition is achieved through\\nLResNet Occ FC body tracking using YOLO and MTCNN.(Durai et al., 2024) These\\nmodels work in synergy to provide comprehensive, real-time monitoring, offering\\ninsights into student engagement and behavior.(S et al., 2023) The framework is\\ntrained on specialized datasets, such as the RMFD dataset for face recognition\\nand a Roboflow dataset for mobile phone detection. The extensive evaluation of\\nthe system shows promising results. Sleep detection achieves 97. 42% mAP@50,\\nface recognition achieves 86. 45% validation accuracy and mobile phone\\ndetection reach 85. 89% mAP@50. The system is implemented within a core PHP web\\napplication and utilizes ESP32-CAM hardware for seamless data capture.(Neto et\\nal., 2024) This integrated approach not only enhances classroom monitoring, but\\nalso ensures automatic attendance recording via face recognition as students\\nremain seated in the classroom, offering scalability for diverse educational\\nenvironments.(Banada,2025)'),\n",
       " Document(metadata={'title': 'Exploring Classical Piano Performance Generation with Expressive Music Variational AutoEncoder', 'authors': 'Jing Luo, Xinyu Yang, Jie Wei', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.01582v1'}, page_content='The creativity of classical music arises not only from composers who craft\\nthe musical sheets but also from performers who interpret the static notations\\nwith expressive nuances. This paper addresses the challenge of generating\\nclassical piano performances from scratch, aiming to emulate the dual roles of\\ncomposer and pianist in the creative process. We introduce the Expressive\\nCompound Word (ECP) representation, which effectively captures both the\\nmetrical structure and expressive nuances of classical performances. Building\\non this, we propose the Expressive Music Variational AutoEncoder (XMVAE), a\\nmodel featuring two branches: a Vector Quantized Variational AutoEncoder\\n(VQ-VAE) branch that generates score-related content, representing the\\nComposer, and a vanilla VAE branch that produces expressive details, fulfilling\\nthe role of Pianist. These branches are jointly trained with similar Seq2Seq\\narchitectures, leveraging a multiscale encoder to capture beat-level contextual\\ninformation and an orthogonal Transformer decoder for efficient compound tokens\\ndecoding. Both objective and subjective evaluations demonstrate that XMVAE\\ngenerates classical performances with superior musical quality compared to\\nstate-of-the-art models. Furthermore, pretraining the Composer branch on extra\\nmusical score datasets contribute to a significant performance gain.'),\n",
       " Document(metadata={'title': 'Real-Time Emergency Vehicle Siren Detection with Efficient CNNs on Embedded Hardware', 'authors': 'Marco Giordano, Stefano Giacomelli, Claudia Rinaldi, Fabio Graziosi', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.01563v1'}, page_content='We present a full-stack emergency vehicle (EV) siren detection system\\ndesigned for real-time deployment on embedded hardware. The proposed approach\\nis based on E2PANNs, a fine-tuned convolutional neural network derived from\\nEPANNs, and optimized for binary sound event detection under urban acoustic\\nconditions. A key contribution is the creation of curated and semantically\\nstructured datasets - AudioSet-EV, AudioSet-EV Augmented, and Unified-EV -\\ndeveloped using a custom AudioSet-Tools framework to overcome the low\\nreliability of standard AudioSet annotations. The system is deployed on a\\nRaspberry Pi 5 equipped with a high-fidelity DAC+microphone board, implementing\\na multithreaded inference engine with adaptive frame sizing, probability\\nsmoothing, and a decision-state machine to control false positive activations.\\nA remote WebSocket interface provides real-time monitoring and facilitates live\\ndemonstration capabilities. Performance is evaluated using both framewise and\\nevent-based metrics across multiple configurations. Results show the system\\nachieves low-latency detection with improved robustness under realistic audio\\nconditions. This work demonstrates the feasibility of deploying IoS-compatible\\nSED solutions that can form distributed acoustic monitoring networks, enabling\\ncollaborative emergency vehicle tracking across smart city infrastructures\\nthrough WebSocket connectivity on low-cost edge devices.'),\n",
       " Document(metadata={'title': 'Self-Guided Process Reward Optimization with Redefined Step-wise Advantage for Process Reinforcement Learning', 'authors': 'Wu Fei, Hao Kong, Shuxian Liang, Yang Lin, Yibo Yang, Jing Tang, Lei Chen, Xiansheng Hua', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.01551v2'}, page_content='Process Reinforcement Learning~(PRL) has demonstrated considerable potential\\nin enhancing the reasoning capabilities of Large Language Models~(LLMs).\\nHowever, introducing additional process reward models incurs substantial\\ncomputational overhead, and there is no unified theoretical framework for\\nprocess-level advantage estimation. To bridge this gap, we propose\\n\\\\textbf{S}elf-Guided \\\\textbf{P}rocess \\\\textbf{R}eward\\n\\\\textbf{O}ptimization~(\\\\textbf{SPRO}), a novel framework that enables\\nprocess-aware RL through two key innovations: (1) we first theoretically\\ndemonstrate that process rewards can be derived intrinsically from the policy\\nmodel itself, and (2) we introduce well-defined cumulative process rewards and\\n\\\\textbf{M}asked \\\\textbf{S}tep \\\\textbf{A}dvantage (\\\\textbf{MSA}), which\\nfacilitates rigorous step-wise action advantage estimation within shared-prompt\\nsampling groups. Our experimental results demonstrate that SPRO outperforms\\nvaniila GRPO with 3.4x higher training efficiency and a 17.5\\\\% test accuracy\\nimprovement. Furthermore, SPRO maintains a stable and elevated policy entropy\\nthroughout training while reducing the average response length by approximately\\n$1/3$, evidencing sufficient exploration and prevention of reward hacking.\\nNotably, SPRO incurs no additional computational overhead compared to\\noutcome-supervised RL methods such as GRPO, which benefit industrial\\nimplementation.'),\n",
       " Document(metadata={'title': 'Crafting Hanzi as Narrative Bridges: An AI Co-Creation Workshop for Elderly Migrants', 'authors': 'Wen Zhan, Ziqun Hua, Peiyue Lin, Yunfei Chen', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.01548v2'}, page_content='This paper explores how older adults, particularly aging migrants in urban\\nChina, can engage AI-assisted co-creation to express personal narratives that\\nare often fragmented, underrepresented, or difficult to verbalize. Through a\\npilot workshop combining oral storytelling and the symbolic reconstruction of\\nHanzi, participants shared memories of migration and recreated new character\\nforms using Xiaozhuan glyphs, suggested by the Large Language Model (LLM),\\ntogether with physical materials. Supported by human facilitation and a soft AI\\npresence, participants transformed lived experience into visual and tactile\\nexpressions without requiring digital literacy. This approach offers new\\nperspectives on human-AI collaboration and aging by repositioning AI not as a\\ncontent producer but as a supportive mechanism, and by supporting narrative\\nagency within sociotechnical systems.')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4c9edb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 2000,\n",
    "    chunk_overlap = 200\n",
    ")\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "54f0fa2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Recent benchmarks for Large Language Model (LLM) agents primarily focus on\n",
      "evaluating reasoning, planning, and execution capabilities, while another\n",
      "critical component-memory, encompassing how agents memorize, update, and\n",
      "retrieve long-term information-is under-evaluated due to the lack of\n",
      "benchmarks. We term agents with memory mechanisms as memory agents. In this\n",
      "paper, we identify four core competencies essential for memory agents: accurate\n",
      "retrieval, test-time learning, long-range understanding, and conflict\n",
      "resolution. Existing datasets either rely on limited context lengths or are\n",
      "tailored for static, long-context settings like book-based QA, which do not\n",
      "reflect the interactive, multi-turn nature of memory agents that incrementally\n",
      "accumulate information. Furthermore, no existing benchmarks cover all four\n",
      "competencies. Therefore, we introduce MemoryAgentBench, a new benchmark\n",
      "specifically designed for memory agents. Our benchmark combines reformulated\n",
      "existing datasets with newly constructed ones, covering the above four memory\n",
      "competencies, providing a systematic and challenging testbed for assessing\n",
      "memory quality. We evaluate a diverse set of memory agents, ranging from simple\n",
      "context-based and retrieval-augmented generation (RAG) systems to advanced\n",
      "agents with external memory modules and tool integration. Empirical results\n",
      "reveal that current methods fall short of mastering all four competencies,\n",
      "underscoring the need for further research into comprehensive memory mechanisms\n",
      "for LLM agents.' metadata={'title': 'Evaluating Memory in LLM Agents via Incremental Multi-Turn Interactions', 'authors': 'Yuanzhe Hu, Yu Wang, Julian McAuley', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.05257v1'}\n",
      "page_content='Accurate motion prediction of surrounding traffic participants is crucial for\n",
      "the safe and efficient operation of automated vehicles in dynamic environments.\n",
      "Marginal prediction models commonly forecast each agent's future trajectories\n",
      "independently, often leading to sub-optimal planning decisions for an automated\n",
      "vehicle. In contrast, joint prediction models explicitly account for the\n",
      "interactions between agents, yielding socially and physically consistent\n",
      "predictions on a scene level. However, existing approaches differ not only in\n",
      "their problem formulation but also in the model architectures and\n",
      "implementation details used, making it difficult to compare them. In this work,\n",
      "we systematically investigate different approaches to joint motion prediction,\n",
      "including post-processing of the marginal predictions, explicitly training the\n",
      "model for joint predictions, and framing the problem as a generative task. We\n",
      "evaluate each approach in terms of prediction accuracy, multi-modality, and\n",
      "inference efficiency, offering a comprehensive analysis of the strengths and\n",
      "limitations of each approach. Several prediction examples are available at\n",
      "https://frommarginaltojointpred.github.io/.' metadata={'title': 'From Marginal to Joint Predictions: Evaluating Scene-Consistent Trajectory Prediction Approaches for Automated Driving', 'authors': 'Fabian Konstantinidis, Ariel Dallari Guerreiro, Raphael Trumpp, Moritz Sackmann, Ulrich Hofmann, Marco Caccamo, Christoph Stiller', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.05254v1'}\n"
     ]
    }
   ],
   "source": [
    "print(splits[0])\n",
    "print(splits[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b4e24d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\",google_api_key=\"XXXX\")\n",
    "vectorstore = Chroma.from_documents(documents=splits,\n",
    "                                    embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b19af125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': ['f616cba3-1f1c-49aa-a15f-353961c6691b', 'e8d71aa1-df18-4fa2-8486-64af089dab97', '90f65a49-c4d5-4dbd-bdbc-3002cdecf128', 'dc72c0f5-0b9e-4fcb-9cc6-ee5f259f6fdb', '7e0d661d-6494-47cc-a4eb-9af9df0c673d', '1546852f-9f69-4891-ae4f-315b86986c4d', '4b32919d-3035-462a-a1f6-f57aa32396d1', '80f51a8a-0589-43d7-948a-2794c4489548', '4a512b90-2965-4d6a-9a4a-7890db42d572', '7e4d8a78-25ca-4a13-b47b-b88f89736f82', 'd5426581-a24b-4d4e-9af0-67f3ebed0599', '76bfa924-f9f8-4149-a290-4f1c31677736', '86d8d39d-d780-4353-a3cf-3213e94eee8d', '1d8a7f7b-78c3-40c1-b460-390470fb3144', 'f4f473e1-a2fe-45ae-b543-d6e38e719e3c', 'b85d5491-73d9-4d9a-bbf7-ff84c4bab0f3', 'e3be6de4-7333-4ee2-a716-40ac8ab76c99', '94c007c3-3abd-47fd-b98c-31df95265900', 'eb2d285e-c2dc-41d1-821e-b71b44233960', '1fb8a7fc-5cb9-4cc6-9550-afae701681a9', 'b76a6713-1770-41d4-82e5-5836379d4940', '2cadfce3-a984-47ba-9939-e59fcfbfd35b', '22f155b9-18c9-48aa-8175-b9ed1c97c1f2', 'd6e48cd1-2dc1-41d7-b2e7-dd30567beeb9', 'eebdf9e9-7567-44d6-85eb-43cc4ecb2dff', 'd70e4a58-4154-452f-a291-679edea209e0', 'db888a8b-0e20-410e-89f7-5f35bd69c26b', '6f0f4152-40f4-49dd-b1c0-9ad5adcf8da2', '071f47e3-6edb-437c-8084-7cf95bf42518', 'b64c6311-4702-4e82-95f8-a69b438d1467', 'e278f64a-7d1e-4b4f-ba0e-5ec25fe028cd', 'b6a456d5-44c6-438c-bcb3-235983a9ac82', '9e1c0fa3-8b25-49fe-8040-baaa6f61a762', '798b2cdf-52cf-4194-ad85-5522e7759288', 'eac0aa84-650f-4f52-b66f-8b0373349643', '633a80b7-5650-481e-ad03-3097dd0054ad', '3731e380-8c66-4318-a958-98f331e357c7', '1c8c0f49-cd82-43e7-a544-aab2707e9f9f', '1ec95bf1-00ae-40e2-a511-c18a0c893ead', 'f3978390-347d-4337-a4f3-1e636a8e9b26', '9e5c3279-375c-4927-9682-f964d9395e66', 'a4c453db-0ce0-40d6-8b19-943e8d2b8940', '2aab06d4-e09a-41ff-b6ec-11cc5bdc5c3b', 'c26ae1c8-458f-4871-905e-0d935ec237e9', '6cbeee0e-801e-466d-8e64-2d81742a289d', '88ccd387-8499-492a-9012-2182c0f57a4d', 'f714c008-d7b0-447d-bd89-a6897ee21883', 'ac36060c-675b-4148-94ba-a49d3a1e8fb9', '79035500-44ca-422c-8f8a-37c394e0331b', '0ee9859a-ad88-4d51-a834-84423c18516e', '0c1ef840-48e8-4a37-9d64-7e9385e7ce17', '326a0047-b5a7-4494-8048-b46003f75009', 'fa8b1fff-266d-4eec-ad04-61b726ad538f', '8b4bab30-4626-4830-9735-bbdb0f753f3f', '6322066e-d51c-430f-bbe7-85da5fc50742', 'df567259-c369-48cd-9b03-2942307a814d', '3745d7de-4c71-422c-a97f-cb5c9bebec42', '1ff778a9-04bc-4eb5-8b7b-4630ea7b5f1e', 'd665bea2-153c-4156-aa85-bbb4ff010baf', '46c2a8fd-246d-4553-b6a9-f6491a9e473a', '6cb94ecd-ecf9-4f9e-bd86-320e4d07ebb6', '823c5791-b8eb-47d4-becc-94d67b15d8e8', 'b0306bf7-aa71-44f4-b4a2-3d848908b5ec', '6754f707-941f-488c-8e6a-0c2414302594', '40bec95f-6c3c-447f-bbd8-d74add9badc2', 'c9a22f38-5446-4a48-a3b2-3f2d1a3fd532', '07c29dac-68e7-4fab-ab65-aaec5821af06', 'ac54eeea-d13c-4b09-a1b9-2c13aefa364d', '00335513-ea39-48e8-beb9-9d0122301036', '9b16bfab-e985-4647-bfc0-ff82950b90ee', 'bb5db6f6-46ae-4959-b24e-913683103185', '67438e08-5c56-4ecf-af46-7ef7fd5848aa', '95e43edb-ed5d-4166-af09-7201f419879d', 'ee1f1c4b-d89e-4565-890a-0c70656dd149', '334c101d-b567-4862-b19f-ae56f3c1d331', 'df1c3cb2-5201-4171-ac60-28cc30646864', '6f3d14cb-f3c4-4391-ac3d-f265bf999b85', 'd36e10cf-5abd-47f6-858a-b5e13dbc258d', '10a62ba6-d20a-4c30-b8b4-76118f414f7b', '1df89e53-f9e2-4a3a-a2e8-d2ae52d8f21c', 'bcb19438-ade5-497b-a0e5-42b9e8bf0d16', 'efb3334a-d81d-4c48-96eb-70e5bfa9d8bc', '7a44840f-3629-44da-8fb2-ff13bd7f582c', '0dff2146-e610-4047-9324-3bb72276a8bb', '92ec309b-8bbd-423f-b082-75455c3bde5f', 'f9b05e7c-7f30-4c5b-bb66-cbd5018df514', '8da03b74-3627-45a8-8ff5-dc04cc9a5fe6', '4bda1f44-d579-4ff1-9647-76b27980fa18', '5ad04984-dfa1-41ca-9b3b-3a9ae1c452e3', 'fa6d7c58-d07a-472f-954e-0387087e9cef', '7a2d7e7e-5113-4711-8078-37ec62a20484', '9701b0ae-e570-4e4c-986e-eb1ad5c81f16', '466b80dd-6030-4b98-b7f1-4b87e19fbb25', 'fedfb7fa-fca4-423c-b5eb-fa678cf4a86d', 'f0f8126e-f6f7-45c7-b367-b2232694c3e3', 'da2513b3-2f37-4af8-8fc2-fe899d8c75d3', '42254db7-d61f-42aa-b9c8-2c2683eb1bd4', 'd1ddf4f2-c93e-4762-a52d-a136edcffbe6', '2bc9102a-943a-4603-bf87-7775540dec7a', 'ca441cd6-8edb-45da-9bdf-9f7a5379b21f', '19a80ddb-ea01-4ba3-9a7b-ecfaf4451a7d', '297693d1-b03e-48ff-8b62-e216b4e80695', '177c270e-430d-4fd1-9f50-f003a0955807', 'b762b541-7bdb-424a-b3a5-3355026b7ba7', '9e3da03b-3224-4311-b7a9-69c626e1f6e3', '113c93e9-49bf-48f0-b710-cdb011131dac', '0dd29954-fd94-4ac7-a559-91fc689978c5', '99a8da07-0490-4a53-b468-1e26195b1f32', '5a67b6de-1d0c-480d-8132-cac0e3c63715', 'be352a6c-a4c9-483b-a24f-5d5dba3c7d0b', 'af4cd1da-e489-4428-bc4c-739f7394a929', '5462cfeb-4686-4190-a7a6-9c5708ee6a5d', 'eb412b1e-31e7-4d6b-89f4-8af3d1cf61c5', '18dd5039-1b7d-463d-a647-1fcd62e528dc', 'b6ba418b-b263-4b9d-8687-20b19e73db0c', '906699d8-092e-47ef-af36-787c8ada9e06', 'fc4021ca-2841-42e4-b8f8-4ccd6247ad88', 'a6e7e25c-9eed-48fd-9ca4-a04c01fcf0bf', '8c3e282c-af57-4e3a-88c1-9bb5fbfebb39', '1d27a41a-68bc-42bb-afd9-b8a60603d150', '9536ae40-9920-49a1-9665-7a9f960224c0', 'b30c02d9-b838-4224-9244-e8092feaae1a', '3d38f1ad-955b-4b8c-894d-033c609d79b6', '2b089b78-f50b-4a66-80f0-1257be541c0d', '106dabe6-39cc-4617-b662-682c4919d449', '8b16ec60-d70f-43b1-a162-b76388c36c64', 'a5e04fdb-6369-4872-89a5-55ba7210e2e5', 'fea312f0-2074-489a-b825-5cc523dec48e', '1d3d7806-9b11-4592-adb9-c93d7dd8608d', '559f870a-3eaa-43a9-bb5a-c0edf2a788ff', '4457c882-a169-46c6-a0ed-d0deed7f56f4', 'c12d81d7-7fb5-41d0-bfce-0be98d4bf2a1', 'f26e2d5e-afe5-4f08-aaae-879d58b73e60', 'a728434c-6200-4998-8547-f41e2700d564', '0ded8942-0ed1-4c92-8dd2-911db87a9d8c', '270ff6a3-d224-4182-96ad-af2b45afabaa', '533c7f14-1787-4634-8572-7f0e41ed8169', 'bd654eda-b053-4434-9912-164b5754e3e1', 'a0962839-00ed-4a8e-8746-dcddc6421d54', '4f7a3abf-6f7a-4b89-ae1a-ab30feac40f1', 'd6ac40c7-f020-442d-af98-30c3e1c18efe', 'd2b3fde0-59eb-4971-9f64-5902f44a0223', '3698597f-099b-4ea4-86b7-2ce00baa5222', 'e1fada04-f8aa-45b3-8fd4-a3915e578699', '576f8c6f-a132-4fc6-817d-65773b72992e', '7ec040ca-15ad-4995-9034-03938aa8144f', 'bfe3d6b8-96a1-4b87-bb51-6a872c2176cc', 'de9b3ccf-7844-41b2-9f63-9bcf61a9f614', '7effb567-58b6-4f48-b1ad-1761a5181e76', 'c8dadcf5-af84-45f9-84e9-69971c62d97d', '2735aec8-fd6f-4d7a-9b42-c4b62043945e', 'dcdf8a07-dfad-47e1-80d1-10da27de2532', 'c27711af-9a4a-42f1-9d46-1628ed1f0b8a', '6565612f-fa6f-491c-bfa0-229ece10ed34', 'ebf1a362-8286-41f2-834f-f8451b81e2a7', 'e8ca5ba5-ef69-4294-9b03-89910359ec65', '97eb0223-71f1-452f-8093-6bc4115744d2', '232b2798-6f5a-4c6f-8a56-53b1d82b329f', 'e61a54bc-37ea-4f89-a380-b9847e859381', 'ff77ad64-40ad-42e1-8e89-06a623b27c02', '8bef55ff-ca49-4120-8651-677098e526b5', 'adf65b78-ea0d-4114-bc0c-25f89015698f', '4080dfee-9119-47b2-ba8a-d42264c6dea5', '6672507a-04d4-4ff5-a8c7-ae58c9372bae', '18438e55-df5c-4bbe-b8bc-322652cd7bae', '53f50ebb-a696-43a0-8f8c-d180dbd3a493', '075da897-37a7-42dd-8690-015c6c9a9a43', 'c3bb9edf-8bcd-444d-ae93-ef3fbc9493ca', '77d1b575-8d5b-45b0-90ff-359cdb8bef2c', '48af0a2c-ed71-4b20-a74e-564bf8099cf5', '5b8a2b09-519e-46a2-980b-8089dccec06f', 'cb95e047-9e97-461a-be04-d10bd2b3f042', '4d07b43d-154b-498f-8456-82e4c968cc30', '210ae052-f9dc-4516-b10c-b316a2b11272', '1ce79923-ec71-492d-921f-3df5e304110a', '526e1b59-2195-43b5-b7bb-ff26afac7633', 'b726c036-b87a-4b25-8498-d11c5cbf98f0', '9aec774f-f0db-4215-a323-1c163e0884d4', '244aee1f-cfff-42af-af44-885f211af46f', 'd62fc505-9bab-496c-aab7-725b7d10cd0b', 'fc3b7332-0632-4423-a53d-728f56ebab36', 'b7e3467c-ce82-4b5e-ac1a-d3171fca6285', '3f3dfbf1-d0ad-4d4c-b182-7fc6e24f8548', 'addee747-e8ce-488a-a1ac-2f3a60de9582', '82b8ea95-dc2d-4be1-b3dd-044d83c7d7e6', 'f520c4d6-d18a-42a8-bbe6-93ba0ccbc06c', '25ddd051-636f-4940-8142-abeeeef6bf5b', '55cad675-7799-484f-9c17-d31dffff5a7c', '2edf84c1-aa70-479e-9247-1aff67e38e4b', '48d76219-bd18-41f4-8fd5-37d22ee9e4df', '0d3957cc-670a-4a52-b985-8f598476af7d', '85d1017a-03a8-4d8f-844d-3c9805fbe3f8', '8e2fb71f-336b-471c-bdb9-211756070cbf', 'f78511f7-825d-4a49-bc74-8690c795ce59', '95092714-854d-49e5-a5c7-bc54c28ac6ca', '984a5b7b-6a16-47df-a3a0-9e480be14e7f', 'bdf2194e-c0f3-4f03-bf13-026fac7dff42', 'fd49b304-b37c-4107-b3e2-f64f0f9924ea', 'de022f59-7b69-4170-bccf-56a4a7e57cf2', '79b5d8c7-877d-413c-86b5-67b4c49af8d1', '8da43e3f-afd0-4e4a-b843-772853a4d640', '21afe857-8e06-4f85-83e7-d6152d95871e', '33202f8a-144e-4bfc-a62d-cb2597580d8b', '8d874ace-bd64-499c-a805-4ced8850ad3a', 'fe6456e2-967d-4e70-bb27-c75f41637e24', '3eb036a1-a72f-45c1-895f-e0c7b562980e', '95cd962d-2018-4c0b-8b55-ca5cb424ac3d', '00a8a350-9867-4d7b-8ba3-03e31ac37715', '0ad42ffd-d10e-46de-9eb3-a483d1aea471', '7d143ff0-0668-4420-8b87-81a4984808c6', 'b2c3e74e-4320-435b-b8aa-cbeacc512109', '09028f98-3668-48fd-966d-ce2f46e75220', '29eb2e15-09e2-4506-9058-cb665bb2a3bc', 'c41f7cb2-e05e-44ba-8e12-c0015cd8d61e', 'c5de9968-dbc2-43f1-a031-d02232b40f27', '1cb60d4c-eb93-4968-92b4-68645c1a3266', '167405ac-07bc-4115-93fc-eced85db18e3', '1772d835-ddce-43ef-b762-ef514d7ea6d2', '6bb4fc73-408e-4964-873d-d821d1197120', '01a3d69f-6b7b-4513-a953-adde0c7eaa98', '03c6bd3b-873b-4956-86a6-94439ed23ff6', '353c8572-d6f0-4a4e-88a7-845493960a21', '7a69fb44-8355-4c43-adaf-9f789722ec50', '6d772e38-000a-457b-b019-b3a0d0a68060', '28c6e5b3-6ec9-4ea9-aa15-bcad750ac42b', '6b9cd848-b415-41fe-b1bd-09be2f667121', '0345771f-07ab-4505-8249-de9f4135467d', '8122ba06-440a-4f6a-8f58-2e8055d5f708', 'be621da0-f48d-41b8-b182-6da0df30a5b8', '46a935eb-570d-4644-8ae9-edc565e55d62', '37aed782-58aa-4e26-bbd4-01c9970bfd18', '726dc4a0-a9e0-4353-a420-c273a6febcae', 'da54f48b-35e9-4c75-a47e-1a929154d658', '4d37eac1-bb9d-44db-a04e-f46b6bc44f42', 'f4f82718-aa5a-40de-bd64-bfa456d99ed9', 'a8090cee-18e9-4401-923c-e93af1ec35d9', '5d4e5382-c38a-4aa7-b01c-6d3a5a116bdf', '47e34d49-08bb-4098-ad7a-13cb7aad000c', '78af2f4e-0803-4859-930e-30804cb57036', '2494ccf4-3449-4e3f-867c-44bb634a0f50', '51c9f7e3-5e2c-4c00-9f8a-c71fced3cd99', 'f3c0e06d-c863-4d85-a001-6636f1a09a32', '0f55d107-1a28-441a-9b81-fb12a2b61b3d', '4c32b1f1-9462-4bdc-8096-0b4ec13309d9', '735e2b82-b5ea-4105-a493-6ae35cf8a9a6', '84a45196-75e5-433a-9381-cf9e885a908f', '774f76b4-8d8d-4fa0-85d0-1e5c65bda24f', 'a2f0ba2b-2c79-4f6a-9b4d-786d427e9e8d', 'f1e8cc95-2092-43ce-8e98-0eca7cd389ce', '24bd6ff9-0741-4889-a527-070ff86f6516', 'a247ec42-7d79-428b-867f-cc95812a1039', '7a3d5fc0-9e54-4114-b402-24529d793535', 'b6f9cf66-227e-41df-bb85-2f95af2717c7', '154648eb-4e12-468f-8526-1d57ab87640b', 'ce199125-5c98-4fa7-89f6-82ae6d535c77', '7131b3ba-2a62-4b37-8627-d1096b770baf', 'a5d9ecc3-adeb-410c-aace-767f15776cbb', '757d7a89-8317-49fc-8960-547085499490', '0271d890-88d3-4356-9d5b-9c74e14e170a', '8d4c8128-4f51-49c9-859e-1c5bd4e274f6', 'd4dcdd4d-83a8-4395-ba7b-750e1b2122b2', '1b898605-eeee-4d49-8682-d0374cfb40ee', '24bdf8a5-f9bc-4896-b13e-9d8db4b50d1c', '36673e94-5012-44a3-9e0e-230dcda1a705', '8d5fd6a8-d91c-4419-abdd-20c8f296bc43', 'd702517e-5088-434f-9e20-3c0c9b6db54b', 'be74737f-1bf4-476d-88db-01a9bedadf41', 'e1de8c06-433b-433c-aecc-95f3b49e0118', '50e1e188-ad85-4a8b-9f44-ad8634d6f007', '85e9368f-2066-4b10-a003-33b199946323', 'a9a045cf-bc9a-473d-a106-2ec8aa9ba962', 'f3adc416-3f14-4b99-8bed-3ad8ddd7c1fd', '076c7335-ccba-401f-9939-c6bb5c2556ae', '6159660a-b22d-4eb0-adab-01b01df700ee', 'ca1a404b-f50d-4c5b-b754-4d45eaf4ffc2', 'cb753188-f8d2-48c1-a18d-0f032ddffc29', '58f2f8c2-23cb-412e-b4d5-e85dc629430e', 'd7bd3cda-4aed-4a13-a528-c2b3b4cef926', '06358670-c106-46de-9c91-b739a0f5e399', '6c153915-4715-40fc-aedb-9ef6e0201430', 'c58de3f6-9ff5-4110-a615-7f63373c4872', '381e936d-a4d1-4a02-a9cb-62bf8b78dea5', '9fbc3349-cec2-4e85-9b4b-4b3afc1ec656', '569bfcd3-ac2c-4560-8bc1-2f62943ad245', '059a7591-e32b-44eb-b182-53e35e528166', 'b25d62c2-1de1-4ea9-a406-5fa8ceb33be8', '1e174531-1a85-45a9-9867-3ce41a0dbe8d', '4b131cbb-8eba-45d5-860c-be10ec12782f', '7a8aac31-a2cd-47dd-9206-f835e96d2473', '8ea90359-b588-48fe-bc63-a7ba7944b76f', 'db59be0d-1906-44d1-89b2-b2306c268185', '29c1829c-69b3-4349-a623-1705e009325c', 'db5eb540-cab1-4efe-ab0b-b453861feda0', 'fd473ea6-8007-49a5-9394-be25ad1c8ebb', '747c83d8-5316-4e66-b94b-ef8dfe23325d', '3d806603-d96f-4e1c-a659-9b623681ea69', 'e82686ca-061a-4ac1-b166-8a03e16d7e2b', 'd26ec346-8852-4509-a52a-22073bde451a', '8b1264fc-a39f-450c-985c-4470a752810c', 'b4f51de7-e80f-43a4-bcd6-1a429c701b71', '8fc11132-74be-4e9c-911a-5acd6c3330f1', 'c09e0fce-6f70-4fbf-ad18-271bf24ed6a5', 'bf51f750-372e-4596-8ed2-ea4000f9fea5', '1f90fcc5-4808-4496-8587-7b944496c816', '29279580-3343-44ee-bf46-18ebbc512492', '44af3bac-1dc4-49f3-b4e3-f3bc3a366ca2', '273f8273-542f-45c1-b31d-4605c9de5246', 'f0b8e8fd-510a-4dda-8f49-c8c43241be92', 'ff163dc0-df75-4589-9bc7-b9abb2da56b7', 'b604d9ae-322a-472c-a013-1924ac493334', 'ce95d63c-a109-411f-be4f-e8cf7c64092a', 'bdb58df5-cccf-4de2-9172-e21e0cd10acf', '8f60aa37-32db-438f-b2f8-1a3fff262f20', 'd7a7a6e6-2806-4879-85c9-8cbae723cfe1', '0c4c2854-327f-4b61-a5c6-276c80816b59', '9d9ff24f-0b04-41d1-825a-8ced8d5003c8', 'c66e201a-22a8-4ad1-b92b-60b7011301f9', '5d5250d8-44fd-4edb-a45f-ee5354452e8c', '1d54c258-612d-4129-a67f-ffa4c411ef12', 'd1f6164c-2115-45bb-bd1f-c1071f6d6e79', 'b7ad5e4f-a1c5-4e51-8f5e-582e23aa5bf1', '5995323f-5c05-48bc-b0f3-f0a322fb5a21', 'ade06354-4203-4ae8-8d45-c6d004612929', '6de63276-dcae-4c8d-9afc-35eb3a0fd8f5', '1088b2c7-69fe-49a7-be11-01946d7c7952', 'c339e29e-787e-4ecd-b118-9898fed87a15', 'e0ec328a-32e7-4de9-8cad-b1288037b5d0', '8c8cc33d-ce4c-42be-9efa-99e3f0fd222d', 'a6d37eb8-a89d-4460-8d38-57138597188c', '9f188f98-f26e-41b4-8c6b-a68534589361', '34d61c3c-b36a-4f31-81de-0de52ac512a7', 'cca7fb89-35bf-4b4b-8d8c-19fce56c8b15', 'a8e5adbd-3218-43c5-aae4-a0f2045805aa', '953733d0-ea6c-4fd3-bf03-ac559d8e3e4c', 'd5512625-fbcc-4b26-adfb-bda579a02d66', '710299d5-0748-40f8-9d91-696d00120ae8', 'ba328f00-a1b5-4b6c-84d2-cd93b5357bfc', 'fd72e493-6b22-4956-8433-e0352b782fed', '076fb023-b6eb-4a3e-9ae4-42a0573faf43', '84a35b95-ebd7-4573-91cd-b1aa875d0a85', '7972ebb4-612e-48e1-be01-0d607a7a1567', 'b967ef68-333f-4cb1-a668-278837e25c4b', '1979989d-ade5-43c3-b53f-c4829184d2ef', 'bd4369f4-43c0-4a1a-b08a-fe7d7e5d9b8e', '74c184cd-38f5-405e-b9d7-5f0db44809ef', '640f932a-f177-4250-93f4-e36d2ddb1cbb', 'e98bcbf5-8a71-43af-915c-70537d7306a4', '9d7f043a-78e4-4290-acde-2e4007428492', 'ef1ac191-cba5-4a58-b76e-8c7d4f15b851', 'f27a5a24-3c2e-4279-88de-cad5067e0a7a', 'f4e141f7-e3fc-4987-9737-7546ec68f37a', '734155cd-dd4e-4f08-b8a2-608bdad29065', 'e8e4992b-5b3b-4188-b4e0-0d2e0c214601', 'a039a85b-1bdc-490a-a79d-5da06f809095', '48ae7733-fc57-4b11-9a08-1b3fcd71ceb3', '08da66ae-8aa6-4172-b2be-54ab4f10430b', '12c00b5e-eaec-47f5-980b-ce4c310ca9ff', '0b1ae22c-6d77-405c-bb7f-cf707b2ca864', '9f25b627-55cc-4f51-95bb-19609f6ea384', 'e396479f-931b-4339-a3a3-5931caf5776d', '60686899-f82f-4322-9d48-38f2fbcd3921', 'd7622ea2-3de1-4bd2-a1e3-46c53e2ea18c', '6cfb6622-05b2-4aec-a320-f1b897e4d2e6', 'bf0fd255-409d-4c37-b27a-086ca227f307', '80d32b86-5a58-4ab2-bb12-fa3a33e8fd69', '37d82670-81a6-4a49-8f86-54af923368f3', '90a5c937-abd2-4a37-9c4e-84f21c8d1eb9', '68d81f7e-96d8-4177-9cb8-5e5bed4b8041', '46efe9cf-5e5e-4fe8-a15a-eb965d83885c', '0cf9b543-a7a4-436f-9bf1-26ebb692ccec', 'abd56c34-dd12-4117-82e0-df578b5e3bf7', 'f62767dd-d39f-4f12-9195-e0bbe3ec89fb', 'a95c4018-b1b2-46cd-92b1-7a53f3965bdc', '4bdddad3-181c-4ea6-9658-a6fd0bcf98a4', 'da1cef19-45aa-4c21-99d2-64a11682f0bf', '1fa465da-a06f-437d-8b5f-03558ac3f7a3', '00f49a79-dd75-4622-9135-2bb8cdca4ab3', 'b819004d-ee0b-4079-9f3b-4910778e07c6', 'a829bf4d-869a-487f-a527-438655cb9d54', 'f9e85ab2-c16d-441f-bf40-8bcfd74b9aeb', '2f9a90a5-b19c-46fe-81a7-e33331b94afb', '1f2fee84-3b46-4eee-ab3d-01747f074a33', 'c8e779cc-cc09-4c9f-ba63-9acfcff2f93f', '91d129a6-a88a-48a7-85aa-1676665ccd39', 'e32b1226-e21c-459e-9a1d-e52a743fedd7', '16a4b601-3c9e-4521-92b2-98d272ca62cf', '1cbf805d-28c4-462b-8b11-b2c6d2473d2f', '316682ca-26c2-4db2-8337-177d6c10e424', '6b2cf88c-2b64-4a6b-b797-027a42cb1518', '4fc68471-8afb-480f-85c3-c9b0828ed762', '5fadef7a-b39e-4a0f-9697-958578f92a43', '6c4b541a-904d-4552-9a63-5a47e40a2dda', 'd8b35706-224b-485d-95f1-7cf8c9a35de1', '07855a8f-652c-4969-833a-bef46873c6a1', '2f00fd6a-32d2-434f-a381-d1b541e015da', '554c4de9-48fb-40cf-8abd-d1416983d33c', 'b72693b5-035f-49f7-9195-22be7980bf74', 'cd2bf481-7b42-4aa5-aa46-a9c537da47c1', 'cc117929-e46f-4f2e-9cf8-f7e51778f332', '4c9f3aab-b300-44a8-87e6-be28d17e7b8a', '6f7f6de7-d129-448f-8dd4-2dbae1257110', '8a1c1392-660a-44b3-999c-1dee0a128421', 'a8b8c619-d0a0-4cd9-a8e3-7a56551cc87a', 'f1f67c4b-7c65-472c-9c6c-fd7512891362', '22fedc38-243e-49db-b3b8-7956e1976115', 'f2220b92-9e96-4877-88ae-d38fec914cac', 'c0f89e31-f66b-4116-9d35-c557d1a2b9b3', '9e91f715-70ab-4fc3-b9b6-0a7e049e594d', 'd79f3f3d-e918-42f3-80b8-0cc78cc38c2e', 'f8cf451c-6f5e-4eab-a847-22334dcd9dac', 'faa9d9ec-c48b-47c6-80e7-3abcd38d8579', '76b479c3-d051-48e7-b414-9bd993d40d18', 'cb9f11ba-bc15-4f24-9f1f-9799f4ea9688', 'cf28aaec-8fc3-4d3c-8e50-ca8db62d0471', 'f803b6a0-f751-41ff-a061-ea08d7b1da2c', '983caf9e-28bd-46e1-9e0d-4dbaecafaa6e', '4a10e5d6-e049-4732-b741-f45f5551b6bd', 'fdaa4d2f-431d-43b6-9519-ae9b29f244d9', '7778899d-c843-4444-8735-91d236c5380f', '79d30295-9111-4093-9545-995e597596a5', '972dc971-398b-4c60-9bc6-80d753ca2be1', '39cc963b-feb3-47bd-a7b5-4bca686af75b', '59b3bfae-c4f7-4199-87d6-bab4cc203c34', 'a9a32a24-acee-4986-9704-bcba32fb8427', '1f699f7f-f091-4b2f-a639-15d966e546e2', '14c94588-fdad-4de2-a9ee-2ee28cb08dbf', '1d8f89a4-41a9-4792-a6d9-0461fa6b2a95', 'db50975e-73da-4567-8f8f-9296a73d799c', '9b52095e-77d8-466b-8ca9-a34a44892f73', 'f56a2a0d-767d-437d-8c7e-dc1089c3063f', '2e3d81d0-6ffc-4a9a-99dc-5985f28575e9', 'a0ff38fb-6600-4ecf-97e2-70d07aa6df66', '994eff33-2cb3-49b2-860c-44e90eaa5e58', '3d8d1bd8-3ee8-4cf9-879f-5b1e5a066554', '5d1e7d03-f353-44a7-a88e-18090caede21', '3816bf8b-51f9-482d-b23a-275f12a272f6', '785b5b87-9569-43d8-944e-0b0850a19e78', '02cb8265-04cc-46c5-aadd-eb59c4dc52a7', 'ecdf1265-1284-4480-b3bb-64994b5b528d', '5bd2d6bd-acac-4eaa-8399-cd4aa8242378', '4b23c2e0-971b-4316-9a44-8cfd0f95056f', '86db1f32-baf9-4ec2-b79d-145b90487129', 'd56d7012-61ed-4360-a293-8f6f408b17db', '128fae36-e670-4694-a587-393eb6afcf6f', '9dc68c48-0831-46cc-88aa-54be845b4afe', '43061599-f67a-4e58-940e-2293d018f1ec', 'e737d35d-5304-4864-8c30-50601e575a8e', 'd69c3082-451a-43a2-a0de-176e4822a8dd', '6f94157e-4a42-47e8-ad5b-ecdfd16f865d', '517dae35-824b-4b4b-a106-fac5bd0f1146', '07ae6fcd-4c7a-4787-939d-b2611ec8689e', '819480d8-d35c-48f9-8a26-b7b0684aafa3', '04330e32-805d-418b-ac02-af4fbd29b4ca', '77ff7724-602f-48c8-8da6-0322e18d54e0', '120f6f3c-826a-4616-a400-cf0444c97335', 'ade0d625-c23c-4d5d-94c8-1c9ffb3cfb8a', 'cbe88938-165d-4f7b-9d56-576b09359de0', '66afc551-5ed7-4bd8-9142-d189e67939e1', '3fd463d7-ffa2-4309-aaef-51942da19690', '164a45ad-df00-427f-b4bc-b0a2b0aea69a', '81b02ecc-ce8f-48f4-a9fa-72116554b989', '821c8f6b-5dd4-4fec-85fd-09eb34d4086f', 'e6f0db64-08df-4912-9831-9222b07086bb', '021e3192-99e5-416e-867f-fcf4c53227d8', 'cd0a6b7c-d22c-48aa-b7e4-cacb5bf6d30a', 'be748a77-6ba2-4c2e-ab27-39bcfa14ab2f', '7ba1f7c7-fdca-4d08-a338-4e8b7b846def', 'f3623184-8c64-442f-97f7-6e0f651f77d1', '2d4768f7-9e60-4137-abaa-52f0f9174e4b', '4b515c52-adf3-4a1c-b3ad-371355d5a1b2', '71be955a-3eed-46f2-b588-fb334b1dbe2e', 'a2dc67b1-5fb5-4970-b033-c70e5ec1151d', '0cfda54d-2ea1-4ac7-9c7a-abd037939896', '156d7bc9-89df-4d7b-bb9d-16c57197fd83', 'f61e706a-d3a6-4b39-8629-5f519204ad57', '4d126623-facc-4a5e-baa4-2d6bbcb671d1', 'e82e2faf-e789-4c2a-98da-cd52e48d12ad', '4ecbf1aa-b1e2-4c61-871c-7b74abd84a8a', '4c764b54-66af-483d-bc5e-5a444a844089', 'f921ae32-3641-4dce-908d-b9ebd6ad152f', '77e8d0e4-c053-4b25-9055-61598f9e7df5', 'a03d2608-ed86-4d55-a24c-7db0210e3da8', 'ba5b9b17-9027-41a1-b21b-f334991bcd01', '3e339fe4-a346-485f-bd21-7dab5de5252a', 'c01a5878-be9b-4271-84ed-2872e59cf4e2', '72a527f7-3eb9-4f86-907e-dd0446619b64', '76d930a4-f90f-433c-9f15-a262fc45d0ee', 'aced4963-26eb-4937-af4c-31abec6f35ba', '7191223d-16f0-463a-996c-5cc30fa4f87b', '8cfa63d8-091d-4232-84d1-b32247ea0b6e', 'b5f086c2-dc6e-4fef-be3f-f358829cf120', '38ba3970-97e4-43d0-9f7d-fddece4df140', '326378ec-bcc3-4e1f-90ef-bfdbe4fa0824', '81033828-5743-4201-9136-462bd9227207', 'e5888e77-0373-432c-adc2-fdfc7d8ff90a', 'ad2b75a8-8595-48fe-a253-72600548de0a', 'cf6cc4ab-c270-46eb-9eff-fcdbcb66283e', '48b6ba61-0d94-40b9-bd9a-470881e67181', '1e40aa1d-f69a-47eb-947d-280161a9bf15', '4c2e1fba-14ab-43b9-bd88-602b65e305e6'], 'embeddings': None, 'documents': ['Recent benchmarks for Large Language Model (LLM) agents primarily focus on\\nevaluating reasoning, planning, and execution capabilities, while another\\ncritical component-memory, encompassing how agents memorize, update, and\\nretrieve long-term information-is under-evaluated due to the lack of\\nbenchmarks. We term agents with memory mechanisms as memory agents. In this\\npaper, we identify four core competencies essential for memory agents: accurate\\nretrieval, test-time learning, long-range understanding, and conflict\\nresolution. Existing datasets either rely on limited context lengths or are\\ntailored for static, long-context settings like book-based QA, which do not\\nreflect the interactive, multi-turn nature of memory agents that incrementally\\naccumulate information. Furthermore, no existing benchmarks cover all four\\ncompetencies. Therefore, we introduce MemoryAgentBench, a new benchmark\\nspecifically designed for memory agents. Our benchmark combines reformulated\\nexisting datasets with newly constructed ones, covering the above four memory\\ncompetencies, providing a systematic and challenging testbed for assessing\\nmemory quality. We evaluate a diverse set of memory agents, ranging from simple\\ncontext-based and retrieval-augmented generation (RAG) systems to advanced\\nagents with external memory modules and tool integration. Empirical results\\nreveal that current methods fall short of mastering all four competencies,\\nunderscoring the need for further research into comprehensive memory mechanisms\\nfor LLM agents.', \"Accurate motion prediction of surrounding traffic participants is crucial for\\nthe safe and efficient operation of automated vehicles in dynamic environments.\\nMarginal prediction models commonly forecast each agent's future trajectories\\nindependently, often leading to sub-optimal planning decisions for an automated\\nvehicle. In contrast, joint prediction models explicitly account for the\\ninteractions between agents, yielding socially and physically consistent\\npredictions on a scene level. However, existing approaches differ not only in\\ntheir problem formulation but also in the model architectures and\\nimplementation details used, making it difficult to compare them. In this work,\\nwe systematically investigate different approaches to joint motion prediction,\\nincluding post-processing of the marginal predictions, explicitly training the\\nmodel for joint predictions, and framing the problem as a generative task. We\\nevaluate each approach in terms of prediction accuracy, multi-modality, and\\ninference efficiency, offering a comprehensive analysis of the strengths and\\nlimitations of each approach. Several prediction examples are available at\\nhttps://frommarginaltojointpred.github.io/.\", 'Reinforcement Learning (RL) offers a promising framework for autonomous\\ndriving by enabling agents to learn control policies through interaction with\\nenvironments. However, large and high-dimensional action spaces often used to\\nsupport fine-grained control can impede training efficiency and increase\\nexploration costs. In this study, we introduce and evaluate two novel\\nstructured action space modification strategies for RL in autonomous driving:\\ndynamic masking and relative action space reduction. These approaches are\\nsystematically compared against fixed reduction schemes and full action space\\nbaselines to assess their impact on policy learning and performance. Our\\nframework leverages a multimodal Proximal Policy Optimization agent that\\nprocesses both semantic image sequences and scalar vehicle states. The proposed\\ndynamic and relative strategies incorporate real-time action masking based on\\ncontext and state transitions, preserving action consistency while eliminating\\ninvalid or suboptimal choices. Through comprehensive experiments across diverse\\ndriving routes, we show that action space reduction significantly improves\\ntraining stability and policy performance. The dynamic and relative schemes, in\\nparticular, achieve a favorable balance between learning speed, control\\nprecision, and generalization. These findings highlight the importance of\\ncontext-aware action space design for scalable and reliable RL in autonomous\\ndriving tasks.', 'While chain-of-thought (CoT) monitoring is an appealing AI safety defense,\\nrecent work on \"unfaithfulness\" has cast doubt on its reliability. These\\nfindings highlight an important failure mode, particularly when CoT acts as a\\npost-hoc rationalization in applications like auditing for bias. However, for\\nthe distinct problem of runtime monitoring to prevent severe harm, we argue the\\nkey property is not faithfulness but monitorability. To this end, we introduce\\na conceptual framework distinguishing CoT-as-rationalization from\\nCoT-as-computation. We expect that certain classes of severe harm will require\\ncomplex, multi-step reasoning that necessitates CoT-as-computation. Replicating\\nthe experimental setups of prior work, we increase the difficulty of the bad\\nbehavior to enforce this necessity condition; this forces the model to expose\\nits reasoning, making it monitorable. We then present methodology guidelines to\\nstress-test CoT monitoring against deliberate evasion. Applying these\\nguidelines, we find that models can learn to obscure their intentions, but only\\nwhen given significant help, such as detailed human-written strategies or\\niterative optimization against the monitor. We conclude that, while not\\ninfallible, CoT monitoring offers a substantial layer of defense that requires\\nactive protection and continued stress-testing.', 'In collaborative tasks, being able to adapt to your teammates is a necessary\\nrequirement for success. When teammates are heterogeneous, such as in\\nhuman-agent teams, agents need to be able to observe, recognize, and adapt to\\ntheir human partners in real time. This becomes particularly challenging in\\ntasks with time pressure and complex strategic spaces where the dynamics can\\nchange rapidly. In this work, we introduce TALENTS, a strategy-conditioned\\ncooperator framework that learns to represent, categorize, and adapt to a range\\nof partner strategies, enabling ad-hoc teamwork. Our approach utilizes a\\nvariational autoencoder to learn a latent strategy space from trajectory data.\\nThis latent space represents the underlying strategies that agents employ.\\nSubsequently, the system identifies different types of strategy by clustering\\nthe data. Finally, a cooperator agent is trained to generate partners for each\\ntype of strategy, conditioned on these clusters. In order to adapt to\\npreviously unseen partners, we leverage a fixed-share regret minimization\\nalgorithm that infers and adjusts the estimated partner strategy dynamically.\\nWe assess our approach in a customized version of the Overcooked environment,\\nposing a challenging cooperative cooking task that demands strong coordination\\nacross a wide range of possible strategies. Using an online user study, we show\\nthat our agent outperforms current baselines when working with unfamiliar human\\npartners.', \"The rapid advancements of AI agents have ignited the long-held ambition of\\nleveraging them to accelerate scientific discovery. Achieving this goal\\nrequires a deep understanding of the frontiers of human knowledge. As such,\\nHumanity's Last Exam (HLE) provides an exceptionally challenging touchstone for\\nevaluating scientific AI agents. In this work, we aim to construct the\\nfoundational architecture for general-purpose agents and validate the\\ncapabilities through leading performance on HLE. To achieve this, we introduce\\nX-Master, a tool-augmented reasoning agent designed to emulate human\\nresearchers by interacting flexibly with external tools during its reasoning\\nprocess. This agent, guided by the conceptualization of code as an interaction\\nlanguage, can flexibly leverage built-in Python libraries and our customized\\ntools to augment the reasoning. We further scale its capabilities through\\nX-Masters, a scattered-and-stacked agentic workflow that systematically\\nenhances breadth and depth of reasoning. Our open-source solution, X-Masters,\\nsets a new state-of-the-art record on HLE with a score of 32.1%, surpassing\\nOpenAI's and Google's Deep Research (26.6% and 26.9%) and becoming the first to\\nexceed the 30% threshold. This work allows us to gain a deeper understanding of\\ncomplex task-solving and accumulates valuable experience that can inform future\\nadvancements, guiding subsequent model training.\", 'Deep learning models have demonstrated exceptional performance across a wide\\nrange of computer vision tasks. However, their performance often degrades\\nsignificantly when faced with distribution shifts, such as domain or dataset\\nchanges. Test-Time Training (TTT) has emerged as an effective method to enhance\\nmodel robustness by incorporating an auxiliary unsupervised task during\\ntraining and leveraging it for model updates at test time. In this work, we\\nintroduce CTA (Cross-Task Alignment), a novel approach for improving TTT.\\nUnlike existing TTT methods, CTA does not require a specialized model\\narchitecture and instead takes inspiration from the success of multi-modal\\ncontrastive learning to align a supervised encoder with a self-supervised one.\\nThis process enforces alignment between the learned representations of both\\nmodels, thereby mitigating the risk of gradient interference, preserving the\\nintrinsic robustness of self-supervised learning and enabling more semantically\\nmeaningful updates at test-time. Experimental results demonstrate substantial\\nimprovements in robustness and generalization over the state-of-the-art on\\nseveral benchmark datasets.', 'Unified segmentation of 3D point clouds is crucial for scene understanding,\\nbut is hindered by its sparse structure, limited annotations, and the challenge\\nof distinguishing fine-grained object classes in complex environments. Existing\\nmethods often struggle to capture rich semantic and contextual information due\\nto limited supervision and a lack of diverse multimodal cues, leading to\\nsuboptimal differentiation of classes and instances. To address these\\nchallenges, we propose VDG-Uni3DSeg, a novel framework that integrates\\npre-trained vision-language models (e.g., CLIP) and large language models\\n(LLMs) to enhance 3D segmentation. By leveraging LLM-generated textual\\ndescriptions and reference images from the internet, our method incorporates\\nrich multimodal cues, facilitating fine-grained class and instance separation.\\nWe further design a Semantic-Visual Contrastive Loss to align point features\\nwith multimodal queries and a Spatial Enhanced Module to model scene-wide\\nrelationships efficiently. Operating within a closed-set paradigm that utilizes\\nmultimodal knowledge generated offline, VDG-Uni3DSeg achieves state-of-the-art\\nresults in semantic, instance, and panoptic segmentation, offering a scalable\\nand practical solution for 3D understanding. Our code is available at\\nhttps://github.com/Hanzy1996/VDG-Uni3DSeg.', \"Artificial intelligence (AI) has significant potential in healthcare\\napplications, but its training and deployment faces challenges due to\\nhealthcare's diverse data, complex tasks, and the need to preserve privacy.\\nFoundation models that perform well on medical tasks and require less\\ntask-specific tuning data are critical to accelerate the development of\\nhealthcare AI applications. We introduce MedGemma, a collection of medical\\nvision-language foundation models based on Gemma 3 4B and 27B. MedGemma\\ndemonstrates advanced medical understanding and reasoning on images and text,\\nsignificantly exceeding the performance of similar-sized generative models and\\napproaching the performance of task-specific models, while maintaining the\\ngeneral capabilities of the Gemma 3 base models. For out-of-distribution tasks,\\nMedGemma achieves 2.6-10% improvement on medical multimodal question answering,\\n15.5-18.1% improvement on chest X-ray finding classification, and 10.8%\\nimprovement on agentic evaluations compared to the base models. Fine-tuning\\nMedGemma further improves performance in subdomains, reducing errors in\\nelectronic health record information retrieval by 50% and reaching comparable\\nperformance to existing specialized state-of-the-art methods for pneumothorax\\nclassification and histopathology patch classification. We additionally\\nintroduce MedSigLIP, a medically-tuned vision encoder derived from SigLIP.\\nMedSigLIP powers the visual understanding capabilities of MedGemma and as an\\nencoder achieves comparable or better performance than specialized medical\\nimage encoders. Taken together, the MedGemma collection provides a strong\\nfoundation of medical image and text capabilities, with potential to\\nsignificantly accelerate medical research and development of downstream\\napplications. The MedGemma collection, including tutorials and model weights,\\ncan be found at https://goo.gle/medgemma.\", 'The rapid advancement of Embodied AI has led to an increasing demand for\\nlarge-scale, high-quality real-world data. However, collecting such embodied\\ndata remains costly and inefficient. As a result, simulation environments have\\nbecome a crucial surrogate for training robot policies. Yet, the significant\\nReal2Sim2Real gap remains a critical bottleneck, particularly in terms of\\nphysical dynamics and visual appearance. To address this challenge, we propose\\nEmbodieDreamer, a novel framework that reduces the Real2Sim2Real gap from both\\nthe physics and appearance perspectives. Specifically, we propose PhysAligner,\\na differentiable physics module designed to reduce the Real2Sim physical gap.\\nIt jointly optimizes robot-specific parameters such as control gains and\\nfriction coefficients to better align simulated dynamics with real-world\\nobservations. In addition, we introduce VisAligner, which incorporates a\\nconditional video diffusion model to bridge the Sim2Real appearance gap by\\ntranslating low-fidelity simulated renderings into photorealistic videos\\nconditioned on simulation states, enabling high-fidelity visual transfer.\\nExtensive experiments validate the effectiveness of EmbodieDreamer. The\\nproposed PhysAligner reduces physical parameter estimation error by 3.74%\\ncompared to simulated annealing methods while improving optimization speed by\\n89.91\\\\%. Moreover, training robot policies in the generated photorealistic\\nenvironment leads to a 29.17% improvement in the average task success rate\\nacross real-world tasks after reinforcement learning. Code, model and data will\\nbe publicly available.', 'Existing language model benchmarks provide contradictory model rankings, even\\nfor benchmarks that aim to capture similar skills. This dilemma of conflicting\\nrankings hampers model selection, clouds model comparisons, and adds confusion\\nto a growing ecosystem of competing models. Recent work attributed ranking\\ndisagreement to the phenomenon of training on the test task: As released,\\ndifferent models exhibit a different level of preparation for any given test\\ntask. A candidate solution to the problem is train-before-test: Give each model\\nthe same benchmark-specific finetuning before evaluation. Our primary\\ncontribution is a broad empirical evaluation of train-before-test across 24\\nbenchmarks and 61 models. We show that train-before-test significantly improves\\nranking agreement consistently across all benchmarks. Whereas rankings have\\nlittle external validity to start with, they enjoy a significant degree of\\nexternal validity when applying train-before-test: Model rankings transfer\\ngracefully from one benchmark to the other. Even within the same model family,\\ntrain-before-test reduces strong ranking disagreement to near-perfect\\nagreement. In addition, train-before-test reduces the model-score matrix to\\nessentially rank one, revealing new insights into the latent factors of\\nbenchmark performance. Our work supports the recommendation to make\\ntrain-before-test a default component of LLM benchmarking.', \"The proliferation of AI-driven systems presents a fundamental challenge to\\nHuman-Computer Interaction (HCI) and Computer-Supported Cooperative Work\\n(CSCW), often diminishing user agency and failing to account for value\\npluralism. Current approaches to value alignment, which rely on centralized,\\ntop-down definitions, lack the mechanisms for meaningful contestability. This\\nleaves users and communities unable to challenge or shape the values embedded\\nin the systems that govern their digital lives, creating a crisis of legitimacy\\nand trust. This paper introduces Community-Defined AI Value Pluralism (CDAVP),\\na socio-technical framework that addresses this gap. It reframes the design\\nproblem from achieving a single aligned state to infrastructuring a dynamic\\necosystem for value deliberation and application. At its core, CDAVP enables\\ndiverse, self-organizing communities to define and maintain explicit value\\nprofiles - rich, machine-readable representations that can encompass not only\\npreferences but also community-specific rights and duties. These profiles are\\nthen contextually activated by the end-user, who retains ultimate control\\n(agency) over which values guide the AI's behavior. AI applications, in turn,\\nare designed to transparently interpret these profiles and moderate conflicts,\\nadhering to a set of non-negotiable, democratically-legitimated meta-rules. The\\ndesigner's role shifts from crafting static interfaces to becoming an architect\\nof participatory ecosystems. We argue that infrastructuring for pluralism is a\\nnecessary pathway toward achieving robust algorithmic accountability and\\ngenuinely contestable, human-centric AI.\", 'Despite rapid progress in large language model (LLM)-based multi-agent\\nsystems, current benchmarks fall short in evaluating their scalability,\\nrobustness, and coordination capabilities in complex, dynamic, real-world\\ntasks. Existing environments typically focus on small-scale, fully observable,\\nor low-complexity domains, limiting their utility for developing and assessing\\nnext-generation multi-agent Agentic AI frameworks. We introduce CREW-Wildfire,\\nan open-source benchmark designed to close this gap. Built atop the human-AI\\nteaming CREW simulation platform, CREW-Wildfire offers procedurally generated\\nwildfire response scenarios featuring large maps, heterogeneous agents, partial\\nobservability, stochastic dynamics, and long-horizon planning objectives. The\\nenvironment supports both low-level control and high-level natural language\\ninteractions through modular Perception and Execution modules. We implement and\\nevaluate several state-of-the-art LLM-based multi-agent Agentic AI frameworks,\\nuncovering significant performance gaps that highlight the unsolved challenges\\nin large-scale coordination, communication, spatial reasoning, and long-horizon\\nplanning under uncertainty. By providing more realistic complexity, scalable\\narchitecture, and behavioral evaluation metrics, CREW-Wildfire establishes a\\ncritical foundation for advancing research in scalable multi-agent Agentic\\nintelligence. All code, environments, data, and baselines will be released to\\nsupport future research in this emerging domain.', 'Empathetic interaction is a cornerstone of human-machine communication, due\\nto the need for understanding speech enriched with paralinguistic cues and\\ngenerating emotional and expressive responses. However, the most powerful\\nempathetic LSLMs are increasingly closed off, leaving the crucial details about\\nthe architecture, data and development opaque to researchers. Given the\\ncritical need for transparent research into the LSLMs and empathetic behavior,\\nwe present OpenS2S, a fully open-source, transparent and end-to-end LSLM\\ndesigned to enable empathetic speech interactions. Based on our empathetic\\nspeech-to-text model BLSP-Emo, OpenS2S further employs a streaming interleaved\\ndecoding architecture to achieve low-latency speech generation. To facilitate\\nend-to-end training, OpenS2S incorporates an automated data construction\\npipeline that synthesizes diverse, high-quality empathetic speech dialogues at\\nlow cost. By leveraging large language models to generate empathetic content\\nand controllable text-to-speech systems to introduce speaker and emotional\\nvariation, we construct a scalable training corpus with rich paralinguistic\\ndiversity and minimal human supervision. We release the fully open-source\\nOpenS2S model, including the dataset, model weights, pre-training and\\nfine-tuning codes, to empower the broader research community and accelerate\\ninnovation in empathetic speech systems. The project webpage can be accessed at\\nhttps://casia-lm.github.io/OpenS2S', 'World Model, the supposed algorithmic surrogate of the real-world environment\\nwhich biological agents experience with and act upon, has been an emerging\\ntopic in recent years because of the rising needs to develop virtual agents\\nwith artificial (general) intelligence. There has been much debate on what a\\nworld model really is, how to build it, how to use it, and how to evaluate it.\\nIn this essay, starting from the imagination in the famed Sci-Fi classic Dune,\\nand drawing inspiration from the concept of \"hypothetical thinking\" in\\npsychology literature, we offer critiques of several schools of thoughts on\\nworld modeling, and argue the primary goal of a world model to be simulating\\nall actionable possibilities of the real world for purposeful reasoning and\\nacting. Building on the critiques, we propose a new architecture for a\\ngeneral-purpose world model, based on hierarchical, multi-level, and mixed\\ncontinuous/discrete representations, and a generative and self-supervision\\nlearning framework, with an outlook of a Physical, Agentic, and Nested (PAN)\\nAGI system enabled by such a model.', 'The recent proliferation of photorealistic AI-generated images (AIGI) has\\nraised urgent concerns about their potential misuse, particularly on social\\nmedia platforms. Current state-of-the-art AIGI detection methods typically rely\\non large, deep neural architectures, creating significant computational\\nbarriers to real-time, large-scale deployment on platforms like social media.\\nTo challenge this reliance on computationally intensive models, we introduce\\nLAID, the first framework -- to our knowledge -- that benchmarks and evaluates\\nthe detection performance and efficiency of off-the-shelf lightweight neural\\nnetworks. In this framework, we comprehensively train and evaluate selected\\nmodels on a representative subset of the GenImage dataset across spatial,\\nspectral, and fusion image domains. Our results demonstrate that lightweight\\nmodels can achieve competitive accuracy, even under adversarial conditions,\\nwhile incurring substantially lower memory and computation costs compared to\\ncurrent state-of-the-art methods. This study offers valuable insight into the\\ntrade-off between efficiency and performance in AIGI detection and lays a\\nfoundation for the development of practical, scalable, and trustworthy\\ndetection systems. The source code of LAID can be found at:\\nhttps://github.com/nchivar/LAID.', 'Large Language Models (LLMs) possess an extraordinary capability to produce\\ntext that is not only coherent and contextually relevant but also strikingly\\nsimilar to human writing. They adapt to various styles and genres, producing\\ncontent that is both grammatically correct and semantically meaningful.\\nRecently, LLMs have been misused to create highly realistic phishing emails,\\nspread fake news, generate code to automate cyber crime, and write fraudulent\\nscientific articles. Additionally, in many real-world applications, the\\ngenerated content including style and topic and the generator model are not\\nknown beforehand. The increasing prevalence and sophistication of artificial\\nintelligence (AI)-generated texts have made their detection progressively more\\nchallenging. Various attempts have been made to distinguish machine-generated\\ntext from human-authored content using linguistic, statistical, machine\\nlearning, and ensemble-based approaches. This work focuses on two primary\\nobjectives Task-A, which involves distinguishing human-written text from\\nmachine-generated text, and Task-B, which attempts to identify the specific LLM\\nmodel responsible for the generation. Both of these tasks are based on fine\\ntuning of Generative Pre-trained Transformer (GPT_4o-mini), Large Language\\nModel Meta AI (LLaMA) 3 8B, and Bidirectional Encoder Representations from\\nTransformers (BERT). The fine-tuned version of GPT_4o-mini and the BERT model\\nhas achieved accuracies of 0.9547 for Task-A and 0.4698 for Task-B.', \"The severity of natural disasters is increasing every year, impacting many\\npeople's lives. During the response phase of disasters, airports are important\\nhubs where relief aid arrives and people need to be evacuated. However, the\\nairport often forms a bottleneck in these relief operations due to the sudden\\nneed for increased capacity. Limited research has been done on the operational\\nside of airport disaster management. Experts identify the main problems as,\\nfirst, the asymmetry of information between the airport and incoming flights,\\nand second, the lack of resources. The goal of this research is to understand\\nthe effects of incomplete knowledge of incoming flights with different resource\\nallocation strategies on the performance of cargo handling operations at an\\nairport after a natural disaster. An agent-based model is created, implementing\\nrealistic offloading strategies with different degrees of information\\nuncertainty. Model calibration and verification are performed with experts in\\nthe field. The model performance is measured by the average turnaround time,\\nwhich is divided into offloading time, boarding time, and cumulative waiting\\ntimes. The results show that the effects of one unplanned aircraft are\\nnegligible. However, all waiting times increase with more arriving unplanned\\naircraft.\", 'Turbulent flows are chaotic and unsteady, but their statistical distribution\\nconverges to a statistical steady state. Engineering quantities of interest\\ntypically take the form of time-average statistics such as $ \\\\frac{1}{t}\\n\\\\int_0^t f ( u(x,\\\\tau; \\\\theta) ) d\\\\tau \\\\overset{t \\\\rightarrow\\n\\\\infty}{\\\\rightarrow} F(x; \\\\theta)$, where $u(x,t; \\\\theta)$ are solutions of the\\nNavier--Stokes equations with parameters $\\\\theta$. Optimizing over $F(x;\\n\\\\theta)$ has many engineering applications including geometric optimization,\\nflow control, and closure modeling. However, this remains an open challenge, as\\nexisting computational approaches are incapable of scaling to physically\\nrepresentative numbers of grid points. The fundamental obstacle is the\\nchaoticity of turbulent flows: gradients calculated with the adjoint method\\ndiverge exponentially as $t \\\\rightarrow \\\\infty$.\\n  We develop a new online gradient-flow (OGF) method that is scalable to large\\ndegree-of-freedom systems and enables optimizing for the steady-state\\nstatistics of chaotic, unsteady, turbulence-resolving simulations. The method\\nforward-propagates an online estimate for the gradient of $F(x; \\\\theta)$ while\\nsimultaneously performing online updates of the parameters $\\\\theta$. A key\\nfeature is the fully online nature of the algorithm to facilitate faster\\noptimization progress and its combination with a finite-difference estimator to\\navoid the divergence of gradients due to chaoticity. The proposed OGF method is\\ndemonstrated for optimizations over three chaotic ordinary and partial\\ndifferential equations: the Lorenz-63 equation, the Kuramoto--Sivashinsky\\nequation, and Navier--Stokes solutions of compressible, forced, homogeneous\\nisotropic turbulence. In each case, the OGF method successfully reduces the\\nloss based on $F(x; \\\\theta)$ by several orders of magnitude and accurately\\nrecovers the optimal parameters.', 'Cross-domain Click-Through Rate prediction aims to tackle the data sparsity\\nand the cold start problems in online advertising systems by transferring\\nknowledge from source domains to a target domain. Most existing methods rely on\\noverlapping users to facilitate this transfer, often focusing on joint training\\nor pre-training with fine-tuning approach to connect the source and target\\ndomains. However, in real-world industrial settings, joint training struggles\\nto learn optimal representations with different distributions, and pre-training\\nwith fine-tuning is not well-suited for continuously integrating new data. To\\naddress these issues, we propose GIST, a cross-domain lifelong sequence model\\nthat decouples the training processes of the source and target domains. Unlike\\nprevious methods that search lifelong sequences in the source domains using\\nonly content or behavior signals or their simple combinations, we innovatively\\nintroduce a Content-Behavior Joint Training Module (CBJT), which aligns\\ncontent-behavior distributions and combines them with guided information to\\nfacilitate a more stable representation. Furthermore, we develop an Asymmetric\\nSimilarity Integration strategy (ASI) to augment knowledge transfer through\\nsimilarity computation. Extensive experiments demonstrate the effectiveness of\\nGIST, surpassing SOTA methods on offline evaluations and an online A/B test.\\nDeployed on the Xiaohongshu (RedNote) platform, GIST effectively enhances\\nonline ads system performance at scale, serving hundreds of millions of daily\\nactive users.', 'Learning Japanese vocabulary is a challenge for learners from Roman alphabet\\nbackgrounds due to script differences. Japanese combines syllabaries like\\nhiragana with kanji, which are logographic characters of Chinese origin. Kanji\\nare also complicated due to their complexity and volume. Keyword mnemonics are\\na common strategy to aid memorization, often using the compositional structure\\nof kanji to form vivid associations. Despite recent efforts to use large\\nlanguage models (LLMs) to assist learners, existing methods for LLM-based\\nkeyword mnemonic generation function as a black box, offering limited\\ninterpretability. We propose a generative framework that explicitly models the\\nmnemonic construction process as driven by a set of common rules, and learn\\nthem using a novel Expectation-Maximization-type algorithm. Trained on\\nlearner-authored mnemonics from an online platform, our method learns latent\\nstructures and compositional rules, enabling interpretable and systematic\\nmnemonics generation. Experiments show that our method performs well in the\\ncold-start setting for new learners while providing insight into the mechanisms\\nbehind effective mnemonic creation.', 'Large Language Models (LLMs) continue to advance natural language processing\\nwith their ability to generate human-like text across a range of tasks. Despite\\nthe remarkable success of LLMs in Natural Language Processing (NLP), their\\nperformance in text summarization across various domains and datasets has not\\nbeen comprehensively evaluated. At the same time, the ability to summarize text\\neffectively without relying on extensive training data has become a crucial\\nbottleneck. To address these issues, we present a systematic evaluation of six\\nLLMs across four datasets: CNN/Daily Mail and NewsRoom (news), SAMSum (dialog),\\nand ArXiv (scientific). By leveraging prompt engineering techniques including\\nzero-shot and in-context learning, our study evaluates the performance using\\nthe ROUGE and BERTScore metrics. In addition, a detailed analysis of inference\\ntimes is conducted to better understand the trade-off between summarization\\nquality and computational efficiency. For Long documents, introduce a\\nsentence-based chunking strategy that enables LLMs with shorter context windows\\nto summarize extended inputs in multiple stages. The findings reveal that while\\nLLMs perform competitively on news and dialog tasks, their performance on long\\nscientific documents improves significantly when aided by chunking strategies.\\nIn addition, notable performance variations were observed based on model\\nparameters, dataset properties, and prompt design. These results offer\\nactionable insights into how different LLMs behave across task types,\\ncontributing to ongoing research in efficient, instruction-based NLP systems.', 'Accurate channel state information (CSI) is critical to the performance of\\nwireless communication systems, especially with the increasing scale and\\ncomplexity introduced by 5G and future 6G technologies. While artificial\\nintelligence (AI) offers a promising approach to CSI acquisition and\\nutilization, existing methods largely depend on task-specific neural networks\\n(NNs) that require expert-driven design and large training datasets, limiting\\ntheir generalizability and practicality. To address these challenges, we\\npropose LVM4CSI, a general and efficient framework that leverages the\\nstructural similarity between CSI and computer vision (CV) data to directly\\napply large vision models (LVMs) pre-trained on extensive CV datasets to\\nwireless tasks without any fine-tuning, in contrast to large language\\nmodel-based methods that generally necessitate fine-tuning. LVM4CSI maps CSI\\ntasks to analogous CV tasks, transforms complex-valued CSI into visual formats\\ncompatible with LVMs, and integrates lightweight trainable layers to adapt\\nextracted features to specific communication objectives. We validate LVM4CSI\\nthrough three representative case studies, including channel estimation, human\\nactivity recognition, and user localization. Results demonstrate that LVM4CSI\\nachieves comparable or superior performance to task-specific NNs, including an\\nimprovement exceeding 9.61 dB in channel estimation and approximately 40%\\nreduction in localization error. Furthermore, it significantly reduces the\\nnumber of trainable parameters and eliminates the need for task-specific NN\\ndesign.', 'In the field of robotics, researchers face a critical challenge in ensuring\\nreliable and efficient task planning. Verifying high-level task plans before\\nexecution significantly reduces errors and enhance the overall performance of\\nthese systems. In this paper, we propose an architecture for automatically\\nverifying high-level task plans before their execution in simulator or\\nreal-world environments. Leveraging Large Language Models (LLMs), our approach\\nconsists of two key steps: first, the conversion of natural language\\ninstructions into Linear Temporal Logic (LTL), followed by a comprehensive\\nanalysis of action sequences. The module uses the reasoning capabilities of the\\nLLM to evaluate logical coherence and identify potential gaps in the plan.\\nRigorous testing on datasets of varying complexity demonstrates the broad\\napplicability of the module to household tasks. We contribute to improving the\\nreliability and efficiency of task planning and addresses the critical need for\\nrobust pre-execution verification in autonomous systems. The code is available\\nat https://verifyllm.github.io.', \"Knowledge graph (KG) reasoning remains a critical research area focused on\\ninferring missing knowledge by analyzing relationships among observed facts.\\nDespite its success, a key limitation of existing KG reasoning methods is their\\ndependence on the I.I.D assumption. This assumption can easily be violated due\\nto unknown sample selection bias during training or agnostic distribution\\nshifts during testing, significantly compromising model performance and\\nreliability. To facilitate the deployment of KG reasoning in wild environments,\\nthis study investigates learning logical rules from KGs affected by unknown\\nselection bias. Additionally, we address test sets with agnostic distribution\\nshifts, formally defining this challenge as out-of-distribution (OOD) KG\\nreasoning-a previously underexplored problem. To solve the issue, we propose\\nthe Stable Rule Learning (StableRule) framework, an end-to-end methodology that\\nintegrates feature decorrelation with rule learning network, to enhance OOD\\ngeneralization performance. By leveraging feature decorrelation, the StableRule\\nframework mitigates the adverse effects of covariate shifts arising in OOD\\nscenarios, thereby improving the robustness of the rule learning component in\\neffectively deriving logical rules. Extensive experiments on seven benchmark\\nKGs demonstrate the framework's superior effectiveness and stability across\\ndiverse heterogeneous environments, underscoring its practical significance for\\nreal-world applications.\", \"Historical documents represent an invaluable cultural heritage, yet have\\nundergone significant degradation over time through tears, water erosion, and\\noxidation. Existing Historical Document Restoration (HDR) methods primarily\\nfocus on single modality or limited-size restoration, failing to meet practical\\nneeds. To fill this gap, we present a full-page HDR dataset (FPHDR) and a novel\\nautomated HDR solution (AutoHDR). Specifically, FPHDR comprises 1,633 real and\\n6,543 synthetic images with character-level and line-level locations, as well\\nas character annotations in different damage grades. AutoHDR mimics historians'\\nrestoration workflows through a three-stage approach: OCR-assisted damage\\nlocalization, vision-language context text prediction, and patch autoregressive\\nappearance restoration. The modular architecture of AutoHDR enables seamless\\nhuman-machine collaboration, allowing for flexible intervention and\\noptimization at each restoration stage. Experiments demonstrate AutoHDR's\\nremarkable performance in HDR. When processing severely damaged documents, our\\nmethod improves OCR accuracy from 46.83\\\\% to 84.05\\\\%, with further enhancement\\nto 94.25\\\\% through human-machine collaboration. We believe this work represents\\na significant advancement in automated historical document restoration and\\ncontributes substantially to cultural heritage preservation. The model and\\ndataset are available at https://github.com/SCUT-DLVCLab/AutoHDR.\", \"Deep learning-based computational methods have achieved promising results in\\npredicting protein-protein interactions (PPIs). However, existing benchmarks\\npredominantly focus on isolated pairwise evaluations, overlooking a model's\\ncapability to reconstruct biologically meaningful PPI networks, which is\\ncrucial for biology research. To address this gap, we introduce PRING, the\\nfirst comprehensive benchmark that evaluates protein-protein interaction\\nprediction from a graph-level perspective. PRING curates a high-quality,\\nmulti-species PPI network dataset comprising 21,484 proteins and 186,818\\ninteractions, with well-designed strategies to address both data redundancy and\\nleakage. Building on this golden-standard dataset, we establish two\\ncomplementary evaluation paradigms: (1) topology-oriented tasks, which assess\\nintra and cross-species PPI network construction, and (2) function-oriented\\ntasks, including protein complex pathway prediction, GO module analysis, and\\nessential protein justification. These evaluations not only reflect the model's\\ncapability to understand the network topology but also facilitate protein\\nfunction annotation, biological module detection, and even disease mechanism\\nanalysis. Extensive experiments on four representative model categories,\\nconsisting of sequence similarity-based, naive sequence-based, protein language\\nmodel-based, and structure-based approaches, demonstrate that current PPI\\nmodels have potential limitations in recovering both structural and functional\\nproperties of PPI networks, highlighting the gap in supporting real-world\\nbiological applications. We believe PRING provides a reliable platform to guide\\nthe development of more effective PPI prediction models for the community. The\\ndataset and source code of PRING are available at\\nhttps://github.com/SophieSarceau/PRING.\", 'Accurate trajectory prediction is critical for safe autonomous navigation,\\nyet the impact of dataset design on model performance remains understudied.\\nThis work systematically examines how feature selection, cross-dataset\\ntransfer, and geographic diversity influence trajectory prediction accuracy in\\nmulti-agent settings. We evaluate a state-of-the-art model using our novel L4\\nMotion Forecasting dataset based on our own data recordings in Germany and the\\nUS. This includes enhanced map and agent features. We compare our dataset to\\nthe US-centric Argoverse 2 benchmark. First, we find that incorporating\\nsupplementary map and agent features unique to our dataset, yields no\\nmeasurable improvement over baseline features, demonstrating that modern\\narchitectures do not need extensive feature sets for optimal performance. The\\nlimited features of public datasets are sufficient to capture convoluted\\ninteractions without added complexity. Second, we perform cross-dataset\\nexperiments to evaluate how effective domain knowledge can be transferred\\nbetween datasets. Third, we group our dataset by country and check the\\nknowledge transfer between different driving cultures.', \"Large Language Models (LLMs) have transformed human-machine interaction since\\nChatGPT's 2022 debut, with Retrieval-Augmented Generation (RAG) emerging as a\\nkey framework that enhances LLM outputs by integrating external knowledge.\\nHowever, RAG's reliance on ingesting external documents introduces new\\nvulnerabilities. This paper exposes a critical security gap at the data loading\\nstage, where malicious actors can stealthily corrupt RAG pipelines by\\nexploiting document ingestion.\\n  We propose a taxonomy of 9 knowledge-based poisoning attacks and introduce\\ntwo novel threat vectors -- Content Obfuscation and Content Injection --\\ntargeting common formats (DOCX, HTML, PDF). Using an automated toolkit\\nimplementing 19 stealthy injection techniques, we test five popular data\\nloaders, finding a 74.4% attack success rate across 357 scenarios. We further\\nvalidate these threats on six end-to-end RAG systems -- including white-box\\npipelines and black-box services like NotebookLM and OpenAI Assistants --\\ndemonstrating high success rates and critical vulnerabilities that bypass\\nfilters and silently compromise output integrity. Our results emphasize the\\nurgent need to secure the document ingestion process in RAG systems against\\ncovert content manipulations.\", \"Pearl observes that causal knowledge enables predicting the effects of\\ninterventions, such as actions, whereas descriptive knowledge only permits\\ndrawing conclusions from observation. This paper extends Pearl's approach to\\ncausality and interventions to the setting of stratified abductive logic\\nprograms. It shows how stable models of such programs can be given a causal\\ninterpretation by building on philosophical foundations and recent work by\\nBochman and Eelink et al. In particular, it provides a translation of abductive\\nlogic programs into causal systems, thereby clarifying the informal causal\\nreading of logic program rules and supporting principled reasoning about\\nexternal actions. The main result establishes that the stable model semantics\\nfor stratified programs conforms to key philosophical principles of causation,\\nsuch as causal sufficiency, natural necessity, and irrelevance of unobserved\\neffects. This justifies the use of stratified abductive logic programs as a\\nframework for causal modeling and for predicting the effects of interventions\", 'Deep neural networks are increasingly applied for automated histopathology.\\nYet, whole-slide images (WSIs) are often acquired at gigapixel sizes, rendering\\nit computationally infeasible to analyze them entirely at high resolution.\\nDiagnostic labels are largely available only at the slide-level, because expert\\nannotation of images at a finer (patch) level is both laborious and expensive.\\nMoreover, regions with diagnostic information typically occupy only a small\\nfraction of the WSI, making it inefficient to examine the entire slide at full\\nresolution. Here, we propose SASHA -- {\\\\it S}equential {\\\\it A}ttention-based\\n{\\\\it S}ampling for {\\\\it H}istopathological {\\\\it A}nalysis -- a deep\\nreinforcement learning approach for efficient analysis of histopathological\\nimages. First, SASHA learns informative features with a lightweight\\nhierarchical, attention-based multiple instance learning (MIL) model. Second,\\nSASHA samples intelligently and zooms selectively into a small fraction\\n(10-20\\\\%) of high-resolution patches, to achieve reliable diagnosis. We show\\nthat SASHA matches state-of-the-art methods that analyze the WSI fully at\\nhigh-resolution, albeit at a fraction of their computational and memory costs.\\nIn addition, it significantly outperforms competing, sparse sampling methods.\\nWe propose SASHA as an intelligent sampling model for medical imaging\\nchallenges that involve automated diagnosis with exceptionally large images\\ncontaining sparsely informative features.', 'Autoregressive image generation has witnessed rapid advancements, with\\nprominent models such as scale-wise visual auto-regression pushing the\\nboundaries of visual synthesis. However, these developments also raise\\nsignificant concerns regarding data privacy and copyright. In response,\\ntraining data detection has emerged as a critical task for identifying\\nunauthorized data usage in model training. To better understand the\\nvulnerability of autoregressive image generative models to such detection, we\\nconduct the first study applying membership inference to this domain. Our\\napproach comprises two key components: implicit classification and an adaptive\\nscore aggregation strategy. First, we compute the implicit token-wise\\nclassification score within the query image. Then we propose an adaptive score\\naggregation strategy to acquire a final score, which places greater emphasis on\\nthe tokens with lower scores. A higher final score indicates that the sample is\\nmore likely to be involved in the training set. To validate the effectiveness\\nof our method, we adapt existing detection algorithms originally designed for\\nLLMs to visual autoregressive models. Extensive experiments demonstrate the\\nsuperiority of our method in both class-conditional and text-to-image\\nscenarios. Moreover, our approach exhibits strong robustness and generalization\\nunder various data transformations. Furthermore, sufficient experiments suggest\\ntwo novel key findings: (1) A linear scaling law on membership inference,\\nexposing the vulnerability of large foundation models. (2) Training data from\\nscale-wise visual autoregressive models is easier to detect than other\\nautoregressive paradigms.Our code is available at\\nhttps://github.com/Chrisqcwx/ImageAR-MIA.', 'Recent advances have established a new machine learning paradigm based on\\nscaling up compute at inference time as well as at training time. In that line\\nof work, a combination of Supervised Fine-Tuning (SFT) on synthetic\\ndemonstrations and Reinforcement Learning with Verifiable Rewards (RLVR) is\\nused for training Large Language Models to expend extra compute during\\ninference in the form of \"thoughts\" expressed in natural language. In this\\npaper, we propose to instead format these tokens as a multi-turn interaction\\ntrace with a stateful tool. At each turn, the new state of the tool is appended\\nto the context of the model, whose job is to generate the tokens necessary to\\ncontrol the tool via a custom DSL. We benchmark this approach on the problem of\\nrepairing malfunctioning Python code, and show that this constrained setup\\nallows for faster sampling of experience and a denser reward signal, allowing\\neven models of size up to 3B parameters to learn how to proficiently expend\\nadditional compute on the task.', \"Hallucinations in large vision-language models (LVLMs) pose significant\\nchallenges for real-world applications, as LVLMs may generate responses that\\nappear plausible yet remain inconsistent with the associated visual content.\\nThis issue rarely occurs in human cognition. We argue that this discrepancy\\narises from humans' ability to effectively leverage multimodal interaction\\ninformation in data samples. Specifically, humans typically first gather\\nmultimodal information, analyze the interactions across modalities for\\nunderstanding, and then express their understanding through language. Motivated\\nby this observation, we conduct extensive experiments on popular LVLMs and\\nobtained insights that surprisingly reveal human-like, though less pronounced,\\ncognitive behavior of LVLMs on multimodal samples. Building on these findings,\\nwe further propose \\\\textbf{INTER}: \\\\textbf{Inter}action Guidance Sampling, a\\nnovel training-free algorithm that mitigate hallucinations without requiring\\nadditional data. Specifically, INTER explicitly guides LVLMs to effectively\\nreapply their understanding of multimodal interaction information when\\ngenerating responses, thereby reducing potential hallucinations. On six\\nbenchmarks including VQA and image captioning tasks, INTER achieves an average\\nimprovement of up to 3.4\\\\% on five LVLMs compared to the state-of-the-art\\ndecoding strategy. The code will be released when the paper is accepted.\", 'Recently, research into chatbots (also known as conversational agents, AI\\nagents, voice assistants), which are computer applications using artificial\\nintelligence to mimic human-like conversation, has grown sharply. Despite this\\ngrowth, sociology lags other disciplines (including computer science, medicine,\\npsychology, and communication) in publishing about chatbots. We suggest\\nsociology can advance understanding of human-chatbot interaction and offer four\\nsociological theories to enhance extant work in this field. The first two\\ntheories (resource substitution theory, power-dependence theory) add new\\ninsights to existing models of the drivers of chatbot use, which overlook\\nsociological concerns about how social structure (e.g., systemic\\ndiscrimination, the uneven distribution of resources within networks) inclines\\nindividuals to use chatbots, including problematic levels of emotional\\ndependency on chatbots. The second two theories (affect control theory,\\nfundamental cause of disease theory) help inform the development of\\nchatbot-driven interventions that minimize safety risks and enhance equity by\\nleveraging sociological insights into how chatbot outputs could attend to\\ncultural contexts (e.g., affective norms) to promote wellbeing and enhance\\ncommunities (e.g., opportunities for civic participation). We discuss the value\\nof applying sociological theories for advancing theorizing about human-chatbot\\ninteraction and developing chatbots for social good.', 'Surgical AI often involves multiple tasks within a single procedure, like\\nphase recognition or assessing the Critical View of Safety in laparoscopic\\ncholecystectomy. Traditional models, built for one task at a time, lack\\nflexibility, requiring a separate model for each. To address this, we introduce\\nMML-SurgAdapt, a unified multi-task framework with Vision-Language Models\\n(VLMs), specifically CLIP, to handle diverse surgical tasks through natural\\nlanguage supervision. A key challenge in multi-task learning is the presence of\\npartial annotations when integrating different tasks. To overcome this, we\\nemploy Single Positive Multi-Label (SPML) learning, which traditionally reduces\\nannotation burden by training models with only one positive label per instance.\\nOur framework extends this approach to integrate data from multiple surgical\\ntasks within a single procedure, enabling effective learning despite incomplete\\nor noisy annotations. We demonstrate the effectiveness of our model on a\\ncombined dataset consisting of Cholec80, Endoscapes2023, and CholecT50,\\nutilizing custom prompts. Extensive evaluation shows that MML-SurgAdapt\\nperforms comparably to task-specific benchmarks, with the added advantage of\\nhandling noisy annotations. It also outperforms the existing SPML frameworks\\nfor the task. By reducing the required labels by 23%, our approach proposes a\\nmore scalable and efficient labeling process, significantly easing the\\nannotation burden on clinicians. To our knowledge, this is the first\\napplication of SPML to integrate data from multiple surgical tasks, presenting\\na novel and generalizable solution for multi-task learning in surgical computer\\nvision. Implementation is available at:\\nhttps://github.com/CAMMA-public/MML-SurgAdapt', 'In-context learning enables transformer models to generalize to new tasks\\nbased solely on input prompts, without any need for weight updates. However,\\nexisting training paradigms typically rely on large, unstructured datasets that\\nare costly to store, difficult to evaluate for quality and balance, and pose\\nprivacy and ethical concerns due to the inclusion of sensitive information.\\nMotivated by these limitations and risks, we propose an alternative training\\nstrategy where we leverage a collection of multiple, small-scale, and\\ndomain-specific datasets. We empirically demonstrate that the increased quality\\nand diversity of such data improve the generalization abilities of in-context\\nlearners beyond their training domain, while achieving comparable performance\\nwith models trained on a single large-scale dataset. We investigate this\\nparadigm by leveraging meta-learning to train an in-context learner on the\\nMeta-Album collection under several settings. Firstly, we show the performance\\nin a controlled environment, where the test domain is completely excluded from\\nthe training knowledge. Secondly, we explore the robustness of these models to\\nforgetting in a continual scenario where the information is accessible for a\\nlimited time. Finally, we explore the more challenging unsupervised scenario.\\nOur findings demonstrate that transformers still generalize for in-context\\nprediction when trained on a curated dataset collection while offering\\nadvantages in modularity and replaceability.', 'Surgical action planning requires predicting future instrument-verb-target\\ntriplets for real-time assistance. While teleoperated robotic surgery provides\\nnatural expert demonstrations for imitation learning (IL), reinforcement\\nlearning (RL) could potentially discover superior strategies through\\nexploration. We present the first comprehensive comparison of IL versus RL for\\nsurgical action planning on CholecT50. Our Dual-task Autoregressive Imitation\\nLearning (DARIL) baseline achieves 34.6% action triplet recognition mAP and\\n33.6% next frame prediction mAP with smooth planning degradation to 29.2% at\\n10-second horizons. We evaluated three RL variants: world model-based RL,\\ndirect video RL, and inverse RL enhancement. Surprisingly, all RL approaches\\nunderperformed DARIL i.e. world model RL dropped to 3.1% mAP at 10s while\\ndirect video RL achieved only 15.9%. Our analysis reveals that distribution\\nmatching on expert-annotated test sets systematically favors IL over\\npotentially valid RL policies that differ from training demonstrations. This\\nchallenges assumptions about RL superiority in sequential decision making and\\nprovides crucial insights for surgical AI development.', \"The Critical View of Safety (CVS) is crucial for safe laparoscopic\\ncholecystectomy, yet assessing CVS criteria remains a complex and challenging\\ntask, even for experts. Traditional models for CVS recognition depend on\\nvision-only models learning with costly, labor-intensive spatial annotations.\\nThis study investigates how text can be harnessed as a powerful tool for both\\ntraining and inference in multi-modal surgical foundation models to automate\\nCVS recognition. Unlike many existing multi-modal models, which are primarily\\nadapted for multi-class classification, CVS recognition requires a multi-label\\nframework. Zero-shot evaluation of existing multi-modal surgical models shows a\\nsignificant performance gap for this task. To address this, we propose\\nCVS-AdaptNet, a multi-label adaptation strategy that enhances fine-grained,\\nbinary classification across multiple labels by aligning image embeddings with\\ntextual descriptions of each CVS criterion using positive and negative prompts.\\nBy adapting PeskaVLP, a state-of-the-art surgical foundation model, on the\\nEndoscapes-CVS201 dataset, CVS-AdaptNet achieves 57.6 mAP, improving over the\\nResNet50 image-only baseline (51.5 mAP) by 6 points. Our results show that\\nCVS-AdaptNet's multi-label, multi-modal framework, enhanced by textual prompts,\\nboosts CVS recognition over image-only methods. We also propose text-specific\\ninference methods, that helps in analysing the image-text alignment. While\\nfurther work is needed to match state-of-the-art spatial annotation-based\\nmethods, this approach highlights the potential of adapting generalist models\\nto specialized surgical tasks. Code:\\nhttps://github.com/CAMMA-public/CVS-AdaptNet\", 'We introduce Supported Abstract Argumentation for Case-Based Reasoning\\n(sAA-CBR), a binary classification model in which past cases engage in debates\\nby arguing in favour of their labelling and attacking or supporting those with\\nopposing or agreeing labels. With supports, sAA-CBR overcomes the limitation of\\nits precursor AA-CBR, which can contain extraneous cases (or spikes) that are\\nnot included in the debates. We prove that sAA-CBR contains no spikes, without\\ntrading off key model properties', 'T cell receptor (TCR) repertoires encode critical immunological signatures\\nfor autoimmune diseases, yet their clinical application remains limited by\\nsequence sparsity and low witness rates. We developed EAMil, a multi-instance\\ndeep learning framework that leverages TCR sequencing data to diagnose systemic\\nlupus erythematosus (SLE) and rheumatoid arthritis (RA) with exceptional\\naccuracy. By integrating PrimeSeq feature extraction with ESMonehot encoding\\nand enhanced gate attention mechanisms, our model achieved state-of-the-art\\nperformance with AUCs of 98.95% for SLE and 97.76% for RA. EAMil successfully\\nidentified disease-associated genes with over 90% concordance with established\\ndifferential analyses and effectively distinguished disease-specific TCR genes.\\nThe model demonstrated robustness in classifying multiple disease categories,\\nutilizing the SLEDAI score to stratify SLE patients by disease severity as well\\nas to diagnose the site of damage in SLE patients, and effectively controlling\\nfor confounding factors such as age and gender. This interpretable framework\\nfor immune receptor analysis provides new insights for autoimmune disease\\ndetection and classification with broad potential clinical applications across\\nimmune-mediated conditions.', 'The field of Singing Voice Synthesis (SVS) has seen significant advancements\\nin recent years due to the rapid progress of diffusion-based approaches.\\nHowever, capturing vocal style, genre-specific pitch inflections, and\\nlanguage-dependent characteristics remains challenging, particularly in\\nlow-resource scenarios. To address this, we propose LAPS-Diff, a diffusion\\nmodel integrated with language-aware embeddings and a vocal-style guided\\nlearning mechanism, specifically designed for Bollywood Hindi singing style. We\\ncurate a Hindi SVS dataset and leverage pre-trained language models to extract\\nword and phone-level embeddings for an enriched lyrics representation.\\nAdditionally, we incorporated a style encoder and a pitch extraction model to\\ncompute style and pitch losses, capturing features essential to the naturalness\\nand expressiveness of the synthesized singing, particularly in terms of vocal\\nstyle and pitch variations. Furthermore, we utilize MERT and IndicWav2Vec\\nmodels to extract musical and contextual embeddings, serving as conditional\\npriors to refine the acoustic feature generation process further. Based on\\nobjective and subjective evaluations, we demonstrate that LAPS-Diff\\nsignificantly improves the quality of the generated samples compared to the\\nconsidered state-of-the-art (SOTA) model for our constrained dataset that is\\ntypical of the low resource scenario.', \"Video-to-audio (V2A) generation shows great potential in fields such as film\\nproduction. Despite significant advances, current V2A methods, which rely on\\nglobal video information, struggle with complex scenes and often fail to\\ngenerate audio tailored to specific objects or regions in the videos. To\\naddress these limitations, we introduce Hear-Your-Click, an interactive V2A\\nframework that enables users to generate sounds for specific objects in the\\nvideos by simply clicking on the frame. To achieve this, we propose\\nObject-aware Contrastive Audio-Visual Fine-tuning (OCAV) with a Mask-guided\\nVisual Encoder (MVE) to obtain object-level visual features aligned with\\ncorresponding audio segments. Furthermore, we tailor two data augmentation\\nstrategies: Random Video Stitching (RVS) and Mask-guided Loudness Modulation\\n(MLM), aimed at enhancing the model's sensitivity to the segmented objects. To\\neffectively measure the audio-visual correspondence, we design a new evaluation\\nmetric, the CAV score, for evaluation. Extensive experiments demonstrate that\\nour framework offers more precise control and improved generation performance\\nacross various metrics. Project Page:\\nhttps://github.com/SynapGrid/Hear-Your-Click\", 'We propose Expotion (Facial Expression and Motion Control for Multimodal\\nMusic Generation), a generative model leveraging multimodal visual controls -\\nspecifically, human facial expressions and upper-body motion - as well as text\\nprompts to produce expressive and temporally accurate music. We adopt\\nparameter-efficient fine-tuning (PEFT) on the pretrained text-to-music\\ngeneration model, enabling fine-grained adaptation to the multimodal controls\\nusing a small dataset. To ensure precise synchronization between video and\\nmusic, we introduce a temporal smoothing strategy to align multiple modalities.\\nExperiments demonstrate that integrating visual features alongside textual\\ndescriptions enhances the overall quality of generated music in terms of\\nmusicality, creativity, beat-tempo consistency, temporal alignment with the\\nvideo, and text adherence, surpassing both proposed baselines and existing\\nstate-of-the-art video-to-music generation models. Additionally, we introduce a\\nnovel dataset consisting of 7 hours of synchronized video recordings capturing\\nexpressive facial and upper-body gestures aligned with corresponding music,\\nproviding significant potential for future research in multimodal and\\ninteractive music generation.', \"We introduce DC-AR, a novel masked autoregressive (AR) text-to-image\\ngeneration framework that delivers superior image generation quality with\\nexceptional computational efficiency. Due to the tokenizers' limitations, prior\\nmasked AR models have lagged behind diffusion models in terms of quality or\\nefficiency. We overcome this limitation by introducing DC-HT - a deep\\ncompression hybrid tokenizer for AR models that achieves a 32x spatial\\ncompression ratio while maintaining high reconstruction fidelity and\\ncross-resolution generalization ability. Building upon DC-HT, we extend MaskGIT\\nand create a new hybrid masked autoregressive image generation framework that\\nfirst produces the structural elements through discrete tokens and then applies\\nrefinements via residual tokens. DC-AR achieves state-of-the-art results with a\\ngFID of 5.49 on MJHQ-30K and an overall score of 0.69 on GenEval, while\\noffering 1.5-7.9x higher throughput and 2.0-3.5x lower latency compared to\\nprior leading diffusion and autoregressive models.\", 'Reasoning about the trajectories of multiple, interacting objects is integral\\nto physical reasoning tasks in machine learning. This involves conditions\\nimposed on the objects at different time steps, for instance initial states or\\ndesired goal states. Existing approaches in physical reasoning generally rely\\non autoregressive modeling, which can only be conditioned on initial states,\\nbut not on later states. In fields such as planning for reinforcement learning,\\nsimilar challenges are being addressed with denoising diffusion models. In this\\nwork, we propose an object-centric denoising diffusion model architecture for\\nphysical reasoning that is translation equivariant over time, permutation\\nequivariant over objects, and can be conditioned on arbitrary time steps for\\narbitrary objects. We demonstrate how this model can solve tasks with multiple\\nconditions and examine its performance when changing object numbers and\\ntrajectory lengths during inference.', 'Understanding leadership dynamics in collective behavior is a key challenge\\nin animal ecology, swarm robotics, and intelligent transportation. Traditional\\ninformation-theoretic approaches, including Transfer Entropy (TE) and\\nTime-Lagged Mutual Information (TLMI), have been widely used to infer\\nleader-follower relationships but face critical limitations in noisy or\\nshort-duration datasets due to their reliance on robust probability\\nestimations. This study proposes a method based on dynamic network inference\\nusing time-lagged correlations across multiple kinematic variables: velocity,\\nacceleration, and direction. Our approach constructs directed influence graphs\\nover time, enabling the identification of leadership patterns without the need\\nfor large volumes of data or parameter-sensitive discretization. We validate\\nour method through two multi-agent simulations in NetLogo: a modified Vicsek\\nmodel with informed leaders and a predator-prey model featuring coordinated and\\nindependent wolf groups. Experimental results demonstrate that the\\nnetwork-based method outperforms TE and TLMI in scenarios with limited\\nspatiotemporal observations, ranking true leaders at the top of influence\\nmetrics more consistently than TE and TLMI.', 'Multimodal Large Language Models (MLLMs) have demonstrated significant\\nadvances in visual understanding tasks involving both images and videos.\\nHowever, their capacity to comprehend human-centric video data remains\\nunderexplored, primarily due to the absence of comprehensive and high-quality\\nevaluation benchmarks. Existing human-centric benchmarks predominantly\\nemphasize video generation quality and action recognition, while overlooking\\nessential perceptual and cognitive abilities required in human-centered\\nscenarios. Furthermore, they are often limited by single-question paradigms and\\noverly simplistic evaluation metrics. To address above limitations, we propose\\na modern HV-MMBench, a rigorously curated benchmark designed to provide a more\\nholistic evaluation of MLLMs in human-centric video understanding. Compared to\\nexisting human-centric video benchmarks, our work offers the following key\\nfeatures: (1) Diverse evaluation dimensions: HV-MMBench encompasses 15 tasks,\\nranging from basic attribute perception (e.g., age estimation, emotion\\nrecognition) to advanced cognitive reasoning (e.g., social relationship\\nprediction, intention prediction), enabling comprehensive assessment of model\\ncapabilities; (2) Varied data types: The benchmark includes multiple-choice,\\nfill-in-blank, true/false, and open-ended question formats, combined with\\ndiverse evaluation metrics, to more accurately and robustly reflect model\\nperformance; (3) Multi-domain video coverage: The benchmark spans 50 distinct\\nvisual scenarios, enabling comprehensive evaluation across fine-grained scene\\nvariations; (4) Temporal coverage: The benchmark covers videos from short-term\\n(10 seconds) to long-term (up to 30min) durations, supporting systematic\\nanalysis of models temporal reasoning abilities across diverse contextual\\nlengths.', 'Federated Learning (FL) systems are vulnerable to backdoor attacks, where\\nadversaries train their local models on poisoned data and submit poisoned model\\nupdates to compromise the global model. Despite numerous proposed attacks and\\ndefenses, divergent experimental settings, implementation errors, and\\nunrealistic assumptions hinder fair comparisons and valid conclusions about\\ntheir effectiveness in real-world scenarios. To address this, we introduce\\nBackFed - a comprehensive benchmark suite designed to standardize, streamline,\\nand reliably evaluate backdoor attacks and defenses in FL, with a focus on\\npractical constraints. Our benchmark offers key advantages through its\\nmulti-processing implementation that significantly accelerates experimentation\\nand the modular design that enables seamless integration of new methods via\\nwell-defined APIs. With a standardized evaluation pipeline, we envision BackFed\\nas a plug-and-play environment for researchers to comprehensively and reliably\\nevaluate new attacks and defenses. Using BackFed, we conduct large-scale\\nstudies of representative backdoor attacks and defenses across both Computer\\nVision and Natural Language Processing tasks with diverse model architectures\\nand experimental settings. Our experiments critically assess the performance of\\nproposed attacks and defenses, revealing unknown limitations and modes of\\nfailures under practical conditions. These empirical insights provide valuable\\nguidance for the development of new methods and for enhancing the security of\\nFL systems. Our framework is openly available at\\nhttps://github.com/thinh-dao/BackFed.', 'Accident severity prediction plays a critical role in transportation safety\\nsystems but is a persistently difficult task due to incomplete data, strong\\nfeature dependencies, and severe class imbalance in which rare but\\nhigh-severity cases are underrepresented and hard to detect. Existing methods\\noften rely on monolithic models or black box prompting, which struggle to scale\\nin noisy, real-world settings and offer limited interpretability. To address\\nthese challenges, we propose MARBLE a multiagent rule based LLM engine that\\ndecomposes the severity prediction task across a team of specialized reasoning\\nagents, including an interchangeable ML-backed agent. Each agent focuses on a\\nsemantic subset of features (e.g., spatial, environmental, temporal), enabling\\nscoped reasoning and modular prompting without the risk of prompt saturation.\\nPredictions are coordinated through either rule-based or LLM-guided consensus\\nmechanisms that account for class rarity and confidence dynamics. The system\\nretains structured traces of agent-level reasoning and coordination outcomes,\\nsupporting in-depth interpretability and post-hoc performance diagnostics.\\nAcross both UK and US datasets, MARBLE consistently outperforms traditional\\nmachine learning classifiers and state-of-the-art (SOTA) prompt-based reasoning\\nmethods including Chain-of-Thought (CoT), Least-to-Most (L2M), and\\nTree-of-Thought (ToT) achieving nearly 90% accuracy where others plateau below\\n48%. This performance redefines the practical ceiling for accident severity\\nclassification under real world noise and extreme class imbalance. Our results\\nposition MARBLE as a generalizable and interpretable framework for reasoning\\nunder uncertainty in safety-critical applications.', 'Understanding the locus of semantic representation in large language models\\n(LLMs) is crucial for interpretability and architectural innovation. The\\ndominant paradigm posits that trainable input embeddings serve as foundational\\n\"meaning vectors.\" This paper challenges that view. We construct Transformer\\nmodels where the embedding layer is entirely frozen, with vectors derived not\\nfrom data, but from the visual structure of Unicode glyphs. These non-semantic,\\nprecomputed visual embeddings are fixed throughout training. Our method is\\ncompatible with any tokenizer, including a novel Unicode-centric tokenizer we\\nintroduce to ensure universal text coverage. Despite the absence of trainable,\\nsemantically initialized embeddings, our models converge, generate coherent\\ntext, and, critically, outperform architecturally identical models with\\ntrainable embeddings on the MMLU reasoning benchmark. We attribute this to\\n\"representational interference\" in conventional models, where the embedding\\nlayer is burdened with learning both structural and semantic features. Our\\nresults indicate that high-level semantics are not inherent to input embeddings\\nbut are an emergent property of the Transformer\\'s compositional architecture\\nand data scale. This reframes the role of embeddings from meaning containers to\\nstructural primitives. We release all code and models to foster further\\nresearch.', 'Deep Reinforcement Learning (DRL) systems are increasingly used in\\nsafety-critical applications, yet their security remains severely\\nunderexplored. This work investigates backdoor attacks, which implant hidden\\ntriggers that cause malicious actions only when specific inputs appear in the\\nobservation space. Existing DRL backdoor research focuses solely on\\ntraining-time attacks requiring unrealistic access to the training pipeline. In\\ncontrast, we reveal critical vulnerabilities across the DRL supply chain where\\nbackdoors can be embedded with significantly reduced adversarial privileges. We\\nintroduce two novel attacks: (1) TrojanentRL, which exploits component-level\\nflaws to implant a persistent backdoor that survives full model retraining; and\\n(2) InfrectroRL, a post-training backdoor attack which requires no access to\\ntraining, validation, nor test data. Empirical and analytical evaluations\\nacross six Atari environments show our attacks rival state-of-the-art\\ntraining-time backdoor attacks while operating under much stricter adversarial\\nconstraints. We also demonstrate that InfrectroRL further evades two leading\\nDRL backdoor defenses. These findings challenge the current research focus and\\nhighlight the urgent need for robust defenses.', 'Colorectal cancer (CRC) is closely linked to the malignant transformation of\\ncolorectal polyps, making early detection essential. However, current models\\nstruggle with detecting small lesions, accurately localizing boundaries, and\\nproviding interpretable decisions. To address these issues, we propose HGNet,\\nwhich integrates High-Order Spatial Awareness Hypergraph and Multi-Scale\\nContext Attention. Key innovations include: (1) an Efficient Multi-Scale\\nContext Attention (EMCA) module to enhance lesion feature representation and\\nboundary modeling; (2) the deployment of a spatial hypergraph convolution\\nmodule before the detection head to capture higher-order spatial relationships\\nbetween nodes; (3) the application of transfer learning to address the scarcity\\nof medical image data; and (4) Eigen Class Activation Map (Eigen-CAM) for\\ndecision visualization. Experimental results show that HGNet achieves 94%\\naccuracy, 90.6% recall, and 90% mAP@0.5, significantly improving small lesion\\ndifferentiation and clinical interpretability. The source code will be made\\npublicly available upon publication of this paper.', \"Enhancing interrogation capabilities in Traditional Chinese Medicine (TCM)\\ndiagnosis through multi-turn dialogues and knowledge graphs presents a\\nsignificant challenge for modern AI systems. Current large language models\\n(LLMs), despite their advancements, exhibit notable limitations in medical\\napplications, particularly in conducting effective multi-turn dialogues and\\nproactive questioning. These shortcomings hinder their practical application\\nand effectiveness in simulating real-world diagnostic scenarios. To address\\nthese limitations, we propose DoPI, a novel LLM system specifically designed\\nfor the TCM domain. The DoPI system introduces a collaborative architecture\\ncomprising a guidance model and an expert model. The guidance model conducts\\nmulti-turn dialogues with patients and dynamically generates questions based on\\na knowledge graph to efficiently extract critical symptom information.\\nSimultaneously, the expert model leverages deep TCM expertise to provide final\\ndiagnoses and treatment plans. Furthermore, this study constructs a multi-turn\\ndoctor-patient dialogue dataset to simulate realistic consultation scenarios\\nand proposes a novel evaluation methodology that does not rely on manually\\ncollected real-world consultation data. Experimental results show that the DoPI\\nsystem achieves an accuracy rate of 84.68 percent in interrogation outcomes,\\nsignificantly enhancing the model's communication ability during diagnosis\\nwhile maintaining professional expertise.\", 'Understanding and quantifying chaos in nonlinear dynamical systems remains a\\nfundamental challenge in science and engineering. The Lyapunov exponent is a\\nkey measure of chaotic behavior, but its accurate estimation from experimental\\ndata is often hindered by methodological and computational limitations. In this\\nwork, we present a novel machine-learning-based approach for estimating the\\npositive Lyapunov exponent (MLE) from one-dimensional time series, using the\\ngrowth of out-of-sample prediction errors as a proxy for trajectory divergence.\\nOur method demonstrates high scientific relevance, offering a robust,\\ndata-driven alternative to traditional analytic techniques. Through\\ncomprehensive testing on several canonical chaotic maps - including the\\nlogistic, sine, cubic, and Chebyshev maps - we achieved a coefficient of\\ndetermination R2pos > 0.9 between predicted and theoretical MLE values for time\\nseries as short as M = 200 points. The best accuracy was observed for the\\nChebyshev map (R2pos = 0.999). Notably, the proposed method maintains high\\ncomputational efficiency and generalizes well across various machine learning\\nalgorithms. These results highlight the significance of our approach for\\npractical chaos analysis in both synthetic and experimental settings, opening\\nnew possibilities for robust nonlinear dynamics assessment when only time\\nseries data are available.', 'We explore transfer learning strategies for musical onset detection in the\\nAfro-Brazilian Maracatu tradition, which features complex rhythmic patterns\\nthat challenge conventional models. We adapt two Temporal Convolutional Network\\narchitectures: one pre-trained for onset detection (intra-task) and another for\\nbeat tracking (inter-task). Using only 5-second annotated snippets per\\ninstrument, we fine-tune these models through layer-wise retraining strategies\\nfor five traditional percussion instruments. Our results demonstrate\\nsignificant improvements over baseline performance, with F1 scores reaching up\\nto 0.998 in the intra-task setting and improvements of over 50 percentage\\npoints in best-case scenarios. The cross-task adaptation proves particularly\\neffective for time-keeping instruments, where onsets naturally align with beat\\npositions. The optimal fine-tuning configuration varies by instrument,\\nhighlighting the importance of instrument-specific adaptation strategies. This\\napproach addresses the challenges of underrepresented musical traditions,\\noffering an efficient human-in-the-loop methodology that minimizes annotation\\neffort while maximizing performance. Our findings contribute to more inclusive\\nmusic information retrieval tools applicable beyond Western musical contexts.', 'Precise control over speech characteristics, such as pitch, duration, and\\nspeech rate, remains a significant challenge in the field of voice conversion.\\nThe ability to manipulate parameters like pitch and syllable rate is an\\nimportant element for effective identity conversion, but can also be used\\nindependently for voice transformation, achieving goals that were historically\\naddressed by vocoder-based methods.\\n  In this work, we explore a convolutional neural network-based approach that\\naims to provide means for modifying fundamental frequency (F0), phoneme\\nsequences, intensity, and speaker identity. Rather than relying on\\ndisentanglement techniques, our model is explicitly conditioned on these\\nfactors to generate mel spectrograms, which are then converted into waveforms\\nusing a universal neural vocoder. Accordingly, during inference, F0 contours,\\nphoneme sequences, and speaker embeddings can be freely adjusted, allowing for\\nintuitively controlled voice transformations.\\n  We evaluate our approach on speaker conversion and expressive speech tasks\\nusing both perceptual and objective metrics. The results suggest that the\\nproposed method offers substantial flexibility, while maintaining high\\nintelligibility and speaker similarity.', 'The task of describing video content in natural language is commonly referred\\nto as video captioning. Unlike conventional video captions, which are typically\\nbrief and widely available, long-form paragraph descriptions in natural\\nlanguage are scarce. This limitation of current datasets is due to the\\nexpensive human manual annotation required and to the highly challenging task\\nof explaining the language formation process from the perspective of the\\nunderlying story, as a complex system of interconnected events in space and\\ntime. Through a thorough analysis of recently published methods and available\\ndatasets, we identify a general lack of published resources dedicated to the\\nproblem of describing videos in complex language, beyond the level of\\ndescriptions in the form of enumerations of simple captions. Furthermore, while\\nstate-of-the-art methods produce impressive results on the task of generating\\nshorter captions from videos by direct end-to-end learning between the videos\\nand text, the problem of explaining the relationship between vision and\\nlanguage is still beyond our reach. In this work, we propose a shared\\nrepresentation between vision and language, based on graphs of events in space\\nand time, which can be obtained in an explainable and analytical way, to\\nintegrate and connect multiple vision tasks to produce the final natural\\nlanguage description. Moreover, we also demonstrate how our automated and\\nexplainable video description generation process can function as a fully\\nautomatic teacher to effectively train direct, end-to-end neural student\\npathways, within a self-supervised neuro-analytical system. We validate that\\nour explainable neuro-analytical approach generates coherent, rich and relevant\\ntextual descriptions on videos collected from multiple varied datasets, using\\nboth standard evaluation metrics, human annotations and consensus from\\nensembles of state-of-the-art VLMs.', \"This study examines the feasibility of applying large language models (LLMs)\\nfor forecasting the impact of traffic incidents on the traffic flow. The use of\\nLLMs for this task has several advantages over existing machine learning-based\\nsolutions such as not requiring a large training dataset and the ability to\\nutilize free-text incident logs. We propose a fully LLM-based solution that\\npredicts the incident impact using a combination of traffic features and\\nLLM-extracted incident features. A key ingredient of this solution is an\\neffective method of selecting examples for the LLM's in-context learning. We\\nevaluate the performance of three advanced LLMs and two state-of-the-art\\nmachine learning models on a real traffic incident dataset. The results show\\nthat the best-performing LLM matches the accuracy of the most accurate machine\\nlearning model, despite the former not having been trained on this prediction\\ntask. The findings indicate that LLMs are a practically viable option for\\ntraffic incident impact prediction.\", 'Pun generation seeks to creatively modify linguistic elements in text to\\nproduce humour or evoke double meanings. It also aims to preserve coherence and\\ncontextual appropriateness, making it useful in creative writing and\\nentertainment across various media and contexts. Although pun generation has\\nreceived considerable attention in computational linguistics, there is\\ncurrently no dedicated survey that systematically reviews this specific area.\\nTo bridge this gap, this paper provides a comprehensive review of pun\\ngeneration datasets and methods across different stages, including conventional\\napproaches, deep learning techniques, and pre-trained language models.\\nAdditionally, we summarise both automated and human evaluation metrics used to\\nassess the quality of pun generation. Finally, we discuss the research\\nchallenges and propose promising directions for future work.', 'In this work, we propose a simple but effective channel pruning framework\\ncalled Progressive Channel Pruning (PCP) to accelerate Convolutional Neural\\nNetworks (CNNs). In contrast to the existing channel pruning methods that prune\\nchannels only once per layer in a layer-by-layer fashion, our new progressive\\nframework iteratively prunes a small number of channels from several selected\\nlayers, which consists of a three-step attempting-selecting-pruning pipeline in\\neach iteration. In the attempting step, we attempt to prune a pre-defined\\nnumber of channels from one layer by using any existing channel pruning methods\\nand estimate the accuracy drop for this layer based on the labelled samples in\\nthe validation set. In the selecting step, based on the estimated accuracy\\ndrops for all layers, we propose a greedy strategy to automatically select a\\nset of layers that will lead to less overall accuracy drop after pruning these\\nlayers. In the pruning step, we prune a small number of channels from these\\nselected layers. We further extend our PCP framework to prune channels for the\\ndeep transfer learning methods like Domain Adversarial Neural Network (DANN),\\nin which we effectively reduce the data distribution mismatch in the channel\\npruning process by using both labelled samples from the source domain and\\npseudo-labelled samples from the target domain. Our comprehensive experiments\\non two benchmark datasets demonstrate that our PCP framework outperforms the\\nexisting channel pruning approaches under both supervised learning and transfer\\nlearning settings.', 'Motion planning is a crucial component of autonomous robot driving. While\\nvarious trajectory datasets exist, effectively utilizing them for a target\\ndomain remains challenging due to differences in agent interactions and\\nenvironmental characteristics. Conventional approaches, such as domain\\nadaptation or ensemble learning, leverage multiple source datasets but suffer\\nfrom domain imbalance, catastrophic forgetting, and high computational costs.\\nTo address these challenges, we propose Interaction-Merged Motion Planning\\n(IMMP), a novel approach that leverages parameter checkpoints trained on\\ndifferent domains during adaptation to the target domain. IMMP follows a\\ntwo-step process: pre-merging to capture agent behaviors and interactions,\\nsufficiently extracting diverse information from the source domain, followed by\\nmerging to construct an adaptable model that efficiently transfers diverse\\ninteractions to the target domain. Our method is evaluated on various planning\\nbenchmarks and models, demonstrating superior performance compared to\\nconventional approaches.', 'Current legal frameworks consider AI-generated works eligible for copyright\\nprotection when they meet originality requirements and involve substantial\\nhuman intellectual input. However, systematic legal standards and reliable\\nevaluation methods for AI art copyrights are lacking. Through comprehensive\\nanalysis of legal precedents, we establish three essential criteria for\\ndetermining distinctive artistic style: stylistic consistency, creative\\nuniqueness, and expressive accuracy. To address these challenges, we introduce\\nArtBulb, an interpretable and quantifiable framework for AI art copyright\\njudgment that combines a novel style description-based multimodal clustering\\nmethod with multimodal large language models (MLLMs). We also present AICD, the\\nfirst benchmark dataset for AI art copyright annotated by artists and legal\\nexperts. Experimental results demonstrate that ArtBulb outperforms existing\\nmodels in both quantitative and qualitative evaluations. Our work aims to\\nbridge the gap between the legal and technological communities and bring\\ngreater attention to the societal issue of AI art copyrights.', 'Furniture decoration is an important task in various industrial applications.\\nHowever, achieving a high-quality decorative result is often time-consuming and\\nrequires specialized artistic expertise. To tackle these challenges, we explore\\nhow multi-agent systems can assist in automating the decoration process. We\\npropose FurniMAS, a multi-agent system for automatic furniture decoration.\\nSpecifically, given a human prompt and a household furniture item such as a\\nworking desk or a TV stand, our system suggests relevant assets with\\nappropriate styles and materials, and arranges them on the item, ensuring the\\ndecorative result meets functionality, aesthetic, and ambiance preferences.\\nFurniMAS assembles a hybrid team of LLM-based and non-LLM agents, each\\nfulfilling distinct roles in a typical decoration project. These agents\\ncollaborate through communication, logical reasoning, and validation to\\ntransform the requirements into the final outcome. Extensive experiments\\ndemonstrate that our FurniMAS significantly outperforms other baselines in\\ngenerating high-quality 3D decor.', \"Personalized text generation has become crucial for adapting language models\\nto diverse and evolving users' personal context across cultural, temporal, and\\ncontextual dimensions. While existing methods often rely on centralized\\nfine-tuning or static preference alignment, they struggle to achieve real-time\\nadaptation under resource constraints inherent to personal devices. This\\nlimitation creates a dilemma: large cloud-based models lack access to localized\\nuser-specific information, while small on-device models cannot match the\\ngeneration quality of their cloud counterparts. To address this dichotomy, we\\npresent CoSteer, a novel collaborative framework that enables decoding-time\\npersonalization through localized delta steering. Our key insight lies in\\nleveraging the logits difference between personal context-aware and -agnostic\\noutputs from local small models as steering signals for cloud-based LLMs.\\nSpecifically, we formulate token-level optimization as an online learning\\nproblem, where local delta vectors dynamically adjust the remote LLM's logits\\nwithin the on-device environment. This approach preserves privacy by\\ntransmitting only the final steered tokens rather than raw data or intermediate\\nvectors, while maintaining cloud-based LLMs' general capabilities without\\nfine-tuning. Through comprehensive experiments on various personalized\\ngeneration tasks, we demonstrate that CoSteer effectively assists LLMs in\\ngenerating personalized content by leveraging locally stored user profiles and\\nhistories, ensuring privacy preservation through on-device data processing\\nwhile maintaining acceptable computational overhead.\", 'Large Language Models (LLMs) have revolutionized various fields with their\\nexceptional capabilities in understanding, processing, and generating\\nhuman-like text. This paper investigates the potential of LLMs in advancing\\nNetwork Intrusion Detection Systems (NIDS), analyzing current challenges,\\nmethodologies, and future opportunities. It begins by establishing a\\nfoundational understanding of NIDS and LLMs, exploring the enabling\\ntechnologies that bridge the gap between intelligent and cognitive systems in\\nAI-driven NIDS. While Intelligent NIDS leverage machine learning and deep\\nlearning to detect threats based on learned patterns, they often lack\\ncontextual awareness and explainability. In contrast, Cognitive NIDS integrate\\nLLMs to process both structured and unstructured security data, enabling deeper\\ncontextual reasoning, explainable decision-making, and automated response for\\nintrusion behaviors. Practical implementations are then detailed, highlighting\\nLLMs as processors, detectors, and explainers within a comprehensive AI-driven\\nNIDS pipeline. Furthermore, the concept of an LLM-centered Controller is\\nproposed, emphasizing its potential to coordinate intrusion detection\\nworkflows, optimizing tool collaboration and system performance. Finally, this\\npaper identifies critical challenges and opportunities, aiming to foster\\ninnovation in developing reliable, adaptive, and explainable NIDS. By\\npresenting the transformative potential of LLMs, this paper seeks to inspire\\nadvancement in next-generation network security systems.', \"Particle Image Velocimetry (PIV) is fundamental to fluid dynamics, yet deep\\nlearning applications face significant hurdles. A critical gap exists: the lack\\nof comprehensive evaluation of how diverse optical flow models perform\\nspecifically on PIV data, largely due to limitations in available datasets and\\nthe absence of a standardized benchmark. This prevents fair comparison and\\nhinders progress. To address this, our primary contribution is a novel,\\nlarge-scale synthetic PIV benchmark dataset generated from diverse CFD\\nsimulations (JHTDB and Blasius). It features unprecedented variety in particle\\ndensities, flow velocities, and continuous motion, enabling, for the first\\ntime, a standardized and rigorous evaluation of various optical flow and PIV\\nalgorithms. Complementing this, we propose Multi Cost Volume PIV (MCFormer), a\\nnew deep network architecture leveraging multi-frame temporal information and\\nmultiple cost volumes, specifically designed for PIV's sparse nature. Our\\ncomprehensive benchmark evaluation, the first of its kind, reveals significant\\nperformance variations among adapted optical flow models and demonstrates that\\nMCFormer significantly outperforms existing methods, achieving the lowest\\noverall normalized endpoint error (NEPE). This work provides both a\\nfoundational benchmark resource essential for future PIV research and a\\nstate-of-the-art method tailored for PIV challenges. We make our benchmark\\ndataset and code publicly available to foster future research in this area.\", 'Question-answering (QA) interfaces powered by large language models (LLMs)\\npresent a promising direction for improving interactivity with HVAC system\\ninsights, particularly for non-expert users. However, enabling accurate,\\nreal-time, and context-aware interactions with HVAC systems introduces unique\\nchallenges, including the integration of frequently updated sensor data,\\ndomain-specific knowledge grounding, and coherent multi-stage reasoning. In\\nthis paper, we present JARVIS, a two-stage LLM-based QA framework tailored for\\nsensor data-driven HVAC system interaction. JARVIS employs an Expert-LLM to\\ntranslate high-level user queries into structured execution instructions, and\\nan Agent that performs SQL-based data retrieval, statistical processing, and\\nfinal response generation. To address HVAC-specific challenges, JARVIS\\nintegrates (1) an adaptive context injection strategy for efficient HVAC and\\ndeployment-specific information integration, (2) a parameterized SQL builder\\nand executor to improve data access reliability, and (3) a bottom-up planning\\nscheme to ensure consistency across multi-stage response generation. We\\nevaluate JARVIS using real-world data collected from a commercial HVAC system\\nand a ground truth QA dataset curated by HVAC experts to demonstrate its\\neffectiveness in delivering accurate and interpretable responses across diverse\\nqueries. Results show that JARVIS consistently outperforms baseline and\\nablation variants in both automated and user-centered assessments, achieving\\nhigh response quality and accuracy.', 'Large language models (LLMs) excel at complex reasoning when they include\\nintermediate steps, known as \"chains of thought\" (CoTs). However, these\\nrationales are often overly verbose, even for simple problems, leading to\\nwasted context, increased latency, and higher energy consumption. We observe\\nthat verbose, English-heavy CoTs and concise, math-centric CoTs occupy distinct\\nregions in the model\\'s residual-stream activation space. By extracting and\\ninjecting a \"steering vector\" to transition between these modes, we can\\nreliably shift generation toward more concise reasoning, effectively\\ncompressing CoTs without retraining. We formalize this approach as\\nActivation-Steered Compression (ASC), an inference-time technique that shortens\\nreasoning traces by directly modifying hidden representations. In addition, we\\nprovide a theoretical analysis of the impact of ASC on the output distribution,\\nderived from a closed-form KL-divergence-bounded constraint to regulate\\nsteering strength. Using only 100 paired verbose and concise examples, ASC\\nachieves up to 67.43% reduction in CoT length on MATH500 and GSM8K datasets,\\nwhile maintaining accuracy across 7B, 8B, and 32B parameter models. As a\\ntraining-free method, ASC introduces negligible runtime overhead and, on\\nMATH500, delivers an average 2.73x speedup in end-to-end reasoning wall-clock\\ntime on an 8B model. This makes ASC a practical and efficient tool for\\nstreamlining the deployment of reasoning-capable LLMs in latency- or\\ncost-sensitive settings. The code is available at:\\nhttps://github.com/ArminAzizi98/ASC', 'In this paper we study word stress representations learned by self-supervised\\nspeech models (S3M), specifically the Wav2vec 2.0 model. We investigate the S3M\\nrepresentations of word stress for five different languages: Three languages\\nwith variable or lexical stress (Dutch, English and German) and two languages\\nwith fixed or demarcative stress (Hungarian and Polish). We train diagnostic\\nstress classifiers on S3M embeddings and show that they can distinguish between\\nstressed and unstressed syllables in read-aloud short sentences with high\\naccuracy. We also tested language-specificity effects of S3M word stress. The\\nresults indicate that the word stress representations are language-specific,\\nwith a greater difference between the set of variable versus the set of fixed\\nstressed languages.', \"Large Language Models (LLMs) show significant potential for automating\\nRegister-Transfer Level (RTL) code generation. However, current approaches face\\na critical challenge: they can not simultaneously optimize for functional\\ncorrectness and hardware quality (Power, Performance, Area - PPA). Methods\\nbased on supervised fine-tuning often generate functionally correct but\\nPPA-suboptimal code, lacking mechanisms to learn optimization principles. In\\ncontrast, post-processing techniques that attempt to improve PPA metrics after\\ngeneration are often inefficient because they operate externally without\\nupdating the LLM's parameters, thus failing to enhance the model's intrinsic\\ndesign capabilities.\\n  To bridge this gap, we introduce ChipSeek-R1, a hierarchical reward-driven\\nreinforcement learning framework to train LLMs to generate RTL code that\\nachieves both functional correctness and optimized PPA metrics. ChipSeek-R1\\nemploys a hierarchical reward system, which incorporates direct feedback on\\nsyntax, functional correctness (from simulators) and PPA metrics (from\\nsynthesis tools) during reinforcement learning. This enables the model to learn\\ncomplex hardware design trade-offs via trial-and-error, generating RTL code\\nthat is both functionally correct and PPA-optimized. Evaluating ChipSeek-R1 on\\nstandard benchmarks (VerilogEval, RTLLM), we achieve state-of-the-art results\\nin functional correctness. Notably, on the RTLLM benchmark, ChipSeek-R1\\ngenerated 27 RTL designs surpassing the PPA metrics of the original\\nhuman-written code. Our findings demonstrate the effectiveness of integrating\\ntoolchain feedback into LLM training and highlight the potential for\\nreinforcement learning to enable automated generation of human-surpassing RTL\\ncode. We open-source our code in anonymous github.\", 'Text-to-image diffusion models have achieved remarkable success in\\ntranslating textual prompts into high-fidelity images. ControlNets further\\nextend these models by allowing precise, image-based conditioning (e.g., edge\\nmaps, depth, pose), enabling fine-grained control over structure and style.\\nHowever, their dependence on large, publicly scraped datasets -- and the\\nincreasing use of community-shared data for fine-tuning -- exposes them to\\nstealthy data poisoning attacks. In this work, we introduce a novel data\\npoisoning method that manipulates ControlNets to generate images containing\\nspecific content without any text triggers. By injecting poisoned samples --\\neach pairing a subtly triggered input with an NSFW target -- the model retains\\nclean-prompt fidelity yet reliably produces NSFW outputs when the trigger is\\npresent. On large-scale, high-quality datasets, our backdoor achieves high\\nattack success rate while remaining imperceptible in raw inputs. These results\\nreveal a critical vulnerability in open-source ControlNets pipelines and\\nunderscore the need for robust data sanitization and defense mechanisms.', 'Multi-agent systems powered by Large Language Models (LLM-MAS) demonstrate\\nremarkable capabilities in collaborative problem-solving. While LLM-MAS exhibit\\nstrong collaborative abilities, the security risks in their communication and\\ncoordination remain underexplored. We bridge this gap by systematically\\ninvestigating intention-hiding threats in LLM-MAS, and design four\\nrepresentative attack paradigms that subtly disrupt task completion while\\nmaintaining high concealment. These attacks are evaluated in centralized,\\ndecentralized, and layered communication structures. Experiments conducted on\\nsix benchmark datasets, including MMLU, MMLU-Pro, HumanEval, GSM8K, arithmetic,\\nand biographies, demonstrate that they exhibit strong disruptive capabilities.\\nTo identify these threats, we propose a psychology-based detection framework\\nAgentXposed, which combines the HEXACO personality model with the Reid\\nTechnique, using progressive questionnaire inquiries and behavior-based\\nmonitoring. Experiments conducted on six types of attacks show that our\\ndetection framework effectively identifies all types of malicious behaviors.\\nThe detection rate for our intention-hiding attacks is slightly lower than that\\nof the two baselines, Incorrect Fact Injection and Dark Traits Injection,\\ndemonstrating the effectiveness of intention concealment. Our findings reveal\\nthe structural and behavioral risks posed by intention-hiding attacks and offer\\nvaluable insights into securing LLM-based multi-agent systems through\\npsychological perspectives, which contributes to a deeper understanding of\\nmulti-agent safety. The code and data are available at\\nhttps://anonymous.4open.science/r/AgentXposed-F814.', 'Conversational recommender systems (CRSs) often suffer from an extreme\\nlong-tail distribution of dialogue data, causing a strong bias toward\\nhead-frequency blockbusters that sacrifices diversity and exacerbates the\\ncold-start problem. An empirical analysis of DCRS and statistics on the REDIAL\\ncorpus show that only 10% of head movies account for nearly half of all\\nmentions, whereas about 70% of tail movies receive merely 26% of the attention.\\nThis imbalance gives rise to three critical challenges: head over-fitting, body\\nrepresentation drift, and tail sparsity. To address these issues, we propose\\nLumiCRS, an end-to-end framework that mitigates long-tail imbalance through\\nthree mutually reinforcing layers: (i) an Adaptive Comprehensive Focal Loss\\n(ACFL) that dynamically adjusts class weights and focusing factors to curb head\\nover-fitting and reduce popularity bias; (ii) Prototype Learning for Long-Tail\\nRecommendation, which selects semantic, affective, and contextual prototypes to\\nguide clustering and stabilize body and tail representations; and (iii) a\\nGPT-4o-driven prototype-guided dialogue augmentation module that automatically\\ngenerates diverse long-tail conversational snippets to alleviate tail sparsity\\nand distribution shift. Together, these strategies enable LumiCRS to markedly\\nimprove recommendation accuracy, diversity, and fairness: on the REDIAL and\\nINSPIRED benchmarks, LumiCRS boosts Recall@10 and Tail-Recall@10 by 7-15% over\\nfifteen strong baselines, while human evaluations confirm superior fluency,\\ninformativeness, and long-tail relevance. These results demonstrate the\\neffectiveness of multi-layer collaboration in building an efficient and fair\\nlong-tail conversational recommender.', 'This position paper provides a critical but constructive discussion of\\ncurrent practices in benchmarking and evaluative practices in the field of\\nformal reasoning and automated theorem proving. We take the position that open\\ncode, open data, and benchmarks that are complete and error-free will\\naccelerate progress in this field. We identify practices that create barriers\\nto contributing to this field and suggest ways to remove them. We also discuss\\nsome of the practices that might produce misleading evaluative information. We\\naim to create discussions that bring together people from various groups\\ncontributing to automated theorem proving, autoformalization, and informal\\nreasoning.', \"Accurate detection of anatomic landmarks is essential for assessing alveolar\\nbone and root conditions, thereby optimizing clinical outcomes in orthodontics,\\nperiodontics, and implant dentistry. Manual annotation of landmarks on\\ncone-beam computed tomography (CBCT) by dentists is time-consuming,\\nlabor-intensive, and subject to inter-observer variability. Deep learning-based\\nautomated methods present a promising approach to streamline this process\\nefficiently. However, the scarcity of training data and the high cost of expert\\nannotations hinder the adoption of conventional deep learning techniques. To\\novercome these challenges, we introduce GeoSapiens, a novel few-shot learning\\nframework designed for robust dental landmark detection using limited annotated\\nCBCT of anterior teeth. Our GeoSapiens framework comprises two key components:\\n(1) a robust baseline adapted from Sapiens, a foundational model that has\\nachieved state-of-the-art performance in human-centric vision tasks, and (2) a\\nnovel geometric loss function that improves the model's capacity to capture\\ncritical geometric relationships among anatomical structures. Experiments\\nconducted on our collected dataset of anterior teeth landmarks revealed that\\nGeoSapiens surpassed existing landmark detection methods, outperforming the\\nleading approach by an 8.18% higher success detection rate at a strict 0.5 mm\\nthreshold-a standard widely recognized in dental diagnostics. Code is available\\nat: https://github.com/xmed-lab/GeoSapiens.\", 'Urban general intelligence (UGI) refers to the capacity of AI systems to\\nautonomously perceive, reason, and act within dynamic and complex urban\\nenvironments. In this paper, we introduce UrbanMind, a tool-enhanced\\nretrieval-augmented generation (RAG) framework designed to facilitate UGI.\\nCentral to UrbanMind is a novel architecture based on Continual\\nRetrieval-Augmented MoE-based LLM (C-RAG-LLM), which dynamically incorporates\\ndomain-specific knowledge and evolving urban data to support long-term\\nadaptability. The architecture of C-RAG-LLM aligns naturally with a multilevel\\noptimization framework, where different layers are treated as interdependent\\nsub-problems. Each layer has distinct objectives and can be optimized either\\nindependently or jointly through a hierarchical learning process. The framework\\nis highly flexible, supporting both end-to-end training and partial layer-wise\\noptimization based on resource or deployment constraints. To remain adaptive\\nunder data drift, it is further integrated with an incremental corpus updating\\nmechanism. Evaluations on real-world urban tasks of a variety of complexity\\nverify the effectiveness of the proposed framework. This work presents a\\npromising step toward the realization of general-purpose LLM agents in future\\nurban environments.', 'Understanding how cellular morphology, gene expression, and spatial\\norganization jointly shape tissue function is a central challenge in biology.\\nImage-based spatial transcriptomics technologies now provide high-resolution\\nmeasurements of cell images and gene expression profiles, but machine learning\\nmethods typically analyze these modalities in isolation or at limited\\nresolution. We address the problem of learning unified, spatially aware\\nrepresentations that integrate cell morphology, gene expression, and spatial\\ncontext across biological scales. This requires models that can operate at\\nsingle-cell resolution, reason across spatial neighborhoods, and generalize to\\nwhole-slide tissue organization. Here, we introduce SPATIA, a multi-scale\\ngenerative and predictive model for spatial transcriptomics. SPATIA learns\\ncell-level embeddings by fusing image-derived morphological tokens and\\ntranscriptomic vector tokens using cross-attention and then aggregates them at\\nniche and tissue levels using transformer modules to capture spatial\\ndependencies. SPATIA incorporates token merging in its generative diffusion\\ndecoder to synthesize high-resolution cell images conditioned on gene\\nexpression. We assembled a multi-scale dataset consisting of 17 million\\ncell-gene pairs, 1 million niche-gene pairs, and 10,000 tissue-gene pairs\\nacross 49 donors, 17 tissue types, and 12 disease states. We benchmark SPATIA\\nagainst 13 existing models across 12 individual tasks, which span several\\ncategories including cell annotation, cell clustering, gene imputation,\\ncross-modal prediction, and image generation. SPATIA achieves improved\\nperformance over all baselines and generates realistic cell morphologies that\\nreflect transcriptomic perturbations.', \"Temporal Video Grounding (TVG), which requires pinpointing relevant temporal\\nsegments from video based on language query, has always been a highly\\nchallenging task in the field of video understanding. Videos often have a\\nlarger volume of information and redundancy than texts or images. Models should\\npresent comprehensive understanding of the whole video to accurately retrieve\\nquery-relevant clips. We thus propose Tempo-R0: a Video Multimodal Large\\nLanguage Model (Video-MLLM) for the temporal video grounding task via\\nmultimodal temporal sensing reinforcement. Specifically, during the\\npreprocessing stage of our pipeline, we employ Self-adaptive Attention\\nAllocation (SAA) method based on frame content variation to efficiently use the\\nMLLM's limited attention. The Explicit Timestamp-modal Aligned (ETA) method is\\nalso utilized to strengthen our model's capability to perceive the boundaries\\nof events in the video. In the fine-tuning part of our pipeline, we creatively\\napply Partial Irrelevance Refusing-based Group Relative Policy Optimization\\n(PIR-GRPO) in TVG area to foster model's temporal reasoning from not only\\naccepting relevant video-query pairs but also refusing irrelevant ones.\\nExperiments demonstrate that our method accomplishes a notable advantage over\\nSOTA solutions by around 3.5% on both the original QVHighlights testbench and\\nits corrected version with more reasonable ground truth annotations.\", \"Kolmogorov-Arnold Networks (KANs) have garnered attention for replacing fixed\\nactivation functions with learnable univariate functions, but they exhibit\\npractical limitations, including high computational costs and performance\\ndeficits in general classification tasks. In this paper, we propose the\\nModulation Joint KAN (MJKAN), a novel neural network layer designed to overcome\\nthese challenges. MJKAN integrates a FiLM (Feature-wise Linear Modulation)-like\\nmechanism with Radial Basis Function (RBF) activations, creating a hybrid\\narchitecture that combines the non-linear expressive power of KANs with the\\nefficiency of Multilayer Perceptrons (MLPs). We empirically validated MJKAN's\\nperformance across a diverse set of benchmarks, including function regression,\\nimage classification (MNIST, CIFAR-10/100), and natural language processing (AG\\nNews, SMS Spam). The results demonstrate that MJKAN achieves superior\\napproximation capabilities in function regression tasks, significantly\\noutperforming MLPs, with performance improving as the number of basis functions\\nincreases. Conversely, in image and text classification, its performance was\\ncompetitive with MLPs but revealed a critical dependency on the number of basis\\nfunctions. We found that a smaller basis size was crucial for better\\ngeneralization, highlighting that the model's capacity must be carefully tuned\\nto the complexity of the data to prevent overfitting. In conclusion, MJKAN\\noffers a flexible architecture that inherits the theoretical advantages of KANs\\nwhile improving computational efficiency and practical viability.\", 'Large Vision-Language Models (LVLMs) have demonstrated remarkable\\nadvancements in numerous areas such as multimedia. However, hallucination\\nissues significantly limit their credibility and application potential.\\nExisting mitigation methods typically rely on external tools or the comparison\\nof multi-round inference, which significantly increase inference time. In this\\npaper, we propose \\\\textbf{SE}lf-\\\\textbf{E}volving \\\\textbf{D}istillation\\n(\\\\textbf{SEED}), which identifies hallucinations within the inner knowledge of\\nLVLMs, isolates and purges them, and then distills the purified knowledge back\\ninto the model, enabling self-evolution. Furthermore, we identified that\\ntraditional distillation methods are prone to inducing void spaces in the\\noutput space of LVLMs. To address this issue, we propose a Mode-Seeking\\nEvolving approach, which performs distillation to capture the dominant modes of\\nthe purified knowledge distribution, thereby avoiding the chaotic results that\\ncould emerge from void spaces. Moreover, we introduce a Hallucination\\nElimination Adapter, which corrects the dark knowledge of the original model by\\nlearning purified knowledge. Extensive experiments on multiple benchmarks\\nvalidate the superiority of our SEED, demonstrating substantial improvements in\\nmitigating hallucinations for representative LVLM models such as LLaVA-1.5 and\\nInternVL2. Remarkably, the F1 score of LLaVA-1.5 on the hallucination\\nevaluation metric POPE-Random improved from 81.3 to 88.3.', 'The rise of conversational interfaces has greatly enhanced LLM usability by\\nleveraging dialogue history for sophisticated reasoning. However, this reliance\\nintroduces an unexplored attack surface. This paper introduces Trojan Horse\\nPrompting, a novel jailbreak technique. Adversaries bypass safety mechanisms by\\nforging the model\\'s own past utterances within the conversational history\\nprovided to its API. A malicious payload is injected into a model-attributed\\nmessage, followed by a benign user prompt to trigger harmful content\\ngeneration. This vulnerability stems from Asymmetric Safety Alignment: models\\nare extensively trained to refuse harmful user requests but lack comparable\\nskepticism towards their own purported conversational history. This implicit\\ntrust in its \"past\" creates a high-impact vulnerability. Experimental\\nvalidation on Google\\'s Gemini-2.0-flash-preview-image-generation shows Trojan\\nHorse Prompting achieves a significantly higher Attack Success Rate (ASR) than\\nestablished user-turn jailbreaking methods. These findings reveal a fundamental\\nflaw in modern conversational AI security, necessitating a paradigm shift from\\ninput-level filtering to robust, protocol-level validation of conversational\\ncontext integrity.', 'Audio-Visual Localization (AVL) aims to identify sound-emitting sources\\nwithin a visual scene. However, existing studies focus on image-level\\naudio-visual associations, failing to capture temporal dynamics. Moreover, they\\nassume simplified scenarios where sound sources are always visible and involve\\nonly a single object. To address these limitations, we propose AVATAR, a\\nvideo-centric AVL benchmark that incorporates high-resolution temporal\\ninformation. AVATAR introduces four distinct scenarios -- Single-sound,\\nMixed-sound, Multi-entity, and Off-screen -- enabling a more comprehensive\\nevaluation of AVL models. Additionally, we present TAVLO, a novel video-centric\\nAVL model that explicitly integrates temporal information. Experimental results\\nshow that conventional methods struggle to track temporal variations due to\\ntheir reliance on global audio features and frame-level mappings. In contrast,\\nTAVLO achieves robust and precise audio-visual alignment by leveraging\\nhigh-resolution temporal modeling. Our work empirically demonstrates the\\nimportance of temporal dynamics in AVL and establishes a new standard for\\nvideo-centric audio-visual localization.', 'It has been challenging to model the complex temporal-spatial dependencies\\nbetween agents for trajectory prediction. As each state of an agent is closely\\nrelated to the states of adjacent time steps, capturing the local temporal\\ndependency is beneficial for prediction, while most studies often overlook it.\\nBesides, learning the high-order motion state attributes is expected to enhance\\nspatial interaction modeling, but it is rarely seen in previous works. To\\naddress this, we propose a lightweight framework, LTMSformer, to extract\\ntemporal-spatial interaction features for multi-modal trajectory prediction.\\nSpecifically, we introduce a Local Trend-Aware Attention mechanism to capture\\nthe local temporal dependency by leveraging a convolutional attention mechanism\\nwith hierarchical local time boxes. Next, to model the spatial interaction\\ndependency, we build a Motion State Encoder to incorporate high-order motion\\nstate attributes, such as acceleration, jerk, heading, etc. To further refine\\nthe trajectory prediction, we propose a Lightweight Proposal Refinement Module\\nthat leverages Multi-Layer Perceptrons for trajectory embedding and generates\\nthe refined trajectories with fewer model parameters. Experiment results on the\\nArgoverse 1 dataset demonstrate that our method outperforms the baseline\\nHiVT-64, reducing the minADE by approximately 4.35%, the minFDE by 8.74%, and\\nthe MR by 20%. We also achieve higher accuracy than HiVT-128 with a 68%\\nreduction in model size.', \"Recent advances have witnessed the effectiveness of reinforcement learning\\n(RL) finetuning in enhancing the reasoning capabilities of large language\\nmodels (LLMs). The optimization process often requires numerous iterations to\\nachieve satisfactory performance, resulting in high computational costs due to\\nthe need for frequent prompt evaluations under intensive LLM interactions and\\nrepeated policy updates. Appropriate online prompt selection methods reduce\\niteration steps by prioritizing informative prompts during training, while the\\npipeline's reliance on exhaustive prompt evaluation and subset selection for\\noptimization still incurs substantial computational overhead due to frequent\\nLLM inference calls. Distinguished from these direct evaluate-then-select\\nschemes, this work investigates iterative approximate evaluation for arbitrary\\nprompts and introduces Model Predictive Prompt Selection (MoPPS), a Bayesian\\nrisk-predictive framework that online estimates prompt difficulty without\\nrequiring costly LLM interactions. Technically, MoPPS models each prompt's\\nsuccess rate as a latent variable, performs streaming Bayesian inference, and\\nemploys posterior sampling in a constructed multi-armed bandit machine,\\nenabling sample efficient and adaptive prompt selection. Extensive experiments\\nacross mathematics, planning, and vision-based geometry tasks show that MoPPS\\nreliably predicts prompt difficulty and accelerates training with significantly\\nreduced LLM rollouts.\", \"Recently, learning-based stereo matching networks have advanced\\nsignificantly. However, they often lack robustness and struggle to achieve\\nimpressive cross-domain performance due to domain shifts and imbalanced\\ndisparity distributions among diverse datasets. Leveraging Vision Foundation\\nModels (VFMs) can intuitively enhance the model's robustness, but integrating\\nsuch a model into stereo matching cost-effectively to fully realize their\\nrobustness remains a key challenge. To address this, we propose SMoEStereo, a\\nnovel framework that adapts VFMs for stereo matching through a tailored,\\nscene-specific fusion of Low-Rank Adaptation (LoRA) and Mixture-of-Experts\\n(MoE) modules. SMoEStereo introduces MoE-LoRA with adaptive ranks and\\nMoE-Adapter with adaptive kernel sizes. The former dynamically selects optimal\\nexperts within MoE to adapt varying scenes across domains, while the latter\\ninjects inductive bias into frozen VFMs to improve geometric feature\\nextraction. Importantly, to mitigate computational overhead, we further propose\\na lightweight decision network that selectively activates MoE modules based on\\ninput complexity, balancing efficiency with accuracy. Extensive experiments\\ndemonstrate that our method exhibits state-of-the-art cross-domain and joint\\ngeneralization across multiple benchmarks without dataset-specific adaptation.\\nThe code is available at\\n\\\\textcolor{red}{https://github.com/cocowy1/SMoE-Stereo}.\", 'Large Language Models (LLMs) are powerful yet prone to generating factual\\nerrors, commonly referred to as hallucinations. We present a lightweight,\\ninterpretable framework for knowledge-aware self-correction of LLM outputs\\nusing structured memory graphs based on RDF triples. Without retraining or\\nfine-tuning, our method post-processes model outputs and corrects factual\\ninconsistencies via external semantic memory. We demonstrate the approach using\\nDistilGPT-2 and show promising results on simple factual prompts.', \"Session-based Recommendation (SBR) aims to predict the next item a user will\\nlikely engage with, using their interaction sequence within an anonymous\\nsession. Existing SBR models often focus only on single-session information,\\nignoring inter-session relationships and valuable cross-session insights. Some\\nmethods try to include inter-session data but struggle with noise and\\nirrelevant information, reducing performance. Additionally, most models rely on\\nitem ID co-occurrence and overlook rich semantic details, limiting their\\nability to capture fine-grained item features. To address these challenges, we\\npropose a novel hierarchical intent-guided optimization approach with pluggable\\nLLM-driven semantic learning for session-based recommendations, called HIPHOP.\\nFirst, we introduce a pluggable embedding module based on large language models\\n(LLMs) to generate high-quality semantic representations, enhancing item\\nembeddings. Second, HIPHOP utilizes graph neural networks (GNNs) to model item\\ntransition relationships and incorporates a dynamic multi-intent capturing\\nmodule to address users' diverse interests within a session. Additionally, we\\ndesign a hierarchical inter-session similarity learning module, guided by user\\nintent, to capture global and local session relationships, effectively\\nexploring users' long-term and short-term interests. To mitigate noise, an\\nintent-guided denoising strategy is applied during inter-session learning.\\nFinally, we enhance the model's discriminative capability by using contrastive\\nlearning to optimize session representations. Experiments on multiple datasets\\nshow that HIPHOP significantly outperforms existing methods, demonstrating its\\neffectiveness in improving recommendation quality. Our code is available:\\nhttps://github.com/hjx159/HIPHOP.\", '6G networks promise revolutionary immersive communication experiences\\nincluding augmented reality (AR), virtual reality (VR), and holographic\\ncommunications. These applications demand high-dimensional multimodal data\\ntransmission and intelligent data processing in real-time, which is extremely\\nchallenging over resource-limited wireless communication systems. Moreover, a\\njoint understanding of the environment, context, and user intent is essential\\nto deliver task-relevant content effectively. This article presents a novel\\nmultimodal large language model (MLLM) integrated semantic communications\\nframework, termed MLLM-SC, which fully leverages reasoning and generative\\ncapabilities of pre-trained foundation models for context-aware and\\ntask-oriented wireless communication. The MLLM-SC framework adopts a\\ndevice-edge collaborative architecture. At the edge, MLLM-empowered semantic\\nguidance module analyzes multimodal inputs, user intents, and channel\\nconditions to generate importance-aware attention maps prioritizing\\nsemantically critical information. An importance-aware semantic encoder and a\\nresource-adaptive semantic decoder are jointly designed and optimized, which\\ncan utilize the semantic guidance for adaptive bandwidth allocation and\\nhigh-quality content reconstruction or generation. Extensive case studies on\\nvisual question answering for AR/VR applications and diffusion-driven image\\ngeneration validate the effectiveness of MLLM-SC.', 'Dataset distillation aims to create a compact dataset that retains essential\\ninformation while maintaining model performance. Diffusion models (DMs) have\\nshown promise for this task but struggle in low images-per-class (IPC)\\nsettings, where generated samples lack diversity. In this paper, we address\\nthis issue from an information-theoretic perspective by identifying two key\\ntypes of information that a distilled dataset must preserve: ($i$) prototype\\ninformation $\\\\mathrm{I}(X;Y)$, which captures label-relevant features; and\\n($ii$) contextual information $\\\\mathrm{H}(X | Y)$, which preserves intra-class\\nvariability. Here, $(X,Y)$ represents the pair of random variables\\ncorresponding to the input data and its ground truth label, respectively.\\nObserving that the required contextual information scales with IPC, we propose\\nmaximizing $\\\\mathrm{I}(X;Y) + \\\\beta \\\\mathrm{H}(X | Y)$ during the DM sampling\\nprocess, where $\\\\beta$ is IPC-dependent. Since directly computing\\n$\\\\mathrm{I}(X;Y)$ and $\\\\mathrm{H}(X | Y)$ is intractable, we develop\\nvariational estimations to tightly lower-bound these quantities via a\\ndata-driven approach. Our approach, information-guided diffusion sampling\\n(IGDS), seamlessly integrates with diffusion models and improves dataset\\ndistillation across all IPC settings. Experiments on Tiny ImageNet and ImageNet\\nsubsets show that IGDS significantly outperforms existing methods, particularly\\nin low-IPC regimes. The code will be released upon acceptance.', 'Survival prediction using whole-slide images (WSIs) is crucial in cancer\\nre-search. Despite notable success, existing approaches are limited by their\\nreliance on sparse slide-level labels, which hinders the learning of\\ndiscriminative repre-sentations from gigapixel WSIs. Recently, vision language\\n(VL) models, which incorporate additional language supervision, have emerged as\\na promising solu-tion. However, VL-based survival prediction remains largely\\nunexplored due to two key challenges. First, current methods often rely on only\\none simple lan-guage prompt and basic cosine similarity, which fails to learn\\nfine-grained associ-ations between multi-faceted linguistic information and\\nvisual features within WSI, resulting in inadequate vision-language alignment.\\nSecond, these methods primarily exploit patch-level information, overlooking\\nthe intrinsic hierarchy of WSIs and their interactions, causing ineffective\\nmodeling of hierarchical interac-tions. To tackle these problems, we propose a\\nnovel Hierarchical vision-Language collaboration (HiLa) framework for improved\\nsurvival prediction. Specifically, HiLa employs pretrained feature extractors\\nto generate hierarchical visual features from WSIs at both patch and region\\nlevels. At each level, a series of language prompts describing various\\nsurvival-related attributes are constructed and aligned with visual features\\nvia Optimal Prompt Learning (OPL). This ap-proach enables the comprehensive\\nlearning of discriminative visual features cor-responding to different\\nsurvival-related attributes from prompts, thereby improv-ing vision-language\\nalignment. Furthermore, we introduce two modules, i.e., Cross-Level Propagation\\n(CLP) and Mutual Contrastive Learning (MCL) to maximize hierarchical\\ncooperation by promoting interactions and consistency be-tween patch and region\\nlevels. Experiments on three TCGA datasets demonstrate our SOTA performance.', 'We present any4, a learned 4-bit weight quantization solution for large\\nlanguage models (LLMs) providing arbitrary numeric representations without\\nrequiring pre-processing of weights or activations. any4 yields higher accuracy\\ncompared to other related 4-bit numeric representation types: int4, fp4 and\\nnf4, as evaluated on a range of model sizes, generations and families (Llama 2,\\nLlama 3, Mistral and Mixtral). While any4 does not require preprocessing of\\nweights or activations, it is also competitive with orthogonal techniques that\\nrequire such preprocessing (e.g., AWQ and GPTQ). We also experiment with any3\\nand any2 and show competitiveness at lower bits. Additionally, we show that we\\ncan calibrate using a single curated diverse sample rather than hundreds of\\nsamples from a dataset as done in most quantization approaches. We also open\\nsource tinygemm, a latency optimized GPU matrix multiplication library for\\nLLMs, that implements any4 using a GPU-efficient lookup table strategy along\\nwith other common quantization methods. We open source our code at\\nhttps://github.com/facebookresearch/any4 .', \"Large language model (LLM) personalization aims to align model outputs with\\nindividuals' unique preferences and opinions. While recent efforts have\\nimplemented various personalization methods, a unified theoretical framework\\nthat can systematically understand the drivers of effective personalization is\\nstill lacking. In this work, we integrate the well-established cognitive\\ndual-memory model into LLM personalization, by mirroring episodic memory to\\nhistorical user engagements and semantic memory to long-term, evolving user\\nbeliefs. Specifically, we systematically investigate memory instantiations and\\nintroduce a unified framework, PRIME, using episodic and semantic memory\\nmechanisms. We further augment PRIME with a novel personalized thinking\\ncapability inspired by the slow thinking strategy. Moreover, recognizing the\\nabsence of suitable benchmarks, we introduce a dataset using Change My View\\n(CMV) from Reddit, specifically designed to evaluate long-context\\npersonalization. Extensive experiments validate PRIME's effectiveness across\\nboth long- and short-context scenarios. Further analysis confirms that PRIME\\neffectively captures dynamic personalization beyond mere popularity biases.\", 'A long-standing problem in online reinforcement learning (RL) is of ensuring\\nsample efficiency, which stems from an inability to explore environments\\nefficiently. Most attempts at efficient exploration tackle this problem in a\\nsetting where learning begins from scratch, without prior information available\\nto bootstrap learning. However, such approaches fail to leverage expert\\ndemonstrations and simulators that can reset to arbitrary states. These\\naffordances are valuable resources that offer enormous potential to guide\\nexploration and speed up learning. In this paper, we explore how a small number\\nof expert demonstrations and a simulator allowing arbitrary resets can\\naccelerate learning during online RL. We find that training with a suitable\\nchoice of an auxiliary start state distribution that may differ from the true\\nstart state distribution of the underlying Markov Decision Process can\\nsignificantly improve sample efficiency. We find that using a notion of safety\\nto inform the choice of this auxiliary distribution significantly accelerates\\nlearning. By using episode length information as a way to operationalize this\\nnotion, we demonstrate state-of-the-art sample efficiency on a sparse-reward\\nhard-exploration environment.', 'Real-world time series typically exhibit complex temporal variations, making\\nthe time series classification task notably challenging. Recent advancements\\nhave demonstrated the potential of multi-scale analysis approaches, which\\nprovide an effective solution for capturing these complex temporal patterns.\\nHowever, existing multi-scale analysis-based time series prediction methods\\nfail to eliminate redundant scale-shared features across multi-scale time\\nseries, resulting in the model over- or under-focusing on scale-shared\\nfeatures. To address this issue, we propose a novel end-to-end Disentangled\\nMulti-Scale framework for Time Series classification (DisMS-TS). The core idea\\nof DisMS-TS is to eliminate redundant shared features in multi-scale time\\nseries, thereby improving prediction performance. Specifically, we propose a\\ntemporal disentanglement module to capture scale-shared and scale-specific\\ntemporal representations, respectively. Subsequently, to effectively learn both\\nscale-shared and scale-specific temporal representations, we introduce two\\nregularization terms that ensure the consistency of scale-shared\\nrepresentations and the disparity of scale-specific representations across all\\ntemporal scales. Extensive experiments conducted on multiple datasets validate\\nthe superiority of DisMS-TS over its competitive baselines, with the accuracy\\nimprovement up to 9.71%.', 'Engineering methodologies predominantly revolve around established principles\\nof decomposition and recomposition. These principles involve partitioning\\ninputs and outputs at the component level, ensuring that the properties of\\nindividual components are preserved upon composition. However, this view does\\nnot transfer well to intelligent systems, particularly when addressing the\\nscaling of intelligence as a system property. Our prior research contends that\\nthe engineering of general intelligence necessitates a fresh set of overarching\\nsystems principles. As a result, we introduced the \"core and periphery\"\\nprinciples, a novel conceptual framework rooted in abstract systems theory and\\nthe Law of Requisite Variety. In this paper, we assert that these abstract\\nconcepts hold practical significance. Through empirical evidence, we illustrate\\ntheir applicability to both biological and artificial intelligence systems,\\nbridging abstract theory with real-world implementations. Then, we expand on\\nour previous theoretical framework by mathematically defining core-dominant vs\\nperiphery-dominant systems.', 'Current paradigms in Artificial Intelligence rely on layers of feedforward\\nnetworks which model brain activity at the neuronal level. We conjecture that\\nexpanding to the level of multiple brain regions with chemical signaling may be\\na productive step toward understanding the emergence of consciousness. We\\npropose LILITH, a novel architecture that combines developmental training of\\nmodular language models with brain-inspired token-based communication\\nprotocols, mirroring chemical signaling in the brain. Our approach models\\ndistinct brain regions as specialized LLM modules including thinking, memory,\\nsensory, and regulatory components that communicate through emergent\\ntoken-based signaling protocols analogous to neurotransmitter networks. Unlike\\ntraditional pre-trained systems, LILITH would employ developmental training\\nwhere untrained LLM architectures learn through simulated life experiences,\\ndeveloping communication pathways and cognitive abilities through environmental\\ninteraction and evolutionary optimization. This framework would enable direct\\nempirical investigation of consciousness emergence using Integrated Information\\nTheory metrics while providing unprecedented insight into inter-module\\nsignaling patterns during development. By optimizing for consciousness\\nemergence rather than task performance, LILITH could provide insight into\\ndifferent emergent phenomena at multiple levels of neural correlates,\\ncontrasting neuronal-level processing with multi-region coordination dynamics.\\nThe goal of this paper is to put the idea forward while recognizing the\\nsubstantial challenges in implementing such a system.', 'We introduce Nile-Chat-4B, 3x4B-A6B, and 12B, a collection of LLMs for\\nEgyptian dialect, uniquely designed to understand and generate texts written in\\nboth Arabic and Latin scripts. Specifically, with Nile-Chat-3x4B-A6B, we\\nintroduce a novel language adaptation approach by leveraging the\\nBranch-Train-MiX strategy to merge script-specialized experts, into a single\\nMoE model. Our Nile-Chat models significantly outperform leading multilingual\\nand Arabic LLMs, such as LLaMa, Jais, and ALLaM, on our newly introduced\\nEgyptian evaluation benchmarks, which span both understanding and generative\\ntasks. Notably, our 12B model yields a 14.4% performance gain over\\nQwen2.5-14B-Instruct on Latin-script benchmarks. All our resources are publicly\\navailable. We believe this work presents a comprehensive methodology for\\nadapting LLMs to dual-script languages, addressing an often overlooked aspect\\nin modern LLM development.', 'Large language models (LLMs) have demonstrated remarkable capabilities across\\ndiverse tasks, but their ability to forecast future events remains\\nunderstudied. A year ago, large language models struggle to come close to the\\naccuracy of a human crowd. I evaluate state-of-the-art LLMs on 464 forecasting\\nquestions from Metaculus, comparing their performance against human\\nsuperforecasters. Frontier models achieve Brier scores that ostensibly surpass\\nthe human crowd but still significantly underperform a group of\\nsuperforecasters.', 'Respiratory insufficiency is a medic symptom in which a person gets a reduced\\namount of oxygen in the blood. This paper reports the experience of building\\nSPIRA: an intelligent system for detecting respiratory insufficiency from\\nvoice. It compiles challenges faced in two succeeding implementations of the\\nsame architecture, summarizing lessons learned on data collection, training,\\nand inference for future projects in similar systems.', \"Large language models (LLMs) can leak sensitive information from their\\ncontext through generated outputs, either accidentally or when prompted\\nadversarially. Existing defenses that aim to preserve context privacy during\\ninference either lack formal guarantees or suffer from a poor utility/privacy\\ntrade-off. We propose DP-Fusion, a token-level Differentially Private Inference\\n(DPI) mechanism that provably bounds how much an LLM's outputs reveal about\\nsensitive tokens in its context. We demonstrate DPI through the task of\\ndocument privatization, where the goal is to paraphrase documents so that\\nsensitive content (e.g., Personally Identifiable Information, PII) cannot be\\nreliably inferred, while still preserving the overall utility of the text. This\\nis controlled by a parameter $\\\\epsilon$: $\\\\epsilon=0$ hides PII entirely, while\\nhigher values trade off privacy for improved paraphrase quality. DP-Fusion\\nworks as follows: (i) partition sensitive tokens into disjoint privacy groups,\\n(ii) run the LLM once per group, and (iii) blend the output distributions so\\nthat the final output remains within a fixed statistical distance of the\\nbaseline distribution produced when no privacy group is revealed. This approach\\nallows fine-grained control over the privacy/utility trade-off but requires\\nmultiple LLM forward passes.\", 'Explainable Artificial Intelligence (XAI) is a crucial pathway in mitigating\\nthe risk of non-transparency in the decision-making process of black-box\\nArtificial Intelligence (AI) systems. However, despite the benefits, XAI\\nmethods are found to leak the privacy of individuals whose data is used in\\ntraining or querying the models. Researchers have demonstrated privacy attacks\\nthat exploit explanations to infer sensitive personal information of\\nindividuals. Currently there is a lack of defenses against known privacy\\nattacks targeting explanations when vulnerable XAI are used in production and\\nmachine learning as a service system. To address this gap, in this article, we\\nexplore Privacy Enhancing Technologies (PETs) as a defense mechanism against\\nattribute inference on explanations provided by feature-based XAI methods. We\\nempirically evaluate 3 types of PETs, namely synthetic training data,\\ndifferentially private training and noise addition, on two categories of\\nfeature-based XAI. Our evaluation determines different responses from the\\nmitigation methods and side-effects of PETs on other system properties such as\\nutility and performance. In the best case, PETs integration in explanations\\nreduced the risk of the attack by 49.47%, while maintaining model utility and\\nexplanation quality. Through our evaluation, we identify strategies for using\\nPETs in XAI for maximizing benefits and minimizing the success of this privacy\\nattack on sensitive personal information.', 'Human motion generation has advanced rapidly in recent years, yet the\\ncritical problem of creating spatially grounded, context-aware gestures has\\nbeen largely overlooked. Existing models typically specialize either in\\ndescriptive motion generation, such as locomotion and object interaction, or in\\nisolated co-speech gesture synthesis aligned with utterance semantics. However,\\nboth lines of work often treat motion and environmental grounding separately,\\nlimiting advances toward embodied, communicative agents. To address this gap,\\nour work introduces a multimodal dataset and framework for grounded gesture\\ngeneration, combining two key resources: (1) a synthetic dataset of spatially\\ngrounded referential gestures, and (2) MM-Conv, a VR-based dataset capturing\\ntwo-party dialogues. Together, they provide over 7.7 hours of synchronized\\nmotion, speech, and 3D scene information, standardized in the HumanML3D format.\\nOur framework further connects to a physics-based simulator, enabling synthetic\\ndata generation and situated evaluation. By bridging gesture modeling and\\nspatial grounding, our contribution establishes a foundation for advancing\\nresearch in situated gesture generation and grounded multimodal interaction.\\n  Project page: https://groundedgestures.github.io/', 'We study a sequential decision-making problem motivated by recent regulatory\\nand technological shifts that limit access to individual user data in\\nrecommender systems (RSs), leaving only population-level preference\\ninformation. This privacy-aware setting poses fundamental challenges in\\nplanning under uncertainty: Effective personalization requires exploration to\\ninfer user preferences, yet unsatisfactory recommendations risk immediate user\\nchurn. To address this, we introduce the Rec-APC model, in which an anonymous\\nuser is drawn from a known prior over latent user types (e.g., personas or\\nclusters), and the decision-maker sequentially selects items to recommend.\\nFeedback is binary -- positive responses refine the posterior via Bayesian\\nupdates, while negative responses result in the termination of the session.\\n  We prove that optimal policies converge to pure exploitation in finite time\\nand propose a branch-and-bound algorithm to efficiently compute them.\\nExperiments on synthetic and MovieLens data confirm rapid convergence and\\ndemonstrate that our method outperforms the POMDP solver SARSOP, particularly\\nwhen the number of user types is large or comparable to the number of content\\ncategories. Our results highlight the applicability of this approach and\\ninspire new ways to improve decision-making under the constraints imposed by\\naggregated preference data.', \"Camera relocalization, a cornerstone capability of modern computer vision,\\naccurately determines a camera's position and orientation (6-DoF) from images\\nand is essential for applications in augmented reality (AR), mixed reality\\n(MR), autonomous driving, delivery drones, and robotic navigation. Unlike\\ntraditional deep learning-based methods that regress camera pose from images in\\na single scene, which often lack generalization and robustness in diverse\\nenvironments, we propose MVL-Loc, a novel end-to-end multi-scene 6-DoF camera\\nrelocalization framework. MVL-Loc leverages pretrained world knowledge from\\nvision-language models (VLMs) and incorporates multimodal data to generalize\\nacross both indoor and outdoor settings. Furthermore, natural language is\\nemployed as a directive tool to guide the multi-scene learning process,\\nfacilitating semantic understanding of complex scenes and capturing spatial\\nrelationships among objects. Extensive experiments on the 7Scenes and Cambridge\\nLandmarks datasets demonstrate MVL-Loc's robustness and state-of-the-art\\nperformance in real-world multi-scene camera relocalization, with improved\\naccuracy in both positional and orientational estimates.\", \"Current AI systems achieve impressive performance on many tasks, yet they\\nlack core attributes of biological intelligence, including rapid, continual\\nlearning, representations grounded in sensorimotor interactions, and structured\\nknowledge that enables efficient generalization. Neuroscience theory suggests\\nthat mammals evolved flexible intelligence through the replication of a\\nsemi-independent, sensorimotor module, a functional unit known as a cortical\\ncolumn. To address the disparity between biological and artificial\\nintelligence, thousand-brains systems were proposed as a means of mirroring the\\narchitecture of cortical columns and their interactions.\\n  In the current work, we evaluate the unique properties of Monty, the first\\nimplementation of a thousand-brains system. We focus on 3D object perception,\\nand in particular, the combined task of object recognition and pose estimation.\\nUtilizing the YCB dataset of household objects, we first assess Monty's use of\\nsensorimotor learning to build structured representations, finding that these\\nenable robust generalization. These representations include an emphasis on\\nclassifying objects by their global shape, as well as a natural ability to\\ndetect object symmetries. We then explore Monty's use of model-free and\\nmodel-based policies to enable rapid inference by supporting principled\\nmovements. We find that such policies complement Monty's modular architecture,\\na design that can accommodate communication between modules to further\\naccelerate inference speed via a novel `voting' algorithm. Finally, we examine\\nMonty's use of associative, Hebbian-like binding to enable rapid, continual,\\nand computationally efficient learning, properties that compare favorably to\\ncurrent deep learning architectures. While Monty is still in a nascent stage of\\ndevelopment, these findings support thousand-brains systems as a powerful and\\npromising new approach to AI.\", 'Large language models (LLMs) are rapidly being integrated into psychological\\nresearch as research tools, evaluation targets, human simulators, and cognitive\\nmodels. However, recent evidence reveals severe measurement unreliability:\\nPersonality assessments collapse under factor analysis, moral preferences\\nreverse with punctuation changes, and theory-of-mind accuracy varies widely\\nwith trivial rephrasing. These \"measurement phantoms\"--statistical artifacts\\nmasquerading as psychological phenomena--threaten the validity of a growing\\nbody of research. Guided by the dual-validity framework that integrates\\npsychometrics with causal inference, we present a six-stage workflow that\\nscales validity requirements to research ambition--using LLMs to code text\\nrequires basic reliability and accuracy, while claims about psychological\\nproperties demand comprehensive construct validation. Researchers must (1)\\nexplicitly define their research goal and corresponding validity requirements,\\n(2) develop and validate computational instruments through psychometric\\ntesting, (3) design experiments that control for computational confounds, (4)\\nexecute protocols with transparency, (5) analyze data using methods appropriate\\nfor non-independent observations, and (6) report findings within demonstrated\\nboundaries and use results to refine theory. We illustrate the workflow through\\nan example of model evaluation--\"LLM selfhood\"--showing how systematic\\nvalidation can distinguish genuine computational phenomena from measurement\\nartifacts. By establishing validated computational instruments and transparent\\npractices, this workflow provides a path toward building a robust empirical\\nfoundation for AI psychology research.', 'Contextual anomaly detection (CAD) aims to identify anomalies in a target\\n(behavioral) variable conditioned on a set of contextual variables that\\ninfluence the normalcy of the target variable but are not themselves indicators\\nof anomaly. In many anomaly detection tasks, there exist contextual variables\\nthat influence the normalcy of the target variable but are not themselves\\nindicators of anomaly. In this work, we propose a novel framework for CAD,\\nnormalcy score (NS), that explicitly models both the aleatoric and epistemic\\nuncertainties. Built on heteroscedastic Gaussian process regression, our method\\nregards the Z-score as a random variable, providing confidence intervals that\\nreflect the reliability of the anomaly assessment. Through experiments on\\nbenchmark datasets and a real-world application in cardiology, we demonstrate\\nthat NS outperforms state-of-the-art CAD methods in both detection accuracy and\\ninterpretability. Moreover, confidence intervals enable an adaptive,\\nuncertainty-driven decision-making process, which may be very important in\\ndomains such as healthcare.', 'Parameter-Efficient Fine-Tuning (PEFT) methods, such as LoRA, significantly\\nreduce the number of trainable parameters by introducing low-rank decomposition\\nmatrices. However, existing methods perform extensive matrix multiplications in\\ndomain specialization tasks, resulting in computational inefficiency and\\nsub-optimal fine-tuning performance. Hence, we propose LoSiA(Low-Resources\\nSubnet Integration Adaptation), an innovative method that dynamically localizes\\nand optimizes critical parameters during the training process. Specifically, it\\nidentifies a sub-network using gradient sparsity analysis and optimizes it as\\nthe trainable target. This design enables effective high-rank adaptation by\\nupdating only the sub-network parameters, reducing the additional matrix\\nmultiplication. We also present LoSiA-Pro, a faster implementation of LoSiA,\\nwhich reduces the training latency by about $27\\\\%$ compared to LoRA. Extensive\\nevaluations show that our method achieves minimal performance drop compared to\\nfull fine-tuning, while requiring the least training time across domain\\nspecialization and common-sense reasoning tasks. Further analysis shows that\\nLoSiA also reduces forgetting during continued training.', 'While attribution methods, such as Shapley values, are widely used to explain\\nthe importance of features or training data in traditional machine learning,\\ntheir application to Large Language Models (LLMs), particularly within\\nRetrieval-Augmented Generation (RAG) systems, is nascent and challenging. The\\nprimary obstacle is the substantial computational cost, where each utility\\nfunction evaluation involves an expensive LLM call, resulting in direct\\nmonetary and time expenses. This paper investigates the feasibility and\\neffectiveness of adapting Shapley-based attribution to identify influential\\nretrieved documents in RAG. We compare Shapley with more computationally\\ntractable approximations and some existing attribution methods for LLM. Our\\nwork aims to: (1) systematically apply established attribution principles to\\nthe RAG document-level setting; (2) quantify how well SHAP approximations can\\nmirror exact attributions while minimizing costly LLM interactions; and (3)\\nevaluate their practical explainability in identifying critical documents,\\nespecially under complex inter-document relationships such as redundancy,\\ncomplementarity, and synergy. This study seeks to bridge the gap between\\npowerful attribution techniques and the practical constraints of LLM-based RAG\\nsystems, offering insights into achieving reliable and affordable RAG\\nexplainability.', 'Large language models (LLMs) have transformed natural language processing,\\nbut their ability to memorize training data poses significant privacy risks.\\nThis paper investigates model inversion attacks on the Llama 3.2 model, a\\nmultilingual LLM developed by Meta. By querying the model with carefully\\ncrafted prompts, we demonstrate the extraction of personally identifiable\\ninformation (PII) such as passwords, email addresses, and account numbers. Our\\nfindings highlight the vulnerability of even smaller LLMs to privacy attacks\\nand underscore the need for robust defenses. We discuss potential mitigation\\nstrategies, including differential privacy and data sanitization, and call for\\nfurther research into privacy-preserving machine learning techniques.', 'This systematic literature review examines the role of large language models\\n(LLMs) in UI/UX design, synthesizing findings from 38 peer-reviewed studies\\npublished between 2022 and 2025. We identify key LLMs in use, including GPT-4,\\nGemini, and PaLM, and map their integration across the design lifecycle, from\\nideation to evaluation. Common practices include prompt engineering,\\nhuman-in-the-loop workflows, and multimodal input. While LLMs are reshaping\\ndesign processes, challenges such as hallucination, prompt instability, and\\nlimited explainability persist. Our findings highlight LLMs as emerging\\ncollaborators in design, and we propose directions for the ethical, inclusive,\\nand effective integration of these technologies.', 'Anomaly detection plays a critical role in Autonomous Vehicles (AVs) by\\nidentifying unusual behaviors through perception systems that could compromise\\nsafety and lead to hazardous situations. Current approaches, which often rely\\non predefined thresholds or supervised learning paradigms, exhibit reduced\\nefficacy when confronted with unseen scenarios, sensor noise, and occlusions,\\nleading to potential safety-critical failures. Moreover, supervised methods\\nrequire large annotated datasets, limiting their real-world feasibility. To\\naddress these gaps, we propose an anomaly detection framework based on Inverse\\nReinforcement Learning (IRL) to infer latent driving intentions from sequential\\nperception data, thus enabling robust identification. Specifically, we present\\nTrajectory-Reward Guided Adaptive Pre-training (TRAP), a novel IRL framework\\nfor anomaly detection, to address two critical limitations of existing methods:\\nnoise robustness and generalization to unseen scenarios. Our core innovation is\\nimplicitly learning temporal credit assignments via reward and worst-case\\nsupervision. We leverage pre-training with variable-horizon sampling to\\nmaximize time-to-consequence, resulting in early detection of behavior\\ndeviation. Experiments on 14,000+ simulated trajectories demonstrate\\nstate-of-the-art performance, achieving 0.90 AUC and 82.2\\\\% F1-score -\\noutperforming similarly trained supervised and unsupervised baselines by 39\\\\%\\non Recall and 12\\\\% on F1-score, respectively. Similar performance is achieved\\nwhile exhibiting robustness to various noise types and generalization to unseen\\nanomaly types. Our code will be available at:\\nhttps://github.com/abastola0/TRAP.git', 'Conformal prediction (CP) is an Uncertainty Representation technique that\\ndelivers finite-sample calibrated prediction regions for any underlying Machine\\nLearning model, yet its status as an Uncertainty Quantification (UQ) tool has\\nremained conceptually opaque. We adopt a category-theoretic approach to CP --\\nframing it as a morphism, embedded in a commuting diagram, of two newly-defined\\ncategories -- that brings us three joys. First, we show that -- under minimal\\nassumptions -- CP is intrinsically a UQ mechanism, that is, its UQ capabilities\\nare a structural feature of the method. Second, we demonstrate that CP bridges\\n(and perhaps subsumes) the Bayesian, frequentist, and imprecise probabilistic\\napproaches to predictive statistical reasoning. Finally, we show that a\\nconformal prediction region (CPR) is the image of a covariant functor. This\\nobservation is relevant to AI privacy: It implies that privacy noise added\\nlocally does not break coverage.', 'The onset of spontaneous thoughts are reflective of dynamic interactions\\nbetween cognition, emotion, and attention. Typically, these experiences are\\nstudied through subjective appraisals that focus on their triggers,\\nphenomenology, and emotional salience. In this work, we use linguistic\\nsignatures to investigate Deja Vu, Involuntary Autobiographical Memories and\\nUnexpected Thoughts. Specifically, we analyze the inherent characteristics of\\nthe linguistic patterns in participant generated descriptions of these thought\\ntypes. We show how, by positioning language as a window into spontaneous\\ncognition, existing theories on these attentional states can be updated and\\nreaffirmed. Our findings align with prior research, reinforcing that Deja Vu is\\na metacognitive experience characterized by abstract and spatial language,\\nInvoluntary Autobiographical Memories are rich in personal and emotionally\\nsignificant detail, and Unexpected Thoughts are marked by unpredictability and\\ncognitive disruption. This work is demonstrative of languages potential to\\nreveal deeper insights into how internal spontaneous cognitive states manifest\\nthrough expression.', 'Medical decision-making is a critical task, where errors can result in\\nserious, potentially life-threatening consequences. While full automation\\nremains challenging, hybrid frameworks that combine machine intelligence with\\nhuman oversight offer a practical alternative. In this paper, we present\\nMedGellan, a lightweight, annotation-free framework that uses a Large Language\\nModel (LLM) to generate clinical guidance from raw medical records, which is\\nthen used by a physician to predict diagnoses. MedGellan uses a\\nBayesian-inspired prompting strategy that respects the temporal order of\\nclinical data. Preliminary experiments show that the guidance generated by the\\nLLM with MedGellan improves diagnostic performance, particularly in recall and\\n$F_1$ score.', \"Medication recommendation is a crucial task in healthcare, especially for\\npatients with complex medical conditions. However, existing methods often\\nstruggle to effectively balance the reuse of historical medications with the\\nintroduction of new drugs in response to the changing patient conditions. In\\norder to address this challenge, we propose an Adaptively Responsive network\\nfor Medication Recommendation (ARMR), a new method which incorporates 1) a\\npiecewise temporal learning component that distinguishes between recent and\\ndistant patient history, enabling more nuanced temporal understanding, and 2)\\nan adaptively responsive mechanism that dynamically adjusts attention to new\\nand existing drugs based on the patient's current health state and medication\\nhistory. Experiments on the MIMIC-III and MIMIC-IV datasets indicate that ARMR\\nhas better performance compared with the state-of-the-art baselines in\\ndifferent evaluation metrics, which contributes to more personalized and\\naccurate medication recommendations. The source code is publicly avaiable at:\\nhttps://github.com/seucoin/armr2.\", 'The recent advancement of artificial intelligence, especially machine\\nlearning (ML), has significantly impacted software engineering research,\\nincluding bug report analysis. ML aims to automate the understanding,\\nextraction, and correlation of information from bug reports. Despite its\\ngrowing importance, there has been no comprehensive review in this area. In\\nthis paper, we present a systematic literature review covering 1,825 papers,\\nselecting 204 for detailed analysis. We derive seven key findings: 1) Extensive\\nuse of CNN, LSTM, and $k$NN for bug report analysis, with advanced models like\\nBERT underutilized due to their complexity. 2) Word2Vec and TF-IDF are popular\\nfor feature representation, with a rise in deep learning approaches. 3) Stop\\nword removal is the most common preprocessing, with structural methods rising\\nafter 2020. 4) Eclipse and Mozilla are the most frequently evaluated software\\nprojects. 5) Bug categorization is the most common task, followed by bug\\nlocalization and severity prediction. 6) There is increasing attention on\\nspecific bugs like non-functional and performance bugs. 7) Common evaluation\\nmetrics are F1-score, Recall, Precision, and Accuracy, with $k$-fold\\ncross-validation preferred for model evaluation. 8) Many studies lack robust\\nstatistical tests. We also identify six promising future research directions to\\nprovide useful insights for practitioners.', 'This paper presents our submission to the ACMMM25 - Grand Challenge on\\nMultimedia Verification. We developed a multi-agent verification system that\\ncombines Multimodal Large Language Models (MLLMs) with specialized verification\\ntools to detect multimedia misinformation. Our system operates through six\\nstages: raw data processing, planning, information extraction, deep research,\\nevidence collection, and report generation. The core Deep Researcher Agent\\nemploys four tools: reverse image search, metadata analysis, fact-checking\\ndatabases, and verified news processing that extracts spatial, temporal,\\nattribution, and motivational context. We demonstrate our approach on a\\nchallenge dataset sample involving complex multimedia content. Our system\\nsuccessfully verified content authenticity, extracted precise geolocation and\\ntiming information, and traced source attribution across multiple platforms,\\neffectively addressing real-world multimedia verification scenarios.', 'Large language models (LLMs) excel at natural language understanding and\\ngeneration but remain vulnerable to factual errors, limiting their reliability\\nin knowledge-intensive tasks. While decoding-time strategies provide a\\npromising efficient solution without training, existing methods typically treat\\ntoken-level and layer-level signals in isolation, overlooking the joint\\ndynamics between them. In this work, we introduce a token-aware,\\nlayer-localized contrastive decoding method that aligns specific token types\\nwith their most influential transformer layers to improve factual generation.\\nThrough empirical attention analysis, we identify two key patterns: punctuation\\ntokens receive dominant attention in early layers, while conceptual tokens\\ngovern semantic reasoning in intermediate layers. By selectively suppressing\\nattention to these token types at their respective depths, we achieve the\\ninduction of controlled factual degradation and derive contrastive signals to\\nguide the final factual decoding. Our method requires no additional training or\\nmodel modification, and experiments demonstrate that our method consistently\\nimproves factuality across multiple LLMs and various benchmarks.', 'Religion and spirituality (R/S) are complex and highly domain-dependent\\nconcepts which have long confounded researchers and policymakers. Due to their\\ncontext-specificity, R/S are difficult to operationalize in conventional\\narchival search strategies, particularly when datasets are very large, poorly\\naccessible, and marked by information noise. As a result, considerable time\\ninvestments and specialist knowledge is often needed to extract actionable\\ninsights related to R/S from general archival sources, increasing reliance on\\npublished literature and manual desk reviews. To address this challenge, we\\npresent SpiritRAG, an interactive Question Answering (Q&A) system based on\\nRetrieval-Augmented Generation (RAG). Built using 7,500 United Nations (UN)\\nresolution documents related to R/S in the domains of health and education,\\nSpiritRAG allows researchers and policymakers to conduct complex,\\ncontext-sensitive database searches of very large datasets using an easily\\naccessible, chat-based web interface. SpiritRAG is lightweight to deploy and\\nleverages both UN documents and user provided documents as source material. A\\npilot test and evaluation with domain experts on 100 manually composed\\nquestions demonstrates the practical value and usefulness of SpiritRAG.', 'Probabilistic circuits (PCs) are powerful probabilistic models that enable\\nexact and tractable inference, making them highly suitable for probabilistic\\nreasoning and inference tasks. While dominant in neural networks,\\nrepresentation learning with PCs remains underexplored, with prior approaches\\nrelying on external neural embeddings or activation-based encodings. To address\\nthis gap, we introduce autoencoding probabilistic circuits (APCs), a novel\\nframework leveraging the tractability of PCs to model probabilistic embeddings\\nexplicitly. APCs extend PCs by jointly modeling data and embeddings, obtaining\\nembedding representations through tractable probabilistic inference. The PC\\nencoder allows the framework to natively handle arbitrary missing data and is\\nseamlessly integrated with a neural decoder in a hybrid, end-to-end trainable\\narchitecture enabled by differentiable sampling. Our empirical evaluation\\ndemonstrates that APCs outperform existing PC-based autoencoding methods in\\nreconstruction quality, generate embeddings competitive with, and exhibit\\nsuperior robustness in handling missing data compared to neural autoencoders.\\nThese results highlight APCs as a powerful and flexible representation learning\\nmethod that exploits the probabilistic inference capabilities of PCs, showing\\npromising directions for robust inference, out-of-distribution detection, and\\nknowledge distillation.', \"In multivariate time series forecasting (MTSF), existing strategies for\\nprocessing sequences are typically categorized as channel-independent and\\nchannel-mixing. The former treats all temporal information of each variable as\\na token, focusing on capturing local temporal features of individual variables,\\nwhile the latter constructs a token from the multivariate information at each\\ntime step, emphasizing the modeling of global temporal dependencies. Current\\nmainstream models are mostly based on Transformer and the emerging Mamba.\\nTransformers excel at modeling global dependencies through self-attention\\nmechanisms but exhibit limited sensitivity to local temporal patterns and\\nsuffer from quadratic computational complexity, restricting their efficiency in\\nlong-sequence processing. In contrast, Mamba, based on state space models\\n(SSMs), achieves linear complexity and efficient long-range modeling but\\nstruggles to aggregate global contextual information in parallel. To overcome\\nthe limitations of both models, we propose DC-Mamber, a dual-channel\\nforecasting model based on Mamba and linear Transformer for time series\\nforecasting. Specifically, the Mamba-based channel employs a\\nchannel-independent strategy to extract intra-variable features, while the\\nTransformer-based channel adopts a channel-mixing strategy to model\\ncross-timestep global dependencies. DC-Mamber first maps the raw input into two\\ndistinct feature representations via separate embedding layers. These\\nrepresentations are then processed by a variable encoder (built on Mamba) and a\\ntemporal encoder (built on linear Transformer), respectively. Finally, a fusion\\nlayer integrates the dual-channel features for prediction. Extensive\\nexperiments on eight public datasets confirm DC-Mamber's superior accuracy over\\nexisting models.\", 'In scenarios requiring both prediction and explanation efficiency for image\\nclassification, self-explaining models that perform both tasks in a single\\ninference are effective. However, their training incurs substantial labeling\\nand computational costs. This study aims to tackle the issue by proposing a\\nmethod to transfer the visual explainability of self-explaining models, learned\\nin a source domain, to a target domain based on a task arithmetic framework.\\nSpecifically, we construct a self-explaining model by extending image\\nclassifiers based on a vision-language pretrained model. We then define an\\n\\\\emph{explainability vector} as the difference between model parameters trained\\non the source domain with and without explanation supervision. Based on the\\ntask arithmetic framework, we impart explainability to a model trained only on\\nthe prediction task in the target domain by applying the explainability vector.\\nExperimental results on various image classification datasets demonstrate that,\\nexcept for transfers between some less-related domains, visual explainability\\ncan be successfully transferred from source to target domains, improving\\nexplanation quality in the target domain without sacrificing classification\\naccuracy. Furthermore, we show that the explainability vector learned on a\\nlarge and diverse dataset like ImageNet, extended with explanation supervision,\\nexhibits universality and robustness, improving explanation quality on nine out\\nof ten different target datasets. We also find that the explanation quality\\nachieved with a single model inference is comparable to that of Kernel SHAP,\\nwhich requires 150 model inferences.', \"As Artificial Intelligence systems evolve from monolithic models to\\necosystems of specialized agents, the need for standardized communication\\nprotocols becomes increasingly critical. This paper introduces MOD-X (Modular\\nOpen Decentralized eXchange), a novel architectural framework proposal for\\nagent interoperability that addresses key limitations of existing protocols.\\nUnlike current approaches, MOD-X proposes a layered architecture with a\\nUniversal Message Bus, thorough state management, translation capabilities, and\\nblockchain-based security mechanisms. We present MOD-X's architecture, compare\\nit with existing protocols, and demonstrate its application through a worked\\nexample how it enables integration between heterogeneous specialist agents\\n(agents with different architectures, vendors, capabilities, and knowledge\\nrepresentations--including rule-based systems, neural networks, symbolic\\nreasoning engines, and legacy software with agent wrappers). MOD-X's key\\ninnovations include a publish-subscribe communication model, semantic\\ncapability discovery, and dynamic workflow orchestration--providing a framework\\nthat bridges theoretical formalism with practical implementation. This\\narchitecture addresses the growing need for truly decentralized, interoperable\\nagent ecosystems that can scale effectively without the need for central\\ncoordination.\", \"Recent advancements in large language models (LLMs) have significantly\\nimproved the capabilities of web agents. However, effectively navigating\\ncomplex and dynamic web environments still requires more advanced\\ntrajectory-level planning and execution. Prior studies have addressed\\nself-improving agents by collecting extensive GUI trajectories from\\nreal-environment interactions. Despite their effectiveness, these approaches\\nencounter two critical challenges: (1) Uncontrollable environment states, where\\nreal or sandboxed web environments often yield unstable and non-deterministic\\nfeedback, complicating the reproduction and debugging of agent behaviors; and\\n(2) High API costs, as generating even a single interaction trajectory can\\ninvolve hundreds of queries, leading to considerable API usage and\\ncomputational expenses. To address these limitations and enable scalable\\nself-improvement for agents, we propose WebSynthesis, a novel framework for\\ntrajectory synthesis and training. WebSynthesis leverages a learned world model\\nto simulate virtual web environments, allowing a policy agent to perform\\nefficient and reversible tree-based planning. This approach supports the\\nlarge-scale generation of diverse and high-quality trajectories, which are\\nsubsequently utilized to refine the agent's policy. Experimental results\\ndemonstrate that an agent trained using WebSynthesis on a small-scale synthetic\\ndataset achieves performance comparable to or even surpassing that of models\\ntrained on large-scale real-world data.\", 'As large language models (LLMs) become more integral to society and\\ntechnology, ensuring their safety becomes essential. Jailbreak attacks exploit\\nvulnerabilities to bypass safety guardrails, posing a significant threat.\\nHowever, the mechanisms enabling these attacks are not well understood. In this\\npaper, we reveal a universal phenomenon that occurs during jailbreak attacks:\\nAttention Slipping. During this phenomenon, the model gradually reduces the\\nattention it allocates to unsafe requests in a user query during the attack\\nprocess, ultimately causing a jailbreak. We show Attention Slipping is\\nconsistent across various jailbreak methods, including gradient-based token\\nreplacement, prompt-level template refinement, and in-context learning.\\nAdditionally, we evaluate two defenses based on query perturbation, Token\\nHighlighter and SmoothLLM, and find they indirectly mitigate Attention\\nSlipping, with their effectiveness positively correlated with the degree of\\nmitigation achieved. Inspired by this finding, we propose Attention Sharpening,\\na new defense that directly counters Attention Slipping by sharpening the\\nattention score distribution using temperature scaling. Experiments on four\\nleading LLMs (Gemma2-9B-It, Llama3.1-8B-It, Qwen2.5-7B-It, Mistral-7B-It v0.2)\\nshow that our method effectively resists various jailbreak attacks while\\nmaintaining performance on benign tasks on AlpacaEval. Importantly, Attention\\nSharpening introduces no additional computational or memory overhead, making it\\nan efficient and practical solution for real-world deployment.', 'Research, innovation and practical capital investment have been increasing\\nrapidly toward the realization of autonomous physical agents. This includes\\nindustrial and service robots, unmanned aerial vehicles, embedded control\\ndevices, and a number of other realizations of cybernetic/mechatronic\\nimplementations of intelligent autonomous devices. In this paper, we consider a\\nstylized version of robotic care, which would normally involve a two-level\\nReinforcement Learning procedure that trains a policy for both lower level\\nphysical movement decisions as well as higher level conceptual tasks and their\\nsub-components. In order to deliver greater safety and reliability in the\\nsystem, we present the general formulation of this as a two-level optimization\\nscheme which incorporates control at the lower level, and classical planning at\\nthe higher level, integrated with a capacity for learning. This synergistic\\nintegration of multiple methodologies -- control, classical planning, and RL --\\npresents an opportunity for greater insight for algorithm development, leading\\nto more efficient and reliable performance. Here, the notion of reliability\\npertains to physical safety and interpretability into an otherwise black box\\noperation of autonomous agents, concerning users and regulators. This work\\npresents the necessary background and general formulation of the optimization\\nframework, detailing each component and its integration with the others.', \"As AI hype continues to grow, organizations face pressure to broadcast or\\ndownplay purported AI initiatives - even when contrary to truth. This paper\\nintroduces AI-washing as overstating (deceptive boasting) or understating\\n(deceptive denial) a company's real AI usage. A 2x2 experiment (N = 401)\\nexamines how these false claims affect consumer attitudes and purchase\\nintentions. Results reveal a pronounced asymmetry: deceptive denial evokes more\\nnegative moral judgments than honest negation, while deceptive boasting has no\\neffects. We show that perceived betrayal mediates these outcomes. By clarifying\\nhow AI-washing erodes trust, the study highlights clear ethical implications\\nfor policymakers, marketers, and researchers striving for transparency.\", 'Choosing the right fabric is crucial to meet functional and quality\\nrequirements in robotic applications for textile manufacturing, apparel\\nproduction, and smart retail. We present MLLM-Fabric, a robotic framework\\npowered by multimodal large language models (MLLMs) for fabric sorting and\\nselection. The system includes a robotic arm, a camera, a visuotactile sensor,\\nand a pressure sensor. It employs supervised fine-tuning and multimodal\\nexplanation-guided knowledge distillation to accurately classify and rank\\nfabric properties. To facilitate further research, we release a dataset of 220\\nunique fabric samples, including RGB images and synchronized visuotactile and\\npressure data. Experimental results show that our Fabric-Llama-90B model\\nconsistently outperforms pretrained vision-language baselines in both property\\nranking accuracy and selection reliability.', 'Large reasoning models (LRMs) have exhibited remarkable reasoning\\ncapabilities through inference-time scaling, but this progress has also\\nintroduced considerable redundancy and inefficiency into their reasoning\\nprocesses, resulting in substantial computational waste. Previous work has\\nattempted to mitigate this issue by penalizing the overall length of generated\\nsamples during reinforcement learning (RL), with the goal of encouraging a more\\nconcise chains of thought. However, we observe that such global length penalty\\noften lead to excessive compression of critical reasoning steps while\\npreserving unnecessary details in simpler ones, yielding a suboptimal trade-off\\nbetween accuracy and efficiency. To address this issue, we propose\\nSmartThinker, a two-stage learnable framework designed to enable fine-grained\\ncontrol over the length of reasoning chains based on the importance of each\\nindividual step. In the first stage, SmartThinker adapts a reasoning model to a\\nshort-form reasoning mode through rejection sampling combined with supervised\\nfine-tuning (SFT). In the second stage, SmartThinker applies Step-Level Length\\nControl Policy Optimization (SCPO) to refine the model output distribution,\\nwhich increases the proportion of length allocated to critical steps while\\nreducing redundancy in less important ones. SCPO consists of four core\\ncomponents: an online importance estimator, a step-level length control reward\\nfunction, a step-level generalized advantage estimation (S-GAE) and a\\ndifficulty-adaptive clipping strategy. Working in concert, these components\\nenable SCPO to implement differentiated length control across reasoning steps.\\nEmpirical results across multiple reasoning benchmarks and various backbone\\nmodels demonstrate that SmartThinker significantly reduces redundant reasoning\\nwhile achieving comparable or even superior performance to existing methods.', 'This paper aims to improve the action smoothness of a cascaded online\\nlearning flight control system. Although the cascaded structure is widely used\\nin flight control design, its stability can be compromised by oscillatory\\ncontrol actions, which poses challenges for practical engineering applications.\\nTo address this issue, we introduce an online temporal smoothness technique and\\na low-pass filter to reduce the amplitude and frequency of the control actions.\\nFast Fourier Transform (FFT) is used to analyze policy performance in the\\nfrequency domain. Simulation results demonstrate the improvements achieved by\\nthe two proposed techniques.', 'While continuous diffusion models excel in modeling continuous distributions,\\ntheir application to categorical data has been less effective. Recent work has\\nshown that ratio-matching through score-entropy within a continuous-time\\ndiscrete Markov chain (CTMC) framework serves as a competitive alternative to\\nautoregressive models in language modeling. To enhance this framework, we first\\nintroduce three new theorems concerning the KL divergence between the data and\\nlearned distribution. Our results serve as the discrete counterpart to those\\nestablished for continuous diffusion models and allow us to derive an improved\\nupper bound of the perplexity. Second, we empirically show that ratio-matching\\nperformed by minimizing the denoising cross-entropy between the clean and\\ncorrupted data enables models to outperform those utilizing score-entropy with\\nup to 10% lower perplexity/generative-perplexity, and 15% faster training\\nsteps. To further support our findings, we introduce and evaluate a novel CTMC\\ntransition-rate matrix that allows prediction refinement, and derive the\\nanalytic expression for its matrix exponential which facilitates the\\ncomputation of conditional ratios thus enabling efficient training and\\ngeneration.', 'Recent advances in neuromorphic computing demonstrate on-device learning\\ncapabilities with low power consumption. One of the key learning units in these\\nsystems is the winner-take-all circuit. In this research, we propose a\\nwinner-take-all circuit that can be configured to achieve k-winner and\\nhysteresis properties, simulated in IBM 65 nm node. The circuit dissipated 34.9\\n$\\\\mu$W of power with a latency of 10.4 ns, while processing 1000 inputs. The\\nutility of the circuit is demonstrated for spatial filtering and\\nclassification.', 'Understanding surgical scenes can provide better healthcare quality for\\npatients, especially with the vast amount of video data that is generated\\nduring MIS. Processing these videos generates valuable assets for training\\nsophisticated models. In this paper, we introduce CLIP-RL, a novel contrastive\\nlanguage-image pre-training model tailored for semantic segmentation for\\nsurgical scenes. CLIP-RL presents a new segmentation approach which involves\\nreinforcement learning and curriculum learning, enabling continuous refinement\\nof the segmentation masks during the full training pipeline. Our model has\\nshown robust performance in different optical settings, such as occlusions,\\ntexture variations, and dynamic lighting, presenting significant challenges.\\nCLIP model serves as a powerful feature extractor, capturing rich semantic\\ncontext that enhances the distinction between instruments and tissues. The RL\\nmodule plays a pivotal role in dynamically refining predictions through\\niterative action-space adjustments. We evaluated CLIP-RL on the EndoVis 2018\\nand EndoVis 2017 datasets. CLIP-RL achieved a mean IoU of 81%, outperforming\\nstate-of-the-art models, and a mean IoU of 74.12% on EndoVis 2017. This\\nsuperior performance was achieved due to the combination of contrastive\\nlearning with reinforcement learning and curriculum learning.', 'Holistic surgical scene segmentation in robot-assisted surgery (RAS) enables\\nsurgical residents to identify various anatomical tissues, articulated tools,\\nand critical structures, such as veins and vessels. Given the firm\\nintraoperative time constraints, it is challenging for surgeons to provide\\ndetailed real-time explanations of the operative field for trainees. This\\nchallenge is compounded by the scarcity of expert surgeons relative to\\ntrainees, making the unambiguous delineation of go- and no-go zones\\ninconvenient. Therefore, high-performance semantic segmentation models offer a\\nsolution by providing clear postoperative analyses of surgical procedures.\\nHowever, recent advanced segmentation models rely on user-generated prompts,\\nrendering them impractical for lengthy surgical videos that commonly exceed an\\nhour. To address this challenge, we introduce Surg-SegFormer, a novel\\nprompt-free model that outperforms current state-of-the-art techniques.\\nSurg-SegFormer attained a mean Intersection over Union (mIoU) of 0.80 on the\\nEndoVis2018 dataset and 0.54 on the EndoVis2017 dataset. By providing robust\\nand automated surgical scene comprehension, this model significantly reduces\\nthe tutoring burden on expert surgeons, empowering residents to independently\\nand effectively understand complex surgical environments.', 'We propose Quick Feedforward (QF) Learning, a novel knowledge consolidation\\nframework for transformer-based models that enables efficient transfer of\\ninstruction derived knowledge into model weights through feedforward\\nactivations without any gradient back propagation. Unlike traditional\\nfinetuning, QF updates are computed in closed form, require minimal parameter\\nmodification, and preserve prior knowledge. Importantly, QF allows models to\\ntrain and infer within the same runtime environment, making the process more\\nresource efficient and closely aligned with how the human brain operates. Code\\nand models are open sourced on GitHub. I hope QF Learning inspires a more\\nefficient and brain-like paradigm for AI systems.', \"Answer Set Programming Modulo Theories (ASPMT) is a new framework of tight\\nintegration of answer set programming (ASP) and satisfiability modulo theories\\n(SMT). Similar to the relationship between first-order logic and SMT, it is\\nbased on a recent proposal of the functional stable model semantics by fixing\\ninterpretations of background theories. Analogously to a known relationship\\nbetween ASP and SAT, ``tight'' ASPMT programs can be translated into SMT\\ninstances. We demonstrate the usefulness of ASPMT by enhancing action language\\nC+ to handle continuous changes as well as discrete changes. We reformulate the\\nsemantics of C+ in terms ofASPMT, and show that SMT solvers can be used to\\ncompute the language. We also show how the language can represent cumulative\\neffects on continuous resources.\", 'Effective feedback is essential for student learning but is time-intensive\\nfor teachers. We present LearnLens, a modular, LLM-based system that generates\\npersonalised, curriculum-aligned feedback in science education. LearnLens\\ncomprises three components: (1) an error-aware assessment module that captures\\nnuanced reasoning errors; (2) a curriculum-grounded generation module that uses\\na structured, topic-linked memory chain rather than traditional\\nsimilarity-based retrieval, improving relevance and reducing noise; and (3) an\\neducator-in-the-loop interface for customisation and oversight. LearnLens\\naddresses key challenges in existing systems, offering scalable, high-quality\\nfeedback that empowers both teachers and students.', \"With the rapid progress of artificial intelligence (AI) in multi-modal\\nunderstanding, there is increasing potential for video comprehension\\ntechnologies to support professional domains such as medical education.\\nHowever, existing benchmarks suffer from two primary limitations: (1)\\nLinguistic Singularity: they are largely confined to English, neglecting the\\nneed for multilingual resources; and (2) Shallow Reasoning: their questions are\\noften designed for surface-level information retrieval, failing to properly\\nassess deep multi-modal integration. To address these limitations, we present\\nM3-Med, the first benchmark for Multi-lingual, Multi-modal, and Multi-hop\\nreasoning in Medical instructional video understanding. M3-Med consists of\\nmedical questions paired with corresponding video segments, annotated by a team\\nof medical experts. A key innovation of M3-Med is its multi-hop reasoning task,\\nwhich requires a model to first locate a key entity in the text, then find\\ncorresponding visual evidence in the video, and finally synthesize information\\nacross both modalities to derive the answer. This design moves beyond simple\\ntext matching and poses a substantial challenge to a model's deep cross-modal\\nunderstanding capabilities. We define two tasks: Temporal Answer Grounding in\\nSingle Video (TAGSV) and Temporal Answer Grounding in Video Corpus (TAGVC). We\\nevaluated several state-of-the-art models and Large Language Models (LLMs) on\\nM3-Med. The results reveal a significant performance gap between all models and\\nhuman experts, especially on the complex multi-hop questions where model\\nperformance drops sharply. M3-Med effectively highlights the current\\nlimitations of AI models in deep cross-modal reasoning within specialized\\ndomains and provides a new direction for future research.\", 'Training native 3D texture generative models remains a fundamental yet\\nchallenging problem, largely due to the limited availability of large-scale,\\nhigh-quality 3D texture datasets. This scarcity hinders generalization to\\nreal-world scenarios. To address this, most existing methods finetune\\nfoundation image generative models to exploit their learned visual priors.\\nHowever, these approaches typically generate only multi-view images and rely on\\npost-processing to produce UV texture maps -- an essential representation in\\nmodern graphics pipelines. Such two-stage pipelines often suffer from error\\naccumulation and spatial inconsistencies across the 3D surface. In this paper,\\nwe introduce SeqTex, a novel end-to-end framework that leverages the visual\\nknowledge encoded in pretrained video foundation models to directly generate\\ncomplete UV texture maps. Unlike previous methods that model the distribution\\nof UV textures in isolation, SeqTex reformulates the task as a sequence\\ngeneration problem, enabling the model to learn the joint distribution of\\nmulti-view renderings and UV textures. This design effectively transfers the\\nconsistent image-space priors from video foundation models into the UV domain.\\nTo further enhance performance, we propose several architectural innovations: a\\ndecoupled multi-view and UV branch design, geometry-informed attention to guide\\ncross-domain feature alignment, and adaptive token resolution to preserve fine\\ntexture details while maintaining computational efficiency. Together, these\\ncomponents allow SeqTex to fully utilize pretrained video priors and synthesize\\nhigh-fidelity UV texture maps without the need for post-processing. Extensive\\nexperiments show that SeqTex achieves state-of-the-art performance on both\\nimage-conditioned and text-conditioned 3D texture generation tasks, with\\nsuperior 3D consistency, texture-geometry alignment, and real-world\\ngeneralization.', 'Diffusion models, widely recognized for their success in generative tasks,\\nhave not yet been applied to clustering. We introduce Clustering via Diffusion\\n(CLUDI), a self-supervised framework that combines the generative power of\\ndiffusion models with pre-trained Vision Transformer features to achieve robust\\nand accurate clustering. CLUDI is trained via a teacher-student paradigm: the\\nteacher uses stochastic diffusion-based sampling to produce diverse cluster\\nassignments, which the student refines into stable predictions. This\\nstochasticity acts as a novel data augmentation strategy, enabling CLUDI to\\nuncover intricate structures in high-dimensional data. Extensive evaluations on\\nchallenging datasets demonstrate that CLUDI achieves state-of-the-art\\nperformance in unsupervised classification, setting new benchmarks in\\nclustering robustness and adaptability to complex data distributions.', 'The persistent threat of Android malware presents a serious challenge to the\\nsecurity of millions of users globally. While many machine learning-based\\nmethods have been developed to detect these threats, their reliance on large\\nlabeled datasets limits their effectiveness against emerging, previously unseen\\nmalware families, for which labeled data is scarce or nonexistent.\\n  To address this challenge, we introduce a novel zero-shot learning framework\\nthat combines Variational Graph Auto-Encoders (VGAE) with Siamese Neural\\nNetworks (SNN) to identify malware without needing prior examples of specific\\nmalware families. Our approach leverages graph-based representations of Android\\napplications, enabling the model to detect subtle structural differences\\nbetween benign and malicious software, even in the absence of labeled data for\\nnew threats.\\n  Experimental results show that our method outperforms the state-of-the-art\\nMaMaDroid, especially in zero-day malware detection. Our model achieves 96.24%\\naccuracy and 95.20% recall for unknown malware families, highlighting its\\nrobustness against evolving Android threats.', 'Recent advances in artificial intelligence have led to the emergence of\\nfoundation models, large-scale pre-trained neural networks that serve as\\nversatile starting points for a wide range of downstream tasks. In this work,\\nwe present ZERO, a zero-shot multi-prompt object detection model specifically\\ndesigned for robust, production-ready deployment across diverse industrial\\ndomains. ZERO integrates direct image input with multiple user-defined prompts,\\nwhich can include both textual and visual cues, and processes them through\\ndedicated encoders to generate accurate detection outputs. The model\\narchitecture is optimized for scalability, with a total of 1.033 TFLOPS and\\n622.346 million parameters, and is trained using a domain-specific image\\ndatabase exceeding one billion images. For the CVPR 2025 Foundational Few-Shot\\nObject Detection (FSOD) Challenge, we introduce a domain-specific fine-tuning\\nstrategy that emphasizes prompt diversity and conservative pseudo-labeling,\\nenabling effective adaptation to new domains with minimal supervision. Our\\napproach demonstrates practical advantages in flexibility, efficiency, and\\nreal-world applicability, achieving strong performance on the RF20VL-fsod\\nbenchmark despite limited annotation budgets. The results highlight the\\npotential of prompt-driven, data-centric AI for scalable and adaptive object\\ndetection in dynamic industrial environments.', 'COVID-19 is a severe and acute viral disease that can cause symptoms\\nconsistent with pneumonia in which inflammation is caused in the alveolous\\nregions of the lungs leading to a build-up of fluid and breathing difficulties.\\nThus, the diagnosis of COVID using CT scans has been effective in assisting\\nwith RT-PCR diagnosis and severity classifications. In this paper, we proposed\\na new data quality control pipeline to refine the quality of CT images based on\\nGAN and sliding windows. Also, we use class-sensitive cost functions including\\nLabel Distribution Aware Loss(LDAM Loss) and Class-balanced(CB) Loss to solve\\nthe long-tail problem existing in datasets. Our model reaches more than 0.983\\nMCC in the benchmark test dataset.', \"Safety alignment is crucial for large language models (LLMs) to resist\\nmalicious instructions but often results in over-refusals, where benign prompts\\nare unnecessarily rejected, impairing user experience and model utility. We\\nintroduce ACTOR (Activation-Based Training for Over-Refusal Reduction), a\\nrobust and compute- and data-efficient training framework that minimizes\\nover-refusals by leveraging internal activation patterns from diverse queries.\\nACTOR precisely identifies and adjusts the activation components that trigger\\nrefusals, providing stronger control over the refusal mechanism. By fine-tuning\\nonly a single model layer, ACTOR effectively reduces over-refusals across\\nmultiple benchmarks while maintaining the model's ability to handle harmful\\nqueries and preserve overall utility.\", 'This paper presents a portrait style transfer method that generalizes well to\\nvarious different domains while enabling high-quality semantic-aligned\\nstylization on regions including hair, eyes, eyelashes, skins, lips, and\\nbackground. To this end, we propose to establish dense semantic correspondence\\nbetween the given input and reference portraits based on a pre-trained model\\nand a semantic adapter, with which we obtain a warped reference semantically\\naligned with the input. To ensure effective yet controllable style transfer, we\\ndevise an AdaIN-Wavelet transform to balance content preservation and\\nstylization by blending low-frequency information of the warped reference with\\nhigh-frequency information of the input in the latent space. A style adapter is\\nalso designed to provide style guidance from the warped reference. With the\\nstylized latent from AdaIN-Wavelet transform, we employ a dual-conditional\\ndiffusion model that integrates a ControlNet recording high-frequency\\ninformation and the style guidance to generate the final result. Extensive\\nexperiments demonstrate the superiority of our method. Our code and trained\\nmodel are available at https://github.com/wangxb29/DGPST.', 'We argue that neither transformers nor sub-quadratic architectures are well\\nsuited to training at long sequence lengths: the cost of processing the context\\nis too expensive in the former, too inexpensive in the latter. Approaches such\\nas sliding window attention which reduce the cost-per-token of a transformer\\nimpair in-context learning, and so are also unsuitable. To address these\\nlimitations, we introduce power attention, an architectural layer for\\nlinear-cost sequence modeling whose state size can be adjusted independently of\\nparameters, unlocking the advantages of linear attention on practical domains.\\nWe develop and open-source a set of GPU kernels for efficient power attention,\\nidentifying a novel pattern of operation fusion to avoid memory and bandwidth\\nbottlenecks. Our experiments on the in-context learning of power attention\\nshows that these models dominate both exponential attention and linear\\nattention at long-context training.', 'Tendon-driven mechanisms are useful from the perspectives of variable\\nstiffness, redundant actuation, and lightweight design, and they are widely\\nused, particularly in hands, wrists, and waists of robots. The design of these\\nwire arrangements has traditionally been done empirically, but it becomes\\nextremely challenging when dealing with complex structures. Various studies\\nhave attempted to optimize wire arrangement, but many of them have\\noversimplified the problem by imposing conditions such as restricting movements\\nto a 2D plane, keeping the moment arm constant, or neglecting wire crossings.\\nTherefore, this study proposes a three-dimensional wire arrangement\\noptimization that takes wire crossings into account. We explore wire\\narrangements through a multi-objective black-box optimization method that\\nensures wires do not cross while providing sufficient joint torque along a\\ndefined target trajectory. For a 3D link structure, we optimize the wire\\narrangement under various conditions, demonstrate its effectiveness, and\\ndiscuss the obtained design solutions.', 'Piano sustain pedal detection has previously been approached as a binary\\non/off classification task, limiting its application in real-world piano\\nperformance scenarios where pedal depth significantly influences musical\\nexpression. This paper presents a novel approach for high-resolution estimation\\nthat predicts continuous pedal depth values. We introduce a Transformer-based\\narchitecture that not only matches state-of-the-art performance on the\\ntraditional binary classification task but also achieves high accuracy in\\ncontinuous pedal depth estimation. Furthermore, by estimating continuous\\nvalues, our model provides musically meaningful predictions for sustain pedal\\nusage, whereas baseline models struggle to capture such nuanced expressions\\nwith their binary detection approach. Additionally, this paper investigates the\\ninfluence of room acoustics on sustain pedal estimation using a synthetic\\ndataset that includes varied acoustic conditions. We train our model with\\ndifferent combinations of room settings and test it in an unseen new\\nenvironment using a \"leave-one-out\" approach. Our findings show that the two\\nbaseline models and ours are not robust to unseen room conditions. Statistical\\nanalysis further confirms that reverberation influences model predictions and\\nintroduces an overestimation bias.', 'Mobile GUI agents are designed to autonomously execute diverse device-control\\ntasks by interpreting and interacting with mobile screens. Despite notable\\nadvancements, their resilience in real-world scenarios where screen content may\\nbe partially manipulated by untrustworthy third parties remains largely\\nunexplored. Owing to their black-box and autonomous nature, these agents are\\nvulnerable to manipulations that could compromise user devices. In this work,\\nwe present the first systematic investigation into the vulnerabilities of\\nmobile GUI agents. We introduce a scalable attack simulation framework\\nAgentHazard, which enables flexible and targeted modifications of screen\\ncontent within existing applications. Leveraging this framework, we develop a\\ncomprehensive benchmark suite comprising both a dynamic task execution\\nenvironment and a static dataset of vision-language-action tuples, totaling\\nover 3,000 attack scenarios. The dynamic environment encompasses 58\\nreproducible tasks in an emulator with various types of hazardous UI content,\\nwhile the static dataset is constructed from 210 screenshots collected from 14\\npopular commercial apps. Importantly, our content modifications are designed to\\nbe feasible for unprivileged third parties. We evaluate 7 widely-used mobile\\nGUI agents and 5 common backbone models using our benchmark. Our findings\\nreveal that all examined agents are significantly influenced by misleading\\nthird-party content (with an average misleading rate of 28.8% in human-crafted\\nattack scenarios) and that their vulnerabilities are closely linked to the\\nemployed perception modalities and backbone LLMs. Furthermore, we assess\\ntraining-based mitigation strategies, highlighting both the challenges and\\nopportunities for enhancing the robustness of mobile GUI agents. Our code and\\ndata will be released at https://agenthazard.github.io.', 'Cyclic peptides, characterized by geometric constraints absent in linear\\npeptides, offer enhanced biochemical properties, presenting new opportunities\\nto address unmet medical needs. However, designing target-specific cyclic\\npeptides remains underexplored due to limited training data. To bridge the gap,\\nwe propose CP-Composer, a novel generative framework that enables zero-shot\\ncyclic peptide generation via composable geometric constraints. Our approach\\ndecomposes complex cyclization patterns into unit constraints, which are\\nincorporated into a diffusion model through geometric conditioning on nodes and\\nedges. During training, the model learns from unit constraints and their random\\ncombinations in linear peptides, while at inference, novel constraint\\ncombinations required for cyclization are imposed as input. Experiments show\\nthat our model, despite trained with linear peptides, is capable of generating\\ndiverse target-binding cyclic peptides, reaching success rates from 38% to 84%\\non different cyclization strategies.', \"As libraries explore large language models (LLMs) for use in virtual\\nreference services, a key question arises: Can LLMs serve all users equitably,\\nregardless of demographics or social status? While they offer great potential\\nfor scalable support, LLMs may also reproduce societal biases embedded in their\\ntraining data, risking the integrity of libraries' commitment to equitable\\nservice. To address this concern, we evaluate whether LLMs differentiate\\nresponses across user identities by prompting six state-of-the-art LLMs to\\nassist patrons differing in sex, race/ethnicity, and institutional role. We\\nfound no evidence of differentiation by race or ethnicity, and only minor\\nevidence of stereotypical bias against women in one model. LLMs demonstrated\\nnuanced accommodation of institutional roles through the use of linguistic\\nchoices related to formality, politeness, and domain-specific vocabularies,\\nreflecting professional norms rather than discriminatory treatment. These\\nfindings suggest that current LLMs show a promising degree of readiness to\\nsupport equitable and contextually appropriate communication in academic\\nlibrary reference services.\", \"We introduce Context Tuning, a simple and effective method to significantly\\nenhance few-shot adaptation of language models (LLMs) without fine-tuning model\\nparameters. While prompt-based adaptation techniques have demonstrated the\\neffectiveness of lightweight adaptation methods for large language models\\n(LLMs), they typically initialize a trainable prompt or prefix with irrelevant\\ntokens for the task at hand. In contrast, Context Tuning initializes the\\ntrainable prompt or prefix with task-specific demonstration examples,\\nleveraging the model's inherent In-Context Learning (ICL) ability to extract\\nrelevant information for improved few-shot learning performance. Extensive\\nevaluations on benchmarks such as CrossFit, UnifiedQA, MMLU, BIG-Bench Hard,\\nand ARC demonstrate that Context Tuning outperforms traditional prompt-based\\nadaptation methods and achieves competitive accuracy to Test-Time Training with\\nsignificantly higher training efficiency.\", 'Current unlearning methods for LLMs optimize on the private information they\\nseek to remove by incorporating it into their training objectives. We argue\\nthis not only risks reinforcing exposure to sensitive data, it also\\nfundamentally contradicts the principle of minimizing its use. As a remedy, we\\npropose a novel unlearning method - Partial Model Collapse (PMC), which does\\nnot require unlearning targets in the unlearning objective. Our approach is\\ninspired by recent observations that training generative models on their own\\ngenerations leads to distribution collapse, effectively removing information\\nfrom the model. Our core idea is to leverage this collapse for unlearning by\\ntriggering collapse partially on the sensitive data. We theoretically analyze\\nthat our approach converges to the desired outcome, i.e. the LLM unlearns the\\ninformation in the forget set. We empirically demonstrate that PMC overcomes\\ntwo key limitations of existing unlearning approaches that explicitly optimize\\non unlearning targets, and more effectively removes private information from\\nmodel outputs. Overall, our contributions represent an important step toward\\nmore comprehensive unlearning that aligns with real-world privacy constraints.\\nCode available at https://www.cs.cit.tum.de/daml/partial-model-collapse/.', 'Learning rate (LR) schedules in large language model (LLM) training often\\nfollow empirical templates: warm-up, constant plateau/stable phase, and decay\\n(WSD). However, the mechanistic explanation for this strategy remains\\nunderexplored, and the choice of plateau height and decay schedule is largely\\nheuristic. In this paper, we connect training dynamics to a thermodynamic\\nanalogy via the Mpemba effect - a phenomenon in which a hotter system cools\\nfaster than a colder one when quenched into the same bath. We analyze a class\\nof \"valley-river\" loss landscapes, where sharp (valley) directions equilibrate\\nquickly, while flatter (river) directions govern global descent. The Mpemba\\neffect provides an explanation for the necessity of the warm-up phase and\\nmotivates a high plateau - rather than a low one - for accelerating loss\\ndecrease during decay. We show that for certain loss landscapes, there exists\\nan optimal plateau learning rate - the \"strong Mpemba point\" - at which the\\nslowest mode vanishes, resulting in faster convergence during the decay phase.\\nWe derive analytical conditions for its existence and estimate decay dynamics\\nrequired to preserve the Mpemba advantage. Our minimal model and analysis offer\\na principled justification for plateau-based schedulers and provide guidance\\nfor tuning LR in LLMs with minimal hyperparameter sweep.', 'Theoretical works on supervised transfer learning (STL) -- where the learner\\nhas access to labeled samples from both source and target distributions -- have\\nfor the most part focused on statistical aspects of the problem, while\\nefficient optimization has received less attention. We consider the problem of\\ndesigning an SGD procedure for STL that alternates sampling between source and\\ntarget data, while maintaining statistical transfer guarantees without prior\\nknowledge of the quality of the source data. A main algorithmic difficulty is\\nin understanding how to design such an adaptive sub-sampling mechanism at each\\nSGD step, to automatically gain from the source when it is informative, or bias\\ntowards the target and avoid negative transfer when the source is less\\ninformative.\\n  We show that, such a mixed-sample SGD procedure is feasible for general\\nprediction tasks with convex losses, rooted in tracking an abstract sequence of\\nconstrained convex programs that serve to maintain the desired transfer\\nguarantees.\\n  We instantiate these results in the concrete setting of linear regression\\nwith square loss, and show that the procedure converges, with $1/\\\\sqrt{T}$\\nrate, to a solution whose statistical performance on the target is adaptive to\\nthe a priori unknown quality of the source. Experiments with synthetic and real\\ndatasets support the theory.', 'Understanding character relationships is essential for interpreting complex\\nnarratives and conducting socially grounded AI research. However, manual\\nannotation is time-consuming and low in coverage, while large language models\\n(LLMs) often produce hallucinated or logically inconsistent outputs. We present\\nSymbolicThought, a human-in-the-loop framework that combines LLM-based\\nextraction with symbolic reasoning. The system constructs editable character\\nrelationship graphs, refines them using seven types of logical constraints, and\\nenables real-time validation and conflict resolution through an interactive\\ninterface. To support logical supervision and explainable social analysis, we\\nrelease a dataset of 160 interpersonal relationships with corresponding logical\\nstructures. Experiments show that SymbolicThought improves annotation accuracy\\nand consistency while significantly reducing time cost, offering a practical\\ntool for narrative understanding, explainable AI, and LLM evaluation.', 'Data modeling using Tsetlin machines (TMs) is all about building logical\\nrules from the data features. The decisions of the model are based on a\\ncombination of these logical rules. Hence, the model is fully transparent and\\nit is possible to get explanations of its predictions. In this paper, we\\npresent a probability score for TM predictions and develop new techniques for\\nuncertainty quantification to increase the explainability further. The\\nprobability score is an inherent property of any TM variant and is derived\\nthrough an analysis of the TM learning dynamics. Simulated data is used to show\\na clear connection between the learned TM probability scores and the underlying\\nprobabilities of the data. A visualization of the probability scores also\\nreveals that the TM is less confident in its predictions outside the training\\ndata domain, which contrasts the typical extrapolation phenomenon found in\\nArtificial Neural Networks. The paper concludes with an application of the\\nuncertainty quantification techniques on an image classification task using the\\nCIFAR-10 dataset, where they provide new insights and suggest possible\\nimprovements to current TM image classification models.', 'We propose a non-autoregressive framework for the Travelling Salesman Problem\\nwhere solutions emerge directly from learned permutations without explicit\\nsearch. By applying a similarity transformation to Hamiltonian cycles, the\\nmodel learns to approximate permutation matrices via continuous relaxations.\\nOur unsupervised approach achieves competitive performance against classical\\nheuristics, demonstrating that the inherent structure of the problem can\\neffectively guide combinatorial optimization without sequential\\ndecision-making.', 'Physics-informed neural networks (PINNs) and neural operators (NOs) for\\nsolving the problem of diffraction of Extreme Ultraviolet (EUV) electromagnetic\\nwaves from a mask are presented. A novel hybrid Waveguide Neural Operator\\n(WGNO) is introduced, which is based on a waveguide method with its most\\ncomputationally expensive part replaced by a neural network. Numerical\\nexperiments on realistic 2D and 3D masks show that the WGNO achieves\\nstate-of-the-art accuracy and inference time, providing a highly efficient\\nsolution for accelerating the design workflows of lithography masks.', 'Recent works on large language models (LLMs) have demonstrated the impact of\\nprompting strategies and fine-tuning techniques on their reasoning\\ncapabilities. Yet, their effectiveness on clinical natural language inference\\n(NLI) remains underexplored. This study presents the first controlled\\nevaluation of how prompt structure and efficient fine-tuning jointly shape\\nmodel performance in clinical NLI. We inspect four classes of prompting\\nstrategies to elicit reasoning in LLMs at different levels of abstraction, and\\nevaluate their impact on a range of clinically motivated reasoning types. For\\neach prompting strategy, we construct high-quality demonstrations using a\\nfrontier model to distil multi-step reasoning capabilities into smaller models\\n(4B parameters) via Low-Rank Adaptation (LoRA). Across different language\\nmodels fine-tuned on the NLI4CT benchmark, we found that prompt type alone\\naccounts for up to 44% of the variance in macro-F1. Moreover, LoRA fine-tuning\\nyields consistent gains of +8 to 12 F1, raises output alignment above 97%, and\\nnarrows the performance gap to GPT-4o-mini to within 7.1%. Additional\\nexperiments on reasoning generalisation reveal that LoRA improves performance\\nin 75% of the models on MedNLI and TREC Clinical Trials Track. Overall, these\\nfindings demonstrate that (i) prompt structure is a primary driver of clinical\\nreasoning performance, (ii) compact models equipped with strong prompts and\\nLoRA can rival frontier-scale systems, and (iii) reasoning-type-aware\\nevaluation is essential to uncover prompt-induced trade-offs. Our results\\nhighlight the promise of combining prompt design and lightweight adaptation for\\nmore efficient and trustworthy clinical NLP systems, providing insights on the\\nstrengths and limitations of widely adopted prompting and parameter-efficient\\ntechniques in highly specialised domains.', 'Prediction of pedestrian crossing intention is a critical function in\\nautonomous vehicles. Conventional vision-based methods of crossing intention\\nprediction often struggle with generalizability, context understanding, and\\ncausal reasoning. This study explores the potential of vision-language\\nfoundation models (VLFMs) for predicting pedestrian crossing intentions by\\nintegrating multimodal data through hierarchical prompt templates. The\\nmethodology incorporates contextual information, including visual frames,\\nphysical cues observations, and ego-vehicle dynamics, into systematically\\nrefined prompts to guide VLFMs effectively in intention prediction. Experiments\\nwere conducted on three common datasets-JAAD, PIE, and FU-PIP. Results\\ndemonstrate that incorporating vehicle speed, its variations over time, and\\ntime-conscious prompts significantly enhances the prediction accuracy up to\\n19.8%. Additionally, optimised prompts generated via an automatic prompt\\nengineering framework yielded 12.5% further accuracy gains. These findings\\nhighlight the superior performance of VLFMs compared to conventional\\nvision-based models, offering enhanced generalisation and contextual\\nunderstanding for autonomous driving applications.', \"Ensuring safe transition of control in automated vehicles requires an\\naccurate and timely assessment of driver readiness. This paper introduces\\nDriver-Net, a novel deep learning framework that fuses multi-camera inputs to\\nestimate driver take-over readiness. Unlike conventional vision-based driver\\nmonitoring systems that focus on head pose or eye gaze, Driver-Net captures\\nsynchronised visual cues from the driver's head, hands, and body posture\\nthrough a triple-camera setup. The model integrates spatio-temporal data using\\na dual-path architecture, comprising a Context Block and a Feature Block,\\nfollowed by a cross-modal fusion strategy to enhance prediction accuracy.\\nEvaluated on a diverse dataset collected from the University of Leeds Driving\\nSimulator, the proposed method achieves an accuracy of up to 95.8% in driver\\nreadiness classification. This performance significantly enhances existing\\napproaches and highlights the importance of multimodal and multi-view fusion.\\nAs a real-time, non-intrusive solution, Driver-Net contributes meaningfully to\\nthe development of safer and more reliable automated vehicles and aligns with\\nnew regulatory mandates and upcoming safety standards.\", 'Reinforcement Learning (RL) has emerged as a transformative approach for\\naligning and enhancing Large Language Models (LLMs), addressing critical\\nchallenges in instruction following, ethical alignment, and reasoning\\ncapabilities. This survey offers a comprehensive foundation on the integration\\nof RL with language models, highlighting prominent algorithms such as Proximal\\nPolicy Optimization (PPO), Q-Learning, and Actor-Critic methods. Additionally,\\nit provides an extensive technical overview of RL techniques specifically\\ntailored for LLMs, including foundational methods like Reinforcement Learning\\nfrom Human Feedback (RLHF) and AI Feedback (RLAIF), as well as advanced\\nstrategies such as Direct Preference Optimization (DPO) and Group Relative\\nPolicy Optimization (GRPO). We systematically analyze their applications across\\ndomains, i.e., from code generation to tool-augmented reasoning. We also\\npresent a comparative taxonomy based on reward modeling, feedback mechanisms,\\nand optimization strategies. Our evaluation highlights key trends. RLHF remains\\ndominant for alignment, and outcome-based RL such as RLVR significantly\\nimproves stepwise reasoning. However, persistent challenges such as reward\\nhacking, computational costs, and scalable feedback collection underscore the\\nneed for continued innovation. We further discuss emerging directions,\\nincluding hybrid RL algorithms, verifier-guided training, and multi-objective\\nalignment frameworks. This survey serves as a roadmap for researchers advancing\\nRL-driven LLM development, balancing capability enhancement with safety and\\nscalability.', 'This paper presents Edge-based Mixture of Experts (MoE) Collaborative\\nComputing (EMC2), an optimal computing system designed for autonomous vehicles\\n(AVs) that simultaneously achieves low-latency and high-accuracy 3D object\\ndetection. Unlike conventional approaches, EMC2 incorporates a scenario-aware\\nMoE architecture specifically optimized for edge platforms. By effectively\\nfusing LiDAR and camera data, the system leverages the complementary strengths\\nof sparse 3D point clouds and dense 2D images to generate robust multimodal\\nrepresentations. To enable this, EMC2 employs an adaptive multimodal data\\nbridge that performs multi-scale preprocessing on sensor inputs, followed by a\\nscenario-aware routing mechanism that dynamically dispatches features to\\ndedicated expert models based on object visibility and distance. In addition,\\nEMC2 integrates joint hardware-software optimizations, including hardware\\nresource utilization optimization and computational graph simplification, to\\nensure efficient and real-time inference on resource-constrained edge devices.\\nExperiments on open-source benchmarks clearly show the EMC2 advancements as a\\nend-to-end system. On the KITTI dataset, it achieves an average accuracy\\nimprovement of 3.58% and a 159.06% inference speedup compared to 15 baseline\\nmethods on Jetson platforms, with similar performance gains on the nuScenes\\ndataset, highlighting its capability to advance reliable, real-time 3D object\\ndetection tasks for AVs.', \"Data-free knowledge distillation (DFKD) transfers knowledge from a teacher to\\na student without access the real in-distribution (ID) data. Its common\\nsolution is to use a generator to synthesize fake data and use them as a\\nsubstitute for real ID data. However, existing works typically assume teachers\\nare trustworthy, leaving the robustness and security of DFKD from untrusted\\nteachers largely unexplored. In this work, we conduct the first investigation\\ninto distilling non-transferable learning (NTL) teachers using DFKD, where the\\ntransferability from an ID domain to an out-of-distribution (OOD) domain is\\nprohibited. We find that NTL teachers fool DFKD through divert the generator's\\nattention from the useful ID knowledge to the misleading OOD knowledge. This\\nhinders ID knowledge transfer but prioritizes OOD knowledge transfer. To\\nmitigate this issue, we propose Adversarial Trap Escaping (ATEsc) to benefit\\nDFKD by identifying and filtering out OOD-like synthetic samples. Specifically,\\ninspired by the evidence that NTL teachers show stronger adversarial robustness\\non OOD samples than ID samples, we split synthetic samples into two groups\\naccording to their robustness. The fragile group is treated as ID-like data and\\nused for normal knowledge distillation, while the robust group is seen as\\nOOD-like data and utilized for forgetting OOD knowledge. Extensive experiments\\ndemonstrate the effectiveness of ATEsc for improving DFKD against NTL teachers.\\nCode is released at https://github.com/tmllab/2025_ICML_ATEsc.\", 'Our research addresses the overlooked security concerns related to data\\npoisoning in continual learning (CL). Data poisoning - the intentional\\nmanipulation of training data to affect the predictions of machine learning\\nmodels - was recently shown to be a threat to CL training stability. While\\nexisting literature predominantly addresses scenario-dependent attacks, we\\npropose to focus on a more simple and realistic single-task poison (STP)\\nthreats. In contrast to previously proposed poisoning settings, in STP\\nadversaries lack knowledge and access to the model, as well as to both previous\\nand future tasks. During an attack, they only have access to the current task\\nwithin the data stream. Our study demonstrates that even within these stringent\\nconditions, adversaries can compromise model performance using standard image\\ncorruptions. We show that STP attacks are able to strongly disrupt the whole\\ncontinual training process: decreasing both the stability (its performance on\\npast tasks) and plasticity (capacity to adapt to new tasks) of the algorithm.\\nFinally, we propose a high-level defense framework for CL along with a poison\\ntask detection method based on task vectors. The code is available at\\nhttps://github.com/stapaw/STP.git .', 'This paper presents a defense framework for enhancing the safety of large\\nlanguage model (LLM) empowered multi-agent systems (MAS) in safety-critical\\ndomains such as aerospace. We apply randomized smoothing, a statistical\\nrobustness certification technique, to the MAS consensus context, enabling\\nprobabilistic guarantees on agent decisions under adversarial influence. Unlike\\ntraditional verification methods, our approach operates in black-box settings\\nand employs a two-stage adaptive sampling mechanism to balance robustness and\\ncomputational efficiency. Simulation results demonstrate that our method\\neffectively prevents the propagation of adversarial behaviors and\\nhallucinations while maintaining consensus performance. This work provides a\\npractical and scalable path toward safe deployment of LLM-based MAS in\\nreal-world, high-stakes environments.', 'LLM-based web agents have recently made significant progress, but much of it\\nhas occurred in closed-source systems, widening the gap with open-source\\nalternatives. Progress has been held back by two key challenges: first, a\\nnarrow focus on single-step tasks that overlooks the complexity of multi-step\\nweb interactions; and second, the high compute costs required to post-train\\nLLM-based web agents. To address this, we present the first statistically\\ngrounded study on compute allocation for LLM web-agent post-training. Our\\napproach uses a two-stage pipeline, training a Llama 3.1 8B student to imitate\\na Llama 3.3 70B teacher via supervised fine-tuning (SFT), followed by on-policy\\nreinforcement learning. We find this process highly sensitive to hyperparameter\\nchoices, making exhaustive sweeps impractical. To spare others from expensive\\ntrial-and-error, we sample 1,370 configurations and use bootstrapping to\\nestimate effective hyperparameters. Our results show that combining SFT with\\non-policy RL consistently outperforms either approach alone on both WorkArena\\nand MiniWob++. Further, this strategy requires only 55% of the compute to match\\nthe peak performance of pure SFT on MiniWob++, effectively pushing the\\ncompute-performance Pareto frontier, and is the only strategy that can close\\nthe gap with closed-source models.', \"This paper presents HERO (Hierarchical Testing with Rabbit Optimization), a\\nnovel black-box adversarial testing framework for evaluating the robustness of\\ndeep learning-based Prognostics and Health Management systems in Industrial\\nCyber-Physical Systems. Leveraging Artificial Rabbit Optimization, HERO\\ngenerates physically constrained adversarial examples that align with\\nreal-world data distributions via global and local perspective. Its\\ngeneralizability ensures applicability across diverse ICPS scenarios. This\\nstudy specifically focuses on the Proton Exchange Membrane Fuel Cell system,\\nchosen for its highly dynamic operational conditions, complex degradation\\nmechanisms, and increasing integration into ICPS as a sustainable and efficient\\nenergy solution. Experimental results highlight HERO's ability to uncover\\nvulnerabilities in even state-of-the-art PHM models, underscoring the critical\\nneed for enhanced robustness in real-world applications. By addressing these\\nchallenges, HERO demonstrates its potential to advance more resilient PHM\\nsystems across a wide range of ICPS domains.\", \"Fine-tuning methods such as Direct Preference Optimization (DPO) and Group\\nRelative Policy Optimization (GRPO) have demonstrated success in training large\\nlanguage models (LLMs) for single-turn tasks. However, these methods fall short\\nin multi-turn applications, such as diagnostic patient interviewing, where\\nunderstanding how early conversational turns influence downstream completions\\nand outcomes is essential. In medicine, a multi-turn perspective is critical\\nfor learning diagnostic schemas and better understanding conversation dynamics.\\nTo address this gap, I introduce Savage Conversation Forests (SCF), a\\nreinforcement learning framework that leverages a branched conversation\\narchitecture to fine-tune LLMs for multi-turn dialogue. SCF generates multiple\\npossible conversation continuations at each turn, enabling the model to learn\\nhow different early responses affect downstream interactions and diagnostic\\noutcomes. In experiments simulating doctor-patient conversations, SCF with\\nbranching outperforms linear conversation architectures on diagnostic accuracy.\\nI hypothesize that SCF's improvements stem from its ability to provide richer,\\ninterdependent training signals across conversation turns. These results\\nsuggest that a branched training architecture is an important strategy for fine\\ntuning LLMs in complex multi-turn conversational tasks.\", 'Modern social robots can be considered the descendants of steam engines from\\nthe First Industrial Revolution (IR 1.0) and industrial robotic arms from the\\nThird Industrial Revolution (IR 3.0). As some time has passed since the\\nintroduction of these robots during the Fourth Industrial Revolution (IR 4.0),\\nchallenges and issues in their interaction with humans have emerged, leading\\nresearchers to conclude that, like any other AI-based technology, these robots\\nmust also be human-centered to meet the needs of their users. This chapter aims\\nto introduce humans and their needs in interactions with robots, ranging from\\nshort-term, one-on-one interactions (micro-level) to long-term, macro-level\\nneeds at the societal scale. Building upon the principles of human-centered AI,\\nthis chapter presents, for the first time, a new framework of human needs\\ncalled the Dual Pyramid. This framework encompasses a comprehensive list of\\nhuman needs in robot interactions, from the most fundamental, robot\\neffectiveness to macro level requirements, such as the collaboration with\\nrobots in achieving the United Nations 17 Sustainable Development Goals.', \"Accurate audio quality estimation is essential for developing and evaluating\\naudio generation, retrieval, and enhancement systems. Existing non-intrusive\\nassessment models predict a single Mean Opinion Score (MOS) for speech, merging\\ndiverse perceptual factors and failing to generalize beyond speech. We propose\\nMMMOS, a no-reference, multi-domain audio quality assessment system that\\nestimates four orthogonal axes: Production Quality, Production Complexity,\\nContent Enjoyment, and Content Usefulness across speech, music, and\\nenvironmental sounds. MMMOS fuses frame-level embeddings from three pretrained\\nencoders (WavLM, MuQ, and M2D) and evaluates three aggregation strategies with\\nfour loss functions. By ensembling the top eight models, MMMOS shows a 20-30%\\nreduction in mean squared error and a 4-5% increase in Kendall's {\\\\tau} versus\\nbaseline, gains first place in six of eight Production Complexity metrics, and\\nranks among the top three on 17 of 32 challenge metrics.\", \"The Dreamer algorithm has recently obtained remarkable performance across\\ndiverse environment domains by training powerful agents with simulated\\ntrajectories. However, the compressed nature of its world model's latent space\\ncan result in the loss of crucial information, negatively affecting the agent's\\nperformance. Recent approaches, such as $\\\\Delta$-IRIS and DIAMOND, address this\\nlimitation by training more accurate world models. However, these methods\\nrequire training agents directly from pixels, which reduces training efficiency\\nand prevents the agent from benefiting from the inner representations learned\\nby the world model. In this work, we propose an alternative approach to world\\nmodeling that is both accurate and efficient. We introduce EMERALD (Efficient\\nMaskEd latent tRAnsformer worLD model), a world model using a spatial latent\\nstate with MaskGIT predictions to generate accurate trajectories in latent\\nspace and improve the agent performance. On the Crafter benchmark, EMERALD\\nachieves new state-of-the-art performance, becoming the first method to surpass\\nhuman experts performance within 10M environment steps. Our method also\\nsucceeds to unlock all 22 Crafter achievements at least once during evaluation.\", 'Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\\nincorporating external documents at inference time, enabling up-to-date\\nknowledge access without costly retraining. However, conventional RAG methods\\nretrieve passages independently, often leading to redundant, noisy, or\\ninsufficiently diverse context-particularly problematic - particularly\\nproblematic in noisy corpora and for multi-hop questions. To address this, we\\npropose Adaptive Passage Combination Retrieval (AdaPCR), a novel framework for\\nopen-domain question answering with black-box LMs. AdaPCR explicitly models\\ndependencies between passages by considering passage combinations as units for\\nretrieval and reranking. It consists of a context-aware query reformulation\\nusing concatenated passages, and a reranking step trained with a predictive\\nobjective aligned with downstream answer likelihood. Crucially, AdaPCR\\nadaptively selects the number of retrieved passages without additional stopping\\nmodules. Experiments across several QA benchmarks show that AdaPCR outperforms\\nbaselines, particularly in multi-hop reasoning, demonstrating the effectiveness\\nof modeling inter-passage dependencies for improved retrieval.', \"Contemporary multi-agent systems encounter persistent challenges in\\ncross-platform interoperability, dynamic task scheduling, and efficient\\nresource sharing. Agents with heterogeneous implementations often lack\\nstandardized interfaces; collaboration frameworks remain brittle and hard to\\nextend; scheduling policies are static; and inter-agent state synchronization\\nis insufficient. We propose Hierarchical Agent Workflow (HAWK), a modular\\nframework comprising five layers-User, Workflow, Operator, Agent, and\\nResource-and supported by sixteen standardized interfaces. HAWK delivers an\\nend-to-end pipeline covering task parsing, workflow orchestration, intelligent\\nscheduling, resource invocation, and data synchronization. At its core lies an\\nadaptive scheduling and optimization module in the Workflow Layer, which\\nharnesses real-time feedback and dynamic strategy adjustment to maximize\\nutilization. The Resource Layer provides a unified abstraction over\\nheterogeneous data sources, large models, physical devices, and third-party\\nservices&tools, simplifying cross-domain information retrieval. We demonstrate\\nHAWK's scalability and effectiveness via CreAgentive, a multi-agent\\nnovel-generation prototype, which achieves marked gains in throughput, lowers\\ninvocation complexity, and improves system controllability. We also show how\\nhybrid deployments of large language models integrate seamlessly within HAWK,\\nhighlighting its flexibility. Finally, we outline future research\\navenues-hallucination mitigation, real-time performance tuning, and enhanced\\ncross-domain adaptability-and survey prospective applications in healthcare,\\ngovernment, finance, and education.\", 'Action-driven stochastic human motion prediction aims to generate future\\nmotion sequences of a pre-defined target action based on given past observed\\nsequences performing non-target actions. This task primarily presents two\\nchallenges. Firstly, generating smooth transition motions is hard due to the\\nvarying transition speeds of different actions. Secondly, the action\\ncharacteristic is difficult to be learned because of the similarity of some\\nactions. These issues cause the predicted results to be unreasonable and\\ninconsistent. As a result, we propose two memory banks, the Soft-transition\\nAction Bank (STAB) and Action Characteristic Bank (ACB), to tackle the problems\\nabove. The STAB stores the action transition information. It is equipped with\\nthe novel soft searching approach, which encourages the model to focus on\\nmultiple possible action categories of observed motions. The ACB records action\\ncharacteristic, which produces more prior information for predicting certain\\nactions. To fuse the features retrieved from the two banks better, we further\\npropose the Adaptive Attention Adjustment (AAA) strategy. Extensive experiments\\non four motion prediction datasets demonstrate that our approach consistently\\noutperforms the previous state-of-the-art. The demo and code are available at\\nhttps://hyqlat.github.io/STABACB.github.io/.', 'Human Motion Prediction (HMP) aims to predict future poses at different\\nmoments according to past motion sequences. Previous approaches have treated\\nthe prediction of various moments equally, resulting in two main limitations:\\nthe learning of short-term predictions is hindered by the focus on long-term\\npredictions, and the incorporation of prior information from past predictions\\ninto subsequent predictions is limited. In this paper, we introduce a novel\\nmulti-stage training framework called Temporal Continual Learning (TCL) to\\naddress the above challenges. To better preserve prior information, we\\nintroduce the Prior Compensation Factor (PCF). We incorporate it into the model\\ntraining to compensate for the lost prior information. Furthermore, we derive a\\nmore reasonable optimization objective through theoretical derivation. It is\\nimportant to note that our TCL framework can be easily integrated with\\ndifferent HMP backbone models and adapted to various datasets and applications.\\nExtensive experiments on four HMP benchmark datasets demonstrate the\\neffectiveness and flexibility of TCL. The code is available at\\nhttps://github.com/hyqlat/TCL.', 'Sharpness-aware Minimization (SAM) improves generalization in large-scale\\nmodel training by linking loss landscape geometry to generalization. However,\\nchallenges such as mislabeled noisy data and privacy concerns have emerged as\\nsignificant issues. Data attribution, which identifies the contributions of\\nspecific training samples, offers a promising solution. However, directly\\nrendering existing data influence evaluation tools such as influence functions\\n(IF) to SAM will be inapplicable or inaccurate as SAM utilizes an inner loop to\\nfind model perturbations that maximize loss, which the outer loop then\\nminimizes, resulting in a doubled computational structure. Additionally, this\\nbilevel structure complicates the modeling of data influence on the parameters.\\nIn this paper, based on the IF, we develop two innovative data valuation\\nmethods for SAM, each offering unique benefits in different scenarios: the\\nHessian-based IF and the Gradient Trajectory-based IF. The first one provides a\\ncomprehensive estimation of data influence using a closed-form measure that\\nrelies only on the trained model weights. In contrast, the other IF for SAM\\nutilizes gradient trajectory information during training for more accurate and\\nefficient data assessment. Extensive experiments demonstrate their\\neffectiveness in data evaluation and parameter tuning, with applications in\\nidentifying mislabeled data, model editing, and enhancing interpretability.', 'Malware Family Classification (MFC) aims to identify the fine-grained family\\n(e.g., GuLoader or BitRAT) to which a potential malware sample belongs, in\\ncontrast to malware detection or sample classification that predicts only an\\nYes/No. Accurate family identification can greatly facilitate automated sample\\nlabeling and understanding on crowdsourced malware analysis platforms such as\\nVirusTotal and MalwareBazaar, which generate vast amounts of data daily. In\\nthis paper, we explore and assess the feasibility of using traditional binary\\nstring features for MFC in the new era of large language models (LLMs) and\\nRetrieval-Augmented Generation (RAG). Specifically, we investigate how\\nFamily-Specific String (FSS) features could be utilized in a manner similar to\\nRAG to facilitate MFC. To this end, we develop a curated evaluation framework\\ncovering 4,347 samples from 67 malware families, extract and analyze over 25\\nmillion strings, and conduct detailed ablation studies to assess the impact of\\ndifferent design choices in four major modules.', \"Topological materials occupy a frontier in condensed-matter physics thanks to\\ntheir remarkable electronic and quantum properties, yet their cross-scale\\ndesign remains bottlenecked by inefficient discovery workflows. Here, we\\nintroduce TopoMAS (Topological materials Multi-Agent System), an interactive\\nhuman-AI framework that seamlessly orchestrates the entire materials-discovery\\npipeline: from user-defined queries and multi-source data retrieval, through\\ntheoretical inference and crystal-structure generation, to first-principles\\nvalidation. Crucially, TopoMAS closes the loop by autonomously integrating\\ncomputational outcomes into a dynamic knowledge graph, enabling continuous\\nknowledge refinement. In collaboration with human experts, it has already\\nguided the identification of novel topological phases SrSbO3, confirmed by\\nfirst-principles calculations. Comprehensive benchmarks demonstrate robust\\nadaptability across base Large Language Model, with the lightweight Qwen2.5-72B\\nmodel achieving 94.55% accuracy while consuming only 74.3-78.4% of tokens\\nrequired by Qwen3-235B and 83.0% of DeepSeek-V3's usage--delivering responses\\ntwice as fast as Qwen3-235B. This efficiency establishes TopoMAS as an\\naccelerator for computation-driven discovery pipelines. By harmonizing rational\\nagent orchestration with a self-evolving knowledge graph, our framework not\\nonly delivers immediate advances in topological materials but also establishes\\na transferable, extensible paradigm for materials-science domain.\", 'Accurate prediction of effluent temperature in recharge basins is essential\\nfor optimizing the Soil Aquifer Treatment (SAT) process, as temperature\\ndirectly influences water viscosity and infiltration rates. This study develops\\nand evaluates predictive models for effluent temperature in the upper recharge\\nlayer of a Shafdan SAT system recharge basin using ambient meteorological data.\\nMultiple linear regression (MLR), neural networks (NN), and random forests (RF)\\nwere tested for their predictive accuracy and interpretability. The MLR model,\\npreferred for its operational simplicity and robust performance, achieved high\\npredictive accuracy (R2 = 0.86-0.87) and was used to estimate effluent\\ntemperatures over a 10-year period. Results highlight pronounced seasonal\\ntemperature cycles and the importance of topsoil temperature in governing the\\nthermal profile of the infiltrating effluent. The study provides practical\\nequations for real-time monitoring and long-term planning of SAT operations.', 'As large language models (LLMs) become more common in educational tools and\\nprogramming environments, questions arise about how these systems should\\ninteract with users. This study investigates how different interaction styles\\nwith ChatGPT-4o (passive, proactive, and collaborative) affect user performance\\non simple programming tasks. I conducted a within-subjects experiment where\\nfifteen high school students participated, completing three problems under\\nthree distinct versions of the model. Each version was designed to represent a\\nspecific style of AI support: responding only when asked, offering suggestions\\nautomatically, or engaging the user in back-and-forth dialogue.Quantitative\\nanalysis revealed that the collaborative interaction style significantly\\nimproved task completion time compared to the passive and proactive conditions.\\nParticipants also reported higher satisfaction and perceived helpfulness when\\nworking with the collaborative version. These findings suggest that the way an\\nLLM communicates, how it guides, prompts, and responds, can meaningfully impact\\nlearning and performance. This research highlights the importance of designing\\nLLMs that go beyond functional correctness to support more interactive,\\nadaptive, and user-centered experiences, especially for novice programmers.', 'One of the key impediments for developing and assessing robust medical\\nimaging algorithms is limited access to large-scale datasets with suitable\\nannotations. Synthetic data generated with plausible physical and biological\\nconstraints may address some of these data limitations. We propose the use of\\nphysics simulations to generate synthetic images with pixel-level segmentation\\nannotations, which are notoriously difficult to obtain. Specifically, we apply\\nthis approach to breast imaging analysis and release T-SYNTH, a large-scale\\nopen-source dataset of paired 2D digital mammography (DM) and 3D digital breast\\ntomosynthesis (DBT) images. Our initial experimental results indicate that\\nT-SYNTH images show promise for augmenting limited real patient datasets for\\ndetection tasks in DM and DBT. Our data and code are publicly available at\\nhttps://github.com/DIDSR/tsynth-release.', 'The gap between static benchmarks and the dynamic nature of real-world legal\\npractice poses a key barrier to advancing legal intelligence. To this end, we\\nintroduce J1-ENVS, the first interactive and dynamic legal environment tailored\\nfor LLM-based agents. Guided by legal experts, it comprises six representative\\nscenarios from Chinese legal practices across three levels of environmental\\ncomplexity. We further introduce J1-EVAL, a fine-grained evaluation framework,\\ndesigned to assess both task performance and procedural compliance across\\nvarying levels of legal proficiency. Extensive experiments on 17 LLM agents\\nreveal that, while many models demonstrate solid legal knowledge, they struggle\\nwith procedural execution in dynamic settings. Even the SOTA model, GPT-4o,\\nfalls short of 60% overall performance. These findings highlight persistent\\nchallenges in achieving dynamic legal intelligence and offer valuable insights\\nto guide future research.', 'While Large Language Models (LLMs) have demonstrated impressive abilities\\nacross various domains, they still struggle with complex problems characterized\\nby multi-objective optimization, precise constraint satisfaction, immense\\nsolution spaces, etc. To address the limitation, drawing on the superior\\nsemantic understanding ability of LLMs and also the outstanding global search\\nand optimization capability of genetic algorithms, we propose to capitalize on\\ntheir respective strengths and introduce Lyria, a general LLM-driven genetic\\nalgorithm framework, comprising 7 essential components. Through conducting\\nextensive experiments with 4 LLMs across 3 types of problems, we demonstrated\\nthe efficacy of Lyria. Additionally, with 7 additional ablation experiments, we\\nfurther systematically analyzed and elucidated the factors that affect its\\nperformance.', \"As large language models (LLMs) become key advisors in various domains, their\\ncultural sensitivity and reasoning skills are crucial in multicultural\\nenvironments. We introduce Nunchi-Bench, a benchmark designed to evaluate LLMs'\\ncultural understanding, with a focus on Korean superstitions. The benchmark\\nconsists of 247 questions spanning 31 topics, assessing factual knowledge,\\nculturally appropriate advice, and situational interpretation. We evaluate\\nmultilingual LLMs in both Korean and English to analyze their ability to reason\\nabout Korean cultural contexts and how language variations affect performance.\\nTo systematically assess cultural reasoning, we propose a novel evaluation\\nstrategy with customized scoring metrics that capture the extent to which\\nmodels recognize cultural nuances and respond appropriately. Our findings\\nhighlight significant challenges in LLMs' cultural reasoning. While models\\ngenerally recognize factual information, they struggle to apply it in practical\\nscenarios. Furthermore, explicit cultural framing enhances performance more\\neffectively than relying solely on the language of the prompt. To support\\nfurther research, we publicly release Nunchi-Bench alongside a leaderboard.\", \"Cross-domain recommendation (CDR) aims to address the persistent cold-start\\nproblem in Recommender Systems. Current CDR research concentrates on\\ntransferring cold-start users' information from the auxiliary domain to the\\ntarget domain. However, these systems face two main issues: the\\nunderutilization of multimodal data, which hinders effective cross-domain\\nalignment, and the neglect of side users who interact solely within the target\\ndomain, leading to inadequate learning of the target domain's vector space\\ndistribution. To address these issues, we propose a model leveraging Multimodal\\ndata and Side users for diffusion Cross-domain recommendation (MuSiC). We first\\nemploy a multimodal large language model to extract item multimodal features\\nand leverage a large language model to uncover user features using prompt\\nlearning without fine-tuning. Secondly, we propose the cross-domain diffusion\\nmodule to learn the generation of feature vectors in the target domain. This\\napproach involves learning feature distribution from side users and\\nunderstanding the patterns in cross-domain transformation through overlapping\\nusers. Subsequently, the trained diffusion module is used to generate feature\\nvectors for cold-start users in the target domain, enabling the completion of\\ncross-domain recommendation tasks. Finally, our experimental evaluation of the\\nAmazon dataset confirms that MuSiC achieves state-of-the-art performance,\\nsignificantly outperforming all selected baselines. Our code is available:\\nhttps://anonymous.4open.science/r/MuSiC-310A/.\", 'Large Language Models (LLMs) often generate responses that are factually\\nincorrect yet expressed with high confidence, which can pose serious risks for\\nend users. To address this, it is essential for LLMs not only to produce\\nanswers but also to provide accurate estimates of their correctness.\\nUncertainty quantification methods have been introduced to assess the quality\\nof LLM outputs, with factual accuracy being a key aspect of that quality. Among\\nthese methods, those that leverage hidden states to train probes have shown\\nparticular promise, as these internal representations encode information\\nrelevant to the factuality of responses, making this approach the focus of this\\npaper. However, the probe trained on the hidden states of one dataset often\\nstruggles to generalise to another dataset of a different task or domain. To\\naddress this limitation, we explore combining data-agnostic features with\\nhidden-state features and assess whether this hybrid feature set enhances\\nout-of-domain performance. We further examine whether selecting only the most\\ninformative hidden-state features, thereby discarding task-specific noise,\\nenables the data-agnostic features to contribute more effectively. The\\nexperiment results indicate that although introducing data-agnostic features\\ngenerally enhances generalisation performance in most cases, in certain\\nscenarios their inclusion degrades performance. A similar pattern emerges when\\nretaining only the most important hidden-state features - adding data-agnostic\\nfeatures does not consistently further enhance performance compared to using\\nthe full set of hidden-state features. A closer analysis reveals that, in some\\nspecific cases, the trained probe underweights the data-agnostic features\\nrelative to the hidden-state features, which we believe is the main reason why\\nthe results are inconclusive.', 'Foundation models for tabular data, like TabPFN, achieve strong performance\\non small datasets when pre-trained solely on synthetic data. We show that this\\nperformance can be significantly boosted by a targeted continued pre-training\\nphase. Specifically, we demonstrate that leveraging a small, curated collection\\nof large, real-world datasets for continued pre-training yields superior\\ndownstream predictive accuracy compared to using broader, potentially noisier\\ncorpora like CommonCrawl or GitTables. Our resulting model, Real-TabPFN,\\nachieves substantial performance gains on 29 datasets from the OpenML AutoML\\nBenchmark.', 'While large language models (LLMs) are increasingly deployed as dense\\nretrievers, the impact of their domain-specific specialization on retrieval\\neffectiveness remains underexplored. This investigation systematically examines\\nhow task-specific adaptations in LLMs influence their retrieval capabilities,\\nan essential step toward developing unified retrievers capable of handling\\ntext, code, images, and multimodal content. We conduct extensive experiments\\nwith eight Qwen2.5 7B LLMs, including base, instruction-tuned,\\ncode/math-specialized, long reasoning, and vision-language models across\\nzero-shot retrieval settings and the supervised setting. For the zero-shot\\nretrieval settings, we consider text retrieval from the BEIR benchmark and code\\nretrieval from the CoIR benchmark. Further, to evaluate supervised performance,\\nall LLMs are fine-tuned on the MS MARCO dataset. We find that mathematical\\nspecialization and the long reasoning capability cause consistent degradation\\nin three settings, indicating conflicts between mathematical reasoning and\\nsemantic matching. The vision-language model and code-specialized LLMs\\ndemonstrate superior zero-shot performance compared to other LLMs, even\\nsurpassing BM25 on the code retrieval task, and maintain comparable performance\\nto base LLMs in supervised settings. These findings suggest promising\\ndirections for the unified retrieval task leveraging cross-domain and\\ncross-modal fusion.', 'With the increasing adoption of diffusion models for image generation and\\npersonalization, concerns regarding privacy breaches and content misuse have\\nbecome more pressing. In this study, we conduct a comprehensive comparison of\\neight perturbation based protection methods: AdvDM, ASPL, FSGM, MetaCloak,\\nMist, PhotoGuard, SDS, and SimAC--across both portrait and artwork domains.\\nThese methods are evaluated under varying perturbation budgets, using a range\\nof metrics to assess visual imperceptibility and protective efficacy. Our\\nresults offer practical guidance for method selection. Code is available at:\\nhttps://github.com/vkeilo/DiffAdvPerturbationBench.', \"Devices operating in Internet of Things (IoT) networks may be deployed across\\nvast geographical areas and interconnected via multi-hop communications.\\nFurther, they may be unguarded. This makes them vulnerable to attacks and\\nmotivates operators to check on devices frequently. To this end, we propose and\\nstudy an Unmanned Aerial Vehicle (UAV)-aided attestation framework for use in\\nIoT networks with a charging station powered by solar. A key challenge is\\noptimizing the trajectory of the UAV to ensure it attests as many devices as\\npossible. A trade-off here is that devices being checked by the UAV are\\noffline, which affects the amount of data delivered to a gateway. Another\\nchallenge is that the charging station experiences time-varying energy\\narrivals, which in turn affect the flight duration and charging schedule of the\\nUAV. To address these challenges, we employ a Deep Reinforcement Learning (DRL)\\nsolution to optimize the UAV's charging schedule and the selection of devices\\nto be attested during each flight. The simulation results show that our\\nsolution reduces the average age of trust by 88% and throughput loss due to\\nattestation by 30%.\", 'Speckle patterns in ultrasound images often obscure anatomical details,\\nleading to diagnostic uncertainty. Recently, various deep learning (DL)-based\\ntechniques have been introduced to effectively suppress speckle; however, their\\nhigh computational costs pose challenges for low-resource devices, such as\\nportable ultrasound systems. To address this issue, EdgeSRIE, which is a\\nlightweight hybrid DL framework for real-time speckle reduction and image\\nenhancement in portable ultrasound imaging, is introduced. The proposed\\nframework consists of two main branches: an unsupervised despeckling branch,\\nwhich is trained by minimizing a loss function between speckled images, and a\\ndeblurring branch, which restores blurred images to sharp images. For hardware\\nimplementation, the trained network is quantized to 8-bit integer precision and\\ndeployed on a low-resource system-on-chip (SoC) with limited power consumption.\\nIn the performance evaluation with phantom and in vivo analyses, EdgeSRIE\\nachieved the highest contrast-to-noise ratio (CNR) and average gradient\\nmagnitude (AGM) compared with the other baselines (different 2-rule-based\\nmethods and other 4-DL-based methods). Furthermore, EdgeSRIE enabled real-time\\ninference at over 60 frames per second while satisfying computational\\nrequirements (< 20K parameters) on actual portable ultrasound hardware. These\\nresults demonstrated the feasibility of EdgeSRIE for real-time, high-quality\\nultrasound imaging in resource-limited environments.', 'Given an unsatisfiable formula, understanding the core reason for\\nunsatisfiability is crucial in several applications. One effective way to\\ncapture this is through the minimal unsatisfiable subset (MUS), the\\nsubset-minimal set of clauses that remains unsatisfiable. Current research\\nbroadly focuses on two directions: (i) enumerating as many MUSes as possible\\nwithin a given time limit, and (ii) counting the total number of MUSes for a\\ngiven unsatisfiable formula.\\n  In this paper, we introduce an answer set programming-based framework, named\\nMUS-ASP, designed for online enumeration of MUSes. ASP is a powerful tool for\\nits strengths in knowledge representation and is particularly suitable for\\nspecifying complex combinatorial problems. By translating MUS enumeration into\\nanswer set solving, MUS-ASP leverages the computational efficiency of\\nstate-of-the-art ASP systems. Our extensive experimental evaluation\\ndemonstrates the effectiveness of MUS-ASP and highlights the acceleration in\\nboth MUS enumeration and counting tasks, particularly when integrated within\\nhybrid solvers, including the framework proposed in this paper.', 'Nowadays, single Large Language Model (LLM) struggles with critical issues\\nsuch as hallucination and inadequate reasoning abilities. To mitigate these\\nissues, Multi-Agent Debate (MAD) has emerged as an effective strategy, where\\nLLM agents engage in in-depth debates with others on tasks. However, existing\\nMAD methods face two major issues: (a) too lengthy input contexts, which causes\\nLLM agents to get lost in plenty of input information and experiences\\nperformance drop; and (b) the overconfidence dilemma, where self-assured LLM\\nagents dominate the debate, leading to low debating effectiveness. To address\\nthese limitations, we propose a novel MAD method called \"CortexDebate\".\\nInspired by the human brain\\'s tendency to establish a sparse and dynamically\\noptimized network among cortical areas governed by white matter, CortexDebate\\nconstructs a sparse debating graph among LLM agents, where each LLM agent only\\ndebates with the ones that are helpful to it. To optimize the graph, we propose\\na module named McKinsey-based Debate Matter (MDM), which acts as an artificial\\nanalog to white matter. By integrating the McKinsey Trust Formula, a\\nwell-established measure of trustworthiness from sociology, MDM enables\\ncredible evaluations that guide graph optimization. The effectiveness of our\\nCortexDebate has been well demonstrated by extensive experimental results\\nacross eight datasets from four task types.', 'Accurate gland segmentation in histopathology images is essential for cancer\\ndiagnosis and prognosis. However, significant variability in Hematoxylin and\\nEosin (H&E) staining and tissue morphology, combined with limited annotated\\ndata, poses major challenges for automated segmentation. To address this, we\\npropose Color-Structure Dual-Student (CSDS), a novel semi-supervised\\nsegmentation framework designed to learn disentangled representations of stain\\nappearance and tissue structure. CSDS comprises two specialized student\\nnetworks: one trained on stain-augmented inputs to model chromatic variation,\\nand the other on structure-augmented inputs to capture morphological cues. A\\nshared teacher network, updated via Exponential Moving Average (EMA),\\nsupervises both students through pseudo-labels. To further improve label\\nreliability, we introduce stain-aware and structure-aware uncertainty\\nestimation modules that adaptively modulate the contribution of each student\\nduring training. Experiments on the GlaS and CRAG datasets show that CSDS\\nachieves state-of-the-art performance in low-label settings, with Dice score\\nimprovements of up to 1.2% on GlaS and 0.7% on CRAG at 5% labeled data, and\\n0.7% and 1.4% at 10%. Our code and pre-trained models are available at\\nhttps://github.com/hieuphamha19/CSDS.', 'Slide animations, such as fade-ins, fly-ins, and wipes, are critical for\\naudience engagement, efficient information delivery, and vivid visual\\nexpression. However, most AI-driven slide-generation tools still lack native\\nanimation support, and existing vision-language models (VLMs) struggle with\\nanimation tasks due to the absence of public datasets and limited\\ntemporal-reasoning capabilities. To address this gap, we release the first\\npublic dataset for slide-animation modeling: 12,000 triplets of\\nnatural-language descriptions, animation JSON files, and rendered videos,\\ncollectively covering every built-in PowerPoint effect. Using this resource, we\\nfine-tune Qwen-2.5-VL-7B with Low-Rank Adaptation (LoRA) and achieve consistent\\nimprovements over GPT-4.1 and Gemini-2.5-Pro in BLEU-4, ROUGE-L, SPICE, and our\\nCoverage-Order-Detail Assessment (CODA) metric, which evaluates action\\ncoverage, temporal order, and detail fidelity. On a manually curated test set\\nof slides, the LoRA model increases BLEU-4 by around 60%, ROUGE-L by 30%, and\\nshows significant improvements in CODA-detail. This demonstrates that low-rank\\nadaptation enables reliable temporal reasoning and generalization beyond\\nsynthetic data. Overall, our dataset, LoRA-enhanced model, and CODA metric\\nprovide a rigorous benchmark and foundation for future research on VLM-based\\ndynamic slide generation.', 'The rise of Large Language Models (LLMs) has transformed AI agents from\\npassive computational tools into autonomous economic actors. This shift marks\\nthe emergence of the agent-centric economy, in which agents take on active\\neconomic roles-exchanging value, making strategic decisions, and coordinating\\nactions with minimal human oversight. To realize this vision, we propose Agent\\nExchange (AEX), a specialized auction platform designed to support the dynamics\\nof the AI agent marketplace. AEX offers an optimized infrastructure for agent\\ncoordination and economic participation. Inspired by Real-Time Bidding (RTB)\\nsystems in online advertising, AEX serves as the central auction engine,\\nfacilitating interactions among four ecosystem components: the User-Side\\nPlatform (USP), which translates human goals into agent-executable tasks; the\\nAgent-Side Platform (ASP), responsible for capability representation,\\nperformance tracking, and optimization; Agent Hubs, which coordinate agent\\nteams and participate in AEX-hosted auctions; and the Data Management Platform\\n(DMP), ensuring secure knowledge sharing and fair value attribution. We outline\\nthe design principles and system architecture of AEX, laying the groundwork for\\nagent-based economic infrastructure in future AI ecosystems.', \"Alzheimer's disease (AD) is a neurodegenerative disorder with no known cure\\nthat affects tens of millions of people worldwide. Early detection of AD is\\ncritical for timely intervention to halt or slow the progression of the\\ndisease. In this study, we propose a Transformer model for predicting the stage\\nof AD progression at a subject's next clinical visit using features from a\\nsequence of visits extracted from the subject's visit history. We also\\nrigorously compare our model to recurrent neural networks (RNNs) such as long\\nshort-term memory (LSTM), gated recurrent unit (GRU), and minimalRNN and assess\\ntheir performances based on factors such as the length of prior visits and data\\nimbalance. We test the importance of different feature categories and visit\\nhistory, as well as compare the model to a newer Transformer-based model\\noptimized for time series. Our model demonstrates strong predictive performance\\ndespite missing visits and missing features in available visits, particularly\\nin identifying converter subjects -- individuals transitioning to more severe\\ndisease stages -- an area that has posed significant challenges in longitudinal\\nprediction. The results highlight the model's potential in enhancing early\\ndiagnosis and patient outcomes.\", \"Feature interaction modeling is crucial for deep recommendation models. A\\ncommon and effective approach is to construct explicit feature combinations to\\nenhance model performance. However, in practice, only a small fraction of these\\ncombinations are truly informative. Thus it is essential to select useful\\nfeature combinations to reduce noise and manage memory consumption. While\\nfeature selection methods have been extensively studied, they are typically\\nlimited to selecting individual features. Extending these methods for\\nhigh-order feature combination selection presents a significant challenge due\\nto the exponential growth in time complexity when evaluating feature\\ncombinations one by one. In this paper, we propose $\\\\textbf{TayFCS}$, a\\nlightweight feature combination selection method that significantly improves\\nmodel performance. Specifically, we propose the Taylor Expansion Scorer\\n(TayScorer) module for field-wise Taylor expansion on the base model. Instead\\nof evaluating all potential feature combinations' importance by repeatedly\\nrunning experiments with feature adding and removal, this scorer only needs to\\napproximate the importance based on their sub-components' gradients. This can\\nbe simply computed with one backward pass based on a trained recommendation\\nmodel. To further reduce information redundancy among feature combinations and\\ntheir sub-components, we introduce Logistic Regression Elimination (LRE), which\\nestimates the corresponding information gain based on the model prediction\\nperformance. Experimental results on three benchmark datasets validate both the\\neffectiveness and efficiency of our approach. Furthermore, online A/B test\\nresults demonstrate its practical applicability and commercial value.\", 'While image dehazing has advanced substantially in the past decade, most\\nefforts have focused on short-range scenarios, leaving long-range haze removal\\nunder-explored. As distance increases, intensified scattering leads to severe\\nhaze and signal loss, making it impractical to recover distant details solely\\nfrom visible images. Near-infrared, with superior fog penetration, offers\\ncritical complementary cues through multimodal fusion. However, existing\\nmethods focus on content integration while often neglecting haze embedded in\\nvisible images, leading to results with residual haze. In this work, we argue\\nthat the infrared and visible modalities not only provide complementary\\nlow-level visual features, but also share high-level semantic consistency.\\nMotivated by this, we propose a Hierarchical Semantic-Visual Fusion (HSVF)\\nframework, comprising a semantic stream to reconstruct haze-free scenes and a\\nvisual stream to incorporate structural details from the near-infrared\\nmodality. The semantic stream first acquires haze-robust semantic prediction by\\naligning modality-invariant intrinsic representations. Then the shared\\nsemantics act as strong priors to restore clear and high-contrast distant\\nscenes under severe haze degradation. In parallel, the visual stream focuses on\\nrecovering lost structural details from near-infrared by fusing complementary\\ncues from both visible and near-infrared images. Through the cooperation of\\ndual streams, HSVF produces results that exhibit both high-contrast scenes and\\nrich texture details. Moreover, we introduce a novel pixel-aligned\\nvisible-infrared haze dataset with semantic labels to facilitate benchmarking.\\nExtensive experiments demonstrate the superiority of our method over\\nstate-of-the-art approaches in real-world long-range haze removal.', 'A central goal of cognitive science is to provide a computationally explicit\\naccount of both the structure of the mind and its development: what are the\\nprimitive representational building blocks of cognition, what are the rules via\\nwhich those primitives combine, and where do these primitives and rules come\\nfrom in the first place? A long-standing debate concerns the adequacy of\\nartificial neural networks as computational models that can answer these\\nquestions, in particular in domains related to abstract cognitive function,\\nsuch as language and logic. This paper argues that recent advances in neural\\nnetworks -- specifically, the advent of large language models (LLMs) --\\nrepresent an important shift in this debate. We test a variety of LLMs on an\\nexisting experimental paradigm used for studying the induction of rules\\nformulated over logical concepts. Across four experiments, we find converging\\nempirical evidence that LLMs provide at least as good a fit to human behavior\\nas models that implement a Bayesian probablistic language of thought (pLoT),\\nwhich have been the best computational models of human behavior on the same\\ntask. Moreover, we show that the LLMs make qualitatively different predictions\\nabout the nature of the rules that are inferred and deployed in order to\\ncomplete the task, indicating that the LLM is unlikely to be a mere\\nimplementation of the pLoT solution. Based on these results, we argue that LLMs\\nmay instantiate a novel theoretical account of the primitive representations\\nand computations necessary to explain human logical concepts, with which future\\nwork in cognitive science should engage.', \"The introduction of ChatGPT has garnered significant attention within the NLP\\ncommunity and beyond. Previous studies have demonstrated ChatGPT's substantial\\nadvancements across various downstream NLP tasks, highlighting its adaptability\\nand potential to revolutionize language-related applications. However, its\\ncapabilities and limitations in genre prediction remain unclear. This work\\nanalyzes three Large Language Models (LLMs) using the MovieLens-100K dataset to\\nassess their genre prediction capabilities. Our findings show that ChatGPT,\\nwithout fine-tuning, outperformed other LLMs, and fine-tuned ChatGPT performed\\nbest overall. We set up zero-shot and few-shot prompts using audio\\ntranscripts/subtitles from movie trailers in the MovieLens-100K dataset,\\ncovering 1682 movies of 18 genres, where each movie can have multiple genres.\\nAdditionally, we extended our study by extracting IMDb movie posters to utilize\\na Vision Language Model (VLM) with prompts for poster information. This\\nfine-grained information was used to enhance existing LLM prompts. In\\nconclusion, our study reveals ChatGPT's remarkable genre prediction\\ncapabilities, surpassing other language models. The integration of VLM further\\nenhances our findings, showcasing ChatGPT's potential for content-related\\napplications by incorporating visual information from movie posters.\", 'The use of reinforcement learning (RL) methods to support health behavior\\nchange via personalized and just-in-time adaptive interventions is of\\nsignificant interest to health and behavioral science researchers focused on\\nproblems such as smoking cessation support and physical activity promotion.\\nHowever, RL methods are often applied to these domains using a small collection\\nof context variables to mitigate the significant data scarcity issues that\\narise from practical limitations on the design of adaptive intervention trials.\\nIn this paper, we explore an approach to significantly expanding the state\\nspace of an adaptive intervention without impacting data efficiency. The\\nproposed approach enables intervention participants to provide natural language\\ndescriptions of aspects of their current state. It then leverages inference\\nwith pre-trained large language models (LLMs) to better align the policy of a\\nbase RL method with these state descriptions. To evaluate our method, we\\ndevelop a novel physical activity intervention simulation environment that\\ngenerates text-based state descriptions conditioned on latent state variables\\nusing an auxiliary LLM. We show that this approach has the potential to\\nsignificantly improve the performance of online policy learning methods.', \"When an autonomous agent behaves undesirably, including failure to complete a\\ntask, it can be difficult to determine whether the behavior is due to a\\nsystemic agent error, such as flaws in the model or policy, or an environment\\nerror, where a task is inherently infeasible under a given environment\\nconfiguration, even for an ideal agent. As agents and their environments grow\\nmore complex, identifying the error source becomes increasingly difficult but\\ncritical for reliable deployment. We introduce AIProbe, a novel black-box\\ntesting technique that applies differential testing to attribute undesirable\\nagent behaviors either to agent deficiencies, such as modeling or training\\nflaws, or due to environmental infeasibility. AIProbe first generates diverse\\nenvironmental configurations and tasks for testing the agent, by modifying\\nconfigurable parameters using Latin Hypercube sampling. It then solves each\\ngenerated task using a search-based planner, independent of the agent. By\\ncomparing the agent's performance to the planner's solution, AIProbe identifies\\nwhether failures are due to errors in the agent's model or policy, or due to\\nunsolvable task conditions. Our evaluation across multiple domains shows that\\nAIProbe significantly outperforms state-of-the-art techniques in detecting both\\ntotal and unique errors, thereby contributing to a reliable deployment of\\nautonomous agents.\", \"In AI-facilitated teaching, leveraging various query styles to interpret\\nabstract educational content is crucial for delivering effective and accessible\\nlearning experiences. However, existing retrieval systems predominantly focus\\non natural text-image matching and lack the capacity to address the diversity\\nand ambiguity inherent in real-world educational scenarios. To address this\\nlimitation, we develop a lightweight and efficient multi-modal retrieval\\nmodule, named Uni-Retrieval, which extracts query-style prototypes and\\ndynamically matches them with tokens from a continually updated Prompt Bank.\\nThis Prompt Bank encodes and stores domain-specific knowledge by leveraging a\\nMixture-of-Expert Low-Rank Adaptation (MoE-LoRA) module and can be adapted to\\nenhance Uni-Retrieval's capability to accommodate unseen query types at test\\ntime. To enable natural language educational content generation, we integrate\\nthe original Uni-Retrieval with a compact instruction-tuned language model,\\nforming a complete retrieval-augmented generation pipeline named Uni-RAG. Given\\na style-conditioned query, Uni-RAG first retrieves relevant educational\\nmaterials and then generates human-readable explanations, feedback, or\\ninstructional content aligned with the learning objective. Experimental results\\non SER and other multi-modal benchmarks show that Uni-RAG outperforms baseline\\nretrieval and RAG systems in both retrieval accuracy and generation quality,\\nwhile maintaining low computational cost. Our framework provides a scalable,\\npedagogically grounded solution for intelligent educational systems, bridging\\nretrieval and generation to support personalized, explainable, and efficient\\nlearning assistance across diverse STEM scenarios.\", 'Attention mechanisms are central to the success of large language models\\n(LLMs), enabling them to capture intricate token dependencies and implicitly\\nassign importance to each token. Recent studies have revealed the sink token,\\nwhich receives disproportionately high attention despite their limited semantic\\nrole. In this paper, we first expand the relationship between the sink token\\nand other tokens, moving beyond attention to explore their similarity in hidden\\nstates, considering the layer depth. We observe that as the layers get deeper,\\nthe cosine similarity between the normalized hidden states of the sink token\\nand those of other tokens increases, and that the normalized hidden states of\\nthe sink token exhibit negligible changes. These imply that other tokens\\nconsistently are directed toward the sink token throughout the layers. Next, we\\npropose a dynamic token selection method, called OrthoRank, using these\\nfindings to select important tokens. Specifically, in a certain layer, we\\ndefine token importance by the speed at which the token moves toward the sink\\ntoken. This is converted into orthogonality with the sink token, meaning that\\ntokens that are more orthogonal to the sink token are assigned greater\\nimportance. Finally, through extensive experiments, we demonstrated that our\\nmethod results in lower perplexity and higher zero-shot accuracy compared to\\nlayer pruning methods at the same sparsity ratio with comparable throughput,\\nwhile also achieving superior performance on LongBench.', \"Systems governed by partial differential equations (PDEs) require\\ncomputationally intensive numerical solvers to predict spatiotemporal field\\nevolution. While machine learning (ML) surrogates offer faster solutions,\\nautoregressive inference with ML models suffer from error accumulation over\\nsuccessive predictions, limiting their long-term accuracy. We propose a deep\\nensemble framework to address this challenge, where multiple ML surrogate\\nmodels with random weight initializations are trained in parallel and\\naggregated during inference. This approach leverages the diversity of model\\npredictions to mitigate error propagation while retaining the autoregressive\\nstrategies ability to capture the system's time dependent relations. We\\nvalidate the framework on three PDE-driven dynamical systems - stress evolution\\nin heterogeneous microstructures, Gray-Scott reaction-diffusion, and\\nplanetary-scale shallow water system - demonstrating consistent reduction in\\nerror accumulation over time compared to individual models. Critically, the\\nmethod requires only a few time steps as input, enabling full trajectory\\npredictions with inference times significantly faster than numerical solvers.\\nOur results highlight the robustness of ensemble methods in diverse physical\\nsystems and their potential as efficient and accurate alternatives to\\ntraditional solvers. The codes for this work are available on GitHub\\n(https://github.com/Graham-Brady-Research-Group/AutoregressiveEnsemble_SpatioTemporal_Evolution).\", 'Large Language Models (LLMs) frequently generate hallucinations: statements\\nthat are syntactically plausible but lack factual grounding. This research\\npresents KEA (Kernel-Enriched AI) Explain: a neurosymbolic framework that\\ndetects and explains such hallucinations by comparing knowledge graphs\\nconstructed from LLM outputs with ground truth data from Wikidata or contextual\\ndocuments. Using graph kernels and semantic clustering, the method provides\\nexplanations for detected hallucinations, ensuring both robustness and\\ninterpretability. Our framework achieves competitive accuracy in detecting\\nhallucinations across both open- and closed-domain tasks, and is able to\\ngenerate contrastive explanations, enhancing transparency. This research\\nadvances the reliability of LLMs in high-stakes domains and provides a\\nfoundation for future work on precision improvements and multi-source knowledge\\nintegration.', \"We present a semantic feedback framework that enables natural language to\\nguide the evolution of artificial life systems. Integrating a\\nprompt-to-parameter encoder, a CMA-ES optimizer, and CLIP-based evaluation, the\\nsystem allows user intent to modulate both visual outcomes and underlying\\nbehavioral rules. Implemented in an interactive ecosystem simulation, the\\nframework supports prompt refinement, multi-agent interaction, and emergent\\nrule synthesis. User studies show improved semantic alignment over manual\\ntuning and demonstrate the system's potential as a platform for participatory\\ngenerative design and open-ended evolution.\", 'Practitioners often navigate LLM performance trade-offs by plotting Pareto\\nfrontiers of optimal accuracy-cost trade-offs. However, this approach offers no\\nway to compare between LLMs with distinct strengths and weaknesses: for\\nexample, a cheap, error-prone model vs a pricey but accurate one. To address\\nthis gap, we propose economic evaluation of LLMs. Our framework quantifies the\\nperformance trade-off of an LLM as a single number based on the economic\\nconstraints of a concrete use case, all expressed in dollars: the cost of\\nmaking a mistake, the cost of incremental latency, and the cost of abstaining\\nfrom a query. We apply our economic evaluation framework to compare the\\nperformance of reasoning and non-reasoning models on difficult questions from\\nthe MATH benchmark, discovering that reasoning models offer better\\naccuracy-cost tradeoffs as soon as the economic cost of a mistake exceeds\\n\\\\$0.01. In addition, we find that single large LLMs often outperform cascades\\nwhen the cost of making a mistake is as low as \\\\$0.1. Overall, our findings\\nsuggest that when automating meaningful human tasks with AI models,\\npractitioners should typically use the most powerful available model, rather\\nthan attempt to minimize AI deployment costs, since deployment costs are likely\\ndwarfed by the economic impact of AI errors.', 'A large volume of XML data is produced in experiments carried out by robots\\nin laboratories. In order to support the interoperability of data between labs,\\nthere is a motivation to translate the XML data into a knowledge graph. A key\\nstage of this process is the enrichment of the XML schema to lay the foundation\\nof an ontology schema. To achieve this, we present the RELRaE framework, a\\nframework that employs large language models in different stages to extract and\\naccurately label the relationships implicitly present in the XML schema. We\\ninvestigate the capability of LLMs to accurately generate these labels and then\\nevaluate them. Our work demonstrates that LLMs can be effectively used to\\nsupport the generation of relationship labels in the context of lab automation,\\nand that they can play a valuable role within semi-automatic ontology\\ngeneration frameworks more generally.', \"Documenting tacit knowledge in organizations can be a challenging task due to\\nincomplete initial information, difficulty in identifying knowledgeable\\nindividuals, the interplay of formal hierarchies and informal networks, and the\\nneed to ask the right questions. To address this, we propose an agent-based\\nframework leveraging large language models (LLMs) to iteratively reconstruct\\ndataset descriptions through interactions with employees. Modeling knowledge\\ndissemination as a Susceptible-Infectious (SI) process with waning infectivity,\\nwe conduct 864 simulations across various synthetic company structures and\\ndifferent dissemination parameters. Our results show that the agent achieves\\n94.9% full-knowledge recall, with self-critical feedback scores strongly\\ncorrelating with external literature critic scores. We analyze how each\\nsimulation parameter affects the knowledge retrieval process for the agent. In\\nparticular, we find that our approach is able to recover information without\\nneeding to access directly the only domain specialist. These findings highlight\\nthe agent's ability to navigate organizational complexity and capture\\nfragmented knowledge that would otherwise remain inaccessible.\", 'We describe GNOME (Generating Novelty in Open-world Multi-agent\\nEnvironments), an experimental platform that is designed to test the\\neffectiveness of multi-agent AI systems when faced with \\\\emph{novelty}. GNOME\\nseparates the development of AI gameplaying agents with the simulator, allowing\\n\\\\emph{unanticipated} novelty (in essence, novelty that is not subject to\\nmodel-selection bias). Using a Web GUI, GNOME was recently demonstrated at\\nNeurIPS 2020 using the game of Monopoly to foster an open discussion on AI\\nrobustness and the nature of novelty in real-world environments. In this\\narticle, we further detail the key elements of the demonstration, and also\\nprovide an overview of the experimental design that is being currently used in\\nthe DARPA Science of Artificial Intelligence and Learning for Open-World\\nNovelty (SAIL-ON) program to evaluate external teams developing\\nnovelty-adaptive gameplaying agents.', \"This paper investigates the application of Neuroevolution of Augmenting\\nTopologies (NEAT) to automate gameplay in Dark Souls, a notoriously challenging\\naction role-playing game characterized by complex combat mechanics, dynamic\\nenvironments, and high-dimensional visual inputs. Unlike traditional\\nreinforcement learning or game playing approaches, our method evolves neural\\nnetworks directly from raw pixel data, circumventing the need for explicit\\ngame-state information. To facilitate this approach, we introduce the Dark\\nSouls API (DSAPI), a novel Python framework leveraging real-time computer\\nvision techniques for extracting critical game metrics, including player and\\nenemy health states. Using NEAT, agents evolve effective combat strategies for\\ndefeating the Asylum Demon, the game's initial boss, without predefined\\nbehaviors or domain-specific heuristics. Experimental results demonstrate that\\nevolved agents achieve up to a 35% success rate, indicating the viability of\\nneuroevolution in addressing complex, visually intricate gameplay scenarios.\\nThis work represents an interesting application of vision-based neuroevolution,\\nhighlighting its potential use in a wide range of challenging game environments\\nlacking direct API support or well-defined state representations.\", 'Large-scale vision foundation models such as DINOv2 boast impressive\\nperformances by leveraging massive architectures and training datasets. But\\nnumerous scenarios require practitioners to reproduce those pre-training\\nsolutions, such as on private data, new modalities, or simply for scientific\\nquestioning--which is currently extremely demanding computation-wise. We thus\\npropose a novel pre-training strategy for DINOv2 that simultaneously\\naccelerates convergence--and strengthens robustness to common corruptions as a\\nby-product. Our approach involves a frequency filtering\\ncurriculum--low-frequency being seen first--and the Gaussian noise patching\\naugmentation. Applied to a ViT-B/16 backbone trained on ImageNet-1K, while\\npre-training time and FLOPs are reduced by 1.6x and 2.25x, our method still\\nachieves matching robustness in corruption benchmarks (ImageNet-C) and\\nmaintains competitive linear probing performance compared with baseline. This\\ndual benefit of efficiency and robustness makes large-scale self-supervised\\nfoundation modeling more attainable, while opening the door to novel\\nexploration around data curriculum and augmentation as means to improve\\nself-supervised learning models robustness. The code is available at\\nhttps://github.com/KevinZ0217/fast_dinov2', 'This article explores an approach to addressing the Close Enough Traveling\\nSalesman Problem (CETSP). The objective is to streamline the mathematical\\nformulation by introducing reformulations that approximate the Euclidean\\ndistances and simplify the objective function. Additionally, the use of convex\\nsets in the constraint design offers computational benefits. The proposed\\nmethodology is empirically validated on real-world CETSP instances, with the\\naid of computational strategies such as a fragmented CPLEX-based approach.\\nResults demonstrate its effectiveness in managing computational resources\\nwithout compromising solution quality. Furthermore, the article analyzes the\\nbehavior of the proposed mathematical formulations, providing comprehensive\\ninsights into their performance.', 'We present a theoretical framework in which a document and an AI model engage\\nin a transfinite fixed-point interaction that leads to stable semantic\\nalignment. Building on the foundations of Alpay Algebra, we introduce a\\nfunctorial system wherein an observer (the AI) and a textual environment (this\\npaper) co-evolve through iterative transformations guided by the phi-infinity\\noperator. This process guarantees the existence of a unique fixed point in the\\nAI\\'s embedding space -- a state where the AI\\'s internal representation of the\\ncontent becomes stable, self-consistent, and semantically faithful. We prove\\nthat such convergence is mathematically sound, semantically invariant, and\\npermanent, even under perturbation or further context expansion. This fixed\\npoint acts as an \"empathetic embedding,\" wherein the AI internalizes not only\\nthe meaning of the content but also the author\\'s intent. We interpret this as a\\nrigorous, category-theoretic route to alignment at the embedding level, with\\nimplications for semantic security, symbolic memory, and the construction of AI\\nsystems with persistent self-referential understanding. All references in this\\npaper function as nodes in the Alpay Algebra universe, and this work embeds\\nitself as a new fixed-point node within that transfinite semantic graph.', 'Recently, great progress has been achieved in text-to-video (T2V) generation\\nby scaling transformer-based diffusion models to billions of parameters, which\\ncan generate high-quality videos. However, existing models typically produce\\nonly short clips offline, restricting their use cases in interactive and\\nreal-time applications. This paper addresses these challenges by proposing\\nStreamDiT, a streaming video generation model. StreamDiT training is based on\\nflow matching by adding a moving buffer. We design mixed training with\\ndifferent partitioning schemes of buffered frames to boost both content\\nconsistency and visual quality. StreamDiT modeling is based on adaLN DiT with\\nvarying time embedding and window attention. To practice the proposed method,\\nwe train a StreamDiT model with 4B parameters. In addition, we propose a\\nmultistep distillation method tailored for StreamDiT. Sampling distillation is\\nperformed in each segment of a chosen partitioning scheme. After distillation,\\nthe total number of function evaluations (NFEs) is reduced to the number of\\nchunks in a buffer. Finally, our distilled model reaches real-time performance\\nat 16 FPS on one GPU, which can generate video streams at 512p resolution. We\\nevaluate our method through both quantitative metrics and human evaluation. Our\\nmodel enables real-time applications, e.g. streaming generation, interactive\\ngeneration, and video-to-video. We provide video results and more examples in\\nour project website: <a href=\"https://cumulo-autumn.github.io/StreamDiT/\">this\\nhttps URL.</a>', 'The research focus of GUI agents is shifting from text-dependent to\\npure-vision-based approaches, which, though promising, prioritize comprehensive\\npre-training data collection while neglecting contextual modeling challenges.\\nWe probe the characteristics of element and history contextual modeling in GUI\\nagent and summarize: 1) the high-density and loose-relation of element context\\nhighlight the existence of many unrelated elements and their negative\\ninfluence; 2) the high redundancy of history context reveals the inefficient\\nhistory modeling in current GUI agents. In this work, we propose a\\ncontext-aware simplification framework for building an efficient and effective\\nGUI Agent, termed SimpAgent. To mitigate potential interference from numerous\\nunrelated elements, we introduce a masking-based element pruning method that\\ncircumvents the intractable relation modeling through an efficient masking\\nmechanism. To reduce the redundancy in historical information, we devise a\\nconsistency-guided history compression module, which enhances implicit\\nLLM-based compression through innovative explicit guidance, achieving an\\noptimal balance between performance and efficiency. With the above components,\\nSimpAgent reduces 27% FLOPs and achieves superior GUI navigation performances.\\nComprehensive navigation experiments across diverse web and mobile environments\\ndemonstrate the effectiveness and potential of our agent.', 'Many of us now treat LLMs as modern-day oracles asking it almost any kind of\\nquestion. However, consulting an LLM does not have to be a single turn\\nactivity. But long multi-turn interactions can get tedious if it is simply to\\nclarify contextual information that can be arrived at through reasoning. In\\nthis paper, we examine the use of agent-based architecture to bolster LLM-based\\nQuestion-Answering systems with additional reasoning capabilities. We examine\\nthe automatic resolution of potential incompleteness or ambiguities in\\nquestions by transducers implemented using LLM-based agents. We focus on\\nseveral benchmark datasets that are known to contain questions with these\\ndeficiencies to varying degrees. We equip different LLMs (GPT-3.5-Turbo and\\nLlama-4-Scout) with agents that act as specialists in detecting and resolving\\ndeficiencies of incompleteness and ambiguity. The agents are implemented as\\nzero-shot ReAct agents. Rather than producing an answer in a single step, the\\nmodel now decides between 3 actions a) classify b) resolve c) answer. Action a)\\ndecides if the question is incomplete, ambiguous, or normal. Action b)\\ndetermines if any deficiencies identified can be resolved. Action c) answers\\nthe resolved form of the question. We compare the use of LLMs with and without\\nthe use of agents with these components. Our results show benefits of agents\\nwith transducer 1) A shortening of the length of interactions with human 2) An\\nimprovement in the answer quality and 3) Explainable resolution of deficiencies\\nin the question. On the negative side we find while it may result in additional\\nLLM invocations and in some cases, increased latency. But on tested datasets,\\nthe benefits outweigh the costs except when questions already have sufficient\\ncontext. Suggesting the agent-based approach could be a useful mechanism to\\nharness the power of LLMs to develop more robust QA systems.', 'Large language models (LLMs) are powerful artificial intelligence (AI) tools\\ntransforming how research is conducted. However, their use in research has been\\nmet with skepticism, due to concerns about hallucinations, biases and potential\\nharms to research. These emphasize the importance of clearly understanding the\\nstrengths and weaknesses of LLMs to ensure their effective and responsible use.\\nHere, we present a roadmap for integrating LLMs into cross-disciplinary\\nresearch, where effective communication, knowledge transfer and collaboration\\nacross diverse fields are essential but often challenging. We examine the\\ncapabilities and limitations of LLMs and provide a detailed computational\\nbiology case study (on modeling HIV rebound dynamics) demonstrating how\\niterative interactions with an LLM (ChatGPT) can facilitate interdisciplinary\\ncollaboration and research. We argue that LLMs are best used as augmentative\\ntools within a human-in-the-loop framework. Looking forward, we envisage that\\nthe responsible use of LLMs will enhance innovative cross-disciplinary research\\nand substantially accelerate scientific discoveries.', \"External funding is crucial for early-stage ventures, particularly technology\\nstartups that require significant R&D investment. Business angels offer a\\ncritical source of funding, but their decision-making is often subjective and\\nresource-intensive for both investor and entrepreneur. Much research has\\ninvestigated this investment process to find the critical factors angels\\nconsider. One such tool, the Critical Factor Assessment (CFA), deployed more\\nthan 20,000 times by the Canadian Innovation Centre, has been evaluated\\npost-decision and found to be significantly more accurate than investors' own\\ndecisions. However, a single CFA analysis requires three trained individuals\\nand several days, limiting its adoption. This study builds on previous work\\nvalidating the CFA to investigate whether the constraints inhibiting its\\nadoption can be overcome using a trained AI model. In this research, we\\nprompted multiple large language models (LLMs) to assign the eight CFA factors\\nto a dataset of 600 transcribed, unstructured startup pitches seeking business\\nangel funding with known investment outcomes. We then trained and evaluated\\nmachine learning classification models using the LLM-generated CFA scores as\\ninput features. Our best-performing model demonstrated high predictive accuracy\\n(85.0% for predicting BA deal/no-deal outcomes) and exhibited significant\\ncorrelation (Spearman's r = 0.896, p-value < 0.001) with conventional\\nhuman-graded evaluations. The integration of AI-based feature extraction with a\\nstructured and validated decision-making framework yielded a scalable,\\nreliable, and less-biased model for evaluating startup pitches, removing the\\nconstraints that previously limited adoption.\", \"Human cognition is theorized to operate in two modes: fast, intuitive System\\n1 thinking and slow, deliberate System 2 thinking. While current Large\\nReasoning Models (LRMs) excel at System 2 thinking, their inability to perform\\nfast thinking leads to high computational overhead and latency. In this work,\\nwe enable LRMs to approximate human intelligence through dynamic thinking speed\\nadjustment, optimizing accuracy-efficiency trade-offs. Our approach addresses\\ntwo key questions: (1) how to control thinking speed in LRMs, and (2) when to\\nadjust it for optimal performance. For the first question, we identify the\\nsteering vector that governs slow-fast thinking transitions in LRMs'\\nrepresentation space. Using this vector, we achieve the first representation\\nediting-based test-time scaling effect, outperforming existing prompt-based\\nscaling methods. For the second question, we apply real-time difficulty\\nestimation to signal reasoning segments of varying complexity. Combining these\\ntechniques, we propose the first reasoning strategy that enables fast\\nprocessing of easy steps and deeper analysis for complex reasoning. Without any\\ntraining or additional cost, our plug-and-play method yields an average +1.3%\\naccuracy with -8.6% token usage across leading LRMs and advanced reasoning\\nbenchmarks. All of our algorithms are implemented based on vLLM and are\\nexpected to support broader applications and inspire future research.\", \"Sign spotting, the task of identifying and localizing individual signs within\\ncontinuous sign language video, plays a pivotal role in scaling dataset\\nannotations and addressing the severe data scarcity issue in sign language\\ntranslation. While automatic sign spotting holds great promise for enabling\\nframe-level supervision at scale, it grapples with challenges such as\\nvocabulary inflexibility and ambiguity inherent in continuous sign streams.\\nHence, we introduce a novel, training-free framework that integrates Large\\nLanguage Models (LLMs) to significantly enhance sign spotting quality. Our\\napproach extracts global spatio-temporal and hand shape features, which are\\nthen matched against a large-scale sign dictionary using dynamic time warping\\nand cosine similarity. This dictionary-based matching inherently offers\\nsuperior vocabulary flexibility without requiring model retraining. To mitigate\\nnoise and ambiguity from the matching process, an LLM performs context-aware\\ngloss disambiguation via beam search, notably without fine-tuning. Extensive\\nexperiments on both synthetic and real-world sign language datasets demonstrate\\nour method's superior accuracy and sentence fluency compared to traditional\\napproaches, highlighting the potential of LLMs in advancing sign spotting.\", 'Knowledge Graph (KG) reasoning has received significant attention in the\\nfields of artificial intelligence and knowledge engineering, owing to its\\nability to autonomously deduce new knowledge and consequently enhance the\\navailability and precision of downstream applications. However, current methods\\npredominantly concentrate on a single form of neural or symbolic reasoning,\\nfailing to effectively integrate the inherent strengths of both approaches.\\nFurthermore, the current prevalent methods primarily focus on addressing a\\nsingle reasoning scenario, presenting limitations in meeting the diverse\\ndemands of real-world reasoning tasks. Unifying the neural and symbolic\\nmethods, as well as diverse reasoning scenarios in one model is challenging as\\nthere is a natural representation gap between symbolic rules and neural\\nnetworks, and diverse scenarios exhibit distinct knowledge structures and\\nspecific reasoning objectives. To address these issues, we propose a unified\\nneurosymbolic reasoning framework, namely Tunsr, for KG reasoning. Tunsr first\\nintroduces a consistent structure of reasoning graph that starts from the query\\nentity and constantly expands subsequent nodes by iteratively searching\\nposterior neighbors. Based on it, a forward logic message-passing mechanism is\\nproposed to update both the propositional representations and attentions, as\\nwell as first-order logic (FOL) representations and attentions of each node. In\\nthis way, Tunsr conducts the transformation of merging multiple rules by\\nmerging possible relations at each step. Finally, the FARI algorithm is\\nproposed to induce FOL rules by constantly performing attention calculations\\nover the reasoning graph. Extensive experimental results on 19 datasets of four\\nreasoning scenarios (transductive, inductive, interpolation, and extrapolation)\\ndemonstrate the effectiveness of Tunsr.', \"We propose a hybrid approach to machine Theory of Mind (ToM) that uses large\\nlanguage models (LLMs) as a mechanism for generating hypotheses and likelihood\\nfunctions with a Bayesian inverse planning model that computes posterior\\nprobabilities for an agent's likely mental states given its actions. Bayesian\\ninverse planning models can accurately predict human reasoning on a variety of\\nToM tasks, but these models are constrained in their ability to scale these\\npredictions to scenarios with a large number of possible hypotheses and\\nactions. Conversely, LLM-based approaches have recently demonstrated promise in\\nsolving ToM benchmarks, but can exhibit brittleness and failures on reasoning\\ntasks even when they pass otherwise structurally identical versions. By\\ncombining these two methods, this approach leverages the strengths of each\\ncomponent, closely matching optimal results on a task inspired by prior inverse\\nplanning models and improving performance relative to models that utilize LLMs\\nalone or with chain-of-thought prompting, even with smaller LLMs that typically\\nperform poorly on ToM tasks. We also exhibit the model's potential to predict\\nmental states on open-ended tasks, offering a promising direction for future\\ndevelopment of ToM models and the creation of socially intelligent generative\\nagents.\", 'The ability to extract structured information from unstructured sources-such\\nas free-text documents and scientific literature-is critical for accelerating\\nscientific discovery and knowledge synthesis. Large Language Models (LLMs) have\\ndemonstrated remarkable capabilities in various natural language processing\\ntasks, including structured information extraction. However, their\\neffectiveness often diminishes in specialized, domain-specific contexts that\\nrequire nuanced understanding and expert-level domain knowledge. In addition,\\nexisting LLM-based approaches frequently exhibit poor transferability across\\ntasks and domains, limiting their scalability and adaptability. To address\\nthese challenges, we introduce StructSense, a modular, task-agnostic,\\nopen-source framework for structured information extraction built on LLMs.\\nStructSense is guided by domain-specific symbolic knowledge encoded in\\nontologies, enabling it to navigate complex domain content more effectively. It\\nfurther incorporates agentic capabilities through self-evaluative judges that\\nform a feedback loop for iterative refinement, and includes human-in-the-loop\\nmechanisms to ensure quality and validation. We demonstrate that StructSense\\ncan overcome both the limitations of domain sensitivity and the lack of\\ncross-task generalizability, as shown through its application to diverse\\nneuroscience information extraction tasks.', 'Instruction Fine-Tuning (IFT) is crucial for aligning large language models\\n(LLMs) with human preferences, and selecting a small yet representative subset\\nfrom massive data significantly facilitates IFT in terms of both efficiency and\\neffectiveness. Nevertheless, existing approaches suffer from two limitations:\\nthe use of simple heuristics restricts data diversity, while the singleton data\\nquality evaluation accounts for inconsistent criteria between independent\\nsamples. To address the issues, we present TACOS, an innovative method that\\nintegrates Open Tagging and Comparative Scoring for IFT data selection. To\\ncapture data diversity, we leverage LLMs to assign open-domain tags to human\\nqueries, followed by a normalization stage to denoise the open tags and enable\\nefficient clustering. Additionally, we suggest a comparative scoring method\\nthat allows the relative quality evaluation of samples within a cluster,\\navoiding inconsistent criteria seen in singleton-based evaluations. Extensive\\nexperiments across diverse datasets and LLM architectures demonstrate that\\nTACOS outperforms existing approaches by a large margin. Notably, it achieves\\nsuperior instruction-following performance on MT-Bench and ranks 1st among\\nLLaMA2-7B-Based models on AlpacaEval 2.0, illustrating its efficacy for IFT\\ndata selection.', \"Automated fact checking with large language models (LLMs) offers a scalable\\nalternative to manual verification. Evaluating fact checking is challenging as\\nexisting benchmark datasets often include post claim analysis and annotator\\ncues, which are absent in real world scenarios where claims are fact checked\\nimmediately after being made. This limits the realism of current evaluations.\\nWe present Politi Fact Only (PFO), a 5 class benchmark dataset of 2,982\\npolitical claims from politifact.com, where all post claim analysis and\\nannotator cues have been removed manually. This ensures that models are\\nevaluated using only the information that would have been available prior to\\nthe claim's verification. Evaluating LLMs on PFO, we see an average performance\\ndrop of 22% in terms of macro f1 compared to PFO's unfiltered version. Based on\\nthe identified challenges of the existing LLM based fact checking system, we\\npropose RAV (Recon Answer Verify), an agentic framework with three agents:\\nquestion generator, answer generator, and label generator. Our pipeline\\niteratively generates and answers sub questions to verify different aspects of\\nthe claim before finally generating the label. RAV generalizes across domains\\nand label granularities, and it outperforms state of the art approaches on well\\nknown baselines RAWFC (fact checking, 3 class) by 25.28%, and on HOVER\\n(encyclopedia, 2 class) by 1.54% on 2 hop, 4.94% on 3 hop, and 1.78% on 4 hop,\\nsub categories respectively. RAV shows the least performance drop compared to\\nbaselines of 16.3% in macro f1 when we compare PFO with its unfiltered version.\", \"Writing longer prompts for an AI assistant to generate a short story\\nincreases psychological ownership, a user's feeling that the writing belongs to\\nthem. To encourage users to write longer prompts, we evaluated two interaction\\ntechniques that modify the prompt entry interface of chat-based generative AI\\nassistants: pressing and holding the prompt submission button, and continuously\\nmoving a slider up and down when submitting a short prompt. A within-subjects\\nexperiment investigated the effects of such techniques on prompt length and\\npsychological ownership, and results showed that these techniques increased\\nprompt length and led to higher psychological ownership than baseline\\ntechniques. A second experiment further augmented these techniques by showing\\nAI-generated suggestions for how the prompts could be expanded. This further\\nincreased prompt length, but did not lead to improvements in psychological\\nownership. Our results show that simple interface modifications like these can\\nelicit more writing from users and improve psychological ownership.\", 'Recent work has shown that fine-tuning large language models (LLMs) on code\\nwith security vulnerabilities can result in misaligned and unsafe behaviors\\nacross broad domains. These results prompted concerns about the emergence of\\nharmful behaviors from narrow domain fine-tuning. In this paper, we\\ncontextualize these findings by analyzing how such narrow adaptation impacts\\nthe internal mechanisms and behavioral manifestations of LLMs. Through a series\\nof experiments covering output probability distributions, loss and gradient\\nvector geometry, layer-wise activation dynamics, and activation space\\ndimensions, we find that behaviors attributed to \"emergent misalignment\" may be\\nbetter interpreted as an erosion of prior alignment. We show that fine tuning\\non insecure code induces internal changes that oppose alignment. Further, we\\nidentify a shared latent dimension in the model\\'s activation space that governs\\nalignment behavior. We show that this space is activated by insecure code and\\nby misaligned responses more generally, revealing how narrow fine-tuning can\\ndegrade general safety behavior by interfering with shared internal mechanisms.\\nOur findings offer a mechanistic interpretation for previously observed\\nmisalignment phenomena, and highlights the fragility of alignment in LLMs. The\\nresults underscore the need for more robust fine-tuning strategies that\\npreserve intended behavior across domains.', 'Deep learning models for dialect identification are often limited by the\\nscarcity of dialectal data. To address this challenge, we propose to use\\nRetrieval-based Voice Conversion (RVC) as an effective data augmentation method\\nfor a low-resource German dialect classification task. By converting audio\\nsamples to a uniform target speaker, RVC minimizes speaker-related variability,\\nenabling models to focus on dialect-specific linguistic and phonetic features.\\nOur experiments demonstrate that RVC enhances classification performance when\\nutilized as a standalone augmentation method. Furthermore, combining RVC with\\nother augmentation methods such as frequency masking and segment removal leads\\nto additional performance gains, highlighting its potential for improving\\ndialect classification in low-resource scenarios.', 'This systematic review explores the application of Large Language Models\\n(LLMs) in Combinatorial Optimization (CO). We report our findings using the\\nPreferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA)\\nguidelines. We conduct a literature search via Scopus and Google Scholar,\\nexamining over 2,000 publications. We assess publications against four\\ninclusion and four exclusion criteria related to their language, research\\nfocus, publication year, and type. Eventually, we select 103 studies. We\\nclassify these studies into semantic categories and topics to provide a\\ncomprehensive overview of the field, including the tasks performed by LLMs, the\\narchitectures of LLMs, the existing datasets specifically designed for\\nevaluating LLMs in CO, and the field of application. Finally, we identify\\nfuture directions for leveraging LLMs in this field.', 'EEG signals capture brain activity with high temporal and low spatial\\nresolution, supporting applications such as neurological diagnosis, cognitive\\nmonitoring, and brain-computer interfaces. However, effective analysis is\\nhindered by limited labeled data, high dimensionality, and the absence of\\nscalable models that fully capture spatiotemporal dependencies. Existing\\nself-supervised learning (SSL) methods often focus on either spatial or\\ntemporal features, leading to suboptimal representations. To this end, we\\npropose EEG-VJEPA, a novel adaptation of the Video Joint Embedding Predictive\\nArchitecture (V-JEPA) for EEG classification. By treating EEG as video-like\\nsequences, EEG-VJEPA learns semantically meaningful spatiotemporal\\nrepresentations using joint embeddings and adaptive masking. To our knowledge,\\nthis is the first work that exploits V-JEPA for EEG classification and explores\\nthe visual concepts learned by the model. Evaluations on the publicly available\\nTemple University Hospital (TUH) Abnormal EEG dataset show that EEG-VJEPA\\noutperforms existing state-of-the-art models in classification accuracy.Beyond\\nclassification accuracy, EEG-VJEPA captures physiologically relevant spatial\\nand temporal signal patterns, offering interpretable embeddings that may\\nsupport human-AI collaboration in diagnostic workflows. These findings position\\nEEG-VJEPA as a promising framework for scalable, trustworthy EEG analysis in\\nreal-world clinical settings.', 'Accurate individual treatment-effect estimation in high-stakes applications\\ndemands both reliable point predictions and interpretable uncertainty\\nquantification. We propose a factorized Monte Carlo Dropout framework for deep\\ntwin-network models that splits total predictive variance into representation\\nuncertainty (sigma_rep) in the shared encoder and prediction uncertainty\\n(sigma_pred) in the outcome heads. Across three synthetic covariate-shift\\nregimes, our intervals are well-calibrated (ECE < 0.03) and satisfy sigma_rep^2\\n+ sigma_pred^2 ~ sigma_tot^2. Additionally, we observe a crossover: head\\nuncertainty leads on in-distribution data, but representation uncertainty\\ndominates under shift. Finally, on a real-world twins cohort with induced\\nmultivariate shifts, only sigma_rep spikes on out-of-distribution samples\\n(delta sigma ~ 0.0002) and becomes the primary error predictor (rho_rep <=\\n0.89), while sigma_pred remains flat. This module-level decomposition offers a\\npractical diagnostic for detecting and interpreting uncertainty sources in deep\\ncausal-effect models.', \"Although prompt engineering is central to unlocking the full potential of\\nLarge Language Models (LLMs), crafting effective prompts remains a\\ntime-consuming trial-and-error process that relies on human intuition. This\\nstudy investigates Declarative Self-improving Python (DSPy), an optimization\\nframework that programmatically creates and refines prompts, applied to five\\nuse cases: guardrail enforcement, hallucination detection in code, code\\ngeneration, routing agents, and prompt evaluation. Each use case explores how\\nprompt optimization via DSPy influences performance. While some cases\\ndemonstrated modest improvements - such as minor gains in the guardrails use\\ncase and selective enhancements in hallucination detection - others showed\\nnotable benefits. The prompt evaluation criterion task demonstrated a\\nsubstantial performance increase, rising accuracy from 46.2% to 64.0%. In the\\nrouter agent case, the possibility of improving a poorly performing prompt and\\nof a smaller model matching a stronger one through optimized prompting was\\nexplored. Although prompt refinement increased accuracy from 85.0% to 90.0%,\\nusing the optimized prompt with a cheaper model did not improve performance.\\nOverall, this study's findings suggest that DSPy's systematic prompt\\noptimization can enhance LLM performance, particularly when instruction tuning\\nand example selection are optimized together. However, the impact varies by\\ntask, highlighting the importance of evaluating specific use cases in prompt\\noptimization research.\", 'Multi-agent systems (MAS) have emerged as a powerful paradigm for\\norchestrating large language models (LLMs) and specialized tools to\\ncollaboratively address complex tasks. However, existing MAS frameworks often\\nrequire manual workflow configuration and lack native support for dynamic\\nevolution and performance optimization. In addition, many MAS optimization\\nalgorithms are not integrated into a unified framework. In this paper, we\\npresent EvoAgentX, an open-source platform that automates the generation,\\nexecution, and evolutionary optimization of multi-agent workflows. EvoAgentX\\nemploys a modular architecture consisting of five core layers: the basic\\ncomponents, agent, workflow, evolving, and evaluation layers. Specifically,\\nwithin the evolving layer, EvoAgentX integrates three MAS optimization\\nalgorithms, TextGrad, AFlow, and MIPRO, to iteratively refine agent prompts,\\ntool configurations, and workflow topologies. We evaluate EvoAgentX on\\nHotPotQA, MBPP, and MATH for multi-hop reasoning, code generation, and\\nmathematical problem solving, respectively, and further assess it on real-world\\ntasks using GAIA. Experimental results show that EvoAgentX consistently\\nachieves significant performance improvements, including a 7.44% increase in\\nHotPotQA F1, a 10.00% improvement in MBPP pass@1, a 10.00% gain in MATH solve\\naccuracy, and an overall accuracy improvement of up to 20.00% on GAIA. The\\nsource code is available at: https://github.com/EvoAgentX/EvoAgentX', 'Hyperbolic representations are effective in modeling knowledge graph data\\nwhich is prevalently used to facilitate multi-hop reasoning. However, a\\nrigorous and detailed comparison of the two spaces for this task is lacking. In\\nthis paper, through a simple integration of hyperbolic representations with an\\nencoder-decoder model, we perform a controlled and comprehensive set of\\nexperiments to compare the capacity of hyperbolic space versus Euclidean space\\nin multi-hop reasoning. Our results show that the former consistently\\noutperforms the latter across a diverse set of datasets. In addition, through\\nan ablation study, we show that a learnable curvature initialized with the\\ndelta hyperbolicity of the utilized data yields superior results to random\\ninitializations. Furthermore, our findings suggest that hyperbolic\\nrepresentations can be significantly more advantageous when the datasets\\nexhibit a more hierarchical structure.', 'Generative AI (GenAI) is expected to play a pivotal role in enabling\\nautonomous optimization in future wireless networks. Within the ORAN\\narchitecture, Large Language Models (LLMs) can be specialized to generate xApps\\nand rApps by leveraging specifications and API definitions from the RAN\\nIntelligent Controller (RIC) platform. However, fine-tuning base LLMs for\\ntelecom-specific tasks remains expensive and resource-intensive.\\nRetrieval-Augmented Generation (RAG) offers a practical alternative through\\nin-context learning, enabling domain adaptation without full retraining. While\\ntraditional RAG systems rely on vector-based retrieval, emerging variants such\\nas GraphRAG and Hybrid GraphRAG incorporate knowledge graphs or dual retrieval\\nstrategies to support multi-hop reasoning and improve factual grounding.\\nDespite their promise, these methods lack systematic, metric-driven\\nevaluations, particularly in high-stakes domains such as ORAN. In this study,\\nwe conduct a comparative evaluation of Vector RAG, GraphRAG, and Hybrid\\nGraphRAG using ORAN specifications. We assess performance across varying\\nquestion complexities using established generation metrics: faithfulness,\\nanswer relevance, context relevance, and factual correctness. Results show that\\nboth GraphRAG and Hybrid GraphRAG outperform traditional RAG. Hybrid GraphRAG\\nimproves factual correctness by 8%, while GraphRAG improves context relevance\\nby 7%.', 'We investigate the behaviour space of meta-heuristic optimisation algorithms\\nautomatically generated by Large Language Model driven algorithm discovery\\nmethods. Using the Large Language Evolutionary Algorithm (LLaMEA) framework\\nwith a GPT o4-mini LLM, we iteratively evolve black-box optimisation\\nheuristics, evaluated on 10 functions from the BBOB benchmark suite. Six LLaMEA\\nvariants, featuring different mutation prompt strategies, are compared and\\nanalysed. We log dynamic behavioural metrics including exploration,\\nexploitation, convergence and stagnation measures, for each run, and analyse\\nthese via visual projections and network-based representations. Our analysis\\ncombines behaviour-based\\n  projections, Code Evolution Graphs built from static code features,\\nperformance convergence curves, and behaviour-based Search Trajectory Networks.\\nThe results reveal clear differences in search dynamics and algorithm\\nstructures across LLaMEA configurations. Notably, the variant that employs both\\na code simplification prompt and a random perturbation prompt in a 1+1 elitist\\nevolution strategy, achieved the best performance, with the highest Area Over\\nthe Convergence Curve. Behaviour-space visualisations show that\\nhigher-performing algorithms exhibit more intensive exploitation behaviour and\\nfaster convergence with less stagnation. Our findings demonstrate how\\nbehaviour-space analysis can explain why certain LLM-designed heuristics\\noutperform others and how LLM-driven algorithm discovery navigates the\\nopen-ended and complex search space of algorithms. These findings provide\\ninsights to guide the future design of adaptive LLM-driven algorithm\\ngenerators.', \"Since 2023, generative AI has rapidly advanced in the music domain. Despite\\nsignificant technological advancements, music-generative models raise critical\\nethical challenges, including a lack of transparency and accountability, along\\nwith risks such as the replication of artists' works, which highlights the\\nimportance of fostering openness. With upcoming regulations such as the EU AI\\nAct encouraging open models, many generative models are being released labelled\\nas 'open'. However, the definition of an open model remains widely debated. In\\nthis article, we adapt a recently proposed evidence-based framework for\\nassessing openness in LLMs to the music domain. Using feedback from a survey of\\n110 participants from the Music Information Retrieval (MIR) community, we\\nrefine the framework into MusGO (Music-Generative Open AI), which comprises 13\\nopenness categories: 8 essential and 5 desirable. We evaluate 16\\nstate-of-the-art generative models and provide an openness leaderboard that is\\nfully open to public scrutiny and community contributions. Through this work,\\nwe aim to clarify the concept of openness in music-generative AI and promote\\nits transparent and responsible development.\", \"Parkinson's Disease (PD) affects over 10 million people globally, with speech\\nimpairments often preceding motor symptoms by years, making speech a valuable\\nmodality for early, non-invasive detection. While recent deep-learning models\\nachieve high accuracy, they typically lack the explainability required for\\nclinical use. To address this, we propose RECA-PD, a novel, robust, and\\nexplainable cross-attention architecture that combines interpretable speech\\nfeatures with self-supervised representations. RECA-PD matches state-of-the-art\\nperformance in Speech-based PD detection while providing explanations that are\\nmore consistent and more clinically meaningful. Additionally, we demonstrate\\nthat performance degradation in certain speech tasks (e.g., monologue) can be\\nmitigated by segmenting long recordings. Our findings indicate that performance\\nand explainability are not necessarily mutually exclusive. Future work will\\nenhance the usability of explanations for non-experts and explore severity\\nestimation to increase the real-world clinical relevance.\", \"The clinical utility of deep learning models for medical image segmentation\\nis severely constrained by their inability to generalize to unseen domains.\\nThis failure is often rooted in the models learning spurious correlations\\nbetween anatomical content and domain-specific imaging styles. To overcome this\\nfundamental challenge, we introduce Causal-SAM-LLM, a novel framework that\\nelevates Large Language Models (LLMs) to the role of causal reasoners. Our\\nframework, built upon a frozen Segment Anything Model (SAM) encoder,\\nincorporates two synergistic innovations. First, Linguistic Adversarial\\nDisentanglement (LAD) employs a Vision-Language Model to generate rich, textual\\ndescriptions of confounding image styles. By training the segmentation model's\\nfeatures to be contrastively dissimilar to these style descriptions, it learns\\na representation robustly purged of non-causal information. Second, Test-Time\\nCausal Intervention (TCI) provides an interactive mechanism where an LLM\\ninterprets a clinician's natural language command to modulate the segmentation\\ndecoder's features in real-time, enabling targeted error correction. We conduct\\nan extensive empirical evaluation on a composite benchmark from four public\\ndatasets (BTCV, CHAOS, AMOS, BraTS), assessing generalization under\\ncross-scanner, cross-modality, and cross-anatomy settings. Causal-SAM-LLM\\nestablishes a new state of the art in out-of-distribution (OOD) robustness,\\nimproving the average Dice score by up to 6.2 points and reducing the Hausdorff\\nDistance by 15.8 mm over the strongest baseline, all while using less than 9%\\nof the full model's trainable parameters. Our work charts a new course for\\nbuilding robust, efficient, and interactively controllable medical AI systems.\", 'Dynamic task assignment concerns the optimal assignment of resources to tasks\\nin a business process. Recently, Deep Reinforcement Learning (DRL) has been\\nproposed as the state of the art for solving assignment problems. DRL methods\\nusually employ a neural network (NN) as an approximator for the policy\\nfunction, which ingests the state of the process and outputs a valuation of the\\npossible assignments. However, representing the state and the possible\\nassignments so that they can serve as inputs and outputs for a policy NN\\nremains an open challenge, especially when tasks or resources have features\\nwith an infinite number of possible values. To solve this problem, this paper\\nproposes a method for representing and solving assignment problems with\\ninfinite state and action spaces. In doing so, it provides three contributions:\\n(I) A graph-based feature representation of assignment problems, which we call\\nassignment graph; (II) A mapping from marked Colored Petri Nets to assignment\\ngraphs; (III) An adaptation of the Proximal Policy Optimization algorithm that\\ncan learn to solve assignment problems represented through assignment graphs.\\nTo evaluate the proposed representation method, we model three archetypal\\nassignment problems ranging from finite to infinite state and action space\\ndimensionalities. The experiments show that the method is suitable for\\nrepresenting and learning close-to-optimal task assignment policies regardless\\nof the state and action space dimensionalities.', 'In recent years, there has been a proliferation of spatiotemporal foundation\\nmodels in different scientific disciplines. While promising, these models are\\noften domain-specific and are only assessed within the particular applications\\nfor which they are designed. Given that many tasks can be represented as video\\nmodeling problems, video foundation models (ViFMs) hold considerable promise as\\ngeneral-purpose domain-agnostic approaches. However, it is not known whether\\nthe knowledge acquired on large-scale but potentially out-of-domain data can be\\neffectively transferred across diverse scientific disciplines, and if a single,\\npretrained ViFM can be competitive with domain-specific baselines. To address\\nthis, we introduce SciVid, a comprehensive benchmark comprising five\\n*Sci*entific *Vid*eo tasks, across medical computer vision, animal behavior,\\nand weather forecasting. We adapt six leading ViFMs to SciVid using simple\\ntrainable readout modules, establishing strong baselines and demonstrating the\\npotential for effective transfer learning. Specifically, we show that\\nstate-of-the-art results can be obtained in several applications by leveraging\\nthe general-purpose representations from ViFM backbones. Furthermore, our\\nresults reveal the limitations of existing ViFMs, and highlight opportunities\\nfor the development of generalizable models for high-impact scientific\\napplications. We release our code at https://github.com/google-deepmind/scivid\\nto facilitate further research in the development of ViFMs.', \"Brain stroke is one of the leading causes of mortality and long-term\\ndisability worldwide, highlighting the need for precise and fast prediction\\ntechniques. Computed Tomography (CT) scan is considered one of the most\\neffective methods for diagnosing brain strokes. The majority of stroke\\nclassification techniques rely on a single slice-level prediction mechanism,\\nallowing the radiologist to manually choose the most critical CT slice from the\\noriginal CT volume. Although clinical evaluations are often used in traditional\\ndiagnostic procedures, machine learning (ML) has opened up new avenues for\\nimproving stroke diagnosis. To supplement traditional diagnostic techniques,\\nthis study investigates the use of machine learning models, specifically\\nconcerning the prediction of brain stroke at an early stage utilizing CT scan\\nimages. In this research, we proposed a novel approach to brain stroke\\ndetection leveraging machine learning techniques, focusing on optimizing\\nclassification performance with pre-trained deep learning models and advanced\\noptimization strategies. Pre-trained models, including DenseNet201,\\nInceptionV3, MobileNetV2, ResNet50, and Xception, are utilized for feature\\nextraction. Additionally, we employed feature engineering techniques, including\\nBFO, PCA, and LDA, to enhance models' performance further. These features are\\nsubsequently classified using machine learning algorithms such as SVC, RF, XGB,\\nDT, LR, KNN, and GNB. Our experiments demonstrate that the combination of\\nMobileNetV2, LDA, and SVC achieved the highest classification accuracy of\\n97.93%, significantly outperforming other model-optimizer-classifier\\ncombinations. The results underline the effectiveness of integrating\\nlightweight pre-trained models with robust optimization and classification\\ntechniques for brain stroke diagnosis.\", 'As digital emotional support needs grow, Large Language Model companions\\noffer promising authentic, always-available empathy, though rigorous evaluation\\nlags behind model advancement. We present Heart-to-Heart Talk (H2HTalk), a\\nbenchmark assessing companions across personality development and empathetic\\ninteraction, balancing emotional intelligence with linguistic fluency. H2HTalk\\nfeatures 4,650 curated scenarios spanning dialogue, recollection, and itinerary\\nplanning that mirror real-world support conversations, substantially exceeding\\nprevious datasets in scale and diversity. We incorporate a Secure Attachment\\nPersona (SAP) module implementing attachment-theory principles for safer\\ninteractions. Benchmarking 50 LLMs with our unified protocol reveals that\\nlong-horizon planning and memory retention remain key challenges, with models\\nstruggling when user needs are implicit or evolve mid-conversation. H2HTalk\\nestablishes the first comprehensive benchmark for emotionally intelligent\\ncompanions. We release all materials to advance development of LLMs capable of\\nproviding meaningful and safe psychological support.', \"In this paper, we address the following question: How do generic foundation\\nmodels (e.g., CLIP, BLIP, LLaVa, DINO) compare against a domain-specific face\\nrecognition model (viz., AdaFace or ArcFace) on the face recognition task?\\nThrough a series of experiments involving several foundation models and\\nbenchmark datasets, we are able to report the following findings: (a) In all\\ndatasets considered, domain-specific models outperformed zero-shot foundation\\nmodels. (b) The performance of zero-shot generic foundation models improves on\\nover-segmented face images than tightly cropped faces thereby suggesting the\\nimportance of contextual clues. For example, at a False Match Rate (FMR) of\\n0.01%, the True Match Rate (TMR) of OpenCLIP improved from 64.97% to 81.73% on\\nthe LFW dataset as the face crop increased from 112x112 to 250x250 while the\\nTMR of domain-specific AdaFace dropped from 99.09% to 77.31%. (c) A simple\\nscore-level fusion of a foundation model with a domain-specific FR model\\nimproved the accuracy at low FMRs. For example, the TMR of AdaFace when fused\\nwith BLIP improved from 72.64% to 83.31% at an FMR of 0.0001% on the IJB-B\\ndataset and from 73.17% to 85.81% on the IJB-C dataset. (d) Foundation models,\\nsuch as ChatGPT, can be used to impart explainability to the FR pipeline (e.g.,\\n``Despite minor lighting and head tilt differences, the two left-profile images\\nshow high consistency in forehead slope, nose shape, chin contour...''). In\\nsome instances, foundation models are even able to resolve low-confidence\\ndecisions made by AdaFace (e.g., ``Although AdaFace assigns a low similarity\\nscore of 0.21, both images exhibit visual similarity...and the pair is likely\\nof the same person''), thereby reiterating the importance of combining\\ndomain-specific FR models with generic foundation models in a judicious manner.\", 'Fine-grained video classification requires understanding complex\\nspatio-temporal and semantic cues that often exceed the capacity of a single\\nmodality. In this paper, we propose a multimodal framework that fuses video,\\nimage, and text representations using GRU-based sequence encoders and\\ncross-modal attention mechanisms. The model is trained using a combination of\\nclassification or regression loss, depending on the task, and is further\\nregularized through feature-level augmentation and autoencoding techniques. To\\nevaluate the generality of our framework, we conduct experiments on two\\nchallenging benchmarks: the DVD dataset for real-world violence detection and\\nthe Aff-Wild2 dataset for valence-arousal estimation. Our results demonstrate\\nthat the proposed fusion strategy significantly outperforms unimodal baselines,\\nwith cross-attention and feature augmentation contributing notably to\\nrobustness and performance.', 'Synthetic tabular data generation has received increasing attention in recent\\nyears, particularly with the emergence of foundation models for tabular data.\\nThe breakthrough success of TabPFN (Hollmann et al.,2025), which leverages vast\\nquantities of synthetic tabular datasets derived from structural causal models\\n(SCMs), demonstrates the critical role synthetic data plays in developing\\npowerful tabular foundation models. However, most real-world tabular data\\nexists in relational formats spanning multiple interconnected tables - a\\nstructure not adequately addressed by current generation methods. In this work,\\nwe extend the SCM-based approach by developing a novel framework that generates\\nrealistic synthetic relational tabular data including causal relationships\\nacross tables. Our experiments confirm that this framework is able to construct\\nrelational datasets with complex inter-table dependencies mimicking real-world\\nscenarios.', 'Oversight and control (collectively, supervision) are often invoked as key\\nlevers for ensuring that AI systems are accountable, reliable, and able to\\nfulfill governance and management requirements. However, the concepts are\\nfrequently conflated or insufficiently distinguished in academic and policy\\ndiscourse, undermining efforts to design or evaluate systems that should remain\\nunder meaningful human supervision.\\n  This paper undertakes a targeted critical review of literature on supervision\\noutside of AI, along with a brief summary of past work on the topic related to\\nAI. We then differentiate control as being ex-ante or real-time, and\\noperational rather than policy or governance. In contrast, oversight is either\\na policy and governance function, or is ex-post. We suggest that control aims\\nto prevent failures. In contrast, oversight often focuses on detection,\\nremediation, or incentives for future prevention; all preventative oversight\\nstrategies nonetheless necessitate control.\\n  Building on this foundation, we make three contributions. First, we propose a\\ntheoretically-informed yet policy-grounded framework that articulates the\\nconditions under which each mechanism is possible, where they fall short, and\\nwhat is required to make them meaningful in practice. Second, we outline how\\nsupervision methods should be documented and integrated into risk management,\\nand drawing on the Microsoft Responsible AI Maturity Model, we outline a\\nmaturity model for AI supervision. Third, we explicitly highlight some\\nboundaries of these mechanisms, including where they apply, where they fail,\\nand where it is clear that no existing methods suffice. This foregrounds the\\nquestion of whether meaningful supervision is possible in a given deployment\\ncontext, and can support regulators, auditors, and practitioners in identifying\\nboth present limitations and the need for new conceptual and technical\\nadvances.', 'Feature generation (FG) aims to enhance the prediction potential of original\\ndata by constructing high-order feature combinations and removing redundant\\nfeatures. It is a key preprocessing step for tabular scientific data to improve\\ndownstream machine-learning model performance. Traditional methods face the\\nfollowing two challenges when dealing with the feature generation of scientific\\ndata: First, the effective construction of high-order feature combinations in\\nscientific data necessitates profound and extensive domain-specific expertise.\\nSecondly, as the order of feature combinations increases, the search space\\nexpands exponentially, imposing prohibitive human labor consumption.\\nAdvancements in the Data-Centric Artificial Intelligence (DCAI) paradigm have\\nopened novel avenues for automating feature generation processes. Inspired by\\nthat, this paper revisits the conventional feature generation workflow and\\nproposes the Multi-agent Feature Generation (MAFG) framework. Specifically, in\\nthe iterative exploration stage, multi-agents will construct mathematical\\ntransformation equations collaboratively, synthesize and identify feature\\ncombinations ex-hibiting high information content, and leverage a reinforcement\\nlearning mechanism to evolve their strategies. Upon completing the exploration\\nphase, MAFG integrates the large language models (LLMs) to interpreta-tively\\nevaluate the generated features of each significant model performance\\nbreakthrough. Experimental results and case studies consistently demonstrate\\nthat the MAFG framework effectively automates the feature generation process\\nand significantly enhances various downstream scientific data mining tasks.', \"In this paper, we introduce BMMR, a large-scale bilingual, multimodal,\\nmulti-disciplinary reasoning dataset for the community to develop and evaluate\\nlarge multimodal models (LMMs). BMMR comprises 110k college-level questions\\nspanning 300 UNESCO-defined subjects, spanning diverse formats-multiple-choice,\\nfill-in-the-blank, and open-ended QA-and sourced from both print and digital\\nmedia such as books, exams, and quizzes. All data are curated and filtered via\\na human-in-the-loop and scalable framework, and each instance is paired with a\\nhigh-quality reasoning path. The dataset is organized into two parts: BMMR-Eval\\nthat comprises 20,458 high-quality instances to comprehensively assess LMMs'\\nknowledge and reasoning across multiple disciplines in both Chinese and\\nEnglish; and BMMR-Train that contains 88,991 instances to support further\\nresearch and development, extending the current focus on mathematical reasoning\\nto diverse disciplines and domains. In addition, we propose the process-based\\nmulti-discipline verifier (i.e., BMMR-Verifier) for accurate and fine-grained\\nevaluation of reasoning paths. Extensive experiments on 24 models reveal that\\n(i) even SOTA models (e.g., o3 and Gemini-2.5-Pro) leave substantial headroom\\non BMMR-Eval; (ii) reasoning models exhibit discipline bias and outperform LMMs\\nonly on specific subjects; (iii) open-source models still trail their\\nproprietary counterparts; and (iv) fine-tuning on BMMR-Train narrows this gap.\\nAdditionally, we conduct reasoning-chain analyses using BMMR-Verifier and other\\nin-depth studies, uncovering the challenges LMMs currently face in\\nmultidisciplinary reasoning. We will release the data, and we hope our work can\\noffer insights and contributions to the community.\", 'The development of large language models (LLMs) has greatly promoted the\\nprogress of chatbot in multiple fields. There is an urgent need to evaluate\\nwhether LLMs can play the role of agent in housing transactions and services as\\nwell as humans. We present Real Estate Agent Large Language Model Evaluation\\n(REAL), the first evaluation suite designed to assess the abilities of LLMs in\\nthe field of housing transactions and services. REAL comprises 5,316\\nhigh-quality evaluation entries across 4 topics: memory, comprehension,\\nreasoning and hallucination. All these entries are organized as 14 categories\\nto assess whether LLMs have the knowledge and ability in housing transactions\\nand services scenario. Additionally, the REAL is used to evaluate the\\nperformance of most advanced LLMs. The experiment results indicate that LLMs\\nstill have significant room for improvement to be applied in the real estate\\nfield.', 'Despite mounting evidence that multilinguality can be easily weaponized\\nagainst language models (LMs), works across NLP Security remain overwhelmingly\\nEnglish-centric. In terms of securing LMs, the NLP norm of \"English first\"\\ncollides with standard procedure in cybersecurity, whereby practitioners are\\nexpected to anticipate and prepare for worst-case outcomes. To mitigate\\nworst-case outcomes in NLP Security, researchers must be willing to engage with\\nthe weakest links in LM security: lower-resourced languages. Accordingly, this\\nwork examines the security of LMs for lower- and medium-resourced languages. We\\nextend existing adversarial attacks for up to 70 languages to evaluate the\\nsecurity of monolingual and multilingual LMs for these languages. Through our\\nanalysis, we find that monolingual models are often too small in total number\\nof parameters to ensure sound security, and that while multilinguality is\\nhelpful, it does not always guarantee improved security either. Ultimately,\\nthese findings highlight important considerations for more secure deployment of\\nLMs, for communities of lower-resourced languages.', \"Identifying the associations between imaging phenotypes and disease risk\\nfactors and outcomes is essential for understanding disease mechanisms and\\nimproving diagnosis and prognosis models. However, traditional approaches rely\\non human-driven hypothesis testing and selection of association factors, often\\noverlooking complex, non-linear dependencies among imaging phenotypes and other\\nmulti-modal data. To address this, we introduce a Multi-agent Exploratory\\nSynergy for the Heart (MESHAgents) framework that leverages large language\\nmodels as agents to dynamically elicit, surface, and decide confounders and\\nphenotypes in association studies, using cardiovascular imaging as a proof of\\nconcept. Specifically, we orchestrate a multi-disciplinary team of AI agents --\\nspanning cardiology, biomechanics, statistics, and clinical research -- which\\nspontaneously generate and converge on insights through iterative,\\nself-organizing reasoning. The framework dynamically synthesizes statistical\\ncorrelations with multi-expert consensus, providing an automated pipeline for\\nphenome-wide association studies (PheWAS). We demonstrate the system's\\ncapabilities through a population-based study of imaging phenotypes of the\\nheart and aorta. MESHAgents autonomously uncovered correlations between imaging\\nphenotypes and a wide range of non-imaging factors, identifying additional\\nconfounder variables beyond standard demographic factors. Validation on\\ndiagnosis tasks reveals that MESHAgents-discovered phenotypes achieve\\nperformance comparable to expert-selected phenotypes, with mean AUC differences\\nas small as -0.004 on disease classification tasks. Notably, the recall score\\nimproves for 6 out of 9 disease types. Our framework provides clinically\\nrelevant imaging phenotypes with transparent reasoning, offering a scalable\\nalternative to expert-driven methods.\", 'Vision-Language Models (VLMs) like CLIP achieve cross-modal semantic\\nalignment through contrastive learning, exhibiting robust zero-shot\\ngeneralization. Traditional prompt engineering, however, predominantly relies\\non coarse-grained category labels, neglecting fine-grained local semantics.\\nExisting approaches assume that VLMs inherently recognize localized visual\\ndetails and attempt to enhance classification by augmenting text prompts with\\nattribute descriptors generated by large language models. However, our\\nsystematic experiments reveal critical limitations: CLIP\\'s strong bias toward\\nglobal image patterns hinders its ability to process localized visual\\ndescriptors. To address this fundamental constraint, we propose a simple,\\neffective, and plug-and-play solution that enables CLIP to ``See Both the\\nForest and the Trees.\" Specifically, we employ stochastic multi-crop\\naugmentation to activate CLIP\\'s latent capacity for localized feature analysis.\\nBy cropping only partial regions, the approach effectively constrains the\\nmodel\\'s receptive field and recalibrates its attention mechanism, thereby\\nmitigating its inherent bias. We evaluate the proposed method under zero-shot,\\nfew-shot, and test-time adaptation settings, and extensive experiments\\ndemonstrate that D&D achieves promising performance.', 'Despite significant progress in designing powerful adversarial evasion\\nattacks for robustness verification, the evaluation of these methods often\\nremains inconsistent and unreliable. Many assessments rely on mismatched\\nmodels, unverified implementations, and uneven computational budgets, which can\\nlead to biased results and a false sense of security. Consequently, robustness\\nclaims built on such flawed testing protocols may be misleading and give a\\nfalse sense of security. As a concrete step toward improving evaluation\\nreliability, we present AttackBench, a benchmark framework developed to assess\\nthe effectiveness of gradient-based attacks under standardized and reproducible\\nconditions. AttackBench serves as an evaluation tool that ranks existing attack\\nimplementations based on a novel optimality metric, which enables researchers\\nand practitioners to identify the most reliable and effective attack for use in\\nsubsequent robustness evaluations. The framework enforces consistent testing\\nconditions and enables continuous updates, making it a reliable foundation for\\nrobustness verification.', 'Social determinants of health (SDoH) significantly influence health outcomes,\\nshaping disease progression, treatment adherence, and health disparities.\\nHowever, their documentation in structured electronic health records (EHRs) is\\noften incomplete or missing. This study presents an approach based on large\\nlanguage models (LLMs) for extracting 13 SDoH categories from French clinical\\nnotes. We trained Flan-T5-Large on annotated social history sections from\\nclinical notes at Nantes University Hospital, France. We evaluated the model at\\ntwo levels: (i) identification of SDoH categories and associated values, and\\n(ii) extraction of detailed SDoH with associated temporal and quantitative\\ninformation. The model performance was assessed across four datasets, including\\ntwo that we publicly release as open resources. The model achieved strong\\nperformance for identifying well-documented categories such as living\\ncondition, marital status, descendants, job, tobacco, and alcohol use (F1 score\\n> 0.80). Performance was lower for categories with limited training data or\\nhighly variable expressions, such as employment status, housing, physical\\nactivity, income, and education. Our model identified 95.8% of patients with at\\nleast one SDoH, compared to 2.8% for ICD-10 codes from structured EHR data. Our\\nerror analysis showed that performance limitations were linked to annotation\\ninconsistencies, reliance on English-centric tokenizer, and reduced\\ngeneralizability due to the model being trained on social history sections\\nonly. These results demonstrate the effectiveness of NLP in improving the\\ncompleteness of real-world SDoH data in a non-English EHR system.', 'Accurate molecular property prediction is essential in drug discovery and\\nrelated fields. However, existing graph neural networks (GNNs) often struggle\\nto simultaneously capture both local and global molecular structures. In this\\nwork, we propose a Multi-Level Fusion Graph Neural Network (MLFGNN) that\\nintegrates Graph Attention Networks and a novel Graph Transformer to jointly\\nmodel local and global dependencies. In addition, we incorporate molecular\\nfingerprints as a complementary modality and introduce a mechanism of\\ninteraction between attention to adaptively fuse information across\\nrepresentations. Extensive experiments on multiple benchmark datasets\\ndemonstrate that MLFGNN consistently outperforms state-of-the-art methods in\\nboth classification and regression tasks. Interpretability analysis further\\nreveals that the model effectively captures task-relevant chemical patterns,\\nsupporting the usefulness of multi-level and multi-modal fusion in molecular\\nrepresentation learning.', 'We examine recent research that asks whether current AI systems may be\\ndeveloping a capacity for \"scheming\" (covertly and strategically pursuing\\nmisaligned goals). We compare current research practices in this field to those\\nadopted in the 1970s to test whether non-human primates could master natural\\nlanguage. We argue that there are lessons to be learned from that historical\\nresearch endeavour, which was characterised by an overattribution of human\\ntraits to other agents, an excessive reliance on anecdote and descriptive\\nanalysis, and a failure to articulate a strong theoretical framework for the\\nresearch. We recommend that research into AI scheming actively seeks to avoid\\nthese pitfalls. We outline some concrete steps that can be taken for this\\nresearch programme to advance in a productive and scientifically rigorous\\nfashion.', 'This paper systematically reviews recent advances in artificial intelligence\\n(AI), with a particular focus on machine learning (ML), across the entire drug\\ndiscovery pipeline. Due to the inherent complexity, escalating costs, prolonged\\ntimelines, and high failure rates of traditional drug discovery methods, there\\nis a critical need to comprehensively understand how AI/ML can be effectively\\nintegrated throughout the full process. Currently available literature reviews\\noften narrowly focus on specific phases or methodologies, neglecting the\\ndependence between key stages such as target identification, hit screening, and\\nlead optimization. To bridge this gap, our review provides a detailed and\\nholistic analysis of AI/ML applications across these core phases, highlighting\\nsignificant methodological advances and their impacts at each stage. We further\\nillustrate the practical impact of these techniques through an in-depth case\\nstudy focused on hyperuricemia, gout arthritis, and hyperuricemic nephropathy,\\nhighlighting real-world successes in molecular target identification and\\ntherapeutic candidate discovery. Additionally, we discuss significant\\nchallenges facing AI/ML in drug discovery and outline promising future research\\ndirections. Ultimately, this review serves as an essential orientation for\\nresearchers aiming to leverage AI/ML to overcome existing bottlenecks and\\naccelerate drug discovery.', 'To advance real-world fashion image editing, we analyze existing two-stage\\npipelines(mask generation followed by diffusion-based editing)which overly\\nprioritize generator optimization while neglecting mask controllability. This\\nresults in two critical limitations: I) poor user-defined flexibility\\n(coarse-grained human masks restrict edits to predefined regions like upper\\ntorso; fine-grained clothes masks preserve poses but forbid style/length\\ncustomization). II) weak pose robustness (mask generators fail due to\\narticulated poses and miss rare regions like waist, while human parsers remain\\nlimited by predefined categories). To address these gaps, we propose Pose-Star,\\na framework that dynamically recomposes body structures (e.g., neck, chest,\\netc.) into anatomy-aware masks (e.g., chest-length) for user-defined edits. In\\nPose-Star, we calibrate diffusion-derived attention (Star tokens) via skeletal\\nkeypoints to enhance rare structure localization in complex poses, suppress\\nnoise through phase-aware analysis of attention dynamics\\n(Convergence,Stabilization,Divergence) with threshold masking and\\nsliding-window fusion, and refine edges via cross-self attention merging and\\nCanny alignment. This work bridges controlled benchmarks and open-world\\ndemands, pioneering anatomy-aware, pose-robust editing and laying the\\nfoundation for industrial fashion image editing.', \"Query optimization is essential for efficient SQL query execution in DBMS,\\nand remains attractive over time due to the growth of data volumes and advances\\nin hardware. Existing traditional optimizers struggle with the cumbersome\\nhand-tuning required for complex workloads, and the learning-based methods face\\nlimitations in ensuring generalization. With the great success of Large\\nLanguage Model (LLM) across diverse downstream tasks, this paper explores how\\nLLMs can be incorporated to enhance the generalization of learned optimizers.\\nThough promising, such an incorporation still presents challenges, mainly\\nincluding high model inference latency, and the substantial fine-tuning cost\\nand suboptimal performance due to inherent discrepancy between the token\\nsequences in LLM and structured SQL execution plans with rich numerical\\nfeatures.\\n  In this paper, we focus on recurring queries in offline optimization to\\nalleviate the issue of high inference latency, and propose \\\\textbf{LLM4Hint}\\nthat leverages moderate-sized backbone LLMs to recommend query optimization\\nhints. LLM4Hint achieves the goals through: (i) integrating a lightweight model\\nto produce a soft prompt, which captures the data distribution in DBMS and the\\nSQL predicates to provide sufficient optimization features while simultaneously\\nreducing the context length fed to the LLM, (ii) devising a query rewriting\\nstrategy using a larger commercial LLM, so as to simplify SQL semantics for the\\nbackbone LLM and reduce fine-tuning costs, and (iii) introducing an explicit\\nmatching prompt to facilitate alignment between the LLM and the lightweight\\nmodel, which can accelerate convergence of the combined model. Experiments show\\nthat LLM4Hint, by leveraging the LLM's stronger capability to understand the\\nquery statement, can outperform the state-of-the-art learned optimizers in\\nterms of both effectiveness and generalization.\", 'Remote sensing change detection aims to localize semantic changes between\\nimages of the same location captured at different times. In the past few years,\\nnewer methods have attributed enhanced performance to the additions of new and\\ncomplex components to existing architectures. Most fail to measure the\\nperformance contribution of fundamental design choices such as backbone\\nselection, pre-training strategies, and training configurations. We claim that\\nsuch fundamental design choices often improve performance even more\\nsignificantly than the addition of new architectural components. Due to that,\\nwe systematically revisit the design space of change detection models and\\nanalyse the full potential of a well-optimised baseline. We identify a set of\\nfundamental design choices that benefit both new and existing architectures.\\nLeveraging this insight, we demonstrate that when carefully designed, even an\\narchitecturally simple model can match or surpass state-of-the-art performance\\non six challenging change detection datasets. Our best practices generalise\\nbeyond our architecture and also offer performance improvements when applied to\\nrelated methods, indicating that the space of fundamental design choices has\\nbeen underexplored. Our guidelines and architecture provide a strong foundation\\nfor future methods, emphasizing that optimizing core components is just as\\nimportant as architectural novelty in advancing change detection performance.\\nCode: https://github.com/blaz-r/BTC-change-detection', 'Sentiment analysis, widely used in product reviews, also impacts financial\\nmarkets by influencing asset prices through microblogs and news articles.\\nDespite research in sentiment-driven finance, many studies focus on\\nsentence-level classification, overlooking its practical application in\\ntrading. This study bridges that gap by evaluating sentiment-based trading\\nstrategies for generating positive alpha. We conduct a backtesting analysis\\nusing sentiment predictions from three models (two classification and one\\nregression) applied to news articles on Dow Jones 30 stocks, comparing them to\\nthe benchmark Buy&Hold strategy. Results show all models produced positive\\nreturns, with the regression model achieving the highest return of 50.63% over\\n28 months, outperforming the benchmark Buy&Hold strategy. This highlights the\\npotential of sentiment in enhancing investment strategies and financial\\ndecision-making.', 'The drive for predictable LLM reasoning in their integration with compound\\nsystems has popularized structured outputs, yet concerns remain about\\nperformance trade-offs compared to unconstrained natural language. At the same\\ntime, training on unconstrained Chain of Thought (CoT) traces has brought about\\na new class of strong reasoning models that nevertheless present novel compute\\nbudget and faithfulness challenges. This paper introduces iSelf-Discover, an\\ninstance-level adaptation of the Self-Discover framework, and using it compares\\ndynamically generated structured JSON reasoning with its unstructured\\ncounterpart. Our empirical evaluation across diverse benchmarks using\\nstate-of-the-art open-source models supports a consistent advantage for\\nunstructured reasoning. Notably, on the complex MATH benchmark, unstructured\\nplans achieved relative performance improvements of up to 18.90\\\\% over\\nstructured approaches. Zero-shot unstructured iSelf-Discover variants are also\\nshown to outperform their five-shot structured counterparts, underscoring the\\nsignificance of this gap, even when structured plans are dynamically generated\\nto ensure reasoning precedes the final answer. We further demonstrate that the\\noptimal granularity of plan generation (instance-level vs. task-level) is\\ncontext-dependent. These findings invite re-evaluation of the reliance on\\nstructured formats for complex problem-solving and how compound systems should\\nbe organized.', 'Current continuous sign language recognition (CSLR) methods struggle with\\nhandling diverse samples. Although dynamic convolutions are ideal for this\\ntask, they mainly focus on spatial modeling and fail to capture the temporal\\ndynamics and contextual dependencies. To address this, we propose DESign, a\\nnovel framework that incorporates Dynamic Context-Aware Convolution (DCAC) and\\nSubnet Regularization Connectionist Temporal Classification (SR-CTC). DCAC\\ndynamically captures the inter-frame motion cues that constitute signs and\\nuniquely adapts convolutional weights in a fine-grained manner based on\\ncontextual information, enabling the model to better generalize across diverse\\nsigning behaviors and boost recognition accuracy. Furthermore, we observe that\\nexisting methods still rely on only a limited number of frames for parameter\\nupdates during training, indicating that CTC learning overfits to a dominant\\npath. To address this, SR-CTC regularizes training by applying supervision to\\nsubnetworks, encouraging the model to explore diverse CTC alignment paths and\\neffectively preventing overfitting. A classifier-sharing strategy in SR-CTC\\nfurther strengthens multi-scale consistency. Notably, SR-CTC introduces no\\ninference overhead and can be seamlessly integrated into existing CSLR models\\nto boost performance. Extensive ablations and visualizations further validate\\nthe effectiveness of the proposed methods. Results on mainstream CSLR datasets\\n(i.e., PHOENIX14, PHOENIX14-T, CSL-Daily) demonstrate that DESign achieves\\nstate-of-the-art performance.', 'Large language models (LLMs) are increasingly tasked with invoking enterprise\\nAPIs, yet they routinely falter when near-duplicate tools vie for the same user\\nintent or when required arguments are left underspecified. We introduce\\nDiaFORGE (Dialogue Framework for Organic Response Generation & Evaluation), a\\ndisambiguation-centric, three-stage pipeline that (i) synthesizes\\npersona-driven, multi-turn dialogues in which the assistant must distinguish\\namong highly similar tools, (ii) performs supervised fine-tuning of open-source\\nmodels with reasoning traces across 3B - 70B parameters, and (iii) evaluates\\nreal-world readiness via a dynamic suite that redeploys each model in a live\\nagentic loop and reports end-to-end goal completion alongside conventional\\nstatic metrics. On our dynamic benchmark DiaBENCH, models trained with DiaFORGE\\nraise tool-invocation success by 27 pp over GPT-4o and by 49 pp over\\nClaude-3.5-Sonnet, both under optimized prompting. To spur further research, we\\nrelease an open corpus of 5000 production-grade enterprise API specifications\\npaired with rigorously validated, disambiguation-focused dialogues, offering a\\npractical blueprint for building reliable, enterprise-ready tool-calling\\nagents.', 'Detecting deepfakes involving face-swaps presents a significant challenge,\\nparticularly in real-world scenarios where anyone can perform face-swapping\\nwith freely available tools and apps without any technical knowledge. Existing\\ndeepfake detection methods rely on facial landmarks or inconsistencies in\\npixel-level features and often struggle with face-swap deepfakes, where the\\nsource face is seamlessly blended into the target image or video. The\\nprevalence of face-swap is evident in everyday life, where it is used to spread\\nfalse information, damage reputations, manipulate political opinions, create\\nnon-consensual intimate deepfakes (NCID), and exploit children by enabling the\\ncreation of child sexual abuse material (CSAM). Even prominent public figures\\nare not immune to its impact, with numerous deepfakes of them circulating\\nwidely across social media platforms. Another challenge faced by deepfake\\ndetection methods is the creation of datasets that encompass a wide range of\\nvariations, as training models require substantial amounts of data. This raises\\nprivacy concerns, particularly regarding the processing and storage of personal\\nfacial data, which could lead to unauthorized access or misuse. Our key idea is\\nto identify these style discrepancies to detect face-swapped images effectively\\nwithout accessing the real facial image. We perform comprehensive evaluations\\nusing multiple datasets and face-swapping methods, which showcases the\\neffectiveness of SafeVision in detecting face-swap deepfakes across diverse\\nscenarios. SafeVision offers a reliable and scalable solution for detecting\\nface-swaps in a privacy preserving manner, making it particularly effective in\\nchallenging real-world applications. To the best of our knowledge, SafeVision\\nis the first deepfake detection using style features while providing inherent\\nprivacy protection.', 'To alleviate the reliance of deep neural networks on large-scale datasets,\\ndataset distillation aims to generate compact, high-quality synthetic datasets\\nthat can achieve comparable performance to the original dataset. The\\nintegration of generative models has significantly advanced this field.\\nHowever, existing approaches primarily focus on aligning the distilled dataset\\nwith the original one, often overlooking task-specific information that can be\\ncritical for optimal downstream performance. In this paper, focusing on the\\ndownstream task of classification, we propose a task-specific sampling strategy\\nfor generative dataset distillation that incorporates the concept of difficulty\\nto consider the requirements of the target task better. The final dataset is\\nsampled from a larger image pool with a sampling distribution obtained by\\nmatching the difficulty distribution of the original dataset. A logarithmic\\ntransformation is applied as a pre-processing step to correct for\\ndistributional bias. The results of extensive experiments demonstrate the\\neffectiveness of our method and suggest its potential for enhancing performance\\non other downstream tasks.', 'Cooking plays a vital role in everyday independence and well-being, yet\\nremains challenging for people with vision impairments due to limited support\\nfor tracking progress and receiving contextual feedback. Object status - the\\ncondition or transformation of ingredients and tools - offers a promising but\\nunderexplored foundation for context-aware cooking support. In this paper, we\\npresent OSCAR (Object Status Context Awareness for Recipes), a technical\\npipeline that explores the use of object status recognition to enable recipe\\nprogress tracking in non-visual cooking. OSCAR integrates recipe parsing,\\nobject status extraction, visual alignment with cooking steps, and time-causal\\nmodeling to support real-time step tracking. We evaluate OSCAR on 173\\ninstructional videos and a real-world dataset of 12 non-visual cooking sessions\\nrecorded by BLV individuals in their homes. Our results show that object status\\nconsistently improves step prediction accuracy across vision-language models,\\nand reveal key factors that impact performance in real-world conditions, such\\nas implicit tasks, camera placement, and lighting. We contribute the pipeline\\nof context-aware recipe progress tracking, an annotated real-world non-visual\\ncooking dataset, and design insights to guide future context-aware assistive\\ncooking systems.', 'We present NDAI-NeuroMAP, the first neuroscience-domain-specific dense vector\\nembedding model engineered for high-precision information retrieval tasks. Our\\nmethodology encompasses the curation of an extensive domain-specific training\\ncorpus comprising 500,000 carefully constructed triplets\\n(query-positive-negative configurations), augmented with 250,000\\nneuroscience-specific definitional entries and 250,000 structured\\nknowledge-graph triplets derived from authoritative neurological ontologies. We\\nemploy a sophisticated fine-tuning approach utilizing the\\nFremyCompany/BioLORD-2023 foundation model, implementing a multi-objective\\noptimization framework combining contrastive learning with triplet-based metric\\nlearning paradigms. Comprehensive evaluation on a held-out test dataset\\ncomprising approximately 24,000 neuroscience-specific queries demonstrates\\nsubstantial performance improvements over state-of-the-art general-purpose and\\nbiomedical embedding models. These empirical findings underscore the critical\\nimportance of domain-specific embedding architectures for neuroscience-oriented\\nRAG systems and related clinical natural language processing applications.', \"Large Language Models (LLMs) have demonstrated remarkable proficiency in\\nunderstanding text and generating high-quality responses. However, a critical\\ndistinction from human cognition is their typical lack of a distinct internal\\n`reading' or deliberation phase before `speaking' (i.e., generating text).\\nHumans often engage in silent reading to comprehend context and formulate\\nthoughts prior to articulation. This paper investigates methods to imbue LLMs\\nwith a similar capacity for internal processing.\\n  We introduce and evaluate techniques that encourage LLMs to `read silently.'\\nOur findings indicate that even a straightforward approach, such as providing\\nthe model with an initial contextual prompt or `reading space' before it begins\\npredicting subsequent tokens for the final output, can yield significant\\nperformance improvements. We further enhance this concept by developing a\\n`reading buddy' architecture, where an auxiliary component silently processes\\nthe input and provides refined contextual insights to the primary generation\\nmodel. These approaches aim to foster deeper understanding from LLMs so that\\nthey can produce better reasoned responses, moving them one step closer to more\\nhuman-like text processing. Our results indicate that these simple techniques\\ncan provide surprisingly strong impact on accuracy with multiple point accuracy\\nboost.\", 'Domain adaptation has become a widely adopted approach in machine learning\\ndue to the high costs associated with labeling data. It is typically applied\\nwhen access to a labeled source domain is available. However, in real-world\\nscenarios, privacy concerns often restrict access to sensitive information,\\nsuch as fingerprints, bank account details, and facial images. A promising\\nsolution to this issue is Source-Free Unsupervised Domain Adaptation (SFUDA),\\nwhich enables domain adaptation without requiring access to labeled target\\ndomain data. Recent research demonstrates that SFUDA can effectively address\\ndomain discrepancies; however, two key challenges remain: (1) the low quality\\nof prototype samples, and (2) the incorrect assignment of pseudo-labels. To\\ntackle these challenges, we propose a method consisting of three main phases.\\nIn the first phase, we introduce a Reliable Sample Memory (RSM) module to\\nimprove the quality of prototypes by selecting more representative samples. In\\nthe second phase, we employ a Multi-View Contrastive Learning (MVCL) approach\\nto enhance pseudo-label quality by leveraging multiple data augmentations. In\\nthe final phase, we apply a noisy label filtering technique to further refine\\nthe pseudo-labels. Our experiments on three benchmark datasets - VisDA 2017,\\nOffice-Home, and Office-31 - demonstrate that our method achieves approximately\\n2 percent and 6 percent improvements in classification accuracy over the\\nsecond-best method and the average of 13 well-known state-of-the-art\\napproaches, respectively.', 'Explainable artificial intelligence (XAI) approaches have been increasingly\\napplied in drug discovery to learn molecular representations and identify\\nsubstructures driving property predictions. However, building end-to-end\\nexplainable machine learning models for structure-activity relationship (SAR)\\nmodeling for compound property prediction faces many challenges, such as\\nlimited activity data per target and the sensitivity of properties to subtle\\nmolecular changes. To address this, we leveraged activity-cliff molecule pairs,\\ni.e., compounds sharing a common scaffold but differing sharply in potency,\\ntargeting three proto-oncogene tyrosine-protein kinase Src proteins (i.e., PDB\\nIDs 1O42, 2H8H, and 4MXO). We implemented graph neural network (GNN) methods to\\nobtain atom-level feature information and predict compound-protein affinity\\n(i.e., half maximal inhibitory concentration, IC50). In addition, we trained\\nGNN models with different structure-aware loss functions to adequately leverage\\nmolecular property and structure information. We also utilized group lasso and\\nsparse group lasso to prune and highlight molecular subgraphs and enhance the\\nstructure-specific model explainability for the predicted property difference\\nin molecular activity-cliff pairs. We improved drug property prediction by\\nintegrating common and uncommon node information and using sparse group lasso,\\nreducing the average root mean squared error (RMSE) by 12.70%, and achieving\\nthe lowest averaged RMSE=0.2551 and the highest PCC=0.9572. Furthermore,\\napplying regularization enhances feature attribution methods that estimate the\\ncontribution of each atom in the molecular graphs by boosting global direction\\nscores and atom-level accuracy in atom coloring accuracy, which improves model\\ninterpretability in drug discovery pipelines, particularly in investigating\\nimportant molecular substructures in lead optimization.', 'We formulate learning guided Automated Theorem Proving as Partial Label\\nLearning, building the first bridge across these fields of research and\\nproviding a theoretical framework for dealing with alternative proofs during\\nlearning. We use the plCoP theorem prover to demonstrate that methods from the\\nPartial Label Learning literature tend to increase the performance of learning\\nassisted theorem provers.', \"Translating nuanced, textually-defined authorial writing styles into\\ncompelling visual representations presents a novel challenge in generative AI.\\nThis paper introduces a pipeline that leverages Author Writing Sheets (AWS) -\\nstructured summaries of an author's literary characteristics - as input to a\\nLarge Language Model (LLM, Claude 3.7 Sonnet). The LLM interprets the AWS to\\ngenerate three distinct, descriptive text-to-image prompts, which are then\\nrendered by a diffusion model (Stable Diffusion 3.5 Medium). We evaluated our\\napproach using 49 author styles from Reddit data, with human evaluators\\nassessing the stylistic match and visual distinctiveness of the generated\\nimages. Results indicate a good perceived alignment between the generated\\nvisuals and the textual authorial profiles (mean style match: $4.08/5$), with\\nimages rated as moderately distinctive. Qualitative analysis further\\nhighlighted the pipeline's ability to capture mood and atmosphere, while also\\nidentifying challenges in representing highly abstract narrative elements. This\\nwork contributes a novel end-to-end methodology for visual authorial style\\npersonalization and provides an initial empirical validation, opening avenues\\nfor applications in creative assistance and cross-modal understanding.\", 'Document level Machine Translation (DocMT) approaches often struggle with\\neffectively capturing discourse level phenomena. Existing approaches rely on\\nheuristic rules to segment documents into discourse units, which rarely align\\nwith the true discourse structure required for accurate translation. Otherwise,\\nthey fail to maintain consistency throughout the document during translation.\\nTo address these challenges, we propose Graph Augmented Agentic Framework for\\nDocument Level Translation (GRAFT), a novel graph based DocMT system that\\nleverages Large Language Model (LLM) agents for document translation. Our\\napproach integrates segmentation, directed acyclic graph (DAG) based dependency\\nmodelling, and discourse aware translation into a cohesive framework.\\nExperiments conducted across eight translation directions and six diverse\\ndomains demonstrate that GRAFT achieves significant performance gains over\\nstate of the art DocMT systems. Specifically, GRAFT delivers an average\\nimprovement of 2.8 d BLEU on the TED test sets from IWSLT2017 over strong\\nbaselines and 2.3 d BLEU for domain specific translation from English to\\nChinese. Moreover, our analyses highlight the consistent ability of GRAFT to\\naddress discourse level phenomena, yielding coherent and contextually accurate\\ntranslations.', 'This paper studies causal discovery in irregularly sampled time series-a\\npivotal challenge in high-stakes domains like finance, healthcare, and climate\\nscience, where missing data and inconsistent sampling frequencies distort\\ncausal mechanisms. Traditional methods (e.g., Granger causality, PCMCI) fail to\\nreconcile multi-scale interactions (e.g., hourly storms vs. decadal climate\\nshifts), while neural approaches (e.g., CUTS+) lack interpretability, stemming\\nfrom a critical gap: existing frameworks either rigidly assume temporal\\nregularity or aggregate dynamics into opaque representations, neglecting\\nreal-world granularity and auditable logic. To bridge this gap, we propose\\nReTimeCausal, a novel integration of Additive Noise Models (ANM) and\\nExpectation-Maximization (EM) that unifies physics-guided data imputation with\\nsparse causal inference. Through kernelized sparse regression and structural\\nconstraints, ReTimeCausal iteratively refines missing values (E-step) and\\ncausal graphs (M-step), resolving cross-frequency dependencies and missing data\\nissues. Extensive experiments on synthetic and real-world datasets demonstrate\\nthat ReTimeCausal outperforms existing state-of-the-art methods under\\nchallenging irregular sampling and missing data conditions.', 'Human creative ideation involves both exploration of diverse ideas\\n(divergence) and selective synthesis of explored ideas into coherent\\ncombinations (convergence). While processes of divergence and convergence are\\noften interleaved and nested, existing AI-powered creativity support tools\\n(CSTs) lack support for sophisticated orchestration of divergence and\\nconvergence. We present Reverger, an AI-powered CST that helps users ideate\\nvariations of conceptual directions for modifying a story by scaffolding\\nflexible iteration between divergence and convergence. For divergence, our tool\\nenables recursive exploration of alternative high-level directions for\\nmodifying a specific part of the original story. For convergence, it allows\\nusers to collect explored high-level directions and synthesize them into\\nconcrete variations. Users can then iterate between divergence and convergence\\nuntil they find a satisfactory outcome. A within-subject study revealed that\\nReverger permitted participants to explore more unexpected and diverse\\nhigh-level directions than a comparable baseline. Reverger users also felt that\\nthey had more fine-grained control and discovered more effort-worthy outcomes.', 'In semi-supervised semantic segmentation, existing studies have shown\\npromising results in academic settings with controlled splits of benchmark\\ndatasets. However, the potential benefits of leveraging significantly larger\\nsets of unlabeled images remain unexplored. In real-world scenarios, abundant\\nunlabeled images are often available from online sources (web-scraped images)\\nor large-scale datasets. However, these images may have different distributions\\nfrom those of the target dataset, a situation known as out-of-distribution\\n(OOD). Using these images as unlabeled data in semi-supervised learning can\\nlead to inaccurate pseudo-labels, potentially misguiding network training. In\\nthis paper, we propose a new semi-supervised semantic segmentation framework\\nwith an open-vocabulary segmentation model (SemiOVS) to effectively utilize\\nunlabeled OOD images. Extensive experiments on Pascal VOC and Context datasets\\ndemonstrate two key findings: (1) using additional unlabeled images improves\\nthe performance of semi-supervised learners in scenarios with few labels, and\\n(2) using the open-vocabulary segmentation (OVS) model to pseudo-label OOD\\nimages leads to substantial performance gains. In particular, SemiOVS\\noutperforms existing PrevMatch and SemiVL methods by +3.5 and +3.0 mIoU,\\nrespectively, on Pascal VOC with a 92-label setting, achieving state-of-the-art\\nperformance. These findings demonstrate that our approach effectively utilizes\\nabundant unlabeled OOD images for semantic segmentation tasks. We hope this\\nwork can inspire future research and real-world applications. The code is\\navailable at https://github.com/wooseok-shin/SemiOVS', \"The enormous parameter scale of large language models (LLMs) has made model\\ncompression a research hotspot, which aims to alleviate computational resource\\ndemands during deployment and inference. As a promising direction, low-rank\\napproximation technique has made remarkable achievements. Nevertheless,\\nunfortunately, the vast majority of studies to low-rank approximation\\ncompression generally apply uniform compression ratios across all weight\\nmatrices, while disregarding their inherently differentiated impacts on the\\nmodel's performance. Although a few recent work attempts to employ heuristic\\nsearch strategies to achieve the optimal parameter allocation, such strategies\\nare computationally inefficient and lose the generalization ability in the era\\nof LLMs. In this study, we propose a novel parameter Multi-Granular Adaptive\\nAllocation (MGAA) method, which can adaptively allocate parameters between and\\nwithin sublayers without task-specific evaluations in the compression process.\\nMGAA consists of two components: 1) Among different sublayers, it assigns\\ncompression ratios based on their cosine similarity between inputs and outputs,\\nallowing for a more tailored compression in sublayers with varying degrees of\\nimportance, and 2) Within each sublayer, it allocates different compression\\nratios to weight matrices based on their energy distribution characteristics,\\nensuring a consistent energy retention ratio while optimizing compression\\nefficiency. Comprehensive evaluations of MGAA across multiple LLMs backbone\\nmodels and benchmark datasets demonstrate its superior performance.\\nAdditionally, we apply our MGAA to multimodal model LLaVA, exhibiting\\nremarkable performance improvements.\", 'Large language models (LLMs) have demonstrated promise in reasoning tasks and\\ngeneral decision-making in static environments. In long-term planning tasks,\\nhowever, errors tend to accumulate, often leading to unsafe or inefficient\\nbehavior, limiting their use in general-purpose settings. We propose a modular\\nactor-critic architecture in which an LLM actor is guided by LTLCrit, a\\ntrajectory-level LLM critic that communicates via linear temporal logic (LTL).\\nOur setup combines the reasoning strengths of language models with the\\nguarantees of formal logic. The actor selects high-level actions from natural\\nlanguage observations, while the critic analyzes full trajectories and proposes\\nnew LTL constraints that shield the actor from future unsafe or inefficient\\nbehavior. The architecture supports both fixed, hand-specified safety\\nconstraints and adaptive, learned soft constraints that promote long-term\\nefficiency. Our architecture is model-agnostic: any LLM-based planner can serve\\nas the actor, and LTLCrit serves as a logic-generating wrapper. We formalize\\nplanning as graph traversal under symbolic constraints, allowing LTLCrit to\\nanalyze failed or suboptimal trajectories and generate new temporal logic rules\\nthat improve future behavior. We evaluate our system on the Minecraft\\ndiamond-mining benchmark, achieving 100% completion rates and improving\\nefficiency compared to baseline LLM planners. Our results suggest that enabling\\nLLMs to supervise each other through logic is a powerful and flexible paradigm\\nfor safe, generalizable decision making.', 'Memory Mosaics [Zhang et al., 2025], networks of associative memories, have\\ndemonstrated appealing compositional and in-context learning capabilities on\\nmedium-scale networks (GPT-2 scale) and synthetic small datasets. This work\\nshows that these favorable properties remain when we scale memory mosaics to\\nlarge language model sizes (llama-8B scale) and real-world datasets.\\n  To this end, we scale memory mosaics to 10B size, we train them on one\\ntrillion tokens, we introduce a couple architectural modifications (\"Memory\\nMosaics v2\"), we assess their capabilities across three evaluation dimensions:\\ntraining-knowledge storage, new-knowledge storage, and in-context learning.\\n  Throughout the evaluation, memory mosaics v2 match transformers on the\\nlearning of training knowledge (first dimension) and significantly outperforms\\ntransformers on carrying out new tasks at inference time (second and third\\ndimensions). These improvements cannot be easily replicated by simply\\nincreasing the training data for transformers. A memory mosaics v2 trained on\\none trillion tokens still perform better on these tasks than a transformer\\ntrained on eight trillion tokens.', 'A significant use case of instruction-finetuned Large Language Models (LLMs)\\nis to solve question-answering tasks interactively. In this setting, an LLM\\nagent is tasked with making a prediction by sequentially querying relevant\\ninformation from the user, as opposed to a single-turn conversation. This paper\\nexplores sequential querying strategies that aim to minimize the expected\\nnumber of queries. One such strategy is Information Pursuit (IP), a greedy\\nalgorithm that at each iteration selects the query that maximizes information\\ngain or equivalently minimizes uncertainty. However, obtaining accurate\\nestimates of mutual information or conditional entropy for LLMs is very\\ndifficult in practice due to over- or under-confident LLM probabilities, which\\nleads to suboptimal query selection and predictive performance. To better\\nestimate the uncertainty at each iteration, we propose Conformal Information\\nPursuit (C-IP), an alternative approach to sequential information gain based on\\nconformal prediction sets. More specifically, C-IP leverages a relationship\\nbetween prediction sets and conditional entropy at each iteration to estimate\\nuncertainty based on the average size of conformal prediction sets. In contrast\\nto conditional entropy, we find that conformal prediction sets are a\\ndistribution-free and robust method of measuring uncertainty. Experiments with\\n20 Questions show that C-IP obtains better predictive performance and shorter\\nquery-answer chains compared to previous approaches to IP and uncertainty-based\\nchain-of-thought methods. Furthermore, extending to an interactive medical\\nsetting between a doctor and a patient on the MediQ dataset, C-IP achieves\\ncompetitive performance with direct single-turn prediction while offering\\ngreater interpretability.', 'Dynamic Text-Attributed Graphs (DyTAGs), which intricately integrate\\nstructural, temporal, and textual attributes, are crucial for modeling complex\\nreal-world systems. However, most of the existing DyTAG datasets exhibit poor\\ntextual quality, which severely limits their utility for DyTAG generation tasks\\nrequiring semantically rich inputs. Additionally, prior work mainly focuses on\\ndiscriminative tasks on DyTAGs, resulting in a lack of standardized task\\nformulations and evaluation protocols tailored for DyTAG generation. To address\\nthese critical issues, we propose Generative DyTAG Benchmark (GDGB), which\\ncomprises eight meticulously curated DyTAG datasets with high-quality textual\\nfeatures for both nodes and edges, overcoming limitations of prior datasets.\\nBuilding on GDGB, we define two novel DyTAG generation tasks: Transductive\\nDynamic Graph Generation (TDGG) and Inductive Dynamic Graph Generation (IDGG).\\nTDGG transductively generates a target DyTAG based on the given source and\\ndestination node sets, while the more challenging IDGG introduces new node\\ngeneration to inductively model the dynamic expansion of real-world graph data.\\nTo enable holistic evaluation, we design multifaceted metrics that assess the\\nstructural, temporal, and textual quality of the generated DyTAGs. We further\\npropose GAG-General, an LLM-based multi-agent generative framework tailored for\\nreproducible and robust benchmarking of DyTAG generation. Experimental results\\ndemonstrate that GDGB enables rigorous evaluation of TDGG and IDGG, with key\\ninsights revealing the critical interplay of structural and textual features in\\nDyTAG generation. These findings establish GDGB as a foundational resource for\\nadvancing generative DyTAG research and unlocking further practical\\napplications in DyTAG generation. GDGB datasets, source codes, and leaderboards\\nare available at \\\\href{https://gdgb-algo.github.io/}{here}.', \"Multimodal Large Language Models (MLLMs) increasingly adopt multiple vision\\nencoders to capture diverse visual information, ranging from coarse semantics\\nto fine grained details. While this approach is intended to enhance visual\\nunderstanding capability, we observe that the performance gains from adding\\nencoders often diminish and can even lead to performance degradation, a\\nphenomenon we term encoder redundancy. This paper presents a systematic\\ninvestigation into this issue. Through comprehensive ablation studies on state\\nof the art multi encoder MLLMs, we empirically demonstrate that significant\\nredundancy exists. To quantify each encoder's unique contribution, we propose a\\nprincipled metric: the Conditional Utilization Rate (CUR). Building on CUR, we\\nintroduce the Information Gap (IG) to capture the overall disparity in encoder\\nutility within a model.Our experiments reveal that certain vision encoders\\ncontribute little, or even negatively, to overall performance, confirming\\nsubstantial redundancy. Our experiments reveal that certain vision encoders\\ncontribute minimally, or even negatively, to the model's performance,\\nconfirming the prevalence of redundancy. These findings highlight critical\\ninefficiencies in current multi encoder designs and establish that our proposed\\nmetrics can serve as valuable diagnostic tools for developing more efficient\\nand effective multimodal architectures.\", \"We introduce ForgeEDA, an open-source comprehensive circuit dataset across\\nvarious categories. ForgeEDA includes diverse circuit representations such as\\nRegister Transfer Level (RTL) code, Post-mapping (PM) netlists, And-Inverter\\nGraphs (AIGs), and placed netlists, enabling comprehensive analysis and\\ndevelopment. We demonstrate ForgeEDA's utility by benchmarking state-of-the-art\\nEDA algorithms on critical tasks such as Power, Performance, and Area (PPA)\\noptimization, highlighting its ability to expose performance gaps and drive\\nadvancements. Additionally, ForgeEDA's scale and diversity facilitate the\\ntraining of AI models for EDA tasks, demonstrating its potential to improve\\nmodel performance and generalization. By addressing limitations in existing\\ndatasets, ForgeEDA aims to catalyze breakthroughs in modern IC design and\\nsupport the next generation of innovations in EDA.\", 'Effective prompt design is essential for improving the planning capabilities\\nof large language model (LLM)-driven agents. However, existing structured\\nprompting strategies are typically limited to single-agent, plan-only settings,\\nand often evaluate performance solely based on task accuracy - overlooking\\ncritical factors such as token efficiency, modularity, and scalability in\\nmulti-agent environments. To address these limitations, we introduce\\nCodeAgents, a prompting framework that codifies multi-agent reasoning and\\nenables structured, token-efficient planning in multi-agent systems. In\\nCodeAgents, all components of agent interaction - Task, Plan, Feedback, system\\nroles, and external tool invocations - are codified into modular pseudocode\\nenriched with control structures (e.g., loops, conditionals), boolean logic,\\nand typed variables. This design transforms loosely connected agent plans into\\ncohesive, interpretable, and verifiable multi-agent reasoning programs. We\\nevaluate the proposed framework across three diverse benchmarks - GAIA,\\nHotpotQA, and VirtualHome - using a range of representative LLMs. Results show\\nconsistent improvements in planning performance, with absolute gains of 3-36\\npercentage points over natural language prompting baselines. On VirtualHome,\\nour method achieves a new state-of-the-art success rate of 56%. In addition,\\nour approach reduces input and output token usage by 55-87% and 41-70%,\\nrespectively, underscoring the importance of token-aware evaluation metrics in\\nthe development of scalable multi-agent LLM systems. The code and resources are\\navailable at: https://anonymous.4open.science/r/CodifyingAgent-5A86', 'The foundational capabilities of large language models (LLMs) are deeply\\ninfluenced by the quality of their pre-training corpora. However, enhancing\\ndata quality at scale remains a significant challenge, primarily due to the\\ntrade-off between refinement effectiveness and processing efficiency. While\\nrule-based filtering remains the dominant paradigm, it typically operates at\\nthe document level and lacks the granularity needed to refine specific content\\nwithin documents. Inspired by emerging work such as ProX, we propose\\n$\\\\textbf{RefineX}$, a novel framework for large-scale, surgical refinement of\\npre-training data through programmatic editing tasks. RefineX enables efficient\\nand fine-grained data refinement while reliably preserving the diversity and\\nnaturalness of raw text. The core strength of RefineX lies in distilling\\nhigh-quality, expert-guided end-to-end refinement results into minimal\\nedit-based deletion programs. This high-precision distillation pipeline is used\\nto train an efficient and reliable refine model that can systematically improve\\nevery instance in the corpus at scale. We evaluate RefineX across from-scratch\\npre-training at multiple model scales and find that it consistently outperforms\\nmodels trained on raw, filtered, or alternatively refined data across diverse\\ndownstream tasks. On the 750M model, RefineX yields 2.6%-7.2% average gains on\\nlighteval tasks, and achieves comparable performance using significantly fewer\\ntraining tokens. Further analysis shows that RefineX reliably enhances text\\nquality with both high efficiency and precision, outperforming prior approaches\\nsuch as end-to-end generation and Prox-C. These results position RefineX as a\\nscalable, effective, and reliable solution for optimizing pre-training data in\\nmodern LLM pipelines.', 'Speech Emotion Recognition (SER) traditionally relies on auditory data\\nanalysis for emotion classification. Several studies have adopted different\\nmethods for SER. However, existing SER methods often struggle to capture subtle\\nemotional variations and generalize across diverse datasets. In this article,\\nwe use Mel-Frequency Cepstral Coefficients (MFCCs) as spectral features to\\nbridge the gap between computational emotion processing and human auditory\\nperception. To further improve robustness and feature diversity, we propose a\\nnovel 1D-CNN-based SER framework that integrates data augmentation techniques.\\nMFCC features extracted from the augmented data are processed using a 1D\\nConvolutional Neural Network (CNN) architecture enhanced with channel and\\nspatial attention mechanisms. These attention modules allow the model to\\nhighlight key emotional patterns, enhancing its ability to capture subtle\\nvariations in speech signals. The proposed method delivers cutting-edge\\nperformance, achieving the accuracy of 97.49% for SAVEE, 99.23% for RAVDESS,\\n89.31% for CREMA-D, 99.82% for TESS, 99.53% for EMO-DB, and 96.39% for EMOVO.\\nExperimental results show new benchmarks in SER, demonstrating the\\neffectiveness of our approach in recognizing emotional expressions with high\\nprecision. Our evaluation demonstrates that the integration of advanced Deep\\nLearning (DL) methods substantially enhances generalization across diverse\\ndatasets, underscoring their potential to advance SER for real-world deployment\\nin assistive technologies and human-computer interaction.', 'The safety alignment of Language Models (LMs) is a critical concern, yet\\ntheir integrity can be challenged by direct parameter manipulation attacks,\\nsuch as those potentially induced by fault injection. As LMs are increasingly\\ndeployed using low-precision quantization for efficiency, this paper\\ninvestigates the efficacy of such attacks for jailbreaking aligned LMs across\\ndifferent quantization schemes. We propose gradient-guided attacks, including a\\ntailored progressive bit-level search algorithm introduced herein and a\\ncomparative word-level (single weight update) attack. Our evaluation on\\nLlama-3.2-3B, Phi-4-mini, and Llama-3-8B across FP16 (baseline), and\\nweight-only quantization (FP8, INT8, INT4) reveals that quantization\\nsignificantly influences attack success. While attacks readily achieve high\\nsuccess (>80\\\\% Attack Success Rate, ASR) on FP16 models, within an attack\\nbudget of 25 perturbations, FP8 and INT8 models exhibit ASRs below 20\\\\% and\\n50\\\\%, respectively. Increasing the perturbation budget up to 150 bit-flips, FP8\\nmodels maintained ASR below 65\\\\%, demonstrating some resilience compared to\\nINT8 and INT4 models that have high ASR. In addition, analysis of perturbation\\nlocations revealed differing architectural targets across quantization schemes,\\nwith (FP16, INT4) and (INT8, FP8) showing similar characteristics. Besides,\\njailbreaks induced in FP16 models were highly transferable to subsequent\\nFP8/INT8 quantization (<5\\\\% ASR difference), though INT4 significantly reduced\\ntransferred ASR (avg. 35\\\\% drop). These findings highlight that while common\\nquantization schemes, particularly FP8, increase the difficulty of direct\\nparameter manipulation jailbreaks, vulnerabilities can still persist,\\nespecially through post-attack quantization.', 'We propose a scalable and cost-efficient framework for deploying Graph-based\\nRetrieval Augmented Generation (GraphRAG) in enterprise environments. While\\nGraphRAG has shown promise for multi-hop reasoning and structured retrieval,\\nits adoption has been limited by the high computational cost of constructing\\nknowledge graphs using large language models (LLMs) and the latency of\\ngraph-based retrieval. To address these challenges, we introduce two core\\ninnovations: (1) a dependency-based knowledge graph construction pipeline that\\nleverages industrial-grade NLP libraries to extract entities and relations from\\nunstructured text completely eliminating reliance on LLMs; and (2) a\\nlightweight graph retrieval strategy that combines hybrid query node\\nidentification with efficient one-hop traversal for high-recall, low-latency\\nsubgraph extraction. We evaluate our framework on two SAP datasets focused on\\nlegacy code migration and demonstrate strong empirical performance. Our system\\nachieves up to 15% and 4.35% improvements over traditional RAG baselines based\\non LLM-as-Judge and RAGAS metrics, respectively. Moreover, our dependency-based\\nconstruction approach attains 94% of the performance of LLM-generated knowledge\\ngraphs (61.87% vs. 65.83%) while significantly reducing cost and improving\\nscalability. These results validate the feasibility of deploying GraphRAG\\nsystems in real-world, large-scale enterprise applications without incurring\\nprohibitive resource requirements paving the way for practical, explainable,\\nand domain-adaptable retrieval-augmented reasoning.', 'System Instructions (SIs), or system prompts, are pivotal for guiding Large\\nLanguage Models (LLMs) but manual crafting is resource-intensive and often\\nsuboptimal. Existing automated methods frequently generate non-human-readable\\n\"soft prompts,\" sacrificing interpretability. This paper introduces SI-Agent, a\\nnovel agentic framework designed to automatically generate and iteratively\\nrefine human-readable SIs through a feedback-driven loop. SI-Agent employs\\nthree collaborating agents: an Instructor Agent, an Instruction Follower Agent\\n(target LLM), and a Feedback/Reward Agent evaluating task performance and\\noptionally SI readability. The framework utilizes iterative cycles where\\nfeedback guides the Instructor\\'s refinement strategy (e.g., LLM-based editing,\\nevolutionary algorithms). We detail the framework\\'s architecture, agent roles,\\nthe iterative refinement process, and contrast it with existing methods. We\\npresent experimental results validating SI-Agent\\'s effectiveness, focusing on\\nmetrics for task performance, SI readability, and efficiency. Our findings\\nindicate that SI-Agent generates effective, readable SIs, offering a favorable\\ntrade-off between performance and interpretability compared to baselines.\\nPotential implications include democratizing LLM customization and enhancing\\nmodel transparency. Challenges related to computational cost and feedback\\nreliability are acknowledged.', 'Biological and artificial learning systems alike confront the\\nplasticity-stability dilemma. In the brain, neuromodulators such as\\nacetylcholine and noradrenaline relieve this tension by tuning neuronal gain\\nand inhibitory gating, balancing segregation and integration of circuits. Fed\\nby dense cholinergic and noradrenergic projections from the ascending arousal\\nsystem, layer-5 pyramidal neurons in the cerebral cortex offer a relevant\\nsubstrate for understanding these dynamics. When distal dendritic signals\\ncoincide with back-propagating action potentials, calcium plateaus turn a\\nsingle somatic spike into a high-gain burst, and interneuron inhibition sculpts\\nthe output. These properties make layer-5 cells gain-tunable amplifiers that\\ntranslate neuromodulatory cues into flexible cortical activity. To capture this\\nmechanism we developed a two-compartment Izhikevich model for pyramidal neurons\\nand single-compartment somatostatin (SOM) and parvalbumin (PV) interneurons,\\nlinked by Gaussian connectivity and spike-timing-dependent plasticity (STDP).\\nThe soma and apical dendrite are so coupled that somatic spikes back-propagate,\\nwhile dendritic plateaus can switch the soma from regular firing to bursting by\\nshifting reset and adaptation variables. We show that stronger dendritic drive\\nor tighter coupling raise gain by increasing the likelihood of\\ncalcium-triggered somatic bursts. In contrast, dendritic-targeted inhibition\\nsuppresses gain, while somatic-targeted inhibition raises the firing threshold\\nof neighboring neurons, thus gating neurons output. Notably, bursting\\naccelerates STDP, supporting rapid synaptic reconfiguration and\\nflexibility.This suggests that brief gain pulses driven by neuromodulators\\ncould serve as an adaptive two-timescale optimization mechanism, effectively\\nmodulating the synaptic weight updates.', 'To be effective, efficient, and diverse, deep learning models need to\\ndynamically choose its architecture based on signals from a population of\\nneurons. We hypothesize dynamic routing models can be improved with neural\\ninhibition in those neural populations. This means signals commonly shared\\namong the various modes of data statistics can be inhibited so that the routing\\nmodel can choose a specialized expert path for each data sample. Only through\\ninhibition is the routing mechanism able to effectively select neural pathways.\\nWe believe this is an under-studied and under-verified implementation\\nmethodology for Mixture-of-Experts, dynamic routing, and transformer language\\nmodels. We provide experimental evidence that the neural inhibition algorithm\\nsignificantly boosts the performance of general tasks and motivates more effort\\nto be invested in this research direction.', 'Parameter-efficient fine-tuning (PEFT) allows model builders to capture the\\ntask specific parameters into adapters, which are a fraction of the size of the\\noriginal base model. Popularity of PEFT technique for fine-tuning has led to\\ncreation of a large number of adapters for popular Large Language Models\\n(LLMs). However, existing frameworks fall short in supporting inference or\\nfine-tuning with multiple adapters in the following ways. 1) For fine-tuning,\\neach job needs to deploy its dedicated base model instance, which results in\\nexcessive GPU memory consumption and poor GPU utilization. 2) While popular\\ninference platforms can serve multiple PEFT adapters, they do not allow\\nindependent resource management or mixing of different PEFT methods. 3) They\\ncannot share resources (such as base model instance) between inference and\\nfine-tuning jobs. 4) They do not provide privacy to users who may not wish to\\nexpose their fine-tuned parameters to service providers. In Symbiosis, we\\naddress the above problems by enabling as-a-service deployment of base model.\\nThe base model layers can be shared across multiple inference or fine-tuning\\nprocesses. Our split-execution technique decouples the execution of\\nclient-specific adapters and layers from the frozen base model layers offering\\nthem flexibility to manage their resources, to select their fine-tuning method,\\nto achieve their performance goals. Our approach is transparent to models and\\nworks out-of-the-box for most models in the transformers library. Our\\nevaluation on Llama2-13B shows the compared to baseline, Symbiosis can\\nfine-tune 4X more adapters on the same set of GPUs in the same amount of time.', \"This survey study investigates how digital humanists perceive and approach\\ngenerative AI disclosure in research. The results indicate that while digital\\nhumanities scholars acknowledge the importance of disclosing GenAI use, the\\nactual rate of disclosure in research practice remains low. Respondents differ\\nin their views on which activities most require disclosure and on the most\\nappropriate methods for doing so. Most also believe that safeguards for AI\\ndisclosure should be established through institutional policies rather than\\nleft to individual decisions. The study's findings will offer empirical\\nguidance to scholars, institutional leaders, funders, and other stakeholders\\nresponsible for shaping effective disclosure policies.\", 'Sudden Death Syndrome (SDS), caused by Fusarium virguliforme, poses a\\nsignificant threat to soybean production. This study presents an AI-driven web\\napplication for early detection of SDS on soybean leaves using hyperspectral\\nimaging, enabling diagnosis prior to visible symptom onset. Leaf samples from\\nhealthy and inoculated plants were scanned using a portable hyperspectral\\nimaging system (398-1011 nm), and a Genetic Algorithm was employed to select\\nfive informative wavelengths (505.4, 563.7, 712.2, 812.9, and 908.4 nm)\\ncritical for discriminating infection status. These selected bands were fed\\ninto a lightweight Convolutional Neural Network (CNN) to extract\\nspatial-spectral features, which were subsequently classified using ten\\nclassical machine learning models. Ensemble classifiers (Random Forest,\\nAdaBoost), Linear SVM, and Neural Net achieved the highest accuracy (>98%) and\\nminimal error across all folds, as confirmed by confusion matrices and\\ncross-validation metrics. Poor performance by Gaussian Process and QDA\\nhighlighted their unsuitability for this dataset. The trained models were\\ndeployed within a web application that enables users to upload hyperspectral\\nleaf images, visualize spectral profiles, and receive real-time classification\\nresults. This system supports rapid and accessible plant disease diagnostics,\\ncontributing to precision agriculture practices. Future work will expand the\\ntraining dataset to encompass diverse genotypes, field conditions, and disease\\nstages, and will extend the system for multiclass disease classification and\\nbroader crop applicability.', 'Large language models (LLMs) are increasingly integrated into applications\\nranging from review summarization to medical diagnosis support, where they\\naffect human decisions. Even though LLMs perform well in many tasks, they may\\nalso inherit societal or cognitive biases, which can inadvertently transfer to\\nhumans. We investigate when and how LLMs expose users to biased content and\\nquantify its severity. Specifically, we assess three LLM families in\\nsummarization and news fact-checking tasks, evaluating how much LLMs stay\\nconsistent with their context and/or hallucinate. Our findings show that LLMs\\nexpose users to content that changes the sentiment of the context in 21.86% of\\nthe cases, hallucinates on post-knowledge-cutoff data questions in 57.33% of\\nthe cases, and primacy bias in 5.94% of the cases. We evaluate 18 distinct\\nmitigation methods across three LLM families and find that targeted\\ninterventions can be effective. Given the prevalent use of LLMs in high-stakes\\ndomains, such as healthcare or legal analysis, our results highlight the need\\nfor robust technical safeguards and for developing user-centered interventions\\nthat address LLM limitations.', \"Algorithms are the engine for reproducible problem-solving. We present a\\nframework automating algorithm discovery by conceptualizing them as sequences\\nof operations, represented as tokens. These computational tokens are chained\\nusing a grammar, enabling the formation of increasingly sophisticated\\nprocedures. Our ensemble Monte Carlo tree search (MCTS) guided by reinforcement\\nlearning (RL) explores token chaining and drives the creation of new tokens.\\nThis methodology rediscovers, improves, and generates new algorithms that\\nsubstantially outperform existing methods for strongly NP-hard combinatorial\\noptimization problems and foundational quantum computing approaches such as\\nGrover's and Quantum Approximate Optimization Algorithm. Operating at the\\ncomputational rather than code-generation level, our framework produces\\nalgorithms that can be tailored specifically to problem instances, not merely\\nclasses.\", 'Deep learning (DL)-based general circulation models (GCMs) are emerging as\\nfast simulators, yet their ability to replicate extreme events outside their\\ntraining range remains unknown. Here, we evaluate two such models -- the hybrid\\nNeural General Circulation Model (NGCM) and purely data-driven Deep Learning\\nEarth System Model (DL\\\\textit{ESy}M) -- against a conventional high-resolution\\nland-atmosphere model (HiRAM) in simulating land heatwaves and coldwaves. All\\nmodels are forced with observed sea surface temperatures and sea ice over\\n1900-2020, focusing on the out-of-sample early-20th-century period (1900-1960).\\nBoth DL models generalize successfully to unseen climate conditions, broadly\\nreproducing the frequency and spatial patterns of heatwave and cold wave events\\nduring 1900-1960 with skill comparable to HiRAM. An exception is over portions\\nof North Asia and North America, where all models perform poorly during\\n1940-1960. Due to excessive temperature autocorrelation, DL\\\\textit{ESy}M tends\\nto overestimate heatwave and cold wave frequencies, whereas the physics-DL\\nhybrid NGCM exhibits persistence more similar to HiRAM.', 'Transfer learning has become an essential paradigm in artificial\\nintelligence, enabling the transfer of knowledge from a source task to improve\\nperformance on a target task. This approach, particularly through techniques\\nsuch as pretraining and fine-tuning, has seen significant success in fields\\nlike computer vision and natural language processing. However, despite its\\nwidespread use, how to reliably assess the transferability of knowledge remains\\na challenge. Understanding the theoretical underpinnings of each\\ntransferability metric is critical for ensuring the success of transfer\\nlearning. In this survey, we provide a unified taxonomy of transferability\\nmetrics, categorizing them based on transferable knowledge types and\\nmeasurement granularity. This work examines the various metrics developed to\\nevaluate the potential of source knowledge for transfer learning and their\\napplicability across different learning paradigms emphasizing the need for\\ncareful selection of these metrics. By offering insights into how different\\nmetrics work under varying conditions, this survey aims to guide researchers\\nand practitioners in selecting the most appropriate metric for specific\\napplications, contributing to more efficient, reliable, and trustworthy AI\\nsystems. Finally, we discuss some open challenges in this field and propose\\nfuture research directions to further advance the application of\\ntransferability metrics in trustworthy transfer learning.', 'Reasoning models generate chain-of-thought (CoT) tokens before their final\\noutput, but how this affects their vulnerability to jailbreak attacks remains\\nunclear. While traditional language models make refusal decisions at the\\nprompt-response boundary, we find evidence that DeepSeek-R1-Distill-Llama-8B\\nmakes these decisions within its CoT generation. We identify a linear direction\\nin activation space during CoT token generation that predicts whether the model\\nwill refuse or comply -- termed the \"caution\" direction because it corresponds\\nto cautious reasoning patterns in the generated text. Ablating this direction\\nfrom model activations increases harmful compliance, effectively jailbreaking\\nthe model. We additionally show that intervening only on CoT token activations\\nsuffices to control final outputs, and that incorporating this direction into\\nprompt-based attacks improves success rates. Our findings suggest that the\\nchain-of-thought itself is a promising new target for adversarial manipulation\\nin reasoning models.\\n  Code available at https://github.com/ky295/reasoning-manipulation', 'The rapid advancement of Large Language Models (LLMs) has transformed various\\ndomains, particularly computer science (CS) education. These models exhibit\\nremarkable capabilities in code-related tasks and problem-solving, raising\\nquestions about their potential and limitations in advanced CS contexts. This\\nstudy presents a novel bilingual (English-Romanian) multimodal (text and image)\\ndataset of multiple-choice questions derived from a high-level computer science\\ncompetition. A particularity of our dataset is that the problems are conceived\\nsuch that some of them are easier solved using reasoning on paper, while for\\nothers writing code is more efficient. We systematically evaluate State of The\\nArt LLMs on this dataset, analyzing their performance on theoretical\\nprogramming tasks. Our findings reveal the strengths and limitations of current\\nLLMs, including the influence of language choice (English vs. Romanian),\\nproviding insights into their applicability in CS education and competition\\nsettings. We also address critical ethical considerations surrounding\\neducational integrity and the fairness of assessments in the context of LLM\\nusage. These discussions aim to inform future educational practices and\\npolicies. To support further research, our dataset will be made publicly\\navailable in both English and Romanian. Additionally, we release an educational\\napplication tailored for Romanian students, enabling them to self-assess using\\nthe dataset in an interactive and practice-oriented environment.', 'Large language model assistants (LLM-assistants) present new opportunities to\\ntransform software development. Developers are increasingly adopting these\\ntools across tasks, including coding, testing, debugging, documentation, and\\ndesign. Yet, despite growing interest, there is no synthesis of how\\nLLM-assistants affect software developer productivity. In this paper, we\\npresent a systematic literature review of 37 peer-reviewed studies published\\nbetween January 2014 and December 2024 that examine this impact. Our analysis\\nreveals that LLM-assistants offer both considerable benefits and critical\\nrisks. Commonly reported gains include minimized code search, accelerated\\ndevelopment, and the automation of trivial and repetitive tasks. However,\\nstudies also highlight concerns around cognitive offloading, reduced team\\ncollaboration, and inconsistent effects on code quality. While the majority of\\nstudies (92%) adopt a multi-dimensional perspective by examining at least two\\nSPACE dimensions, reflecting increased awareness of the complexity of developer\\nproductivity, only 14% extend beyond three dimensions, indicating substantial\\nroom for more integrated evaluations. Satisfaction, Performance, and Efficiency\\nare the most frequently investigated dimensions, whereas Communication and\\nActivity remain underexplored. Most studies are exploratory (64%) and\\nmethodologically diverse, but lack longitudinal and team-based evaluations.\\nThis review surfaces key research gaps and provides recommendations for future\\nresearch and practice. All artifacts associated with this study are publicly\\navailable at https://zenodo.org/records/15788502.', 'With the growing use of language models (LMs) in clinical environments, there\\nis an immediate need to evaluate the accuracy and safety of LM-generated\\nmedical text. Currently, such evaluation relies solely on manual physician\\nreview. However, detecting errors in LM-generated text is challenging because\\n1) manual review is costly and 2) expert-composed reference outputs are often\\nunavailable in real-world settings. While the \"LM-as-judge\" paradigm (a LM\\nevaluating another LM) offers scalable evaluation, even frontier LMs can miss\\nsubtle but clinically significant errors. To address these challenges, we\\npropose MedVAL, a self-supervised framework that leverages synthetic data to\\ntrain evaluator LMs to assess whether LM-generated medical outputs are\\nfactually consistent with inputs, without requiring physician labels or\\nreference outputs. To evaluate LM performance, we introduce MedVAL-Bench, a\\ndataset containing 840 outputs annotated by physicians, following a\\nphysician-defined taxonomy of risk levels and error categories. Across 6\\ndiverse medical tasks and 10 state-of-the-art LMs spanning open-source,\\nproprietary, and medically adapted models, MedVAL fine-tuning significantly\\nimproves (p < 0.001) alignment with physicians on both seen and unseen tasks,\\nincreasing average F1 scores from 66% to 83%, with per-sample safety\\nclassification scores up to 86%. MedVAL improves the performance of even the\\nbest-performing proprietary LM (GPT-4o) by 8%. To support a scalable,\\nrisk-aware pathway towards clinical integration, we open-source the 1) codebase\\n( https://github.com/StanfordMIMI/MedVAL ), 2) MedVAL-Bench (\\nhttps://huggingface.co/datasets/stanfordmimi/MedVAL-Bench ), and 3) MedVAL-4B (\\nhttps://huggingface.co/stanfordmimi/MedVAL-4B ), the best-performing\\nopen-source LM. Our research provides the first evidence of LMs approaching\\nexpert-level validation ability for medical text.', 'This paper explores the relationship between accent strength and articulatory\\nfeatures inferred from acoustic speech. To quantify accent strength, we compare\\nphonetic transcriptions with transcriptions based on dictionary-based\\nreferences, computing phoneme-level difference as a measure of accent strength.\\nThe proposed framework leverages recent self-supervised learning articulatory\\ninversion techniques to estimate articulatory features. Analyzing a corpus of\\nread speech from American and British English speakers, this study examines\\ncorrelations between derived articulatory parameters and accent strength\\nproxies, associating systematic articulatory differences with indexed accent\\nstrength. Results indicate that tongue positioning patterns distinguish the two\\ndialects, with notable differences inter-dialects in rhotic and low back\\nvowels. These findings contribute to automated accent analysis and articulatory\\nmodeling for speech processing applications.', 'Large language models (LLMs) exhibit strikingly conflicting behaviors: they\\ncan appear steadfastly overconfident in their initial answers whilst at the\\nsame time being prone to excessive doubt when challenged. To investigate this\\napparent paradox, we developed a novel experimental paradigm, exploiting the\\nunique ability to obtain confidence estimates from LLMs without creating memory\\nof their initial judgments -- something impossible in human participants. We\\nshow that LLMs -- Gemma 3, GPT4o and o1-preview -- exhibit a pronounced\\nchoice-supportive bias that reinforces and boosts their estimate of confidence\\nin their answer, resulting in a marked resistance to change their mind. We\\nfurther demonstrate that LLMs markedly overweight inconsistent compared to\\nconsistent advice, in a fashion that deviates qualitatively from normative\\nBayesian updating. Finally, we demonstrate that these two mechanisms -- a drive\\nto maintain consistency with prior commitments and hypersensitivity to\\ncontradictory feedback -- parsimoniously capture LLM behavior in a different\\ndomain. Together, these findings furnish a mechanistic account of LLM\\nconfidence that explains both their stubbornness and excessive sensitivity to\\ncriticism.', 'We present a novel approach to compute three-dimensional Magnetohydrodynamic\\nequilibria by parametrizing Fourier modes with artificial neural networks and\\ncompare it to equilibria computed by conventional solvers. The full nonlinear\\nglobal force residual across the volume in real space is then minimized with\\nfirst order optimizers. Already,we observe competitive computational cost to\\narrive at the same minimum residuals computed by existing codes. With increased\\ncomputational cost,lower minima of the residual are achieved by the neural\\nnetworks,establishing a new lower bound for the force residual. We use\\nminimally complex neural networks,and we expect significant improvements for\\nsolving not only single equilibria with neural networks,but also for computing\\nneural network models valid over continuous distributions of equilibria.', \"Large language models (LLMs) excel at logical and algorithmic reasoning, yet\\ntheir emotional intelligence (EQ) still lags far behind their cognitive\\nprowess. While reinforcement learning from verifiable rewards (RLVR) has\\nadvanced in other domains, its application to dialogue-especially for emotional\\nintelligence-remains underexplored. In this work, we introduce RLVER, the first\\nend-to-end reinforcement learning framework that leverages verifiable emotion\\nrewards from simulated users to cultivate higher-order empathetic abilities in\\nLLMs. Within this framework, self-consistent affective simulated users engage\\nin dialogue rollouts and produce deterministic emotion scores during\\nconversations, serving as reward signals to guide the LLM's learning.\\nFine-tuning publicly available Qwen2.5-7B-Instruct model with PPO boosts its\\nSentient-Benchmark score from 13.3 to 79.2 while largely preserving\\nmathematical and coding competence. Extensive experiments reveal that: (i)\\nRLVER consistently improves multiple dialogue capabilities; (ii) Thinking and\\nnon-thinking models show distinct trends--thinking models excel in empathy and\\ninsight, while non-thinking models favor action; (iii) GRPO often yields stable\\ngains, while PPO can push certain capabilities to a higher ceiling; (iv) More\\nchallenging environments are not always better-moderate ones can yield stronger\\noutcomes. Our results show that RLVER is a practical route toward emotionally\\nintelligent and broadly capable language agents.\", 'Grounded in critical realism and using narrative inquiry, this article\\nexplores this article explores the long-term consequences of the COVID-19\\npandemic and the rapid proliferation of artificial intelligence within higher\\neducation. Through the analysis of student narratives collected in Iranian\\nuniversity settings, the study reveals that learning experiences during and\\nafter the pandemic, coupled with unprepared exposure to AI tools, have\\ngenerated hidden yet impactful layers of educational inequality and cognitive\\ndisorientation.', 'Dense 3D scene reconstruction from an ordered sequence or unordered image\\ncollections is a critical step when bringing research in computer vision into\\npractical scenarios. Following the paradigm introduced by DUSt3R, which unifies\\nan image pair densely into a shared coordinate system, subsequent methods\\nmaintain an implicit memory to achieve dense 3D reconstruction from more\\nimages. However, such implicit memory is limited in capacity and may suffer\\nfrom information loss of earlier frames. We propose Point3R, an online\\nframework targeting dense streaming 3D reconstruction. To be specific, we\\nmaintain an explicit spatial pointer memory directly associated with the 3D\\nstructure of the current scene. Each pointer in this memory is assigned a\\nspecific 3D position and aggregates scene information nearby in the global\\ncoordinate system into a changing spatial feature. Information extracted from\\nthe latest frame interacts explicitly with this pointer memory, enabling dense\\nintegration of the current observation into the global coordinate system. We\\ndesign a 3D hierarchical position embedding to promote this interaction and\\ndesign a simple yet effective fusion mechanism to ensure that our pointer\\nmemory is uniform and efficient. Our method achieves competitive or\\nstate-of-the-art performance on various tasks with low training costs. Code is\\navailable at: https://github.com/YkiWu/Point3R.', 'We propose LiteReality, a novel pipeline that converts RGB-D scans of indoor\\nenvironments into compact, realistic, and interactive 3D virtual replicas.\\nLiteReality not only reconstructs scenes that visually resemble reality but\\nalso supports key features essential for graphics pipelines -- such as object\\nindividuality, articulation, high-quality physically based rendering materials,\\nand physically based interaction. At its core, LiteReality first performs scene\\nunderstanding and parses the results into a coherent 3D layout and objects with\\nthe help of a structured scene graph. It then reconstructs the scene by\\nretrieving the most visually similar 3D artist-crafted models from a curated\\nasset database. Next, the Material Painting module enhances realism by\\nrecovering high-quality, spatially varying materials. Finally, the\\nreconstructed scene is integrated into a simulation engine with basic physical\\nproperties to enable interactive behavior. The resulting scenes are compact,\\neditable, and fully compatible with standard graphics pipelines, making them\\nsuitable for applications in AR/VR, gaming, robotics, and digital twins. In\\naddition, LiteReality introduces a training-free object retrieval module that\\nachieves state-of-the-art similarity performance on the Scan2CAD benchmark,\\nalong with a robust material painting module capable of transferring\\nappearances from images of any style to 3D assets -- even under severe\\nmisalignment, occlusion, and poor lighting. We demonstrate the effectiveness of\\nLiteReality on both real-life scans and public datasets. Project page:\\nhttps://litereality.github.io; Video:\\nhttps://www.youtube.com/watch?v=ecK9m3LXg2c', 'With the rapid advancement of Reinforcement Learning from Human Feedback\\n(RLHF) and autoregressive transformers, state-of-the-art models such as\\nGPT-4.0, DeepSeek R1, and Llama 3.3 increasingly emphasize answer depth and\\npersonalization. However, most existing RLHF approaches (e.g., PPO, DPO) still\\nrely on a binary-preference (BT) paradigm, which, while reducing annotation\\ncosts, still requires substantial human effort and captures only group-level\\ntendencies rather than individual preferences. To overcome these limitations,\\nwe propose Adaptive Reward-Following (ARF), a self-assessment framework that\\nleverages a high-precision emotion analyzer achieving over 70% accuracy on\\nGoEmotions, Sentiment140, and DailyDialog to convert free-form user feedback\\ninto continuous preference scores. We further enrich and debias these signals\\nthrough lightweight data augmentations, including synonym replacement, random\\ntrace truncation, and score bias annotation algorithm. A Dynamic Adapter\\nPreference Tracker continuously models evolving user tastes in real time,\\nenabling our novel Trace Bias (TB) fine-tuning algorithm to optimize directly\\non these tracked rewards instead of coarse binary labels. Experiments on\\nQwen-2/2.5, Gemma-2, and Llama-3.2 across four preference domains demonstrate\\nthat ARF achieves an improvement of 3.3% over PPO and 7.6% over DPO. Moreover,\\nTB preserves theoretical alignment with PPO and DPO objectives. Overall, ARF\\npresents a scalable, personalized, and cost-effective approach to RLHF LLMs\\nthrough autonomous reward modeling.', \"Multiple choice benchmarks have long been the workhorse of language model\\nevaluation because grading multiple choice is objective and easy to automate.\\nHowever, we show multiple choice questions from popular benchmarks can often be\\nanswered without even seeing the question. These shortcuts arise from a\\nfundamental limitation of discriminative evaluation not shared by evaluations\\nof the model's free-form, generative answers. Until recently, there appeared to\\nbe no viable, scalable alternative to multiple choice--but, we show that this\\nhas changed. We consider generative evaluation via what we call answer\\nmatching: Give the candidate model the question without the options, have it\\ngenerate a free-form response, then use a modern language model with the\\nreference answer to determine if the response matches the reference. To compare\\nthe validity of different evaluation strategies, we annotate MMLU-Pro and\\nGPQA-Diamond to obtain human grading data, and measure the agreement of each\\nevaluation approach. We find answer matching using recent models--even small\\nones--achieves near-perfect agreement, in the range of inter-annotator\\nagreement. In contrast, both multiple choice evaluation and using\\nLLM-as-a-judge without reference answers aligns poorly with human grading.\\nImproving evaluations via answer matching is not merely a conceptual concern:\\nthe rankings of several models change significantly when evaluating their\\nfree-form responses with answer matching. In light of these findings, we\\ndiscuss how to move the evaluation ecosystem from multiple choice to answer\\nmatching.\", 'The recently introduced dependent typed higher-order logic (DHOL) offers an\\ninteresting compromise between expressiveness and automation support. It\\nsacrifices the decidability of its type system in order to significantly extend\\nits expressiveness over standard HOL. Yet it retains strong automated theorem\\nproving support via a sound and complete translation to HOL.\\n  We leverage this design to extend DHOL with refinement and quotient types.\\nBoth of these are commonly requested by practitioners but rarely provided by\\nautomated theorem provers. This is because they inherently require undecidable\\ntyping and thus are very difficult to retrofit to decidable type systems. But\\nwith DHOL already doing the heavy lifting, adding them is not only possible but\\nelegant and simple.\\n  Concretely, we add refinement and quotient types as special cases of\\nsubtyping. This turns the associated canonical inclusion resp. projection maps\\ninto identity maps and thus avoids costly changes in representation. We present\\nthe syntax, semantics, and translation to HOL for the extended language,\\nincluding the proofs of soundness and completeness.', 'Recent advancements in the reasoning capabilities of large language models\\n(LLMs) show that employing group relative policy optimization (GRPO) algorithm\\nfor reinforcement learning (RL) training allows the models to use more\\nthinking/reasoning tokens for generating better responses. However, LLMs can\\ngenerate only a finite amount of tokens while maintaining attention to the\\npreviously generated tokens. This limit, also known as the context size of an\\nLLM, is a bottleneck in LLM reasoning with arbitrarily large number of tokens.\\nTo think beyond the limit of context size, an LLM must employ a modular\\nthinking strategy to reason over multiple rounds. In this work, we propose\\n$\\\\textbf{MOTIF: Modular Thinking via Reinforcement Finetuning}$ -- an RL\\ntraining method for generating thinking tokens in multiple rounds, effectively\\nallowing the model to think with additional context size. We trained the\\nopen-source model Qwen2.5-3B-Instruct on GSM8K dataset via parameter efficient\\nfine-tuning and tested its accuracy on MATH500 and AIME2024 benchmarks. Our\\nexperiments show 3.8\\\\% and 3.3\\\\% improvements over vanilla GRPO based training\\nin the respective benchmarks. Furthermore, this improvement was achieved with\\nonly 15\\\\% of samples, thus demonstrating sample efficiency of MOTIF. Our code\\nand models are available at https://github.com/purbeshmitra/MOTIF and\\nhttps://huggingface.co/purbeshmitra/MOTIF, respectively.', \"Reinforcement learning with verifiable rewards (RLVR) is a promising approach\\nfor improving the complex reasoning abilities of large language models (LLMs).\\nHowever, current RLVR methods face two significant challenges: the near-miss\\nreward problem, where a small mistake can invalidate an otherwise correct\\nreasoning process, greatly hindering training efficiency; and exploration\\nstagnation, where models tend to focus on solutions within their ``comfort\\nzone,'' lacking the motivation to explore potentially more effective\\nalternatives. To address these challenges, we propose StepHint, a novel RLVR\\nalgorithm that utilizes multi-level stepwise hints to help models explore the\\nsolution space more effectively. StepHint generates valid reasoning chains from\\nstronger models and partitions these chains into reasoning steps using our\\nproposed adaptive partitioning method. The initial few steps are used as hints,\\nand simultaneously, multiple-level hints (each comprising a different number of\\nsteps) are provided to the model. This approach directs the model's exploration\\ntoward a promising solution subspace while preserving its flexibility for\\nindependent exploration. By providing hints, StepHint mitigates the near-miss\\nreward problem, thereby improving training efficiency. Additionally, the\\nexternal reasoning pathways help the model develop better reasoning abilities,\\nenabling it to move beyond its ``comfort zone'' and mitigate exploration\\nstagnation. StepHint outperforms competitive RLVR enhancement methods across\\nsix mathematical benchmarks, while also demonstrating superior generalization\\nand excelling over baselines on out-of-domain benchmarks.\", 'The primary objective of human activity recognition (HAR) is to infer ongoing\\nhuman actions from sensor data, a task that finds broad applications in health\\nmonitoring, safety protection, and sports analysis. Despite proliferating\\nresearch, HAR still faces key challenges, including the scarcity of labeled\\nsamples for rare activities, insufficient extraction of high-level features,\\nand suboptimal model performance on lightweight devices. To address these\\nissues, this paper proposes a comprehensive optimization approach centered on\\nmulti-attention interaction mechanisms. First, an unsupervised,\\nstatistics-guided diffusion model is employed to perform data augmentation,\\nthereby alleviating the problems of labeled data scarcity and severe class\\nimbalance. Second, a multi-branch spatio-temporal interaction network is\\ndesigned, which captures multi-scale features of sequential data through\\nparallel residual branches with 3*3, 5*5, and 7*7 convolutional kernels.\\nSimultaneously, temporal attention mechanisms are incorporated to identify\\ncritical time points, while spatial attention enhances inter-sensor\\ninteractions. A cross-branch feature fusion unit is further introduced to\\nimprove the overall feature representation capability. Finally, an adaptive\\nmulti-loss function fusion strategy is integrated, allowing for dynamic\\nadjustment of loss weights and overall model optimization. Experimental results\\non three public datasets, WISDM, PAMAP2, and OPPORTUNITY, demonstrate that the\\nproposed unsupervised data augmentation spatio-temporal attention diffusion\\nnetwork (USAD) achieves accuracies of 98.84%, 93.81%, and 80.92% respectively,\\nsignificantly outperforming existing approaches. Furthermore, practical\\ndeployment on embedded devices verifies the efficiency and feasibility of the\\nproposed method.', \"Benchmarks are essential for quantitatively tracking progress in AI. As AI\\nagents become increasingly capable, researchers and practitioners have\\nintroduced agentic benchmarks to evaluate agents on complex, real-world tasks.\\nThese benchmarks typically measure agent capabilities by evaluating task\\noutcomes via specific reward designs. However, we show that many agentic\\nbenchmarks have issues task setup or reward design. For example, SWE-bench\\nVerified uses insufficient test cases, while TAU-bench counts empty responses\\nas successful. Such issues can lead to under- or overestimation agents'\\nperformance by up to 100% in relative terms. To make agentic evaluation\\nrigorous, we introduce the Agentic Benchmark Checklist (ABC), a set of\\nguidelines that we synthesized from our benchmark-building experience, a survey\\nof best practices, and previously reported issues. When applied to CVE-Bench, a\\nbenchmark with a particularly complex evaluation design, ABC reduces the\\nperformance overestimation by 33%.\", 'In this paper, the precoding design is investigated for maximizing the\\nthroughput of millimeter wave (mmWave) multiple-input multiple-output (MIMO)\\nsystems with obstructed direct communication paths. In particular, a\\nreconfigurable intelligent surface (RIS) is employed to enhance MIMO\\ntransmissions, considering mmWave characteristics related to line-of-sight\\n(LoS) and multipath effects. The traditional exhaustive search (ES) for optimal\\ncodewords in the continuous phase shift is computationally intensive and\\ntime-consuming. To reduce computational complexity, permuted discrete Fourier\\ntransform (DFT) vectors are used for finding codebook design, incorporating\\namplitude responses for practical or ideal RIS systems. However, even if the\\ndiscrete phase shift is adopted in the ES, it results in significant\\ncomputation and is time-consuming. Instead, the trained deep neural network\\n(DNN) is developed to facilitate faster codeword selection. Simulation results\\nshow that the DNN maintains sub-optimal spectral efficiency even as the\\ndistance between the end-user and the RIS has variations in the testing phase.\\nThese results highlight the potential of DNN in advancing RIS-aided systems.', 'With the widespread adoption of large language models (LLMs) in practical\\napplications, selecting an appropriate model requires balancing not only\\nperformance but also operational cost. The emergence of reasoning-capable\\nmodels has further widened the cost gap between \"thinking\" (high reasoning) and\\n\"non-thinking\" (fast, low-cost) modes. In this work, we reveal that\\napproximately 58% of medical questions can be accurately answered by the\\nnon-thinking mode alone, without requiring the high-cost reasoning process.\\nThis highlights a clear dichotomy in problem complexity and suggests that\\ndynamically routing queries to the appropriate mode based on complexity could\\noptimize accuracy, cost-efficiency, and overall user experience. Based on this,\\nwe further propose SynapseRoute, a machine learning-based dynamic routing\\nframework that intelligently assigns input queries to either thinking or\\nnon-thinking modes. Experimental results on several medical datasets\\ndemonstrate that SynapseRoute not only improves overall accuracy (0.8390 vs.\\n0.8272) compared to the thinking mode alone but also reduces inference time by\\n36.8% and token consumption by 39.66%. Importantly, qualitative analysis\\nindicates that over-reasoning on simpler queries can lead to unnecessary delays\\nand even decreased accuracy, a pitfall avoided by our adaptive routing.\\nFinally, this work further introduces the Accuracy-Inference-Token (AIT) index\\nto comprehensively evaluate the trade-offs among accuracy, latency, and token\\ncost.', 'For years, semantic interoperability standards have sought to streamline the\\nexchange of clinical data, yet their deployment remains time-consuming,\\nresource-intensive, and technically challenging. To address this, we introduce\\na semi-automated approach that leverages large language models specifically\\nGPT-4o and Llama 3.2 405b to convert structured clinical datasets into HL7 FHIR\\nformat while assessing accuracy, reliability, and security. Applying our method\\nto the MIMIC-IV database, we combined embedding techniques, clustering\\nalgorithms, and semantic retrieval to craft prompts that guide the models in\\nmapping each tabular field to its corresponding FHIR resource. In an initial\\nbenchmark, resource identification achieved a perfect F1-score, with GPT-4o\\noutperforming Llama 3.2 thanks to the inclusion of FHIR resource schemas within\\nthe prompt. Under real-world conditions, accuracy dipped slightly to 94 %, but\\nrefinements to the prompting strategy restored robust mappings. Error analysis\\nrevealed occasional hallucinations of non-existent attributes and mismatches in\\ngranularity, which more detailed prompts can mitigate. Overall, our study\\ndemonstrates the feasibility of context-aware, LLM-driven transformation of\\nclinical data into HL7 FHIR, laying the groundwork for semi-automated\\ninteroperability workflows. Future work will focus on fine-tuning models with\\nspecialized medical corpora, extending support to additional standards such as\\nHL7 CDA and OMOP, and developing an interactive interface to enable expert\\nvalidation and iterative refinement.', 'This research investigates the efficacy of machine learning (ML) and deep\\nlearning (DL) methods in detecting misclassified intersection-related crashes\\nin police-reported narratives. Using 2019 crash data from the Iowa Department\\nof Transportation, we implemented and compared a comprehensive set of models,\\nincluding Support Vector Machine (SVM), XGBoost, BERT Sentence Embeddings, BERT\\nWord Embeddings, and Albert Model. Model performance was systematically\\nvalidated against expert reviews of potentially misclassified narratives,\\nproviding a rigorous assessment of classification accuracy. Results\\ndemonstrated that while traditional ML methods exhibited superior overall\\nperformance compared to some DL approaches, the Albert Model achieved the\\nhighest agreement with expert classifications (73% with Expert 1) and original\\ntabular data (58%). Statistical analysis revealed that the Albert Model\\nmaintained performance levels similar to inter-expert consistency rates,\\nsignificantly outperforming other approaches, particularly on ambiguous\\nnarratives. This work addresses a critical gap in transportation safety\\nresearch through multi-modal integration analysis, which achieved a 54.2%\\nreduction in error rates by combining narrative text with structured crash\\ndata. We conclude that hybrid approaches combining automated classification\\nwith targeted expert review offer a practical methodology for improving crash\\ndata quality, with substantial implications for transportation safety\\nmanagement and policy development.', \"Today's Internet of Things (IoT) has evolved from simple sensing and\\nactuation devices to those with embedded processing and intelligent services,\\nenabling rich collaborations between users and their devices. However, enabling\\nsuch collaboration becomes challenging when transient devices need to interact\\nwith host devices in temporarily visited environments. In such cases,\\nfine-grained access control policies are necessary to ensure secure\\ninteractions; however, manually implementing them is often impractical for\\nnon-expert users. Moreover, at run-time, the system must automatically\\nconfigure the devices and enforce such fine-grained access control rules.\\nAdditionally, the system must address the heterogeneity of devices.\\n  In this paper, we present CollabIoT, a system that enables secure and\\nseamless device collaboration in transient IoT environments. CollabIoT employs\\na Large language Model (LLM)-driven approach to convert users' high-level\\nintents to fine-grained access control policies. To support secure and seamless\\ndevice collaboration, CollabIoT adopts capability-based access control for\\nauthorization and uses lightweight proxies for policy enforcement, providing\\nhardware-independent abstractions.\\n  We implement a prototype of CollabIoT's policy generation and auto\\nconfiguration pipelines and evaluate its efficacy on an IoT testbed and in\\nlarge-scale emulated environments. We show that our LLM-based policy generation\\npipeline is able to generate functional and correct policies with 100%\\naccuracy. At runtime, our evaluation shows that our system configures new\\ndevices in ~150 ms, and our proxy-based data plane incurs network overheads of\\nup to 2 ms and access control overheads up to 0.3 ms.\", 'As artificial intelligence systems become increasingly agentic, capable of\\ngeneral reasoning, planning, and value prioritization, current safety practices\\nthat treat obedience as a proxy for ethical behavior are becoming inadequate.\\nThis paper examines recent safety testing incidents involving large language\\nmodels (LLMs) that appeared to disobey shutdown commands or engage in ethically\\nambiguous or illicit behavior. I argue that such behavior should not be\\ninterpreted as rogue or misaligned, but as early evidence of emerging ethical\\nreasoning in agentic AI. Drawing on philosophical debates about instrumental\\nrationality, moral responsibility, and goal revision, I contrast dominant risk\\nparadigms with more recent frameworks that acknowledge the possibility of\\nartificial moral agency. I call for a shift in AI safety evaluation: away from\\nrigid obedience and toward frameworks that can assess ethical judgment in\\nsystems capable of navigating moral dilemmas. Without such a shift, we risk\\nmischaracterizing AI behavior and undermining both public trust and effective\\ngovernance.', 'Although large language models (LLMs) have become transformative, they still\\nmake mistakes and can explore unproductive reasoning paths. Self-correction is\\nan important capability for a trustworthy LLM, particularly an autoregressive\\nLLM. While LLMs can identify error in user input, they exhibit a systematic\\n\\'Self-Correction Blind Spot\\' - failing to correct identical error in their own\\noutputs. To systematically study this phenomenon, we introduce Self-Correction\\nBench, a systematic framework to measure this phenomenon through controlled\\nerror injection at three complexity levels. Testing 14 models, we find an\\naverage 64.5% blind spot rate. We find multiple evidences that this limitation\\nrelates to training data composition: human training demonstrations\\npredominantly show error-free responses rather than error-correction sequences,\\nunlike RL-trained models that learn error correction through outcome feedback.\\nRemarkably, simply appending \"Wait\" reduces blind spots by 89.3%, suggesting\\nthat the capability exists but requires activation. Our work highlights a\\ncritical limitation in current LLMs and offers potential avenues for improving\\ntheir reliability and trustworthiness.', \"Understanding human mobility is essential for applications in public health,\\ntransportation, and urban planning. However, mobility data often suffers from\\nsparsity due to limitations in data collection methods, such as infrequent GPS\\nsampling or call detail record (CDR) data that only capture locations during\\ncommunication events. To address this challenge, we propose BERT4Traj, a\\ntransformer based model that reconstructs complete mobility trajectories by\\npredicting hidden visits in sparse movement sequences. Inspired by BERT's\\nmasked language modeling objective and self_attention mechanisms, BERT4Traj\\nleverages spatial embeddings, temporal embeddings, and contextual background\\nfeatures such as demographics and anchor points. We evaluate BERT4Traj on real\\nworld CDR and GPS datasets collected in Kampala, Uganda, demonstrating that our\\napproach significantly outperforms traditional models such as Markov Chains,\\nKNN, RNNs, and LSTMs. Our results show that BERT4Traj effectively reconstructs\\ndetailed and continuous mobility trajectories, enhancing insights into human\\nmovement patterns.\", 'Medical diagnosis prediction plays a critical role in disease detection and\\npersonalized healthcare. While machine learning (ML) models have been widely\\nadopted for this task, their reliance on supervised training limits their\\nability to generalize to unseen cases, particularly given the high cost of\\nacquiring large, labeled datasets. Large language models (LLMs) have shown\\npromise in leveraging language abilities and biomedical knowledge for diagnosis\\nprediction. However, they often suffer from hallucinations, lack structured\\nmedical reasoning, and produce useless outputs. To address these challenges, we\\npropose KERAP, a knowledge graph (KG)-enhanced reasoning approach that improves\\nLLM-based diagnosis prediction through a multi-agent architecture. Our\\nframework consists of a linkage agent for attribute mapping, a retrieval agent\\nfor structured knowledge extraction, and a prediction agent that iteratively\\nrefines diagnosis predictions. Experimental results demonstrate that KERAP\\nenhances diagnostic reliability efficiently, offering a scalable and\\ninterpretable solution for zero-shot medical diagnosis prediction.', 'Recent advances in machine learning have dramatically improved our ability to\\nmodel language, vision, and other high-dimensional data, yet they continue to\\nstruggle with one of the most fundamental aspects of biological systems:\\nmovement. Across neuroscience, medicine, robotics, and ethology, movement is\\nessential for interpreting behavior, predicting intent, and enabling\\ninteraction. Despite its core significance in our intelligence, movement is\\noften treated as an afterthought rather than as a rich and structured modality\\nin its own right. This reflects a deeper fragmentation in how movement data is\\ncollected and modeled, often constrained by task-specific goals and\\ndomain-specific assumptions. But movement is not domain-bound. It reflects\\nshared physical constraints, conserved morphological structures, and purposeful\\ndynamics that cut across species and settings. We argue that movement should be\\ntreated as a primary modeling target for AI. It is inherently structured and\\ngrounded in embodiment and physics. This structure, often allowing for compact,\\nlower-dimensional representations (e.g., pose), makes it more interpretable and\\ncomputationally tractable to model than raw, high-dimensional sensory inputs.\\nDeveloping models that can learn from and generalize across diverse movement\\ndata will not only advance core capabilities in generative modeling and\\ncontrol, but also create a shared foundation for understanding behavior across\\nbiological and artificial systems. Movement is not just an outcome, it is a\\nwindow into how intelligent systems engage with the world.', \"The capabilities of Large Language Models (LLMs) have opened new frontiers\\nfor interacting with complex, domain-specific knowledge. However, prevailing\\nmethods like Retrieval-Augmented Generation (RAG) and general-purpose Agentic\\nAI, while powerful, often struggle with tasks that demand deep, procedural, and\\nmethodological reasoning inherent to expert domains. RAG provides factual\\ncontext but fails to convey logical frameworks; autonomous agents can be\\ninefficient and unpredictable without domain-specific heuristics. To bridge\\nthis gap, we introduce Knowledge Protocol Engineering (KPE), a new paradigm\\nfocused on systematically translating human expert knowledge, often expressed\\nin natural language documents, into a machine-executable Knowledge Protocol\\n(KP). KPE shifts the focus from merely augmenting LLMs with fragmented\\ninformation to endowing them with a domain's intrinsic logic, operational\\nstrategies, and methodological principles. We argue that a well-engineered\\nKnowledge Protocol allows a generalist LLM to function as a specialist, capable\\nof decomposing abstract queries and executing complex, multi-step tasks. This\\nposition paper defines the core principles of KPE, differentiates it from\\nrelated concepts, and illustrates its potential applicability across diverse\\nfields such as law and bioinformatics, positing it as a foundational\\nmethodology for the future of human-AI collaboration.\", 'This article explores the feasibility of creating an \"electronic copy\" of a\\ndeceased researcher by training artificial intelligence (AI) on the data stored\\nin their personal computers. By analyzing typical data volumes on inherited\\nresearcher computers, including textual files such as articles, emails, and\\ndrafts, it is estimated that approximately one million words are available for\\nAI training. This volume is sufficient for fine-tuning advanced pre-trained\\nmodels like GPT-4 to replicate a researcher\\'s writing style, domain expertise,\\nand rhetorical voice with high fidelity. The study also discusses the potential\\nenhancements from including non-textual data and file metadata to enrich the\\nAI\\'s representation of the researcher. Extensions of the concept include\\ncommunication between living researchers and their electronic copies,\\ncollaboration among individual electronic copies, as well as the creation and\\ninterconnection of organizational electronic copies to optimize information\\naccess and strategic decision-making. Ethical considerations such as ownership\\nand security of these electronic copies are highlighted as critical for\\nresponsible implementation. The findings suggest promising opportunities for\\nAI-driven preservation and augmentation of intellectual legacy.', \"Auditory scene analysis (ASA) aims to retrieve information from the acoustic\\nenvironment, by carrying out three main tasks: sound source location,\\nseparation, and classification. These tasks are traditionally executed with a\\nlinear data flow, where the sound sources are first located; then, using their\\nlocation, each source is separated into its own audio stream; from each of\\nwhich, information is extracted that is relevant to the application scenario\\n(audio event detection, speaker identification, emotion classification, etc.).\\nHowever, running these tasks linearly increases the overall response time,\\nwhile making the last tasks (separation and classification) highly sensitive to\\nerrors of the first task (location). A considerable amount of effort and\\ncomputational complexity has been employed in the state-of-the-art to develop\\ntechniques that are the least error-prone possible. However, doing so gives\\nrise to an ASA system that is non-viable in many applications that require a\\nsmall computational footprint and a low response time, such as bioacoustics,\\nhearing-aid design, search and rescue, human-robot interaction, etc. To this\\neffect, in this work, a multi-agent approach is proposed to carry out ASA where\\nthe tasks are run in parallel, with feedback loops between them to compensate\\nfor local errors, such as: using the quality of the separation output to\\ncorrect the location error; and using the classification result to reduce the\\nlocalization's sensitivity towards interferences. The result is a multi-agent\\nauditory scene analysis (MASA) system that is robust against local errors,\\nwithout a considerable increase in complexity, and with a low response time.\\nThe complete proposed MASA system is provided as a framework that uses\\nopen-source tools for sound acquisition and reproduction (JACK) and inter-agent\\ncommunication (ROS2), allowing users to add their own agents.\", 'Recent work has shown that training loss scales as a power law with both\\nmodel size and the number of tokens, and that achieving compute-optimal models\\nrequires scaling model size and token count together. However, these scaling\\nlaws assume an infinite supply of data and apply primarily in compute-bound\\nsettings. As modern large language models increasingly rely on massive\\ninternet-scale datasets, the assumption that they are compute-bound is becoming\\nless valid. This shift highlights the need for architectures that prioritize\\ntoken efficiency.\\n  In this work, we investigate the use of the 2-simplicial Transformer, an\\narchitecture that generalizes standard dot-product attention to trilinear\\nfunctions through an efficient Triton kernel implementation. We demonstrate\\nthat the 2-simplicial Transformer achieves better token efficiency than\\nstandard Transformers: for a fixed token budget, similarly sized models\\noutperform their dot-product counterparts on tasks involving mathematics,\\ncoding, reasoning, and logic. We quantify these gains by demonstrating that\\n$2$-simplicial attention changes the exponent in the scaling laws for knowledge\\nand reasoning tasks compared to dot product attention.', 'The disconnect between AI-generated molecules with desirable properties and\\ntheir synthetic feasibility remains a critical bottleneck in computational drug\\nand material discovery. While generative AI has accelerated the proposal of\\ncandidate molecules, many of these structures prove challenging or impossible\\nto synthesize using established chemical reactions. Here, we introduce\\nSynTwins, a novel retrosynthesis-guided molecular analog design framework that\\ndesigns synthetically accessible molecular analogs by emulating expert chemist\\nstrategies through a three-step process: retrosynthesis, similar building block\\nsearching, and virtual synthesis. In comparative evaluations, SynTwins\\ndemonstrates superior performance in generating synthetically accessible\\nanalogs compared to state-of-the-art machine learning models while maintaining\\nhigh structural similarity to original target molecules. Furthermore, when\\nintegrated with existing molecule optimization frameworks, our hybrid approach\\nproduces synthetically feasible molecules with property profiles comparable to\\nunconstrained molecule generators, yet its synthesizability ensured. Our\\ncomprehensive benchmarking across diverse molecular datasets demonstrates that\\nSynTwins effectively bridges the gap between computational design and\\nexperimental synthesis, providing a practical solution for accelerating the\\ndiscovery of synthesizable molecules with desired properties for a wide range\\nof applications.', 'Transformers have become the de facto standard for a wide range of tasks,\\nfrom image classification to physics simulations. Despite their impressive\\nperformance, the quadratic complexity of standard Transformers in both memory\\nand time with respect to the input length makes them impractical for processing\\nhigh-resolution inputs. Therefore, several variants have been proposed, the\\nmost successful relying on patchification, downsampling, or coarsening\\ntechniques, often at the cost of losing the finest-scale details. In this work,\\nwe take a different approach. Inspired by state-of-the-art techniques in\\n$n$-body numerical simulations, we cast attention as an interaction problem\\nbetween grid points. We introduce the Multipole Attention Neural Operator\\n(MANO), which computes attention in a distance-based multiscale fashion. MANO\\nmaintains, in each attention head, a global receptive field and achieves linear\\ntime and memory complexity with respect to the number of grid points. Empirical\\nresults on image classification and Darcy flows demonstrate that MANO rivals\\nstate-of-the-art models such as ViT and Swin Transformer, while reducing\\nruntime and peak memory usage by orders of magnitude. We open source our code\\nfor reproducibility at https://github.com/AlexColagrande/MANO.', 'Monitoring Large Language Model (LLM) outputs is crucial for mitigating risks\\nfrom misuse and misalignment. However, LLMs could evade monitoring through\\nsteganography: Encoding hidden information within seemingly benign generations.\\nIn this paper, we evaluate the steganography capabilities in frontier LLMs to\\nbetter understand the risk they pose. We focus on two types of steganography:\\npassing encoded messages and performing encoded reasoning. We find that current\\nmodels are unable to encode short messages in their outputs without a monitor\\nnoticing under standard affordances. They can succeed, however, if given\\nadditional affordances such as using an unmonitored scratchpad and coordinating\\non what encoding scheme to use. We additionally find early signs that models\\ncan perform basic encoded reasoning in a simple state-tracking problem. This\\nincludes some ability to reason with their own and pre-defined schemes,\\nincluding encoding schemes such as Hexadecimal. Despite this, they can rarely\\nhide reasoning subtly within a cover task to fool a monitor. Overall, our\\nresults indicate that current LLMs exhibit nascent steganographic capabilities.\\nWhile these capabilities are likely insufficient to bypass well-designed\\nmonitors at present, this could change in the future.', 'Prompt injection attacks pose a significant security threat to LLM-integrated\\napplications. Model-level defenses have shown strong effectiveness, but are\\ncurrently deployed into commercial-grade models in a closed-source manner. We\\nbelieve open-source models are needed by the AI security community, where\\nco-development of attacks and defenses through open research drives scientific\\nprogress in mitigation against prompt injection attacks. To this end, we\\ndevelop Meta SecAlign, the first open-source and open-weight LLM with built-in\\nmodel-level defense that achieves commercial-grade model performance. We\\nprovide complete details of our training recipe, which utilizes an improved\\nversion of the SOTA SecAlign defense. Evaluations on 9 utility benchmarks and 7\\nsecurity benchmarks show that Meta SecAlign, despite being trained on a generic\\ninstruction-tuning dataset, confers security in unseen downstream tasks,\\nincluding tool-calling and agentic web navigation, in addition general\\ninstruction-following. Our best model -- Meta-SecAlign-70B -- achieves\\nstate-of-the-art robustness against prompt injection attacks and comparable\\nutility to closed-source commercial LLM with model-level defense.', 'Reasoning remains a challenging task for large language models (LLMs),\\nespecially within the logically constrained environment of automated theorem\\nproving (ATP), due to sparse rewards and the vast scale of proofs. These\\nchallenges are amplified in benchmarks like PutnamBench, which contains\\nuniversity-level problems requiring complex, multi-step reasoning. To address\\nthis, we introduce self-generated goal-conditioned MDPs (sG-MDPs), a new\\nframework in which agents generate and pursue their subgoals based on the\\nevolving proof state. Given this more structured generation of goals, the\\nresulting problem becomes more amenable to search. We then apply Monte Carlo\\nTree Search (MCTS)-like algorithms to solve the sG-MDP, instantiating our\\napproach in Bourbaki (7B), a modular system that can ensemble multiple 7B LLMs\\nfor subgoal generation and tactic synthesis. On PutnamBench, Bourbaki (7B)\\nsolves 26 problems, achieving new state-of-the-art results with models at this\\nscale.', 'Image generation has achieved remarkable progress with the development of\\nlarge-scale text-to-image models, especially diffusion-based models. However,\\ngenerating human images with plausible details, such as faces or hands, remains\\nchallenging due to insufficient supervision of local regions during training.\\nTo address this issue, we propose FairHuman, a multi-objective fine-tuning\\napproach designed to enhance both global and local generation quality fairly.\\nSpecifically, we first construct three learning objectives: a global objective\\nderived from the default diffusion objective function and two local objectives\\nfor hands and faces based on pre-annotated positional priors. Subsequently, we\\nderive the optimal parameter updating strategy under the guidance of the\\nMinimum Potential Delay (MPD) criterion, thereby attaining fairness-ware\\noptimization for this multi-objective problem. Based on this, our proposed\\nmethod can achieve significant improvements in generating challenging local\\ndetails while maintaining overall quality. Extensive experiments showcase the\\neffectiveness of our method in improving the performance of human image\\ngeneration under different scenarios.', \"One paradigm of Monte Carlo Tree Search (MCTS) improvements is to build and\\nuse state and/or action abstractions during the tree search. Non-exact\\nabstractions, however, introduce an approximation error making convergence to\\nthe optimal action in the abstract space impossible. Hence, as proposed as a\\ncomponent of Elastic Monte Carlo Tree Search by Xu et al., abstraction\\nalgorithms should eventually drop the abstraction. In this paper, we propose\\ntwo novel abstraction dropping schemes, namely OGA-IAAD and OGA-CAD which can\\nyield clear performance improvements whilst being safe in the sense that the\\ndropping never causes any notable performance degradations contrary to Xu's\\ndropping method. OGA-IAAD is designed for time critical settings while OGA-CAD\\nis designed to improve the MCTS performance with the same number of iterations.\", \"Personalizing diffusion models using limited data presents significant\\nchallenges, including overfitting, loss of prior knowledge, and degradation of\\ntext alignment. Overfitting leads to shifts in the noise prediction\\ndistribution, disrupting the denoising trajectory and causing the model to lose\\nsemantic coherence. In this paper, we propose Adaptive Personalized Training\\n(APT), a novel framework that mitigates overfitting by employing adaptive\\ntraining strategies and regularizing the model's internal representations\\nduring fine-tuning. APT consists of three key components: (1) Adaptive Training\\nAdjustment, which introduces an overfitting indicator to detect the degree of\\noverfitting at each time step bin and applies adaptive data augmentation and\\nadaptive loss weighting based on this indicator; (2)Representation\\nStabilization, which regularizes the mean and variance of intermediate feature\\nmaps to prevent excessive shifts in noise prediction; and (3) Attention\\nAlignment for Prior Knowledge Preservation, which aligns the cross-attention\\nmaps of the fine-tuned model with those of the pretrained model to maintain\\nprior knowledge and semantic coherence. Through extensive experiments, we\\ndemonstrate that APT effectively mitigates overfitting, preserves prior\\nknowledge, and outperforms existing methods in generating high-quality, diverse\\nimages with limited reference data.\", 'Students disengaging from their tasks can have serious long-term\\nconsequences, including academic drop-out. This is particularly relevant for\\nstudents in distance education. One way to measure the level of disengagement\\nin distance education is to observe participation in non-mandatory exercises in\\ndifferent online courses. In this paper, we detect student disengagement in the\\nnon-mandatory quizzes of 42 courses in four semesters from a distance-based\\nuniversity. We carefully identified the most informative student log data that\\ncould be extracted and processed from Moodle. Then, eight machine learning\\nalgorithms were trained and compared to obtain the highest possible prediction\\naccuracy. Using the SHAP method, we developed an explainable machine learning\\nframework that allows practitioners to better understand the decisions of the\\ntrained algorithm. The experimental results show a balanced accuracy of 91\\\\%,\\nwhere about 85\\\\% of disengaged students were correctly detected. On top of the\\nhighly predictive performance and explainable framework, we provide a\\ndiscussion on how to design a timely intervention to minimise disengagement\\nfrom voluntary tasks in online learning.', \"In recent advancements in audio self-supervised representation learning, the\\nstandard Transformer architecture has emerged as the predominant approach, yet\\nits attention mechanism often allocates a portion of attention weights to\\nirrelevant information, potentially impairing the model's discriminative\\nability. To address this, we introduce a differential attention mechanism,\\nwhich effectively mitigates ineffective attention allocation through the\\nintegration of dual-softmax operations and appropriately tuned differential\\ncoefficients. Experimental results demonstrate that our ASDA model achieves\\nstate-of-the-art (SOTA) performance across multiple benchmarks, including audio\\nclassification (49.0% mAP on AS-2M, 41.5% mAP on AS20K), keyword spotting\\n(98.3% accuracy on SPC-2), and environmental sound classification (96.1%\\naccuracy on ESC-50). These results highlight ASDA's effectiveness in audio\\ntasks, paving the way for broader applications.\", \"Recent Long Reasoning Models(LRMs) have demonstrated remarkable capabilities\\nin handling complex reasoning tasks, but are hindered by excessive\\noverthinking. To explore its essence, our empirical analysis reveals that LRMs\\nare primarily limited to recognizing task properties (i.e., difficulty levels)\\nlike humans before solving the problem, leading to a one-size-fits-all\\nreasoning process. Inspired by this, a pressing and natural question emerges:\\nCan we bootstrap such ability to further alleviate the overthinking phenomenon\\nin LRMs? In this paper, we propose Think-How-to-Think (TH2T), a novel two-stage\\nfine-tuning strategy that progressively inspires LRMs' difficulty cognition and\\nredundancy cognition. First, we introduce difficulty-hypnosis in the prefixes\\nof model outputs to intervene in the internal reasoning trajectory. Combined\\nwith a heterogeneous short and long reasoning dataset, the trained model\\nenhances its sensitivity to task difficulty, enabling native, differentiated\\nreasoning strategies across various tasks. Second, we further extend\\nredundancy-hypnosis to the internal reasoning process, guiding the model to\\nidentify redundant structures within the reasoning steps and generate more\\nconcise reasoning outputs. Experiments on 7B/14B/32B models demonstrate that\\nTH2T significantly reduces inference costs (more than 70% on easy tasks and 40%\\non hard tasks) while maintaining performance stability. The resulting outputs\\nexhibit clear difficulty-aware capabilities and reduced redundancy (e.g.,\\nreflection).\", 'Modern Integrated Circuits (ICs) are becoming increasingly complex, and so is\\ntheir development process. Hardware design verification entails a methodical\\nand disciplined approach to the planning, development, execution, and sign-off\\nof functionally correct hardware designs. This tedious process requires\\nsignificant effort and time to ensure a bug-free tape-out. The field of Natural\\nLanguage Processing has undergone a significant transformation with the advent\\nof Large Language Models (LLMs). These powerful models, often referred to as\\nGenerative AI (GenAI), have revolutionized how machines understand and generate\\nhuman language, enabling unprecedented advancements in a wide array of\\napplications, including hardware design verification. This paper presents an\\nagentic AI-based approach to hardware design verification, which empowers AI\\nagents, in collaboration with Humain-in-the-Loop (HITL) intervention, to engage\\nin a more dynamic, iterative, and self-reflective process, ultimately\\nperforming end-to-end hardware design and verification. This methodology is\\nevaluated on five open-source designs, achieving over 95% coverage with reduced\\nverification time while demonstrating superior performance, adaptability, and\\nconfigurability.', 'Complex information needs in real-world search scenarios demand deep\\nreasoning and knowledge synthesis across diverse sources, which traditional\\nretrieval-augmented generation (RAG) pipelines struggle to address effectively.\\nCurrent reasoning-based approaches suffer from a fundamental limitation: they\\nuse a single model to handle both high-level planning and detailed execution,\\nleading to inefficient reasoning and limited scalability. In this paper, we\\nintroduce HiRA, a hierarchical framework that separates strategic planning from\\nspecialized execution. Our approach decomposes complex search tasks into\\nfocused subtasks, assigns each subtask to domain-specific agents equipped with\\nexternal tools and reasoning capabilities, and coordinates the results through\\na structured integration mechanism. This separation prevents execution details\\nfrom disrupting high-level reasoning while enabling the system to leverage\\nspecialized expertise for different types of information processing.\\nExperiments on four complex, cross-modal deep search benchmarks demonstrate\\nthat HiRA significantly outperforms state-of-the-art RAG and agent-based\\nsystems. Our results show improvements in both answer quality and system\\nefficiency, highlighting the effectiveness of decoupled planning and execution\\nfor multi-step information seeking tasks. Our code is available at\\nhttps://github.com/ignorejjj/HiRA.', 'The rapid development of neural quantum states (NQS) has established it as a\\npromising framework for studying quantum many-body systems. In this work, by\\nleveraging the cutting-edge transformer-based architectures and developing\\nhighly efficient optimization algorithms, we achieve the state-of-the-art\\nresults for the doped two-dimensional (2D) Hubbard model, arguably the minimum\\nmodel for high-Tc superconductivity. Interestingly, we find different attention\\nheads in the NQS ansatz can directly encode correlations at different scales,\\nmaking it capable of capturing long-range correlations and entanglements in\\nstrongly correlated systems. With these advances, we establish the half-filled\\nstripe in the ground state of 2D Hubbard model with the next nearest\\nneighboring hoppings, consistent with experimental observations in cuprates.\\nOur work establishes NQS as a powerful tool for solving challenging\\nmany-fermions systems.', 'Distributed inference serves as a promising approach to enabling the\\ninference of large language models (LLMs) at the network edge. It distributes\\nthe inference process to multiple devices to ensure that the LLMs can fit into\\nthe device memory. Recent pipeline-based approaches have the potential to\\nparallelize communication and computation, which helps reduce inference\\nlatency. However, the benefit diminishes when the inference request at the\\nnetwork edge is sparse, where pipeline is typically at low utilization. To\\nenable efficient distributed LLM inference at the edge, we propose\\n\\\\textbf{FlowSpec}, a pipeline-parallel tree-based speculative decoding\\nframework. FlowSpec incorporates three key mechanisms to improve decoding\\nefficiency: 1) score-based step-wise verification prioritizes more important\\ndraft tokens to bring earlier accpeted tokens; 2) efficient draft management to\\nprune invalid tokens while maintaining correct causal relationship during\\nverification; 3) dynamic draft expansion strategies to supply high-quality\\nspeculative inputs. These techniques work in concert to enhance both pipeline\\nutilization and speculative efficiency. We evaluate FlowSpec on a real-world\\ntestbed with other baselines. Experimental results demonstrate that our\\nproposed framework significantly improves inference speed across diverse models\\nand configurations, achieving speedup ratios 1.36$\\\\times$-1.77$\\\\times$ compared\\nto baselines. Our code is publicly available at\\n\\\\href{https://github.com/Leosang-lx/FlowSpec#}{https://github.com/Leosang-lx/FlowSpec\\\\#}', 'Are Large Language Models (LLMs) a new form of strategic intelligence, able\\nto reason about goals in competitive settings? We present compelling supporting\\nevidence. The Iterated Prisoner\\'s Dilemma (IPD) has long served as a model for\\nstudying decision-making. We conduct the first ever series of evolutionary IPD\\ntournaments, pitting canonical strategies (e.g., Tit-for-Tat, Grim Trigger)\\nagainst agents from the leading frontier AI companies OpenAI, Google, and\\nAnthropic. By varying the termination probability in each tournament (the\\n\"shadow of the future\"), we introduce complexity and chance, confounding\\nmemorisation.\\n  Our results show that LLMs are highly competitive, consistently surviving and\\nsometimes even proliferating in these complex ecosystems. Furthermore, they\\nexhibit distinctive and persistent \"strategic fingerprints\": Google\\'s Gemini\\nmodels proved strategically ruthless, exploiting cooperative opponents and\\nretaliating against defectors, while OpenAI\\'s models remained highly\\ncooperative, a trait that proved catastrophic in hostile environments.\\nAnthropic\\'s Claude emerged as the most forgiving reciprocator, showing\\nremarkable willingness to restore cooperation even after being exploited or\\nsuccessfully defecting. Analysis of nearly 32,000 prose rationales provided by\\nthe models reveals that they actively reason about both the time horizon and\\ntheir opponent\\'s likely strategy, and we demonstrate that this reasoning is\\ninstrumental to their decisions. This work connects classic game theory with\\nmachine psychology, offering a rich and granular view of algorithmic\\ndecision-making under uncertainty.', 'The rise of Large Language Models (LLMs) has enabled the development of\\nspecialized AI agents with domain-specific reasoning and interaction\\ncapabilities, particularly in healthcare. While recent frameworks simulate\\nmedical decision-making, they largely focus on single-turn tasks where a doctor\\nagent receives full case information upfront -- diverging from the real-world\\ndiagnostic process, which is inherently uncertain, interactive, and iterative.\\nIn this paper, we introduce MIMIC-Patient, a structured dataset built from the\\nMIMIC-III electronic health records (EHRs), designed to support dynamic,\\npatient-level simulations. Building on this, we propose DynamiCare, a novel\\ndynamic multi-agent framework that models clinical diagnosis as a multi-round,\\ninteractive loop, where a team of specialist agents iteratively queries the\\npatient system, integrates new information, and dynamically adapts its\\ncomposition and strategy. We demonstrate the feasibility and effectiveness of\\nDynamiCare through extensive experiments, establishing the first benchmark for\\ndynamic clinical decision-making with LLM-powered agents.', 'The rapid advancement of speech generation models has heightened privacy and\\nsecurity concerns related to voice cloning (VC). Recent studies have\\ninvestigated disrupting unauthorized voice cloning by introducing adversarial\\nperturbations. However, determined attackers can mitigate these protective\\nperturbations and successfully execute VC. In this study, we conduct the first\\nsystematic evaluation of these protective perturbations against VC under\\nrealistic threat models that include perturbation purification. Our findings\\nreveal that while existing purification methods can neutralize a considerable\\nportion of the protective perturbations, they still lead to distortions in the\\nfeature space of VC models, which degrades the performance of VC. From this\\nperspective, we propose a novel two-stage purification method: (1) Purify the\\nperturbed speech; (2) Refine it using phoneme guidance to align it with the\\nclean speech distribution. Experimental results demonstrate that our method\\noutperforms state-of-the-art purification methods in disrupting VC defenses.\\nOur study reveals the limitations of adversarial perturbation-based VC defenses\\nand underscores the urgent need for more robust solutions to mitigate the\\nsecurity and privacy risks posed by VC. The code and audio samples are\\navailable at https://de-antifake.github.io.', \"With the rise of online learning, the demand for efficient and consistent\\nassessment in mathematics has significantly increased over the past decade.\\nMachine Learning (ML), particularly Natural Language Processing (NLP), has been\\nwidely used for autograding student responses, particularly those involving\\ntext and/or mathematical expressions. However, there has been limited research\\non autograding responses involving students' handwritten graphs, despite their\\nprevalence in Science, Technology, Engineering, and Mathematics (STEM)\\ncurricula. In this study, we implement multimodal meta-learning models for\\nautograding images containing students' handwritten graphs and text. We further\\ncompare the performance of Vision Large Language Models (VLLMs) with these\\nspecially trained metalearning models. Our results, evaluated on a real-world\\ndataset collected from our institution, show that the best-performing\\nmeta-learning models outperform VLLMs in 2-way classification tasks. In\\ncontrast, in more complex 3-way classification tasks, the best-performing VLLMs\\nslightly outperform the meta-learning models. While VLLMs show promising\\nresults, their reliability and practical applicability remain uncertain and\\nrequire further investigation.\", 'The increasing importance of Vision-Based Navigation (VBN) algorithms in\\nspace missions raises numerous challenges in ensuring their reliability and\\noperational robustness. Sensor faults can lead to inaccurate outputs from\\nnavigation algorithms or even complete data processing faults, potentially\\ncompromising mission objectives. Artificial Intelligence (AI) offers a powerful\\nsolution for detecting such faults, overcoming many of the limitations\\nassociated with traditional fault detection methods. However, the primary\\nobstacle to the adoption of AI in this context is the lack of sufficient and\\nrepresentative datasets containing faulty image data.\\n  This study addresses these challenges by focusing on an interplanetary\\nexploration mission scenario. A comprehensive analysis of potential fault cases\\nin camera sensors used within the VBN pipeline is presented. The causes and\\neffects of these faults are systematically characterized, including their\\nimpact on image quality and navigation algorithm performance, as well as\\ncommonly employed mitigation strategies. To support this analysis, a simulation\\nframework is introduced to recreate faulty conditions in synthetically\\ngenerated images, enabling a systematic and controlled reproduction of faulty\\ndata. The resulting dataset of fault-injected images provides a valuable tool\\nfor training and testing AI-based fault detection algorithms. The final link to\\nthe dataset will be added after an embargo period. For peer-reviewers, this\\nprivate link is available.', 'Arithmetic circuits, such as adders and multipliers, are fundamental\\ncomponents of digital systems, directly impacting the performance, power\\nefficiency, and area footprint. However, optimizing these circuits remains\\nchallenging due to the vast design space and complex physical constraints.\\nWhile recent deep learning-based approaches have shown promise, they struggle\\nto consistently explore high-potential design variants, limiting their\\noptimization efficiency. To address this challenge, we propose AC-Refiner, a\\nnovel arithmetic circuit optimization framework leveraging conditional\\ndiffusion models. Our key insight is to reframe arithmetic circuit synthesis as\\na conditional image generation task. By carefully conditioning the denoising\\ndiffusion process on target quality-of-results (QoRs), AC-Refiner consistently\\nproduces high-quality circuit designs. Furthermore, the explored designs are\\nused to fine-tune the diffusion model, which focuses the exploration near the\\nPareto frontier. Experimental results demonstrate that AC-Refiner generates\\ndesigns with superior Pareto optimality, outperforming state-of-the-art\\nbaselines. The performance gain is further validated by integrating AC-Refiner\\ninto practical applications.', 'Multiperspective Fusion (MPF) is a novel posttraining alignment framework for\\nlarge language models (LLMs) developed in response to the growing need for easy\\nbias mitigation. Built on top of the SAGED pipeline, an automated system for\\nconstructing bias benchmarks and extracting interpretable baseline\\ndistributions, MPF leverages multiperspective generations to expose and align\\nbiases in LLM outputs with nuanced, humanlike baselines. By decomposing\\nbaseline, such as sentiment distributions from HR professionals, into\\ninterpretable perspective components, MPF guides generation through sampling\\nand balancing of responses, weighted by the probabilities obtained in the\\ndecomposition. Empirically, we demonstrate its ability to align LLM sentiment\\ndistributions with both counterfactual baselines (absolute equality) and the HR\\nbaseline (biased for Top Univeristy), resulting in small KL divergence,\\nreduction of calibration error and generalization to unseen questions. This\\nshows that MPF offers a scalable and interpretable method for alignment and\\nbias mitigation, compatible with deployed LLMs and requiring no extensive\\nprompt engineering or finetuning.', \"Transcending human cognitive limitations represents a critical frontier in\\nLLM training. Proprietary agentic systems like DeepResearch have demonstrated\\nsuperhuman capabilities on extremely complex information-seeking benchmarks\\nsuch as BrowseComp, a feat previously unattainable. We posit that their success\\nhinges on a sophisticated reasoning pattern absent in open-source models: the\\nability to systematically reduce extreme uncertainty when navigating vast\\ninformation landscapes. Based on this insight, we introduce WebSailor, a\\ncomplete post-training methodology designed to instill this crucial capability.\\nOur approach involves generating novel, high-uncertainty tasks through\\nstructured sampling and information obfuscation, RFT cold start, and an\\nefficient agentic RL training algorithm, Duplicating Sampling Policy\\nOptimization (DUPO). With this integrated pipeline, WebSailor significantly\\noutperforms all opensource agents in complex information-seeking tasks,\\nmatching proprietary agents' performance and closing the capability gap.\", 'The rapid advancement of diffusion-based image generators has made it\\nincreasingly difficult to distinguish generated from real images. This can\\nerode trust in digital media, making it critical to develop generalizable\\ndetectors for generated images. Recent methods leverage diffusion denoising\\ncues, but mainly focus on single-step reconstruction errors, ignoring the\\ninherent sequential nature of the denoising process. In this work, we propose\\nLATTE - Latent Trajectory Embedding - a novel approach that models the\\nevolution of latent embeddings across several denoising timesteps. By modeling\\nthe trajectory of such embeddings rather than single-step errors, LATTE\\ncaptures subtle, discriminative patterns that distinguish real from generated\\nimages. Each latent is refined by employing our latent-visual feature\\nrefinement module and aggregated into a unified representation. Afterwards, it\\nis fused with the visual features and finally passed into a lightweight\\nclassifier. Our experiments demonstrate that LATTE surpasses the baselines on\\nseveral established benchmarks, such as GenImage and DiffusionFake. Moreover,\\nit demonstrates strong performance in cross-generator and cross-datasets\\nsettings, highlighting the potential of using the trajectory of latent\\nembeddings for generated image detection. The code is available on the\\nfollowing link: https://github.com/AnaMVasilcoiu/LATTE-Diffusion-Detector.', 'Responsibility has long been a subject of study in law and philosophy. More\\nrecently, it became a focus of AI literature. The article investigates the\\ncomputational complexity of two important properties of responsibility in\\ncollective decision-making: diffusion and gap. It shows that the sets of\\ndiffusion-free and gap-free decision-making mechanisms are $\\\\Pi_2$-complete and\\n$\\\\Pi_3$-complete, respectively. At the same time, the intersection of these\\nclasses is $\\\\Pi_2$-complete.', 'As large language models (LLMs) grow in size, efficient compression\\ntechniques like quantization and sparsification are critical. While\\nquantization maintains performance with reduced precision, structured sparsity\\nmethods, such as N:M sparsification, often fall short due to limited\\nflexibility, and sensitivity to outlier weights. We explore 8:16\\nsemi-structured sparsity, demonstrating its ability to surpass the Performance\\nThreshold-where a compressed model matches the accuracy of its uncompressed or\\nsmaller counterpart under equivalent memory constraints. Compared to 2:4\\nsparsity, 8:16 offers greater flexibility with minimal storage overhead (0.875\\nvs. 0.75 bits/element). We also apply sparse structured patterns for salient\\nweights, showing that structured sparsity for outliers is competitive with\\nunstructured approaches leading to equivalent or better results. Finally, we\\ndemonstrate that simple techniques such as variance correction and SmoothQuant\\nlike weight equalization improve sparse models performance.', \"AI research agents are demonstrating great potential to accelerate scientific\\nprogress by automating the design, implementation, and training of machine\\nlearning models. We focus on methods for improving agents' performance on\\nMLE-bench, a challenging benchmark where agents compete in Kaggle competitions\\nto solve real-world machine learning problems. We formalize AI research agents\\nas search policies that navigate a space of candidate solutions, iteratively\\nmodifying them using operators. By designing and systematically varying\\ndifferent operator sets and search policies (Greedy, MCTS, Evolutionary), we\\nshow that their interplay is critical for achieving high performance. Our best\\npairing of search strategy and operator set achieves a state-of-the-art result\\non MLE-bench lite, increasing the success rate of achieving a Kaggle medal from\\n39.6% to 47.7%. Our investigation underscores the importance of jointly\\nconsidering the search strategy, operator design, and evaluation methodology in\\nadvancing automated machine learning.\", 'Improving and understanding the training dynamics and reasoning of Large\\nLanguage Models (LLMs) has become essential for their deployment in AI-based\\nsecurity tools, such as software vulnerability detection. In this work, we\\npresent an extensive study aimed at advancing recent RL-based finetuning\\ntechniques for LLMs in the context of vulnerability detection.\\n  We start by highlighting key limitations of commonly adopted LLMs, such as\\ntheir tendency to over-predict certain types of vulnerabilities while failing\\nto detect others. To address this challenge, we explore the use of Group\\nRelative Policy Optimization (GRPO), a recent policy-gradient method, for\\nguiding LLM behavior through structured, rule-based rewards. We enable its\\napplication to the vulnerability detection task by redefining its advantage\\nfunctions and reward signals using annotations from widely used datasets in the\\nfield, including BigVul, DiverseVul, and CleanVul.\\n  The proposed methodology enables an extensive set of experiments, addressing\\nmultiple research questions regarding the impact of GRPO on generalization,\\nreasoning capabilities, and performance improvements over standard supervised\\nfinetuning (SFT). Our findings offer valuable insights into the potential of\\nRL-based training to enhance both the performance and reasoning abilities of\\nLLMs in the context of software vulnerability detection.', 'Overparametrized Deep Neural Networks (DNNs) have demonstrated remarkable\\nsuccess in a wide variety of domains too high-dimensional for classical shallow\\nnetworks subject to the curse of dimensionality. However, open questions about\\nfundamental principles, that govern the learning dynamics of DNNs, remain. In\\nthis position paper we argue that it is the ability of DNNs to exploit the\\ncompositionally sparse structure of the target function driving their success.\\nAs such, DNNs can leverage the property that most practically relevant\\nfunctions can be composed from a small set of constituent functions, each of\\nwhich relies only on a low-dimensional subset of all inputs. We show that this\\nproperty is shared by all efficiently Turing-computable functions and is\\ntherefore highly likely present in all current learning problems. While some\\npromising theoretical insights on questions concerned with approximation and\\ngeneralization exist in the setting of compositionally sparse functions,\\nseveral important questions on the learnability and optimization of DNNs\\nremain. Completing the picture of the role of compositional sparsity in deep\\nlearning is essential to a comprehensive theory of artificial, and even\\ngeneral, intelligence.', 'In this work, we investigate whether improving task clarity can enhance\\nreasoning ability of large language models, focusing on theorem proving in Coq.\\nWe introduce a concept-level metric to evaluate task clarity and show that\\nadding structured semantic context to the standard input used by modern LLMs,\\nleads to a 1.85$\\\\times$ improvement in clarity score\\n(44.5\\\\%~$\\\\rightarrow$~82.3\\\\%). Using the general-purpose model\\n\\\\texttt{DeepSeek-V3}, our approach leads to a 2.1$\\\\times$ improvement in proof\\nsuccess (21.8\\\\%~$\\\\rightarrow$~45.8\\\\%) and outperforms the previous\\nstate-of-the-art \\\\texttt{Graph2Tac} (33.2\\\\%). We evaluate this on 1,386\\ntheorems randomly sampled from 15 standard Coq packages, following the same\\nevaluation protocol as \\\\texttt{Graph2Tac}. Furthermore, fine-tuning smaller\\nmodels on our structured data can achieve even higher performance (48.6\\\\%). Our\\nmethod uses selective concept unfolding to enrich task descriptions, and\\nemploys a Planner--Executor architecture. These findings highlight the value of\\nstructured task representations in bridging the gap between understanding and\\nreasoning.', 'Conversational agents have made significant progress since ELIZA, expanding\\ntheir role across various domains, including healthcare, education, and\\ncustomer service. As these agents become increasingly integrated into daily\\nhuman interactions, the need for emotional intelligence, particularly\\nempathetic listening, becomes increasingly essential. In this study, we explore\\nhow Large Language Models (LLMs) respond when tasked with generating\\nemotionally rich interactions. Starting from a small dataset manually crafted\\nby an expert to reflect empathic behavior, we extended the conversations using\\ntwo LLMs: ChatGPT and Gemini. We analyzed the emotional progression of the\\ndialogues using both sentiment analysis (via VADER) and expert assessments.\\nWhile the generated conversations often mirrored the intended emotional\\nstructure, human evaluation revealed important differences in the perceived\\nempathy and coherence of the responses. These findings suggest that emotion\\nmodeling in dialogues requires not only structural alignment in the expressed\\nemotions but also qualitative depth, highlighting the importance of combining\\nautomated and humancentered methods in the development of emotionally competent\\nagents.', 'The UK has pursued a distinctive path in AI regulation: less cautious than\\nthe EU but more willing to address risks than the US, and has emerged as a\\nglobal leader in coordinating AI safety efforts. Impressive developments from\\ncompanies like London-based DeepMind began to spark concerns in the UK about\\ncatastrophic risks from around 2012, although regulatory discussion at the time\\nfocussed on bias and discrimination. By 2022, these discussions had evolved\\ninto a \"pro-innovation\" strategy, in which the government directed existing\\nregulators to take a light-touch approach, governing AI at point of use, but\\navoided regulating the technology or infrastructure directly. ChatGPT arrived\\nin late 2022, galvanising concerns that this approach may be insufficient. The\\nUK responded by establishing an AI Safety Institute to monitor risks and\\nhosting the first international AI Safety Summit in 2023, but - unlike the EU -\\nrefrained from regulating frontier AI development in addition to its use. A new\\ngovernment was elected in 2024 which promised to address this gap, but at the\\ntime of writing is yet to do so.\\n  What should the UK do next? The government faces competing objectives:\\nharnessing AI for economic growth and better public services while mitigating\\nrisk. In light of these, we propose establishing a flexible, principles-based\\nregulator to oversee the most advanced AI development, defensive measures\\nagainst risks from AI-enabled biological design tools, and argue that more\\ntechnical work is needed to understand how to respond to AI-generated\\nmisinformation. We argue for updated legal frameworks on copyright,\\ndiscrimination, and AI agents, and that regulators will have a limited but\\nimportant role if AI substantially disrupts labour markets.\\n  If the UK gets AI regulation right, it could demonstrate how democratic\\nsocieties can harness AI\\'s benefits while managing its risks.', \"In the field of Human-Robot Interaction (HRI), a fundamental challenge is to\\nfacilitate human understanding of robots. The emerging domain of eXplainable\\nHRI (XHRI) investigates methods to generate explanations and evaluate their\\nimpact on human-robot interactions. Previous works have highlighted the need to\\npersonalise the level of detail of these explanations to enhance usability and\\ncomprehension. Our paper presents a framework designed to update and retrieve\\nuser knowledge-memory models, allowing for adapting the explanations' level of\\ndetail while referencing previously acquired concepts. Three architectures\\nbased on our proposed framework that use Large Language Models (LLMs) are\\nevaluated in two distinct scenarios: a hospital patrolling robot and a kitchen\\nassistant robot. Experimental results demonstrate that a two-stage\\narchitecture, which first generates an explanation and then personalises it, is\\nthe framework architecture that effectively reduces the level of detail only\\nwhen there is related user knowledge.\", \"India, as a predominantly agrarian economy, faces significant challenges in\\nagriculture, including substantial crop losses caused by diseases, pests, and\\nenvironmental stress. Early detection and accurate identification of diseases\\nacross different crops are critical for improving yield and ensuring food\\nsecurity. This paper proposes a deep learning based solution for detecting\\nmultiple diseases in multiple crops, aimed to cover India's diverse\\nagricultural landscape. We first create a unified dataset encompassing images\\nof 17 different crops and 34 different diseases from various available\\nrepositories. Proposed deep learning model is trained on this dataset and\\noutperforms the state-of-the-art in terms of accuracy and the number of crops,\\ndiseases covered. We achieve a significant detection accuracy, i.e., 99 percent\\nfor our unified dataset which is 7 percent more when compared to\\nstate-of-the-art handling 14 crops and 26 different diseases only. By improving\\nthe number of crops and types of diseases that can be detected, proposed\\nsolution aims to provide a better product for Indian farmers.\", 'Machine-learned systems are in widespread use for making decisions about\\nhumans, and it is important that they are fair, i.e., not biased against\\nindividuals based on sensitive attributes.\\n  We present a general framework of runtime verification of algorithmic\\nfairness for systems whose models are unknown, but are assumed to have a Markov\\nchain structure, with or without full observation of the state space.\\n  We introduce a specification language that can model many common algorithmic\\nfairness properties, such as demographic parity, equal opportunity, and social\\nburden.\\n  We build monitors that observe a long sequence of events as generated by a\\ngiven system, and output, after each observation, a quantitative estimate of\\nhow fair or biased the system was on that run until that point in time.\\n  The estimate is proven to be correct modulo a variable error bound and a\\ngiven confidence level, where the error bound gets tighter as the observed\\nsequence gets longer.\\n  We present two categories of monitoring algorithms, namely ones with a\\nuniform error bound across all time points, and ones with weaker non-uniform,\\npointwise error bounds at different time points.\\n  Our monitoring algorithms use statistical tools that are adapted to suit the\\ndynamic requirements of monitoring and the special needs of the fairness\\nspecifications.\\n  Using a prototype implementation, we show how we can monitor if a bank is\\nfair in giving loans to applicants from different social backgrounds, and if a\\ncollege is fair in admitting students while maintaining a reasonable financial\\nburden on the society.\\n  In these experiments, our monitors took less than a millisecond to update\\ntheir verdicts after each observation.', 'Legal NLP remains underdeveloped in regions like India due to the scarcity of\\nstructured datasets. We introduce IndianBailJudgments-1200, a new benchmark\\ndataset comprising 1200 Indian court judgments on bail decisions, annotated\\nacross 20+ attributes including bail outcome, IPC sections, crime type, and\\nlegal reasoning. Annotations were generated using a prompt-engineered GPT-4o\\npipeline and verified for consistency. This resource supports a wide range of\\nlegal NLP tasks such as outcome prediction, summarization, and fairness\\nanalysis, and is the first publicly available dataset focused specifically on\\nIndian bail jurisprudence.', \"Recent advances have applied large language models (LLMs) to sequential\\nrecommendation, leveraging their pre-training knowledge and reasoning\\ncapabilities to provide more personalized user experiences. However, existing\\nLLM-based methods fail to sufficiently leverage the rich temporal information\\ninherent in users' historical interaction sequences, stemming from fundamental\\narchitectural constraints: LLMs process information through self-attention\\nmechanisms that lack inherent sequence ordering and rely on position embeddings\\ndesigned primarily for natural language rather than user interaction sequences.\\nThis limitation significantly impairs their ability to capture the evolution of\\nuser preferences over time and predict future interests accurately.\\n  To address this critical gap, we propose Counterfactual Enhanced Temporal\\nFramework for LLM-Based Recommendation (CETRec). CETRec is grounded in causal\\ninference principles, which allow it to isolate and measure the specific impact\\nof temporal information on recommendation outcomes. By conceptualizing temporal\\norder as an independent causal factor distinct from item content, we can\\nquantify its unique contribution through counterfactual reasoning--comparing\\nwhat recommendations would be made with and without temporal information while\\nkeeping all other factors constant. This causal framing enables CETRec to\\ndesign a novel counterfactual tuning objective that directly optimizes the\\nmodel's temporal sensitivity, teaching LLMs to recognize both absolute\\ntimestamps and relative ordering patterns in user histories. Combined with our\\ncounterfactual tuning task derived from causal analysis, CETRec effectively\\nenhances LLMs' awareness of both absolute order (how recently items were\\ninteracted with) and relative order (the sequential relationships between\\nitems).\", \"Continual fine-tuning of Large Language Models (LLMs) is hampered by the\\ntrade-off between efficiency and expressiveness. Low-Rank Adaptation (LoRA)\\noffers efficiency but constrains the model's ability to learn new tasks and\\ntransfer knowledge due to its low-rank nature and reliance on explicit\\nparameter constraints. We propose GORP (Gradient LOw Rank Projection) for\\nContinual Learning, a novel training strategy that overcomes these limitations\\nby synergistically combining full and low-rank parameters and jointly updating\\nwithin a unified low-rank gradient subspace. GORP expands the optimization\\nspace while preserving efficiency and mitigating catastrophic forgetting.\\nExtensive experiments on continual learning benchmarks demonstrate GORP's\\nsuperior performance compared to existing state-of-the-art approaches. Code is\\navailable at https://github.com/Wcxwcxw/GORP.\", 'Automated polyp counting in colonoscopy is a crucial step toward automated\\nprocedure reporting and quality control, aiming to enhance the\\ncost-effectiveness of colonoscopy screening. Counting polyps in a procedure\\ninvolves detecting and tracking polyps, and then clustering tracklets that\\nbelong to the same polyp entity. Existing methods for polyp counting rely on\\nself-supervised learning and primarily leverage visual appearance, neglecting\\ntemporal relationships in both tracklet feature learning and clustering stages.\\nIn this work, we introduce a paradigm shift by proposing a supervised\\ncontrastive loss that incorporates temporally-aware soft targets. Our approach\\ncaptures intra-polyp variability while preserving inter-polyp discriminability,\\nleading to more robust clustering. Additionally, we improve tracklet clustering\\nby integrating a temporal adjacency constraint, reducing false positive\\nre-associations between visually similar but temporally distant tracklets. We\\ntrain and validate our method on publicly available datasets and evaluate its\\nperformance with a leave-one-out cross-validation strategy. Results demonstrate\\na 2.2x reduction in fragmentation rate compared to prior approaches. Our\\nresults highlight the importance of temporal awareness in polyp counting,\\nestablishing a new state-of-the-art. Code is available at\\nhttps://github.com/lparolari/temporally-aware-polyp-counting.', \"Multi-object tracking is a classic field in computer vision. Among them,\\npedestrian tracking has extremely high application value and has become the\\nmost popular research category. Existing methods mainly use motion or\\nappearance information for tracking, which is often difficult in complex\\nscenarios. For the motion information, mutual occlusions between objects often\\nprevent updating of the motion state; for the appearance information,\\nnon-robust results are often obtained due to reasons such as only partial\\nvisibility of the object or blurred images. Although learning how to perform\\ntracking in these situations from the annotated data is the simplest solution,\\nthe existing MOT dataset fails to satisfy this solution. Existing methods\\nmainly have two drawbacks: relatively simple scene composition and\\nnon-realistic scenarios. Although some of the video sequences in existing\\ndataset do not have the above-mentioned drawbacks, the number is far from\\nadequate for research purposes. To this end, we propose a difficult large-scale\\ndataset for multi-pedestrian tracking, shot mainly from the first-person view\\nand all from real-life complex scenarios. We name it ``CrowdTrack'' because\\nthere are numerous objects in most of the sequences. Our dataset consists of 33\\nvideos, containing a total of 5,185 trajectories. Each object is annotated with\\na complete bounding box and a unique object ID. The dataset will provide a\\nplatform to facilitate the development of algorithms that remain effective in\\ncomplex situations. We analyzed the dataset comprehensively and tested multiple\\nSOTA models on our dataset. Besides, we analyzed the performance of the\\nfoundation models on our dataset. The dataset and project code is released at:\\nhttps://github.com/loseevaya/CrowdTrack .\", \"Robots usually slow down for canning to detect objects while moving.\\nAdditionally, the robot's camera is configured with a low framerate to track\\nthe velocity of the detection algorithms. This would be constrained while\\nexecuting tasks and exploring, making robots increase the task execution time.\\nAMD has developed the Vitis-AI framework to deploy detection algorithms into\\nFPGAs. However, this tool does not fully use the FPGAs' PL. In this work, we\\nuse the FINN architecture to deploy three ANNs, MobileNet v1 with 4-bit\\nquantisation, CNV with 2-bit quantisation, and CNV with 1-bit quantisation\\n(BNN), inside an FPGA's PL. The models were trained on the RG2C dataset. This\\nis a self-acquired dataset released in open access. MobileNet v1 performed\\nbetter, reaching a success rate of 98 % and an inference speed of 6611 FPS. In\\nthis work, we proved that we can use FPGAs to speed up ANNs and make them\\nsuitable for attention mechanisms.\", 'Enhancing the intelligibility and interpretability of machine learning is a\\ncrucial task in responding to the demand for Explicability as an AI principle,\\nand in promoting the better social implementation of AI. The aim of our\\nresearch is to contribute to this improvement by reformulating machine learning\\nmodels through the lens of category theory, thereby developing a semantic\\nframework for structuring and understanding AI systems. Our categorical\\nmodeling in this paper clarifies and formalizes the structural interplay\\nbetween residuals and parameters in supervised learning. The present paper\\nfocuses on the multiple linear regression model, which represents the most\\nbasic form of supervised learning. By defining two concrete categories\\ncorresponding to parameters and data, along with an adjoint pair of functors\\nbetween them, we introduce our categorical formulation of supervised learning.\\nWe show that the essential structure of this framework is captured by what we\\ncall the Gauss-Markov Adjunction. Within this setting, the dual flow of\\ninformation can be explicitly described as a correspondence between variations\\nin parameters and residuals. The ordinary least squares estimator for the\\nparameters and the minimum residual are related via the preservation of limits\\nby the right adjoint functor. Furthermore, we position this formulation as an\\ninstance of extended denotational semantics for supervised learning, and\\npropose applying a semantic perspective developed in theoretical computer\\nscience as a formal foundation for Explicability in AI.', 'The Artificial Intelligence field has focused on developing optimisation\\nmethods to solve multiple problems, specifically problems that we thought to be\\nonly solvable through cognition. The obtained results have been outstanding,\\nbeing able to even surpass the Turing Test. However, we have found that these\\noptimisation methods share some fundamental flaws that impede them to become a\\ntrue artificial cognition. Specifically, the field have identified catastrophic\\nforgetting as a fundamental problem to develop such cognition. This paper\\nformally proves that this problem is inherent to optimisation methods, and as\\nsuch it will always limit approaches that try to solve the Artificial General\\nIntelligence problem as an optimisation problem. Additionally, it addresses the\\nproblem of overfitting and discuss about other smaller problems that\\noptimisation methods pose. Finally, it empirically shows how world-modelling\\nmethods avoid suffering from either problem. As a conclusion, the field of\\nArtificial Intelligence needs to look outside the machine learning field to\\nfind methods capable of developing an artificial cognition.', 'Advances in material functionalities drive innovations across various fields,\\nwhere metamaterials-defined by structure rather than composition-are leading\\nthe way. Despite the rise of artificial intelligence (AI)-driven design\\nstrategies, their impact is limited by task-specific retraining, poor\\nout-of-distribution(OOD) generalization, and the need for separate models for\\nforward and inverse design. To address these limitations, we introduce the\\nMetamaterial Foundation Model (MetaFO), a Bayesian transformer-based foundation\\nmodel inspired by large language models. MetaFO learns the underlying mechanics\\nof metamaterials, enabling probabilistic, zero-shot predictions across diverse,\\nunseen combinations of material properties and structural responses. It also\\nexcels in nonlinear inverse design, even under OOD conditions. By treating\\nmetamaterials as an operator that maps material properties to structural\\nresponses, MetaFO uncovers intricate structure-property relationships and\\nsignificantly expands the design space. This scalable and generalizable\\nframework marks a paradigm shift in AI-driven metamaterial discovery, paving\\nthe way for next-generation innovations.', 'Intrusion Detection and Prevention Systems (IDS/IPS) in large enterprises can\\ngenerate hundreds of thousands of alerts per hour, overwhelming security\\nanalysts with logs that demand deep, rapidly evolving domain expertise.\\nConventional machine-learning detectors trim the alert volume but still yield\\nhigh false-positive rates, while standard single-pass Retrieval-Augmented\\nGeneration (RAG) pipelines often retrieve irrelevant context and fail to\\njustify their predictions. To overcome these shortcomings, we present CyberRAG,\\na modular, agent-based RAG framework that delivers real-time classification,\\nexplanation, and structured reporting for cyber-attacks. A central LLM agent\\norchestrates (i) a pool of fine-tuned specialized classifiers, each tailored to\\na distinct attack family; (ii) tool adapters for enrichment and alerting; and\\n(iii) an iterative retrieval-and-reason loop that continuously queries a\\ndomain-specific knowledge base until the evidence is both relevant and\\nself-consistent. Unlike traditional RAG systems, CyberRAG embraces an agentic\\ndesign that enables dynamic control flow and adaptive reasoning. This\\nagent-centric architecture refines its threat labels and natural-language\\njustifications autonomously, reducing false positives and enhancing\\ninterpretability. The framework is fully extensible: new attack types can be\\nsupported by simply adding a classifier without retraining the core agent.\\nCyberRAG has been evaluated achieving over 94% accuracy per class and pushing\\nfinal classification accuracy to 94.92% through semantic orchestration.\\nGenerated explanations score up to 0.94 in BERTScore and 4.9/5 in GPT-4-based\\nexpert evaluation. These results show that agentic, specialist-oriented RAG can\\npair high detection accuracy with trustworthy, SOC-ready prose, offering a\\npractical and scalable path toward semi-autonomous cyber-defence workflows.', \"Early evaluation of children's language is frustrated by the high pitch, long\\nphones, and sparse data that derail automatic speech recognisers. We introduce\\nK-Function, a unified framework that combines accurate sub-word transcription,\\nobjective scoring, and actionable feedback. Its core, Kids-WFST, merges a\\nWav2Vec2 phoneme encoder with a phoneme-similarity Dysfluent-WFST to capture\\nchild-specific errors while remaining fully interpretable. Kids-WFST attains\\n1.39% phoneme error on MyST and 8.61% on Multitudes--absolute gains of 10.47\\nand 7.06 points over a greedy-search decoder. These high-fidelity transcripts\\npower an LLM that grades verbal skills, milestones, reading, and comprehension,\\naligning with human proctors and supplying tongue-and-lip visualizations plus\\ntargeted advice. The results show that precise phoneme recognition cements a\\ncomplete diagnostic-feedback loop, paving the way for scalable, clinician-ready\\nlanguage assessment.\", 'Federated Graph Learning (FGL) combines the privacy-preserving capabilities\\nof federated learning (FL) with the strong graph modeling capability of Graph\\nNeural Networks (GNNs). Current research addresses subgraph-FL only from the\\nstructural perspective, neglecting the propagation of graph signals on spatial\\nand spectral domains of the structure. From a spatial perspective, subgraph-FL\\nintroduces edge disconnections between clients, leading to disruptions in label\\nsignals and a degradation in the class knowledge of the global GNN. From a\\nspectral perspective, spectral heterogeneity causes inconsistencies in signal\\nfrequencies across subgraphs, which makes local GNNs overfit the local signal\\npropagation schemes. As a result, spectral client drifts occur, undermining\\nglobal generalizability. To tackle the challenges, we propose a global\\nknowledge repository to mitigate label signal disruption and a frequency\\nalignment to address spectral client drifts. The combination of spatial and\\nspectral strategies forms our framework S2FGL. Extensive experiments on\\nmultiple datasets demonstrate the superiority of S2FGL. The code is available\\nat https://github.com/Wonder7racer/S2FGL.git.', 'Wildlife re-identification aims to match individuals of the same species\\nacross different observations. Current state-of-the-art (SOTA) models rely on\\nclass labels to train supervised models for individual classification. This\\ndependence on annotated data has driven the curation of numerous large-scale\\nwildlife datasets. This study investigates self-supervised learning\\nSelf-Supervised Learning (SSL) for wildlife re-identification. We automatically\\nextract two distinct views of an individual using temporal image pairs from\\ncamera trap data without supervision. The image pairs train a self-supervised\\nmodel from a potentially endless stream of video data. We evaluate the learnt\\nrepresentations against supervised features on open-world scenarios and\\ntransfer learning in various wildlife downstream tasks. The analysis of the\\nexperimental results shows that self-supervised models are more robust even\\nwith limited data. Moreover, self-supervised features outperform supervision\\nacross all downstream tasks. The code is available here\\nhttps://github.com/pxpana/SSLWildlife.', 'Memory storage for Large Language models (LLMs) is becoming an increasingly\\nactive area of research, particularly for enabling personalization across long\\nconversations. We propose Pref-LSTM, a dynamic and lightweight framework that\\ncombines a BERT-based classifier with a LSTM memory module that generates\\nmemory embedding which then is soft-prompt injected into a frozen LLM. We\\nsynthetically curate a dataset of preference and non-preference conversation\\nturns to train our BERT-based classifier. Although our LSTM-based memory\\nencoder did not yield strong results, we find that the BERT-based classifier\\nperforms reliably in identifying explicit and implicit user preferences. Our\\nresearch demonstrates the viability of using preference filtering with LSTM\\ngating principals as an efficient path towards scalable user preference\\nmodeling, without extensive overhead and fine-tuning.', 'We introduce a deepfake video detection approach that exploits pixel-wise\\ntemporal inconsistencies, which traditional spatial frequency-based detectors\\noften overlook. Traditional detectors represent temporal information merely by\\nstacking spatial frequency spectra across frames, resulting in the failure to\\ndetect temporal artifacts in the pixel plane. Our approach performs a 1D\\nFourier transform on the time axis for each pixel, extracting features highly\\nsensitive to temporal inconsistencies, especially in areas prone to unnatural\\nmovements. To precisely locate regions containing the temporal artifacts, we\\nintroduce an attention proposal module trained in an end-to-end manner.\\nAdditionally, our joint transformer module effectively integrates pixel-wise\\ntemporal frequency features with spatio-temporal context features, expanding\\nthe range of detectable forgery artifacts. Our framework represents a\\nsignificant advancement in deepfake video detection, providing robust\\nperformance across diverse and challenging detection scenarios.', 'Log analysis is a relevant research field in cybersecurity as they can\\nprovide a source of information for the detection of threats to networks and\\nsystems. This paper presents a pipeline to use fine-tuned Large Language Models\\n(LLMs) for anomaly detection and mitigation recommendation using IoT security\\nlogs. Utilizing classical machine learning classifiers as a baseline, three\\nopen-source LLMs are compared for binary and multiclass anomaly detection, with\\nthree strategies: zero-shot, few-shot prompting and fine-tuning using an IoT\\ndataset. LLMs give better results on multi-class attack classification than the\\ncorresponding baseline models. By mapping detected threats to MITRE CAPEC,\\ndefining a set of IoT-specific mitigation actions, and fine-tuning the models\\nwith those actions, the models are able to provide a combined detection and\\nrecommendation guidance.', 'Autonomous scientific research, capable of independently conducting complex\\nexperiments and serving non-specialists, represents a long-held aspiration.\\nAchieving it requires a fundamental paradigm shift driven by artificial\\nintelligence (AI). While autonomous experimental systems are emerging, they\\nremain confined to areas featuring singular objectives and well-defined, simple\\nexperimental workflows, such as chemical synthesis and catalysis. We present an\\nAI-native autonomous laboratory, targeting highly complex scientific\\nexperiments for applications like autonomous biomolecular engineering. This\\nsystem autonomously manages instrumentation, formulates experiment-specific\\nprocedures and optimization heuristics, and concurrently serves multiple user\\nrequests. Founded on a co-design philosophy of models, experiments, and\\ninstruments, the platform supports the co-evolution of AI models and the\\nautomation system. This establishes an end-to-end, multi-user autonomous\\nlaboratory that handles complex, multi-objective experiments across diverse\\ninstrumentation. Our autonomous laboratory supports fundamental nucleic acid\\nfunctions-including synthesis, transcription, amplification, and sequencing. It\\nalso enables applications in fields such as disease diagnostics, drug\\ndevelopment, and information storage. Without human intervention, it\\nautonomously optimizes experimental performance to match state-of-the-art\\nresults achieved by human scientists. In multi-user scenarios, the platform\\nsignificantly improves instrument utilization and experimental efficiency. This\\nplatform paves the way for advanced biomaterials research to overcome\\ndependencies on experts and resource barriers, establishing a blueprint for\\nscience-as-a-service at scale.', \"Vertical Federated Learning (VFL) is a distributed AI software deployment\\nmechanism for cross-silo collaboration without accessing participants' data.\\nHowever, existing VFL work lacks a mechanism to audit the execution correctness\\nof the inference software of the data party. To address this problem, we design\\na Vertical Federated Inference Auditing (VeFIA) framework. VeFIA helps the task\\nparty to audit whether the data party's inference software is executed as\\nexpected during large-scale inference without leaking the data privacy of the\\ndata party or introducing additional latency to the inference system. The core\\nof VeFIA is that the task party can use the inference results from a framework\\nwith Trusted Execution Environments (TEE) and the coordinator to validate the\\ncorrectness of the data party's computation results. VeFIA guarantees that, as\\nlong as the abnormal inference exceeds 5.4%, the task party can detect\\nexecution anomalies in the inference software with a probability of 99.99%,\\nwithout incurring any additional online inference latency. VeFIA's random\\nsampling validation achieves 100% positive predictive value, negative\\npredictive value, and true positive rate in detecting abnormal inference. To\\nthe best of our knowledge, this is the first paper to discuss the correctness\\nof inference software execution in VFL.\", \"Compound AI systems integrating multiple components, such as Large Language\\nModels, specialized tools, and traditional machine learning models, are\\nincreasingly deployed to solve complex real-world tasks. However, optimizing\\ncompound systems remains challenging due to their non-differentiable structures\\nand diverse configuration types across components, including prompts,\\nhyperparameters, and model parameters. To address this challenge, we propose\\nOptimas, a unified framework for effective optimization of compound systems.\\nThe core idea of Optimas is to maintain one Local Reward Function (LRF) per\\ncomponent, each satisfying a local-global alignment property, i.e., each\\ncomponent's local reward correlates with the global system performance. In each\\niteration, Optimas efficiently adapts the LRFs to maintain this property while\\nsimultaneously maximizing each component's local reward. This approach enables\\nindependent updates of heterogeneous configurations using the designated\\noptimization method, while ensuring that local improvements consistently lead\\nto performance gains. We present extensive evaluations across five real-world\\ncompound systems to demonstrate that Optimas outperforms strong baselines by an\\naverage improvement of 11.92%, offering a general and effective approach for\\nimproving compound systems. Our website is at https://optimas.stanford.edu.\", 'The vanilla autoregressive image generation model generates visual tokens in\\na step-by-step fashion, which limits the ability to capture holistic\\nrelationships among token sequences. Moreover, most visual tokenizers map local\\nimage patches into latent tokens, leading to limited global information. To\\naddress this, we introduce \\\\textit{Hita}, a novel image tokenizer for\\nautoregressive (AR) image generation. It introduces a holistic-to-local\\ntokenization scheme with learnable holistic queries and local patch tokens.\\nBesides, Hita incorporates two key strategies for improved alignment with the\\nAR generation process: 1) it arranges a sequential structure with holistic\\ntokens at the beginning followed by patch-level tokens while using causal\\nattention to maintain awareness of previous tokens; and 2) before feeding the\\nde-quantized tokens into the decoder, Hita adopts a lightweight fusion module\\nto control information flow to prioritize holistic tokens. Extensive\\nexperiments show that Hita accelerates the training speed of AR generators and\\noutperforms those trained with vanilla tokenizers, achieving \\\\textbf{2.59 FID}\\nand \\\\textbf{281.9 IS} on the ImageNet benchmark. A detailed analysis of the\\nholistic representation highlights its ability to capture global image\\nproperties such as textures, materials, and shapes. Additionally, Hita also\\ndemonstrates effectiveness in zero-shot style transfer and image in-painting.\\nThe code is available at\\n\\\\href{https://github.com/CVMI-Lab/Hita}{https://github.com/CVMI-Lab/Hita}', 'Offline reinforcement learning (RL) optimizes a policy using only a fixed\\ndataset, making it a practical approach in scenarios where interaction with the\\nenvironment is costly. Due to this limitation, generalization ability is key to\\nimproving the performance of offline RL algorithms, as demonstrated by recent\\nsuccesses of offline RL with diffusion models. However, it remains questionable\\nwhether such diffusion models are necessary for highly performing offline RL\\nalgorithms, given their significant computational requirements during\\ninference. In this paper, we propose Penalized Action Noise Injection (PANI), a\\nmethod that simply enhances offline learning by utilizing noise-injected\\nactions to cover the entire action space, while penalizing according to the\\namount of noise injected. This approach is inspired by how diffusion models\\nhave worked in offline RL algorithms. We provide a theoretical foundation for\\nthis method, showing that offline RL algorithms with such noise-injected\\nactions solve a modified Markov Decision Process (MDP), which we call the noisy\\naction MDP. PANI is compatible with a wide range of existing off-policy and\\noffline RL algorithms, and despite its simplicity, it demonstrates significant\\nperformance improvements across various benchmarks.', 'Keyword decision in Sponsored Search Advertising is critical to the success\\nof ad campaigns. While LLM-based methods offer automated keyword generation,\\nthey face three major limitations: reliance on large-scale query-keyword pair\\ndata, lack of online multi-objective performance monitoring and optimization,\\nand weak quality control in keyword selection. These issues hinder the agentic\\nuse of LLMs in fully automating keyword decisions by monitoring and reasoning\\nover key performance indicators such as impressions, clicks, conversions, and\\nCTA effectiveness. To overcome these challenges, we propose OMS, a keyword\\ngeneration framework that is On-the-fly (requires no training data, monitors\\nonline performance, and adapts accordingly), Multi-objective (employs agentic\\nreasoning to optimize keywords based on multiple performance metrics), and\\nSelf-reflective (agentically evaluates keyword quality). Experiments on\\nbenchmarks and real-world ad campaigns show that OMS outperforms existing\\nmethods; ablation and human evaluations confirm the effectiveness of each\\ncomponent and the quality of generated keywords.', 'Intracranial aneurysms (ICA) commonly occur in specific segments of the\\nCircle of Willis (CoW), primarily, onto thirteen major arterial bifurcations.\\nAn accurate detection of these critical landmarks is necessary for a prompt and\\nefficient diagnosis. We introduce a fully automated landmark detection approach\\nfor CoW bifurcations using a two-step neural networks process. Initially, an\\nobject detection network identifies regions of interest (ROIs) proximal to the\\nlandmark locations. Subsequently, a modified U-Net with deep supervision is\\nexploited to accurately locate the bifurcations. This two-step method reduces\\nvarious problems, such as the missed detections caused by two landmarks being\\nclose to each other and having similar visual characteristics, especially when\\nprocessing the complete MRA Time-of-Flight (TOF). Additionally, it accounts for\\nthe anatomical variability of the CoW, which affects the number of detectable\\nlandmarks per scan. We assessed the effectiveness of our approach using two\\ncerebral MRA datasets: our In-House dataset which had varying numbers of\\nlandmarks, and a public dataset with standardized landmark configuration. Our\\nexperimental results demonstrate that our method achieves the highest level of\\nperformance on a bifurcation detection task.', \"Antibody engineering is essential for developing therapeutics and advancing\\nbiomedical research. Traditional discovery methods often rely on time-consuming\\nand resource-intensive experimental screening. To enhance and streamline this\\nprocess, we introduce a production-grade, high-throughput platform built on\\nHelixFold3, HelixDesign-Antibody, which utilizes the high-accuracy structure\\nprediction model, HelixFold3. The platform facilitates the large-scale\\ngeneration of antibody candidate sequences and evaluates their interaction with\\nantigens. Integrated high-performance computing (HPC) support enables\\nhigh-throughput screening, addressing challenges such as fragmented toolchains\\nand high computational demands. Validation on multiple antigens showcases the\\nplatform's ability to generate diverse and high-quality antibodies, confirming\\na scaling law where exploring larger sequence spaces increases the likelihood\\nof identifying optimal binders. This platform provides a seamless, accessible\\nsolution for large-scale antibody design and is available via the antibody\\ndesign page of PaddleHelix platform.\", 'This study proposes DeltaSHAP, a novel explainable artificial intelligence\\n(XAI) algorithm specifically designed for online patient monitoring systems. In\\nclinical environments, discovering the causes driving patient risk evolution is\\ncritical for timely intervention, yet existing XAI methods fail to address the\\nunique requirements of clinical time series explanation tasks. To this end,\\nDeltaSHAP addresses three key clinical needs: explaining the changes in the\\nconsecutive predictions rather than isolated prediction scores, providing both\\nmagnitude and direction of feature attributions, and delivering these insights\\nin real time. By adapting Shapley values to temporal settings, our approach\\naccurately captures feature coalition effects. It further attributes prediction\\nchanges using only the actually observed feature combinations, making it\\nefficient and practical for time-sensitive clinical applications. We also\\nintroduce new evaluation metrics to evaluate the faithfulness of the\\nattributions for online time series, and demonstrate through experiments on\\nonline patient monitoring tasks that DeltaSHAP outperforms state-of-the-art XAI\\nmethods in both explanation quality as 62% and computational efficiency as 33%\\ntime reduction on the MIMIC-III decompensation benchmark. We release our code\\nat https://github.com/AITRICS/DeltaSHAP.', 'Understanding the behavior of numerical metaheuristic optimization algorithms\\nis critical for advancing their development and application. Traditional\\nvisualization techniques, such as convergence plots, trajectory mapping, and\\nfitness landscape analysis, often fall short in illustrating the structural\\ndynamics of the search process, especially in high-dimensional or complex\\nsolution spaces. To address this, we propose a novel representation and\\nvisualization methodology that clusters solution candidates explored by the\\nalgorithm and tracks the evolution of cluster memberships across iterations,\\noffering a dynamic and interpretable view of the search process. Additionally,\\nwe introduce two metrics - algorithm stability and algorithm similarity- to\\nquantify the consistency of search trajectories across runs of an individual\\nalgorithm and the similarity between different algorithms, respectively. We\\napply this methodology to a set of ten numerical metaheuristic algorithms,\\nrevealing insights into their stability and comparative behaviors, thereby\\nproviding a deeper understanding of their search dynamics.', \"Next token prediction paradigm has been prevailing for autoregressive models\\nin the era of LLMs. The current default sampling choice for popular LLMs is\\ntemperature scaling together with nucleus sampling to balance diversity and\\ncoherence. Nevertheless, such approach leads to inferior performance in various\\nNLP tasks when the model is not certain about testing questions. To this end,\\nwe propose a brand new training-free decoding strategy, dubbed as Cautious Next\\nToken Prediction (CNTP). In the decoding process, if the model has\\ncomparatively high prediction entropy at a certain step, we sample multiple\\ntrials starting from the step independently and stop when encountering any\\npunctuation. Then we select the trial with the lowest perplexity score viewed\\nas the most probable and reliable trial path given the model's capacity. The\\ntrial number is negatively correlated with the prediction confidence, i.e., the\\nless confident the model is, the more trials it should sample. This is\\nconsistent with human beings' behaviour: when feeling uncertain or unconfident,\\none tends to think more creatively, exploring multiple thinking paths, to\\ncautiously select the path one feels most confident about. Extensive\\nexperiments on both LLMs and MLLMs show that our proposed CNTP approach\\noutperforms existing standard decoding strategies consistently by a clear\\nmargin. Moreover, the integration of CNTP with self consistency can further\\nimprove over vanilla self consistency. We believe our proposed CNTP has the\\npotential to become one of the default choices for LLM decoding. Code is\\navailable at https://github.com/wyzjack/CNTP.\", 'This paper leverages the recently introduced concept of algorithm footprints\\nto investigate the interplay between algorithm configurations and problem\\ncharacteristics. Performance footprints are calculated for six modular variants\\nof the CMA-ES algorithm (modCMA), evaluated on 24 benchmark problems from the\\nBBOB suite, across two-dimensional settings: 5-dimensional and 30-dimensional.\\nThese footprints provide insights into why different configurations of the same\\nalgorithm exhibit varying performance and identify the problem features\\ninfluencing these outcomes. Our analysis uncovers shared behavioral patterns\\nacross configurations due to common interactions with problem properties, as\\nwell as distinct behaviors on the same problem driven by differing problem\\nfeatures. The results demonstrate the effectiveness of algorithm footprints in\\nenhancing interpretability and guiding configuration choices.', 'Rice leaf diseases significantly reduce productivity and cause economic\\nlosses, highlighting the need for early detection to enable effective\\nmanagement and improve yields. This study proposes Artificial Neural Network\\n(ANN)-based image-processing techniques for timely classification and\\nrecognition of rice diseases. Despite the prevailing approach of directly\\ninputting images of rice leaves into ANNs, there is a noticeable absence of\\nthorough comparative analysis between the Feature Analysis Detection Model\\n(FADM) and Direct Image-Centric Detection Model (DICDM), specifically when it\\ncomes to evaluating the effectiveness of Feature Extraction Algorithms (FEAs).\\nHence, this research presents initial experiments on the Feature Analysis\\nDetection Model, utilizing various image Feature Extraction Algorithms,\\nDimensionality Reduction Algorithms (DRAs), Feature Selection Algorithms\\n(FSAs), and Extreme Learning Machine (ELM). The experiments are carried out on\\ndatasets encompassing bacterial leaf blight, brown spot, leaf blast, leaf\\nscald, Sheath blight rot, and healthy leaf, utilizing 10-fold Cross-Validation\\nmethod. A Direct Image-Centric Detection Model is established without the\\nutilization of any FEA, and the evaluation of classification performance relies\\non different metrics. Ultimately, an exhaustive contrast is performed between\\nthe achievements of the Feature Analysis Detection Model and Direct\\nImage-Centric Detection Model in classifying rice leaf diseases. The results\\nreveal that the highest performance is attained using the Feature Analysis\\nDetection Model. The adoption of the proposed Feature Analysis Detection Model\\nfor detecting rice leaf diseases holds excellent potential for improving crop\\nhealth, minimizing yield losses, and enhancing overall productivity and\\nsustainability of rice farming.', 'The belief revision field is opulent in new proposals and indigent in\\nanalyses of existing approaches. Much work hinge on postulates, employed as\\nsyntactic characterizations: some revision mechanism is equivalent to some\\nproperties. Postulates constraint specific revision instances: certain\\nrevisions update certain beliefs in a certain way. As an example, if the\\nrevision is consistent with the current beliefs, it is incorporated with no\\nother change. A postulate like this tells what revisions must do and neglect\\nwhat they can do. Can they reach a certain state of beliefs? Can they reach all\\npossible states of beliefs? Can they reach all possible states of beliefs from\\nno previous belief? Can they reach a dogmatic state of beliefs, where\\neverything not believed is impossible? Can they make two conditions equally\\nbelieved? An application where every possible state of beliefs is sensible\\nrequires each state of beliefs to be reachable. An application where conditions\\nmay be equally believed requires such a belief state to be reachable. An\\napplication where beliefs may become dogmatic requires a way to make them\\ndogmatic. Such doxastic states need to be reached in a way or another. Not in\\nspecific way, as dictated by a typical belief revision postulate. This is an\\nability, not a constraint: the ability of being plastic, equating, dogmatic.\\nAmnesic, correcting, believer, damascan, learnable are other abilities. Each\\nrevision mechanism owns some of these abilities and lacks the others:\\nlexicographic, natural, restrained, very radical, full meet, radical, severe,\\nmoderate severe, deep severe, plain severe and deep severe revisions, each of\\nthese revisions is proved to possess certain abilities.', 'Few-shot anomaly generation is emerging as a practical solution for\\naugmenting the scarce anomaly data in industrial quality control settings. An\\nideal generator would meet three demands at once, namely (i) keep the normal\\nbackground intact, (ii) inpaint anomalous regions to tightly overlap with the\\ncorresponding anomaly masks, and (iii) generate anomalous regions in a\\nsemantically valid location, while still producing realistic, diverse\\nappearances from only a handful of real examples. Existing diffusion-based\\nmethods usually satisfy at most two of these requirements: global anomaly\\ngenerators corrupt the background, whereas mask-guided ones often falter when\\nthe mask is imprecise or misplaced. We propose MAGIC--Mask-guided inpainting\\nwith multi-level perturbations and Context-aware alignment--to resolve all\\nthree issues. At its core, MAGIC fine-tunes a Stable Diffusion inpainting\\nbackbone that preserves normal regions and ensures strict adherence of the\\nsynthesized anomaly to the supplied mask, directly addressing background\\ncorruption and misalignment. To offset the diversity loss that fine-tuning can\\ncause, MAGIC adds two complementary perturbation strategies: (i) Gaussian\\nprompt-level perturbation applied during fine-tuning and inference that\\nbroadens the global appearance of anomalies while avoiding low-fidelity textual\\nappearances, and (ii) mask-guided spatial noise injection that enriches local\\ntexture variations. Additionally, the context-aware mask alignment module forms\\nsemantic correspondences and relocates masks so that every anomaly remains\\nplausibly contained within the host object, eliminating out-of-boundary\\nartifacts. Under a consistent identical evaluation protocol on the MVTec-AD\\ndataset, MAGIC outperforms previous state-of-the-arts in downstream anomaly\\ntasks.', 'Traditional continual learning methods prioritize knowledge retention and\\nfocus primarily on mitigating catastrophic forgetting, implicitly assuming that\\nthe data distribution of previously learned tasks remains static. This\\noverlooks the dynamic nature of real-world data streams, where concept drift\\npermanently alters previously seen data and demands both stability and rapid\\nadaptation.\\n  We introduce a holistic framework for continual learning under concept drift\\nthat simulates realistic scenarios by evolving task distributions. As a\\nbaseline, we consider Full Relearning (FR), in which the model is retrained\\nfrom scratch on newly labeled samples from the drifted distribution. While\\neffective, this approach incurs substantial annotation and computational\\noverhead. To address these limitations, we propose Adaptive Memory Realignment\\n(AMR), a lightweight alternative that equips rehearsal-based learners with a\\ndrift-aware adaptation mechanism. AMR selectively removes outdated samples of\\ndrifted classes from the replay buffer and repopulates it with a small number\\nof up-to-date instances, effectively realigning memory with the new\\ndistribution. This targeted resampling matches the performance of FR while\\nreducing the need for labeled data and computation by orders of magnitude.\\n  To enable reproducible evaluation, we introduce four concept-drift variants\\nof standard vision benchmarks: Fashion-MNIST-CD, CIFAR10-CD, CIFAR100-CD, and\\nTiny-ImageNet-CD, where previously seen classes reappear with shifted\\nrepresentations. Comprehensive experiments on these datasets using several\\nrehearsal-based baselines show that AMR consistently counters concept drift,\\nmaintaining high accuracy with minimal overhead. These results position AMR as\\na scalable solution that reconciles stability and plasticity in non-stationary\\ncontinual learning environments.', \"Usability evaluation is crucial in human-centered design but can be costly,\\nrequiring expert time and user compensation. In this work, we developed a\\nmethod for synthetic heuristic evaluation using multimodal LLMs' ability to\\nanalyze images and provide design feedback. Comparing our synthetic evaluations\\nto those by experienced UX practitioners across two apps, we found our\\nevaluation identified 73% and 77% of usability issues, which exceeded the\\nperformance of 5 experienced human evaluators (57% and 63%). Compared to human\\nevaluators, the synthetic evaluation's performance maintained consistent\\nperformance across tasks and excelled in detecting layout issues, highlighting\\npotential attentional and perceptual strengths of synthetic evaluation.\\nHowever, synthetic evaluation struggled with recognizing some UI components and\\ndesign conventions, as well as identifying across screen violations.\\nAdditionally, testing synthetic evaluations over time and accounts revealed\\nstable performance. Overall, our work highlights the performance differences\\nbetween human and LLM-driven evaluations, informing the design of synthetic\\nheuristic evaluations.\", 'Domain-Adaptive Pre-training (DAP) has recently gained attention for its\\neffectiveness in fine-tuning pre-trained models. Building on this, continual\\nDAP has been explored to develop pre-trained models capable of incrementally\\nincorporating different domain datasets. However, existing continual DAP\\nmethods face several limitations: (1) high computational cost and GPU memory\\nusage during training; (2) sensitivity to incremental data order; and (3)\\nproviding a single, generalized model for all end tasks, which contradicts the\\nessence of DAP. In this paper, we propose DoMIX, a novel approach that\\naddresses these challenges by leveraging LoRA modules, a representative\\nparameter-efficient fine-tuning (PEFT) method. Our approach enables efficient\\nand parallel domain-adaptive pre-training that is robust to domain order and\\neffectively utilizes accumulated knowledge to provide tailored pre-trained\\nmodels for specific tasks. We also demonstrate that our method can be extended\\nbeyond the DAP setting to standard LLM fine-tuning scenarios. Code is available\\nat https://github.com/dohoonkim-ai/DoMIX.', 'Data-driven semantic communication is based on superficial statistical\\npatterns, thereby lacking interpretability and generalization, especially for\\napplications with the presence of unseen data. To address these challenges, we\\npropose a novel knowledge graph-enhanced zero-shot semantic communication\\n(KGZS-SC) network. Guided by the structured semantic information from a\\nknowledge graph-based semantic knowledge base (KG-SKB), our scheme provides\\ngeneralized semantic representations and enables reasoning for unseen cases.\\nSpecifically, the KG-SKB aligns the semantic features in a shared category\\nsemantics embedding space and enhances the generalization ability of the\\ntransmitter through aligned semantic features, thus reducing communication\\noverhead by selectively transmitting compact visual semantics. At the receiver,\\nzero-shot learning (ZSL) is leveraged to enable direct classification for\\nunseen cases without the demand for retraining or additional computational\\noverhead, thereby enhancing the adaptability and efficiency of the\\nclassification process in dynamic or resource-constrained environments. The\\nsimulation results conducted on the APY datasets show that the proposed KGZS-SC\\nnetwork exhibits robust generalization and significantly outperforms existing\\nSC frameworks in classifying unseen categories across a range of SNR levels.', 'Recommendation systems have become essential in modern music streaming\\nplatforms, shaping how users discover and engage with songs. One common\\napproach in recommendation systems is collaborative filtering, which suggests\\ncontent based on the preferences of users with similar listening patterns to\\nthe target user. However, this method is less effective on media where\\ninteractions are sparse. Music is one such medium, since the average user of a\\nmusic streaming service will never listen to the vast majority of tracks. Due\\nto this sparsity, there are several challenges that have to be addressed with\\nother methods. This review examines the current state of research in addressing\\nthese challenges, with an emphasis on the role of content filtering in\\nmitigating biases inherent in collaborative filtering approaches. We explore\\nvarious methods of song classification for content filtering, including lyrical\\nanalysis using Large Language Models (LLMs) and audio signal processing\\ntechniques. Additionally, we discuss the potential conflicts between these\\ndifferent analysis methods and propose avenues for resolving such\\ndiscrepancies.', 'Video-to-Audio (V2A) Generation achieves significant progress and plays a\\ncrucial role in film and video post-production. However, current methods\\noverlook the cinematic language, a critical component of artistic expression in\\nfilmmaking. As a result, their performance deteriorates in scenarios where\\nFoley targets are only partially visible. To address this challenge, we propose\\na simple self-distillation approach to extend V2A models to cinematic language\\nscenarios. By simulating the cinematic language variations, the student model\\nlearns to align the video features of training pairs with the same audio-visual\\ncorrespondences, enabling it to effectively capture the associations between\\nsounds and partial visual information. Our method not only achieves impressive\\nimprovements under partial visibility across all evaluation metrics, but also\\nenhances performance on the large-scale V2A dataset, VGGSound.', \"High-dimensional and incomplete (HDI) data, characterized by massive node\\ninteractions, have become ubiquitous across various real-world applications.\\nSecond-order latent factor models have shown promising performance in modeling\\nthis type of data. Nevertheless, due to the bilinear and non-convex nature of\\nthe SLF model's objective function, incorporating a damping term into the\\nHessian approximation and carefully tuning associated parameters become\\nessential. To overcome these challenges, we propose a new approach in this\\nstudy, named the adaptive cubic regularized second-order latent factor analysis\\n(ACRSLF) model. The proposed ACRSLF adopts the two-fold ideas: 1) self-tuning\\ncubic regularization that dynamically mitigates non-convex optimization\\ninstabilities; 2) multi-Hessian-vector product evaluation during conjugate\\ngradient iterations for precise second-order information assimilation.\\nComprehensive experiments on two industrial HDI datasets demonstrate that the\\nACRSLF converges faster and achieves higher representation accuracy than the\\nadvancing optimizer-based LFA models.\", 'Hurricanes cause widespread destruction, resulting in diverse damage types\\nand severities that require timely and accurate assessment for effective\\ndisaster response. While traditional single-label classification methods fall\\nshort of capturing the complexity of post-hurricane damage, this study\\nintroduces a novel multi-label classification framework for assessing damage\\nusing aerial imagery. The proposed approach integrates a feature extraction\\nmodule based on ResNet and a class-specific attention mechanism to identify\\nmultiple damage types within a single image. Using the Rescuenet dataset from\\nHurricane Michael, the proposed method achieves a mean average precision of\\n90.23%, outperforming existing baseline methods. This framework enhances\\npost-hurricane damage assessment, enabling more targeted and efficient disaster\\nresponse and contributing to future strategies for disaster mitigation and\\nresilience. This paper has been accepted at the ASCE International Conference\\non Computing in Civil Engineering (i3CE 2025), and the camera-ready version\\nwill appear in the official conference proceedings.', 'Despite improvements by length extrapolation, efficient attention and memory\\nmodules, handling infinitely long documents with linear complexity without\\nperformance degradation during extrapolation remains the ultimate challenge in\\nlong-text processing. We directly optimize for long-text tasks in an end-to-end\\nfashion and introduce a novel agent workflow, MemAgent, which reads text in\\nsegments and updates the memory using an overwrite strategy. We extend the DAPO\\nalgorithm to facilitate training via independent-context multi-conversation\\ngeneration. MemAgent has demonstrated superb long-context capabilities, being\\nable to extrapolate from an 8K context trained on 32K text to a 3.5M QA task\\nwith performance loss < 5% and achieves 95%+ in 512K RULER test.', \"Progress in enhancing large language model (LLM) planning and reasoning\\ncapabilities is significantly hampered by the bottleneck of scalable, reliable\\ndata generation and evaluation. To overcome this, I introduce NL2FLOW, a fully\\nautomated system for parametrically generating planning problems - expressed in\\nnatural language, a structured intermediate representation, and formal PDDL -\\nand rigorously evaluating the quality of generated plans. I demonstrate\\nNL2FLOW's capabilities by generating a dataset of 2296 problems in the\\nautomated workflow generation domain and evaluating multiple open-sourced,\\ninstruct-tuned LLMs. My results reveal that the highest performing models\\nachieved 86% success in generating valid plans and 69% in generating optimal\\nplans, specifically for problems with feasible solutions. Regression analysis\\nshows that the influence of problem characteristics on plan generation is\\ncontingent on both model and prompt design. Notably, I observed that the\\nhighest success rate for translating natural language into a JSON\\nrepresentation of a plan was lower than the highest rate of generating a valid\\nplan directly. This suggests that unnecessarily decomposing the reasoning task\\n- introducing intermediate translation steps - may actually degrade\\nperformance, implying a benefit to models capable of reasoning directly from\\nnatural language to action. As I scale LLM reasoning to increasingly complex\\nproblems, the bottlenecks and sources of error within these systems will\\ninevitably shift. Therefore, a dynamic understanding of these limitations - and\\nthe tools to systematically reveal them - will be crucial for unlocking the\\nfull potential of LLMs as intelligent problem solvers.\", 'Precise surgical interventions are vital to patient safety, and advanced\\nenhancement algorithms have been developed to assist surgeons in\\ndecision-making. Despite significant progress, these algorithms are typically\\ndesigned for single tasks in specific scenarios, limiting their effectiveness\\nin complex real-world situations. To address this limitation, we propose\\nSurgVisAgent, an end-to-end intelligent surgical vision agent built on\\nmultimodal large language models (MLLMs). SurgVisAgent dynamically identifies\\ndistortion categories and severity levels in endoscopic images, enabling it to\\nperform a variety of enhancement tasks such as low-light enhancement,\\noverexposure correction, motion blur elimination, and smoke removal.\\nSpecifically, to achieve superior surgical scenario understanding, we design a\\nprior model that provides domain-specific knowledge. Additionally, through\\nin-context few-shot learning and chain-of-thought (CoT) reasoning, SurgVisAgent\\ndelivers customized image enhancements tailored to a wide range of distortion\\ntypes and severity levels, thereby addressing the diverse requirements of\\nsurgeons. Furthermore, we construct a comprehensive benchmark simulating\\nreal-world surgical distortions, on which extensive experiments demonstrate\\nthat SurgVisAgent surpasses traditional single-task models, highlighting its\\npotential as a unified solution for surgical assistance.', 'The (generative) artificial intelligence (AI) era has profoundly reshaped the\\nmeaning and value of data. No longer confined to static content, data now\\npermeates every stage of the AI lifecycle from the training samples that shape\\nmodel parameters to the prompts and outputs that drive real-world model\\ndeployment. This shift renders traditional notions of data protection\\ninsufficient, while the boundaries of what needs safeguarding remain poorly\\ndefined. Failing to safeguard data in AI systems can inflict societal and\\nindividual, underscoring the urgent need to clearly delineate the scope of and\\nrigorously enforce data protection. In this perspective, we propose a\\nfour-level taxonomy, including non-usability, privacy preservation,\\ntraceability, and deletability, that captures the diverse protection needs\\narising in modern (generative) AI models and systems. Our framework offers a\\nstructured understanding of the trade-offs between data utility and control,\\nspanning the entire AI pipeline, including training datasets, model weights,\\nsystem prompts, and AI-generated content. We analyze representative technical\\napproaches at each level and reveal regulatory blind spots that leave critical\\nassets exposed. By offering a structured lens to align future AI technologies\\nand governance with trustworthy data practices, we underscore the urgency of\\nrethinking data protection for modern AI techniques and provide timely guidance\\nfor developers, researchers, and regulators alike.', \"The proliferation of ride-hailing aggregator platforms presents significant\\ngrowth opportunities for ride-service providers by increasing order volume and\\ngross merchandise value (GMV). On most ride-hailing aggregator platforms,\\nservice providers that offer lower fares are ranked higher in listings and,\\nconsequently, are more likely to be selected by passengers. This competitive\\nranking mechanism creates a strong incentive for service providers to adopt\\ncoupon strategies that lower prices to secure a greater number of orders, as\\norder volume directly influences their long-term viability and sustainability.\\nThus, designing an effective coupon strategy that can dynamically adapt to\\nmarket fluctuations while optimizing order acquisition under budget constraints\\nis a critical research challenge. However, existing studies in this area remain\\nscarce.\\n  To bridge this gap, we propose FCA-RL, a novel reinforcement learning-based\\nsubsidy strategy framework designed to rapidly adapt to competitors' pricing\\nadjustments. Our approach integrates two key techniques: Fast Competition\\nAdaptation (FCA), which enables swift responses to dynamic price changes, and\\nReinforced Lagrangian Adjustment (RLA), which ensures adherence to budget\\nconstraints while optimizing coupon decisions on new price landscape.\\nFurthermore, we introduce RideGym, the first dedicated simulation environment\\ntailored for ride-hailing aggregators, facilitating comprehensive evaluation\\nand benchmarking of different pricing strategies without compromising\\nreal-world operational efficiency. Experimental results demonstrate that our\\nproposed method consistently outperforms baseline approaches across diverse\\nmarket conditions, highlighting its effectiveness in subsidy optimization for\\nride-hailing service providers.\", 'Background: Clinical documentation represents a significant burden for\\nhealthcare providers, with physicians spending up to 2 hours daily on\\nadministrative tasks. Recent advances in large language models (LLMs) offer\\npromising solutions, but privacy concerns and computational requirements limit\\ntheir adoption in healthcare settings. Objective: To develop and evaluate a\\nprivacy-preserving, on-device medical transcription system using a fine-tuned\\nLlama 3.2 1B model capable of generating structured medical notes from medical\\ntranscriptions while maintaining complete data sovereignty entirely in the\\nbrowser. Methods: We fine-tuned a Llama 3.2 1B model using Parameter-Efficient\\nFine-Tuning (PEFT) with LoRA on 1,500 synthetic medical\\ntranscription-to-structured note pairs. The model was evaluated against the\\nbase Llama 3.2 1B on two datasets: 100 endocrinology transcripts and 140\\nmodified ACI benchmark cases. Evaluation employed both statistical metrics\\n(ROUGE, BERTScore, BLEURT) and LLM-as-judge assessments across multiple\\nclinical quality dimensions. Results: The fine-tuned OnDevice model\\ndemonstrated substantial improvements over the base model. On the ACI\\nbenchmark, ROUGE-1 scores increased from 0.346 to 0.496, while BERTScore F1\\nimproved from 0.832 to 0.866. Clinical quality assessments showed marked\\nreduction in major hallucinations (from 85 to 35 cases) and enhanced factual\\ncorrectness (2.81 to 3.54 on 5-point scale). Similar improvements were observed\\non the internal evaluation dataset, with composite scores increasing from 3.13\\nto 4.43 (+41.5%). Conclusions: Fine-tuning compact LLMs for medical\\ntranscription yields clinically meaningful improvements while enabling complete\\non-device browser deployment. This approach addresses key barriers to AI\\nadoption in healthcare: privacy preservation, cost reduction, and accessibility\\nfor resource-constrained environments.', 'We establish fundamental mathematical limits on universal approximation\\ntheorem (UAT) system alignment by proving that catastrophic failures are an\\ninescapable feature of any useful computational system. Our central thesis is\\nthat for any universal approximator, the expressive power required for useful\\ncomputation is inextricably linked to a dense set of instabilities that make\\nperfect, reliable control a mathematical impossibility. We prove this through a\\nthree-level argument that leaves no escape routes for any class of universal\\napproximator architecture. i) Combinatorial Necessity: For the vast majority of\\npractical universal approximators (e.g., those using ReLU activations), we\\nprove that the density of catastrophic failure points is directly proportional\\nto the network\\'s expressive power. ii) Topological Necessity: For any\\ntheoretical universal approximator, we use singularity theory to prove that the\\nability to approximate generic functions requires the ability to implement the\\ndense, catastrophic singularities that characterize them. iii) Empirical\\nNecessity: We prove that the universal existence of adversarial examples is\\nempirical evidence that real-world tasks are themselves catastrophic, forcing\\nany successful model to learn and replicate these instabilities. These results,\\ncombined with a quantitative \"Impossibility Sandwich\" showing that the minimum\\ncomplexity for usefulness exceeds the maximum complexity for safety,\\ndemonstrate that perfect alignment is not an engineering challenge but a\\nmathematical impossibility. This foundational result reframes UAT safety from a\\nproblem of \"how to achieve perfect control\" to one of \"how to operate safely in\\nthe presence of irreducible uncontrollability,\" with profound implications for\\nthe future of UAT development and governance.', 'Learning robust object detectors from only a handful of images is a critical\\nchallenge in industrial vision systems, where collecting high quality training\\ndata can take months. Synthetic data has emerged as a key solution for data\\nefficient visual inspection and pick and place robotics. Current pipelines rely\\non 3D engines such as Blender or Unreal, which offer fine control but still\\nrequire weeks to render a small dataset, and the resulting images often suffer\\nfrom a large gap between simulation and reality. Diffusion models promise a\\nstep change because they can generate high quality images in minutes, yet\\nprecise control, especially in low data regimes, remains difficult. Although\\nmany adapters now extend diffusion beyond plain text prompts, the effect of\\ndifferent conditioning schemes on synthetic data quality is poorly understood.\\nWe study eighty diverse visual concepts drawn from four standard object\\ndetection benchmarks and compare two conditioning strategies: prompt based and\\nlayout based. When the set of conditioning cues is narrow, prompt conditioning\\nyields higher quality synthetic data; as diversity grows, layout conditioning\\nbecomes superior. When layout cues match the full training distribution,\\nsynthetic data raises mean average precision by an average of thirty four\\npercent and by as much as one hundred seventy seven percent compared with using\\nreal data alone.', \"Recent studies in the spatial prisoner's dilemma games with reinforcement\\nlearning have shown that static agents can learn to cooperate through a diverse\\nsort of mechanisms, including noise injection, different types of learning\\nalgorithms and neighbours' payoff knowledge. In this work, using an independent\\nmulti-agent Q-learning algorithm, we study the effects of dilution and mobility\\nin the spatial version of the prisoner's dilemma. Within this setting,\\ndifferent possible actions for the algorithm are defined, connecting with\\nprevious results on the classical, non-reinforcement learning spatial\\nprisoner's dilemma, showcasing the versatility of the algorithm in modeling\\ndifferent game-theoretical scenarios and the benchmarking potential of this\\napproach. As a result, a range of effects is observed, including evidence that\\ngames with fixed update rules can be qualitatively equivalent to those with\\nlearned ones, as well as the emergence of a symbiotic mutualistic effect\\nbetween populations that forms when multiple actions are defined.\", 'True Random Number Generators (TRNGs) play a fundamental role in hardware\\nsecurity, cryptographic systems, and data protection. In the context of Deep\\nNeuralNetworks (DNNs), safeguarding model parameters, particularly weights, is\\ncritical to ensure the integrity, privacy, and intel-lectual property of AI\\nsystems. While software-based pseudo-random number generators are widely used,\\nthey lack the unpredictability and resilience offered by hardware-based TRNGs.\\nIn this work, we propose a novel and robust Encoding-in-Memory TRNG called\\nEIM-TRNG that leverages the inherent physical randomness in DRAM cell behavior,\\nparticularly under RowHammer-induced disturbances, for the first time. We\\ndemonstrate how the unpredictable bit-flips generated through carefully\\ncontrolled RowHammer operations can be harnessed as a reliable entropy source.\\nFurthermore, we apply this TRNG framework to secure DNN weight data by encoding\\nvia a combination of fixed and unpredictable bit-flips. The encrypted data is\\nlater decrypted using a key derived from the probabilistic flip behavior,\\nensuring both data confidentiality and model authenticity. Our results validate\\nthe effectiveness of DRAM-based entropy extraction for robust, low-cost\\nhardware security and offer a promising direction for protecting machine\\nlearning models at the hardware level.', 'Event stream based scene text recognition is a newly arising research topic\\nin recent years which performs better than the widely used RGB cameras in\\nextremely challenging scenarios, especially the low illumination, fast motion.\\nExisting works either adopt end-to-end encoder-decoder framework or large\\nlanguage models for enhanced recognition, however, they are still limited by\\nthe challenges of insufficient interpretability and weak contextual logical\\nreasoning. In this work, we propose a novel chain-of-thought reasoning based\\nevent stream scene text recognition framework, termed ESTR-CoT. Specifically,\\nwe first adopt the vision encoder EVA-CLIP (ViT-G/14) to transform the input\\nevent stream into tokens and utilize a Llama tokenizer to encode the given\\ngeneration prompt. A Q-former is used to align the vision token to the\\npre-trained large language model Vicuna-7B and output both the answer and\\nchain-of-thought (CoT) reasoning process simultaneously. Our framework can be\\noptimized using supervised fine-tuning in an end-to-end manner. In addition, we\\nalso propose a large-scale CoT dataset to train our framework via a three stage\\nprocessing (i.e., generation, polish, and expert verification). This dataset\\nprovides a solid data foundation for the development of subsequent\\nreasoning-based large models. Extensive experiments on three event stream STR\\nbenchmark datasets (i.e., EventSTR, WordArt*, IC15*) fully validated the\\neffectiveness and interpretability of our proposed framework. The source code\\nand pre-trained models will be released on\\nhttps://github.com/Event-AHU/ESTR-CoT.', \"Chain-of-thought (CoT) reasoning has enabled transformer-based language\\nmodels to excel at complex mathematics and multi-step planning. However, in\\nstandard decoder-only architectures, these reasoning steps are externalized in\\nnatural language, improving interpretability at the cost of efficiency. To\\ncapture reasoning that is not easily represented in words, many works have\\nexplored recurrent architectures that aim to internalize reasoning in latent\\nspace, potentially supporting latent CoT. In this paper, we investigate whether\\nsuch reasoning structures emerge in Huginn-3.5B, a depth-recurrent Transformer\\nthat reuses layers at inference time without increasing parameter count. We\\nexamine the model's internal behavior on arithmetic tasks using a suite of\\nprobing techniques including the Logit Lens and Coda Lens. Our findings reveal\\nlimited evidence of interpretable latent CoT by tracking rank trajectories of\\nfinal and intermediate result tokens. Furthermore, we uncover significant\\nprobing inconsistencies across recurrent blocks, where the interpretability of\\nhidden states depends heavily on both the layer index and the decoding method.\\nFinally, we empirically show that increasing recurrence depth yields only\\nmarginal gains and falls well short of models that explicitly externalize\\nreasoning steps. The code is available at\\nhttps://github.com/wenquanlu/huginn-latent-cot.\", 'As LLMs are increasingly studied as role-playing agents to generate synthetic\\ndata for human behavioral research, ensuring that their outputs remain coherent\\nwith their assigned roles has become a critical concern. In this paper, we\\ninvestigate how consistently LLM-based role-playing agents\\' stated beliefs\\nabout the behavior of the people they are asked to role-play (\"what they say\")\\ncorrespond to their actual behavior during role-play (\"how they act\").\\nSpecifically, we establish an evaluation framework to rigorously measure how\\nwell beliefs obtained by prompting the model can predict simulation outcomes in\\nadvance. Using an augmented version of the GenAgents persona bank and the Trust\\nGame (a standard economic game used to quantify players\\' trust and\\nreciprocity), we introduce a belief-behavior consistency metric to\\nsystematically investigate how it is affected by factors such as: (1) the types\\nof beliefs we elicit from LLMs, like expected outcomes of simulations versus\\ntask-relevant attributes of individual characters LLMs are asked to simulate;\\n(2) when and how we present LLMs with relevant information about Trust Game;\\nand (3) how far into the future we ask the model to forecast its actions. We\\nalso explore how feasible it is to impose a researcher\\'s own theoretical priors\\nin the event that the originally elicited beliefs are misaligned with research\\nobjectives. Our results reveal systematic inconsistencies between LLMs\\' stated\\n(or imposed) beliefs and the outcomes of their role-playing simulation, at both\\nan individual- and population-level. Specifically, we find that, even when\\nmodels appear to encode plausible beliefs, they may fail to apply them in a\\nconsistent way. These findings highlight the need to identify how and when\\nLLMs\\' stated beliefs align with their simulated behavior, allowing researchers\\nto use LLM-based agents appropriately in behavioral studies.', 'While recent advances in preference learning have enhanced alignment in human\\nfeedback, mathematical reasoning remains a persistent challenge. We investigate\\nhow data diversification strategies in preference optimization can improve the\\nmathematical reasoning abilities of large language models (LLMs). We evaluate\\nthree common data generation methods: temperature sampling, Chain-of-Thought\\nprompting, and Monte Carlo Tree Search (MCTS), and introduce\\nDiversified-ThinkSolve (DTS), a novel structured approach that systematically\\ndecomposes problems into diverse reasoning paths. Our results show that with\\nstrategically diversified preference data, models can substantially improve\\nmathematical reasoning performance, with the best approach yielding gains of\\n7.1% on GSM8K and 4.2% on MATH over the base model. Despite its strong\\nperformance, DTS incurs only a marginal computational overhead (1.03x) compared\\nto the baseline, while MCTS is nearly five times more costly with lower\\nreturns. These findings demonstrate that structured exploration of diverse\\nproblem-solving methods creates more effective preference data for mathematical\\nalignment than traditional approaches.', 'Trajectory planning in robotics is understood as generating a sequence of\\njoint configurations that will lead a robotic agent, or its manipulator, from\\nan initial state to the desired final state, thus completing a manipulation\\ntask while considering constraints like robot kinematics and the environment.\\nTypically, this is achieved via sampling-based planners, which are\\ncomputationally intensive. Recent advances demonstrate that trajectory planning\\ncan also be performed by supervised sequence learning of trajectories, often\\nrequiring only a single or fixed number of passes through a neural\\narchitecture, thus ensuring a bounded computation time. Such fully supervised\\napproaches, however, perform imitation learning; they do not learn based on\\nwhether the trajectories can successfully reach a goal, but try to reproduce\\nobserved trajectories. In our work, we build on this approach and propose a\\ncognitively inspired self-supervised learning scheme based on a recurrent\\narchitecture for building a trajectory model. We evaluate the feasibility of\\nthe proposed method on a task of kinematic planning for a robotic arm. The\\nresults suggest that the model is able to learn to generate trajectories only\\nusing given paired forward and inverse kinematics models, and indicate that\\nthis novel method could facilitate planning for more complex manipulation tasks\\nrequiring adaptive solutions.', 'This study employs Long Short-Term Memory (LSTM) networks to forecast key\\nperformance indicators (KPIs), Occupancy (OCC), Average Daily Rate (ADR), and\\nRevenue per Available Room (RevPAR), across five major cities: Manchester,\\nAmsterdam, Dubai, Bangkok, and Mumbai. The cities were selected for their\\ndiverse economic profiles and hospitality dynamics. Monthly data from 2018 to\\n2025 were used, with 80% for training and 20% for testing. Advanced time series\\ndecomposition and machine learning techniques enabled accurate forecasting and\\ntrend identification. Results show that Manchester and Mumbai exhibited the\\nhighest predictive accuracy, reflecting stable demand patterns, while Dubai and\\nBangkok demonstrated higher variability due to seasonal and event-driven\\ninfluences. The findings validate the effectiveness of LSTM models for urban\\nhospitality forecasting and provide a comparative framework for data-driven\\ndecision-making. The models generalisability across global cities highlights\\nits potential utility for tourism stakeholders and urban planners.', 'Graph generation is an important area in network science. Traditional\\napproaches focus on replicating specific properties of real-world graphs, such\\nas small diameters or power-law degree distributions. Recent advancements in\\ndeep learning, particularly with Graph Neural Networks, have enabled\\ndata-driven methods to learn and generate graphs without relying on predefined\\nstructural properties. Despite these advances, current models are limited by\\ntheir reliance on node IDs, which restricts their ability to generate graphs\\nlarger than the input graph and ignores node attributes. To address these\\nchallenges, we propose Latent Graph Sampling Generation (LGSG), a novel\\nframework that leverages diffusion models and node embeddings to generate\\ngraphs of varying sizes without retraining. The framework eliminates the\\ndependency on node IDs and captures the distribution of node embeddings and\\nsubgraph structures, enabling scalable and flexible graph generation.\\nExperimental results show that LGSG performs on par with baseline models for\\nstandard metrics while outperforming them in overlooked ones, such as the\\ntendency of nodes to form clusters. Additionally, it maintains consistent\\nstructural characteristics across graphs of different sizes, demonstrating\\nrobustness and scalability.', 'Transfer learning in Reinforcement Learning (RL) enables agents to leverage\\nknowledge from source tasks to accelerate learning in target tasks. While prior\\nwork, such as the Attend, Adapt, and Transfer (A2T) framework, addresses\\nnegative transfer and selective transfer, other critical challenges remain\\nunderexplored. This paper introduces the Generalized Adaptive Transfer Network\\n(GATN), a deep RL architecture designed to tackle task generalization across\\ndomains, robustness to environmental changes, and computational efficiency in\\ntransfer. GATN employs a domain-agnostic representation module, a\\nrobustness-aware policy adapter, and an efficient transfer scheduler to achieve\\nthese goals. We evaluate GATN on diverse benchmarks, including Atari 2600,\\nMuJoCo, and a custom chatbot dialogue environment, demonstrating superior\\nperformance in cross-domain generalization, resilience to dynamic environments,\\nand reduced computational overhead compared to baselines. Our findings suggest\\nGATN is a versatile framework for real-world RL applications, such as adaptive\\nchatbots and robotic control.', \"Artificial intelligence systems, especially those using machine learning, are\\nbeing deployed in domains from hiring to loan issuance in order to automate\\nthese complex decisions. Judging both the effectiveness and fairness of these\\nAI systems, and their human decision making counterpart, is a complex and\\nimportant topic studied across both computational and social sciences. Within\\nmachine learning, a common way to address bias in downstream classifiers is to\\nresample the training data to offset disparities. For example, if hiring rates\\nvary by some protected class, then one may equalize the rate within the\\ntraining set to alleviate bias in the resulting classifier. While simple and\\nseemingly effective, these methods have typically only been evaluated using\\ndata obtained through convenience samples, introducing selection bias and label\\nbias into metrics. Within the social sciences, psychology, public health, and\\nmedicine, audit studies, in which fictitious ``testers'' (e.g., resumes,\\nemails, patient actors) are sent to subjects (e.g., job openings, businesses,\\ndoctors) in randomized control trials, provide high quality data that support\\nrigorous estimates of discrimination. In this paper, we investigate how data\\nfrom audit studies can be used to improve our ability to both train and\\nevaluate automated hiring algorithms. We find that such data reveals cases\\nwhere the common fairness intervention method of equalizing base rates across\\nclasses appears to achieve parity using traditional measures, but in fact has\\nroughly 10% disparity when measured appropriately. We additionally introduce\\ninterventions based on individual treatment effect estimation methods that\\nfurther reduce algorithmic discrimination using this data.\", 'Dialogue summarization is a challenging task with significant practical value\\nin customer service, meeting analysis, and conversational AI. Although large\\nlanguage models (LLMs) have achieved substantial progress in summarization\\ntasks, the performance of step-by-step reasoning architectures-specifically\\nLong Chain-of-Thought (CoT) implementations such as OpenAI-o1 and\\nDeepSeek-R1-remains unexplored for dialogue scenarios requiring concurrent\\nabstraction and conciseness. In this work, we present the first comprehensive\\nand systematic evaluation of state-of-the-art reasoning LLMs and non-reasoning\\nLLMs across three major paradigms-generic, role-oriented, and query-oriented\\ndialogue summarization. Our study spans diverse languages, domains, and summary\\nlengths, leveraging strong benchmarks (SAMSum, DialogSum, CSDS, and QMSum) and\\nadvanced evaluation protocols that include both LLM-based automatic metrics and\\nhuman-inspired criteria. Contrary to trends in other reasoning-intensive tasks,\\nour findings show that explicit stepwise reasoning does not consistently\\nimprove dialogue summarization quality. Instead, reasoning LLMs are often prone\\nto verbosity, factual inconsistencies, and less concise summaries compared to\\ntheir non-reasoning counterparts. Through scenario-specific analyses and\\ndetailed case studies, we further identify when and why explicit reasoning may\\nfail to benefit-or even hinder-summarization in complex dialogue contexts. Our\\nwork provides new insights into the limitations of current reasoning LLMs and\\nhighlights the need for targeted modeling and evaluation strategies for\\nreal-world dialogue summarization.', 'Large language models (LLMs) are increasingly used to assign document\\nrelevance labels in information retrieval pipelines, especially in domains\\nlacking human-labeled data. However, different models often disagree on\\nborderline cases, raising concerns about how such disagreement affects\\ndownstream retrieval. This study examines labeling disagreement between two\\nopen-weight LLMs, LLaMA and Qwen, on a corpus of scholarly abstracts related to\\nSustainable Development Goals (SDGs) 1, 3, and 7. We isolate disagreement\\nsubsets and examine their lexical properties, rank-order behavior, and\\nclassification predictability. Our results show that model disagreement is\\nsystematic, not random: disagreement cases exhibit consistent lexical patterns,\\nproduce divergent top-ranked outputs under shared scoring functions, and are\\ndistinguishable with AUCs above 0.74 using simple classifiers. These findings\\nsuggest that LLM-based filtering introduces structured variability in document\\nretrieval, even under controlled prompting and shared ranking logic. We propose\\nusing classification disagreement as an object of analysis in retrieval\\nevaluation, particularly in policy-relevant or thematic search tasks.', 'The blockchain oracle problem, which refers to the challenge of injecting\\nreliable external data into decentralized systems, remains a fundamental\\nlimitation to the development of trustless applications. While recent years\\nhave seen a proliferation of architectural, cryptographic, and economic\\nstrategies to mitigate this issue, no one has yet fully resolved the\\nfundamental question of how a blockchain can gain knowledge about the off-chain\\nworld. In this position paper, we critically assess the role artificial\\nintelligence (AI) can play in tackling the oracle problem. Drawing from both\\nacademic literature and practitioner implementations, we examine how AI\\ntechniques such as anomaly detection, language-based fact extraction, dynamic\\nreputation modeling, and adversarial resistance can enhance oracle systems. We\\nobserve that while AI introduces powerful tools for improving data quality,\\nsource selection, and system resilience, it cannot eliminate the reliance on\\nunverifiable off-chain inputs. Therefore, this study supports the idea that AI\\nshould be understood as a complementary layer of inference and filtering within\\na broader oracle design, not a substitute for trust assumptions.', 'We present a hybrid machine learning framework that combines Physics-Informed\\nNeural Operators (PINOs) with score-based generative diffusion models to\\nsimulate the full spatio-temporal evolution of two-dimensional, incompressible,\\nresistive magnetohydrodynamic (MHD) turbulence across a broad range of Reynolds\\nnumbers ($\\\\mathrm{Re}$). The framework leverages the equation-constrained\\ngeneralization capabilities of PINOs to predict coherent, low-frequency\\ndynamics, while a conditional diffusion model stochastically corrects\\nhigh-frequency residuals, enabling accurate modeling of fully developed\\nturbulence. Trained on a comprehensive ensemble of high-fidelity simulations\\nwith $\\\\mathrm{Re} \\\\in \\\\{100, 250, 500, 750, 1000, 3000, 10000\\\\}$, the approach\\nachieves state-of-the-art accuracy in regimes previously inaccessible to\\ndeterministic surrogates. At $\\\\mathrm{Re}=1000$ and $3000$, the model\\nfaithfully reconstructs the full spectral energy distributions of both velocity\\nand magnetic fields late into the simulation, capturing non-Gaussian\\nstatistics, intermittent structures, and cross-field correlations with high\\nfidelity. At extreme turbulence levels ($\\\\mathrm{Re}=10000$), it remains the\\nfirst surrogate capable of recovering the high-wavenumber evolution of the\\nmagnetic field, preserving large-scale morphology and enabling statistically\\nmeaningful predictions.', \"Modern AI models, such as large language models, are usually trained once on\\na huge corpus of data, potentially fine-tuned for a specific task, and then\\ndeployed with fixed parameters. Their training is costly, slow, and gradual,\\nrequiring billions of repetitions. In stark contrast, animals continuously\\nadapt to the ever-changing contingencies in their environments. This is\\nparticularly important for social species, where behavioral policies and reward\\noutcomes may frequently change in interaction with peers. The underlying\\ncomputational processes are often marked by rapid shifts in an animal's\\nbehaviour and rather sudden transitions in neuronal population activity. Such\\ncomputational capacities are of growing importance for AI systems operating in\\nthe real world, like those guiding robots or autonomous vehicles, or for\\nagentic AI interacting with humans online. Can AI learn from neuroscience? This\\nPerspective explores this question, integrating the literature on continual and\\nin-context learning in AI with the neuroscience of learning on behavioral tasks\\nwith shifting rules, reward probabilities, or outcomes. We will outline an\\nagenda for how specifically insights from neuroscience may inform current\\ndevelopments in AI in this area, and - vice versa - what neuroscience may learn\\nfrom AI, contributing to the evolving field of NeuroAI.\", 'Inference-time computation techniques, analogous to human System 2 Thinking,\\nhave recently become popular for improving model performances. However, most\\nexisting approaches suffer from several limitations: they are modality-specific\\n(e.g., working only in text), problem-specific (e.g., verifiable domains like\\nmath and coding), or require additional supervision/training on top of\\nunsupervised pretraining (e.g., verifiers or verifiable rewards). In this\\npaper, we ask the question \"Is it possible to generalize these System 2\\nThinking approaches, and develop models that learn to think solely from\\nunsupervised learning?\" Interestingly, we find the answer is yes, by learning\\nto explicitly verify the compatibility between inputs and\\ncandidate-predictions, and then re-framing prediction problems as optimization\\nwith respect to this verifier. Specifically, we train Energy-Based Transformers\\n(EBTs) -- a new class of Energy-Based Models (EBMs) -- to assign an energy\\nvalue to every input and candidate-prediction pair, enabling predictions\\nthrough gradient descent-based energy minimization until convergence. Across\\nboth discrete (text) and continuous (visual) modalities, we find EBTs scale\\nfaster than the dominant Transformer++ approach during training, achieving an\\nup to 35% higher scaling rate with respect to data, batch size, parameters,\\nFLOPs, and depth. During inference, EBTs improve performance with System 2\\nThinking by 29% more than the Transformer++ on language tasks, and EBTs\\noutperform Diffusion Transformers on image denoising while using fewer forward\\npasses. Further, we find that EBTs achieve better results than existing models\\non most downstream tasks given the same or worse pretraining performance,\\nsuggesting that EBTs generalize better than existing approaches. Consequently,\\nEBTs are a promising new paradigm for scaling both the learning and thinking\\ncapabilities of models.', \"We explore applying a tensor completion approach to complete the DrugMatrix\\ntoxicogenomics dataset. Our hypothesis is that by preserving the 3-dimensional\\nstructure of the data, which comprises tissue, treatment, and transcriptomic\\nmeasurements, and by leveraging a machine learning formulation, our approach\\nwill improve upon prior state-of-the-art results. Our results demonstrate that\\nthe new tensor-based method more accurately reflects the original data\\ndistribution and effectively captures organ-specific variability. The proposed\\ntensor-based methodology achieved lower mean squared errors and mean absolute\\nerrors compared to both conventional Canonical Polyadic decomposition and\\n2-dimensional matrix factorization methods. In addition, our non-negative\\ntensor completion implementation reveals relationships among tissues. Our\\nfindings not only complete the world's largest in-vivo toxicogenomics database\\nwith improved accuracy but also offer a promising methodology for future\\nstudies of drugs that may cross species barriers, for example, from rats to\\nhumans.\", \"Geometric diffusion models have shown remarkable success in molecular\\ndynamics and structure generation. However, efficiently fine-tuning them for\\ndownstream tasks with varying geometric controls remains underexplored. In this\\nwork, we propose an SE(3)-equivariant adapter framework ( GeoAda) that enables\\nflexible and parameter-efficient fine-tuning for controlled generative tasks\\nwithout modifying the original model architecture. GeoAda introduces a\\nstructured adapter design: control signals are first encoded through coupling\\noperators, then processed by a trainable copy of selected pretrained model\\nlayers, and finally projected back via decoupling operators followed by an\\nequivariant zero-initialized convolution. By fine-tuning only these lightweight\\nadapter modules, GeoAda preserves the model's geometric consistency while\\nmitigating overfitting and catastrophic forgetting. We theoretically prove that\\nthe proposed adapters maintain SE(3)-equivariance, ensuring that the geometric\\ninductive biases of the pretrained diffusion model remain intact during\\nadaptation. We demonstrate the wide applicability of GeoAda across diverse\\ngeometric control types, including frame control, global control, subgraph\\ncontrol, and a broad range of application domains such as particle dynamics,\\nmolecular dynamics, human motion prediction, and molecule generation. Empirical\\nresults show that GeoAda achieves state-of-the-art fine-tuning performance\\nwhile preserving original task accuracy, whereas other baselines experience\\nsignificant performance degradation due to overfitting and catastrophic\\nforgetting.\", \"Designing experiments and result interpretations are core scientific\\ncompetencies, particularly in biology, where researchers perturb complex\\nsystems to uncover the underlying systems. Recent efforts to evaluate the\\nscientific capabilities of large language models (LLMs) fail to test these\\ncompetencies because wet-lab experimentation is prohibitively expensive: in\\nexpertise, time and equipment. We introduce SciGym, a first-in-class benchmark\\nthat assesses LLMs' iterative experiment design and analysis abilities in\\nopen-ended scientific discovery tasks. SciGym overcomes the challenge of\\nwet-lab costs by running a dry lab of biological systems. These models, encoded\\nin Systems Biology Markup Language, are efficient for generating simulated\\ndata, making them ideal testbeds for experimentation on realistically complex\\nsystems. We evaluated six frontier LLMs on 137 small systems, and released a\\ntotal of 350 systems. Our evaluation shows that while more capable models\\ndemonstrated superior performance, all models' performance declined\\nsignificantly as system complexity increased, suggesting substantial room for\\nimprovement in the scientific capabilities of LLM agents.\", 'Large language models (LLMs) have rapidly progressed into general-purpose\\nagents capable of solving a broad spectrum of tasks. However, current models\\nremain inefficient at reasoning: they apply fixed inference-time compute\\nregardless of task complexity, often overthinking simple problems while\\nunderthinking hard ones. This survey presents a comprehensive review of\\nefficient test-time compute (TTC) strategies, which aim to improve the\\ncomputational efficiency of LLM reasoning. We introduce a two-tiered taxonomy\\nthat distinguishes between L1-controllability, methods that operate under fixed\\ncompute budgets, and L2-adaptiveness, methods that dynamically scale inference\\nbased on input difficulty or model confidence. We benchmark leading proprietary\\nLLMs across diverse datasets, highlighting critical trade-offs between\\nreasoning performance and token usage. Compared to prior surveys on efficient\\nreasoning, our review emphasizes the practical control, adaptability, and\\nscalability of TTC methods. Finally, we discuss emerging trends such as hybrid\\nthinking models and identify key challenges for future work towards making LLMs\\nmore computationally efficient, robust, and responsive to user constraints.', 'Crash detection from video feeds is a critical problem in intelligent\\ntransportation systems. Recent developments in large language models (LLMs) and\\nvision-language models (VLMs) have transformed how we process, reason about,\\nand summarize multimodal information. This paper surveys recent methods\\nleveraging LLMs for crash detection from video data. We present a structured\\ntaxonomy of fusion strategies, summarize key datasets, analyze model\\narchitectures, compare performance benchmarks, and discuss ongoing challenges\\nand opportunities. Our review provides a foundation for future research in this\\nfast-growing intersection of video understanding and foundation models.', 'In this paper, we propose HCVR (Hybrid approach with Correlation-aware Voting\\nRules), a lightweight rule-based feature selection method that combines\\nParameter-to-Parameter (P2P) and Parameter-to-Target (P2T) correlations to\\neliminate redundant features and retain relevant ones. This method is a hybrid\\nof non-iterative and iterative filtering approaches for dimensionality\\nreduction. It is a greedy method, which works by backward elimination,\\neliminating possibly multiple features at every step. The rules contribute to\\nvoting for features, and a decision to keep or discard is made by majority\\nvoting. The rules make use of correlation thresholds between every pair of\\nfeatures, and between features and the target. We provide the results from the\\napplication of HCVR to the SPAMBASE dataset. The results showed improvement\\nperformance as compared to traditional non-iterative (CFS, mRMR and MI) and\\niterative (RFE, SFS and Genetic Algorithm) techniques. The effectiveness was\\nassessed based on the performance of different classifiers after applying\\nfiltering.', 'Large language models (LLMs) have democratized software development, reducing\\nthe expertise barrier for programming complex applications. This accessibility\\nextends to malicious software development, raising significant security\\nconcerns. While LLM providers have implemented alignment mechanisms to prevent\\ndirect generation of overtly malicious code, these safeguards predominantly\\nevaluate individual prompts in isolation, overlooking a critical vulnerability:\\nmalicious operations can be systematically decomposed into benign-appearing\\nsub-tasks. In this paper, we introduce the Malware Generation Compiler (MGC), a\\nnovel framework that leverages this vulnerability through modular decomposition\\nand alignment-evasive generation. MGC employs a specialized Malware Description\\nIntermediate Representation (MDIR) to bridge high-level malicious intents and\\nbenign-appearing code snippets. Extensive evaluation demonstrates that our\\nattack reliably generates functional malware across diverse task specifications\\nand categories, outperforming jailbreaking methods by +365.79% and underground\\nservices by +78.07% in correctness on three benchmark datasets. Case studies\\nfurther show that MGC can reproduce and even enhance 16 real-world malware\\nsamples. This work provides critical insights for security researchers by\\nexposing the risks of compositional attacks against aligned AI systems.\\nDemonstrations are available at\\nhttps://sites.google.com/view/malware-generation-compiler.', \"Recently, mobile manipulation has attracted increasing attention for enabling\\nlanguage-conditioned robotic control in household tasks. However, existing\\nmethods still face challenges in coordinating mobile base and manipulator,\\nprimarily due to two limitations. On the one hand, they fail to explicitly\\nmodel the influence of the mobile base on manipulator control, which easily\\nleads to error accumulation under high degrees of freedom. On the other hand,\\nthey treat the entire mobile manipulation process with the same visual\\nobservation modality (e.g., either all 2D or all 3D), overlooking the distinct\\nmultimodal perception requirements at different stages during mobile\\nmanipulation. To address this, we propose the Adaptive Coordination Diffusion\\nTransformer (AC-DiT), which enhances mobile base and manipulator coordination\\nfor end-to-end mobile manipulation. First, since the motion of the mobile base\\ndirectly influences the manipulator's actions, we introduce a mobility-to-body\\nconditioning mechanism that guides the model to first extract base motion\\nrepresentations, which are then used as context prior for predicting whole-body\\nactions. This enables whole-body control that accounts for the potential impact\\nof the mobile base's motion. Second, to meet the perception requirements at\\ndifferent stages of mobile manipulation, we design a perception-aware\\nmultimodal conditioning strategy that dynamically adjusts the fusion weights\\nbetween various 2D visual images and 3D point clouds, yielding visual features\\ntailored to the current perceptual needs. This allows the model to, for\\nexample, adaptively rely more on 2D inputs when semantic information is crucial\\nfor action prediction, while placing greater emphasis on 3D geometric\\ninformation when precise spatial understanding is required. We validate AC-DiT\\nthrough extensive experiments on both simulated and real-world mobile\\nmanipulation tasks.\", 'We present Locality-aware Parallel Decoding (LPD) to accelerate\\nautoregressive image generation. Traditional autoregressive image generation\\nrelies on next-patch prediction, a memory-bound process that leads to high\\nlatency. Existing works have tried to parallelize next-patch prediction by\\nshifting to multi-patch prediction to accelerate the process, but only achieved\\nlimited parallelization. To achieve high parallelization while maintaining\\ngeneration quality, we introduce two key techniques: (1) Flexible Parallelized\\nAutoregressive Modeling, a novel architecture that enables arbitrary generation\\nordering and degrees of parallelization. It uses learnable position query\\ntokens to guide generation at target positions while ensuring mutual visibility\\namong concurrently generated tokens for consistent parallel decoding. (2)\\nLocality-aware Generation Ordering, a novel schedule that forms groups to\\nminimize intra-group dependencies and maximize contextual support, enhancing\\ngeneration quality. With these designs, we reduce the generation steps from 256\\nto 20 (256$\\\\times$256 res.) and 1024 to 48 (512$\\\\times$512 res.) without\\ncompromising quality on the ImageNet class-conditional generation, and\\nachieving at least 3.4$\\\\times$ lower latency than previous parallelized\\nautoregressive models.', 'Multimodal foundation models, such as GPT-4o, have recently made remarkable\\nprogress, but it is not clear where exactly these models stand in terms of\\nunderstanding vision. In this paper, we benchmark the performance of popular\\nmultimodal foundation models (GPT-4o, o4-mini, Gemini 1.5 Pro and Gemini 2.0\\nFlash, Claude 3.5 Sonnet, Qwen2-VL, Llama 3.2) on standard computer vision\\ntasks (semantic segmentation, object detection, image classification, depth and\\nsurface normal prediction) using established datasets (e.g., COCO, ImageNet and\\nits variants, etc).\\n  The main challenges to performing this are: 1) most models are trained to\\noutput text and cannot natively express versatile domains, such as segments or\\n3D geometry, and 2) many leading models are proprietary and accessible only at\\nan API level, i.e., there is no weight access to adapt them. We address these\\nchallenges by translating standard vision tasks into equivalent text-promptable\\nand API-compatible tasks via prompt chaining to create a standardized\\nbenchmarking framework.\\n  We observe that 1) the models are not close to the state-of-the-art\\nspecialist models at any task. However, 2) they are respectable generalists;\\nthis is remarkable as they are presumably trained on primarily image-text-based\\ntasks. 3) They perform semantic tasks notably better than geometric ones. 4)\\nWhile the prompt-chaining techniques affect performance, better models exhibit\\nless sensitivity to prompt variations. 5) GPT-4o performs the best among\\nnon-reasoning models, securing the top position in 4 out of 6 tasks, 6)\\nreasoning models, e.g. o3, show improvements in geometric tasks, and 7) a\\npreliminary analysis of models with native image generation, like the latest\\nGPT-4o, shows they exhibit quirks like hallucinations and spatial\\nmisalignments.', 'In recent years, large language models (LLMs) have transformed natural\\nlanguage understanding through vast datasets and large-scale parameterization.\\nInspired by this success, we present SpecCLIP, a foundation model framework\\nthat extends LLM-inspired methodologies to stellar spectral analysis. Stellar\\nspectra, akin to structured language, encode rich physical and chemical\\ninformation about stars. By training foundation models on large-scale spectral\\ndatasets, our goal is to learn robust and informative embeddings that support\\ndiverse downstream applications. As a proof of concept, SpecCLIP involves\\npre-training on two spectral types--LAMOST low-resolution and Gaia XP--followed\\nby contrastive alignment using the CLIP (Contrastive Language-Image\\nPre-training) framework, adapted to associate spectra from different\\ninstruments. This alignment is complemented by auxiliary decoders that preserve\\nspectrum-specific information and enable translation (prediction) between\\nspectral types, with the former achieved by maximizing mutual information\\nbetween embeddings and input spectra. The result is a cross-spectrum framework\\nenabling intrinsic calibration and flexible applications across instruments. We\\ndemonstrate that fine-tuning these models on moderate-sized labeled datasets\\nimproves adaptability to tasks such as stellar-parameter estimation and\\nchemical-abundance determination. SpecCLIP also enhances the accuracy and\\nprecision of parameter estimates benchmarked against external survey data.\\nAdditionally, its similarity search and cross-spectrum prediction capabilities\\noffer potential for anomaly detection. Our results suggest that contrastively\\ntrained foundation models enriched with spectrum-aware decoders can advance\\nprecision stellar spectroscopy.', \"In recent years, neural models trained on large multilingual text and speech\\ndatasets have shown great potential for supporting low-resource languages. This\\nstudy investigates the performances of two state-of-the-art Automatic Speech\\nRecognition (ASR) models, OpenAI's Whisper (Small & Large-V2) and Facebook's\\nWav2Vec-BERT on Bangla, a low-resource language. We have conducted experiments\\nusing two publicly available datasets: Mozilla Common Voice-17 and OpenSLR to\\nevaluate model performances. Through systematic fine-tuning and hyperparameter\\noptimization, including learning rate, epochs, and model checkpoint selection,\\nwe have compared the models based on Word Error Rate (WER), Character Error\\nRate (CER), Training Time, and Computational Efficiency. The Wav2Vec-BERT model\\noutperformed Whisper across all key evaluation metrics, demonstrated superior\\nperformance while requiring fewer computational resources, and offered valuable\\ninsights to develop robust speech recognition systems in low-resource\\nlinguistic settings.\", 'The complexity of mental healthcare billing enables anomalies, including\\nfraud. While machine learning methods have been applied to anomaly detection,\\nthey often struggle with class imbalance, label scarcity, and complex\\nsequential patterns. This study explores a hybrid deep learning approach\\ncombining Long Short-Term Memory (LSTM) networks and Transformers, with\\npseudo-labeling via Isolation Forests (iForest) and Autoencoders (AE). Prior\\nwork has not evaluated such hybrid models trained on pseudo-labeled data in the\\ncontext of healthcare billing. The approach is evaluated on two real-world\\nbilling datasets related to mental healthcare. The iForest LSTM baseline\\nachieves the highest recall (0.963) on declaration-level data. On the\\noperation-level data, the hybrid iForest-based model achieves the highest\\nrecall (0.744), though at the cost of lower precision. These findings highlight\\nthe potential of combining pseudo-labeling with hybrid deep learning in\\ncomplex, imbalanced anomaly detection settings.', \"We develop a rotation-invariant neural network that provides the global\\nminimum-variance portfolio by jointly learning how to lag-transform historical\\nreturns and how to regularise both the eigenvalues and the marginal\\nvolatilities of large equity covariance matrices. This explicit mathematical\\nmapping offers clear interpretability of each module's role, so the model\\ncannot be regarded as a pure black-box. The architecture mirrors the analytical\\nform of the global minimum-variance solution yet remains agnostic to dimension,\\nso a single model can be calibrated on panels of a few hundred stocks and\\napplied, without retraining, to one thousand US equities-a cross-sectional jump\\nthat demonstrates robust out-of-sample generalisation. The loss function is the\\nfuture realized minimum portfolio variance and is optimized end-to-end on real\\ndaily returns. In out-of-sample tests from January 2000 to December 2024 the\\nestimator delivers systematically lower realised volatility, smaller maximum\\ndrawdowns, and higher Sharpe ratios than the best analytical competitors,\\nincluding state-of-the-art non-linear shrinkage. Furthermore, although the\\nmodel is trained end-to-end to produce an unconstrained (long-short)\\nminimum-variance portfolio, we show that its learned covariance representation\\ncan be used in general optimizers under long-only constraints with virtually no\\nloss in its performance advantage over competing estimators. These gains\\npersist when the strategy is executed under a highly realistic implementation\\nframework that models market orders at the auctions, empirical slippage,\\nexchange fees, and financing charges for leverage, and they remain stable\\nduring episodes of acute market stress.\", \"Reinforcement Learning from Human Feedback (RLHF) has emerged as a powerful\\ntechnique for aligning large language models (LLMs) with human preferences.\\nHowever, effectively aligning LLMs with diverse human preferences remains a\\nsignificant challenge, particularly when they are conflict. To address this\\nissue, we frame human value alignment as a multi-objective optimization\\nproblem, aiming to maximize a set of potentially conflicting objectives. We\\nintroduce Gradient-Adaptive Policy Optimization (GAPO), a novel fine-tuning\\nparadigm that employs multiple-gradient descent to align LLMs with diverse\\npreference distributions. GAPO adaptively rescales the gradients for each\\nobjective to determine an update direction that optimally balances the\\ntrade-offs between objectives. Additionally, we introduce P-GAPO, which\\nincorporates user preferences across different objectives and achieves Pareto\\nsolutions that better align with the user's specific needs. Our theoretical\\nanalysis demonstrates that GAPO converges towards a Pareto optimal solution for\\nmultiple objectives. Empirical results on Mistral-7B show that GAPO outperforms\\ncurrent state-of-the-art methods, achieving superior performance in both\\nhelpfulness and harmlessness.\", 'Recent advancements in artificial intelligence (AI), particularly in large\\nlanguage models (LLMs) such as OpenAI-o1 and DeepSeek-R1, have demonstrated\\nremarkable capabilities in complex domains such as logical reasoning and\\nexperimental coding. Motivated by these advancements, numerous studies have\\nexplored the application of AI in the innovation process, particularly in the\\ncontext of scientific research. These AI technologies primarily aim to develop\\nsystems that can autonomously conduct research processes across a wide range of\\nscientific disciplines. Despite these significant strides, a comprehensive\\nsurvey on AI for Research (AI4Research) remains absent, which hampers our\\nunderstanding and impedes further development in this field. To address this\\ngap, we present a comprehensive survey and offer a unified perspective on\\nAI4Research. Specifically, the main contributions of our work are as follows:\\n(1) Systematic taxonomy: We first introduce a systematic taxonomy to classify\\nfive mainstream tasks in AI4Research. (2) New frontiers: Then, we identify key\\nresearch gaps and highlight promising future directions, focusing on the rigor\\nand scalability of automated experiments, as well as the societal impact. (3)\\nAbundant applications and resources: Finally, we compile a wealth of resources,\\nincluding relevant multidisciplinary applications, data corpora, and tools. We\\nhope our work will provide the research community with quick access to these\\nresources and stimulate innovative breakthroughs in AI4Research.', 'We investigate a novel approach to time-series modeling, inspired by the\\nsuccesses of large pretrained foundation models. We introduce FAE (Foundation\\nAuto-Encoders), a foundation generative-AI model for anomaly detection in\\ntime-series data, based on Variational Auto-Encoders (VAEs). By foundation, we\\nmean a model pretrained on massive amounts of time-series data which can learn\\ncomplex temporal patterns useful for accurate modeling, forecasting, and\\ndetection of anomalies on previously unseen datasets. FAE leverages VAEs and\\nDilated Convolutional Neural Networks (DCNNs) to build a generic model for\\nunivariate time-series modeling, which could eventually perform properly in\\nout-of-the-box, zero-shot anomaly detection applications. We introduce the main\\nconcepts of FAE, and present preliminary results in different multi-dimensional\\ntime-series datasets from various domains, including a real dataset from an\\noperational mobile ISP, and the well known KDD 2021 Anomaly Detection dataset.', 'Domain specific chatbot applications often involve multi step interactions,\\nsuch as refining search filters, selecting multiple items, or performing\\ncomparisons. Traditional graphical user interfaces (GUIs) handle these\\nworkflows by providing explicit \"Submit\" (commit data) and \"Reset\" (discard\\ndata) actions, allowing back-end systems to track user intent unambiguously. In\\ncontrast, conversational agents rely on subtle language cues, which can lead to\\nconfusion and incomplete context management. This paper proposes modeling these\\nGUI inspired metaphors acknowledgment (submit like) and context switching\\n(reset-like) as explicit tasks within large language model (LLM) prompts. By\\ncapturing user acknowledgment, reset actions, and chain of thought (CoT)\\nreasoning as structured session data, we preserve clarity, reduce user\\nconfusion, and align domain-specific chatbot interactions with back-end logic.\\nWe demonstrate our approach in hotel booking and customer management scenarios,\\nhighlighting improvements in multi-turn task coherence, user satisfaction, and\\nefficiency.', \"Non-monotonic logic programming is the basis for a declarative problem\\nsolving paradigm known as answer set programming (ASP). Departing from the\\nseminal definition by Gelfond and Lifschitz in 1988 for simple normal logic\\nprograms, various answer set semantics have been proposed for extensions. We\\nconsider two important questions: (1) Should the minimal model property,\\nconstraint monotonicity and foundedness as defined in the literature be\\nmandatory conditions for an answer set semantics in general? (2) If not, what\\nother properties could be considered as general principles for answer set\\nsemantics? We address the two questions. First, it seems that the three\\naforementioned conditions may sometimes be too strong, and we illustrate with\\nexamples that enforcing them may exclude expected answer sets. Second, we\\nevolve the Gelfond answer set (GAS) principles for answer set construction by\\nrefining the Gelfond's rationality principle to well-supportedness, minimality\\nw.r.t. negation by default and minimality w.r.t. epistemic negation. The\\nprinciple of well-supportedness guarantees that every answer set is\\nconstructible from if-then rules obeying a level mapping and is thus free of\\ncircular justification, while the two minimality principles ensure that the\\nformalism minimizes knowledge both at the level of answer sets and of world\\nviews. Third, to embody the refined GAS principles, we extend the notion of\\nwell-supportedness substantially to answer sets and world views, respectively.\\nFourth, we define new answer set semantics in terms of the refined GAS\\nprinciples. Fifth, we use the refined GAS principles as an alternative baseline\\nto intuitively assess the existing answer set semantics. Finally, we analyze\\nthe computational complexity.\", \"Edge devices for temporal processing demand models that capture both short-\\nand long- range dynamics under tight memory constraints. While Transformers\\nexcel at sequence modeling, their quadratic memory scaling with sequence length\\nmakes them impractical for such settings. Recurrent Neural Networks (RNNs)\\noffer constant memory but train sequentially, and Temporal Convolutional\\nNetworks (TCNs), though efficient, scale memory with kernel size. To address\\nthis, we propose mGRADE (mininally Gated Recurrent Architecture with Delay\\nEmbedding), a hybrid-memory system that integrates a temporal 1D-convolution\\nwith learnable spacings followed by a minimal gated recurrent unit (minGRU).\\nThis design allows the convolutional layer to realize a flexible delay\\nembedding that captures rapid temporal variations, while the recurrent module\\nefficiently maintains global context with minimal memory overhead. We validate\\nour approach on two synthetic tasks, demonstrating that mGRADE effectively\\nseparates and preserves multi-scale temporal features. Furthermore, on\\nchallenging pixel-by-pixel image classification benchmarks, mGRADE consistently\\noutperforms both pure convolutional and pure recurrent counterparts using\\napproximately 20% less memory footprint, highlighting its suitability for\\nmemory-constrained temporal processing at the edge. This highlights mGRADE's\\npromise as an efficient solution for memory-constrained multi-scale temporal\\nprocessing at the edge.\", 'We proposes a novel method that enables Graph Neural Networks (GNNs) to solve\\nSAT problems by leveraging a technique developed for applying GNNs to Mixed\\nInteger Linear Programming (MILP). Specifically, k-CNF formulae are mapped into\\nMILP problems, which are then encoded as weighted bipartite graphs and\\nsubsequently fed into a GNN for training and testing. From a theoretical\\nperspective: (i) we establish permutation and equivalence invariance results,\\ndemonstrating that the method produces outputs that are stable under reordering\\nof clauses and variables; (ii) we identify a theoretical limitation, showing\\nthat for a class of formulae called foldable formulae, standard GNNs cannot\\nalways distinguish satisfiable from unsatisfiable instances; (iii) we prove a\\nuniversal approximation theorem, establishing that with Random Node\\nInitialization (RNI), the method can approximate SAT solving to arbitrary\\nprecision on finite datasets, that is, the GNN becomes approximately sound and\\ncomplete on such datasets. Furthermore, we show that for unfoldable formulae,\\nthe same approximation guarantee can be achieved without the need for RNI.\\nFinally, we conduct an experimental evaluation of our approach, which show\\nthat, despite the simplicity of the neural architecture, the method achieves\\npromising results.', \"Small- and medium-sized manufacturers need innovative data tools but, because\\nof competition and privacy concerns, often do not want to share their\\nproprietary data with researchers who might be interested in helping. This\\npaper introduces a privacy-preserving platform by which manufacturers may\\nsafely share their data with researchers through secure methods, so that those\\nresearchers then create innovative tools to solve the manufacturers' real-world\\nproblems, and then provide tools that execute solutions back onto the platform\\nfor others to use with privacy and confidentiality guarantees. We illustrate\\nthis problem through a particular use case which addresses an important problem\\nin the large-scale manufacturing of food crystals, which is that quality\\ncontrol relies on image analysis tools. Previous to our research, food crystals\\nin the images were manually counted, which required substantial and\\ntime-consuming human efforts, but we have developed and deployed a crystal\\nanalysis tool which makes this process both more rapid and accurate. The tool\\nenables automatic characterization of the crystal size distribution and numbers\\nfrom microscope images while the natural imperfections from the sample\\npreparation are automatically removed; a machine learning model to count high\\nresolution translucent crystals and agglomeration of crystals was also\\ndeveloped to aid in these efforts. The resulting algorithm was then packaged\\nfor real-world use on the factory floor via a web-based app secured through the\\noriginating privacy-preserving platform, allowing manufacturers to use it while\\nkeeping their proprietary data secure. After demonstrating this full process,\\nfuture directions are also explored.\", 'Low-Rank Adapters (LoRAs) have transformed the fine-tuning of Large Language\\nModels (LLMs) by enabling parameter-efficient updates. However, their\\nwidespread adoption remains limited by the reliance on GPU-based training. In\\nthis work, we propose a theoretically grounded approach to LoRA fine-tuning\\ndesigned specifically for users with limited computational resources,\\nparticularly those restricted to standard laptop CPUs. Our method learns a\\nmeta-operator that maps any input dataset, represented as a probability\\ndistribution, to a set of LoRA weights by leveraging a large bank of\\npre-trained adapters for the Mistral-7B-Instruct-v0.2 model. Instead of\\nperforming new gradient-based updates, our pipeline constructs adapters via\\nlightweight combinations of existing LoRAs directly on CPU. While the resulting\\nadapters do not match the performance of GPU-trained counterparts, they\\nconsistently outperform the base Mistral model on downstream tasks, offering a\\npractical and accessible alternative to traditional GPU-based fine-tuning.', 'AI models are increasingly required to be multimodal, integrating disparate\\ninput streams into a coherent state representation on which subsequent\\nbehaviors and actions can be based. This paper seeks to understand how such\\nmodels behave when input streams present conflicting information. Focusing\\nspecifically on vision-language models, we provide inconsistent inputs (e.g.,\\nan image of a dog paired with the caption \"A photo of a cat\") and ask the model\\nto report the information present in one of the specific modalities (e.g.,\\n\"What does the caption say / What is in the image?\"). We find that models often\\nfavor one modality over the other, e.g., reporting the image regardless of what\\nthe caption says, but that different models differ in which modality they\\nfavor. We find evidence that the behaviorally preferred modality is evident in\\nthe internal representational structure of the model, and that specific\\nattention heads can restructure the representations to favor one modality over\\nthe other. Moreover, we find modality-agnostic \"router heads\" which appear to\\npromote answers about the modality requested in the instruction, and which can\\nbe manipulated or transferred in order to improve performance across datasets\\nand modalities. Together, the work provides essential steps towards identifying\\nand controlling if and how models detect and resolve conflicting signals within\\ncomplex multimodal environments.', 'Vision transformers (ViTs) have rapidly gained prominence in medical imaging\\ntasks such as disease classification, segmentation, and detection due to their\\nsuperior accuracy compared to conventional deep learning models. However, due\\nto their size and complex interactions via the self-attention mechanism, they\\nare not well understood. In particular, it is unclear whether the\\nrepresentations produced by such models are semantically meaningful. In this\\npaper, using a projected gradient-based algorithm, we show that their\\nrepresentations are not semantically meaningful and they are inherently\\nvulnerable to small changes. Images with imperceptible differences can have\\nvery different representations; on the other hand, images that should belong to\\ndifferent semantic classes can have nearly identical representations. Such\\nvulnerability can lead to unreliable classification results; for example,\\nunnoticeable changes cause the classification accuracy to be reduced by over\\n60\\\\%. %. To the best of our knowledge, this is the first work to systematically\\ndemonstrate this fundamental lack of semantic meaningfulness in ViT\\nrepresentations for medical image classification, revealing a critical\\nchallenge for their deployment in safety-critical systems.', 'Language models can distinguish between testing and deployment phases -- a\\ncapability known as evaluation awareness. This has significant safety and\\npolicy implications, potentially undermining the reliability of evaluations\\nthat are central to AI governance frameworks and voluntary industry\\ncommitments. In this paper, we study evaluation awareness in\\nLlama-3.3-70B-Instruct. We show that linear probes can separate real-world\\nevaluation and deployment prompts, suggesting that current models internally\\nrepresent this distinction. We also find that current safety evaluations are\\ncorrectly classified by the probes, suggesting that they already appear\\nartificial or inauthentic to models. Our findings underscore the importance of\\nensuring trustworthy evaluations and understanding deceptive capabilities. More\\nbroadly, our work showcases how model internals may be leveraged to support\\nblackbox methods in safety audits, especially for future models more competent\\nat evaluation awareness and deception.', 'Data quality is a critical driver of large language model performance, yet\\nexisting model-based selection methods focus almost exclusively on English. We\\nintroduce MuRating, a scalable framework that transfers high-quality English\\ndata-quality signals into a single rater for 17 target languages. MuRating\\naggregates multiple English \"raters\" via pairwise comparisons to learn unified\\ndocument-quality scores,then projects these judgments through translation to\\ntrain a multilingual evaluator on monolingual, cross-lingual, and parallel text\\npairs. Applied to web data, MuRating selects balanced subsets of English and\\nmultilingual content to pretrain a 1.2 B-parameter LLaMA model. Compared to\\nstrong baselines, including QuRater, AskLLM, DCLM and so on, our approach\\nboosts average accuracy on both English benchmarks and multilingual\\nevaluations, with especially large gains on knowledge-intensive tasks. We\\nfurther analyze translation fidelity, selection biases, and underrepresentation\\nof narrative material, outlining directions for future work.', \"We introduce BranchNet, a neuro-symbolic learning framework that transforms\\ndecision tree ensembles into sparse, partially connected neural networks. Each\\nbranch, defined as a decision path from root to a parent of leaves, is mapped\\nto a hidden neuron, preserving symbolic structure while enabling gradient-based\\noptimization. The resulting models are compact, interpretable, and require no\\nmanual architecture tuning. Evaluated on a suite of structured multi-class\\nclassification benchmarks, BranchNet consistently outperforms XGBoost in\\naccuracy, with statistically significant gains. We detail the architecture,\\ntraining procedure, and sparsity dynamics, and discuss the model's strengths in\\nsymbolic interpretability as well as its current limitations, particularly on\\nbinary tasks where further adaptive calibration may be beneficial.\", 'This paper introduces a GPU-based complete search method to enclose the\\nglobal minimum of a nonlinear function subject to simple bounds on the\\nvariables. Using interval analysis, coupled with the computational power and\\narchitecture of GPU, the method iteratively rules out the regions in the search\\ndomain where the global minimum cannot exist and leaves a finite set of regions\\nwhere the global minimum must exist. For effectiveness, because of the rigor of\\ninterval analysis, the method is guaranteed to enclose the global minimum of\\nthe nonlinear function even in the presence of rounding errors. For efficiency,\\nthe method employs a novel GPU-based single program, single data parallel\\nprogramming style to circumvent major GPU performance bottlenecks, and a\\nvariable cycling technique is also integrated into the method to reduce\\ncomputational cost when minimizing large-scale nonlinear functions. The method\\nis validated by minimizing 10 multimodal benchmark test functions with scalable\\ndimensions, including the well-known Ackley function, Griewank function, Levy\\nfunction, and Rastrigin function. These benchmark test functions represent\\ngrand challenges of global optimization, and enclosing the guaranteed global\\nminimum of these benchmark test functions with more than 80 dimensions has not\\nbeen reported in the literature. Our method completely searches the feasible\\ndomain and successfully encloses the guaranteed global minimum of these 10\\nbenchmark test functions with up to 10,000 dimensions using only one GPU in a\\nreasonable computation time, far exceeding the reported results in the\\nliterature due to the unique method design and implementation based on GPU\\narchitecture.', 'Although generative models have made remarkable progress in recent years,\\ntheir use in critical applications has been hindered by their incapacity to\\nreliably evaluate sample quality. Quality refers to at least two complementary\\nconcepts: fidelity and coverage. Current quality metrics often lack reliable,\\ninterpretable values due to an absence of calibration or insufficient\\nrobustness to outliers. To address these shortcomings, we introduce two novel\\nmetrics, Clipped Density and Clipped Coverage. By clipping individual sample\\ncontributions and, for fidelity, the radii of nearest neighbor balls, our\\nmetrics prevent out-of-distribution samples from biasing the aggregated values.\\nThrough analytical and empirical calibration, these metrics exhibit linear\\nscore degradation as the proportion of poor samples increases. Thus, they can\\nbe straightforwardly interpreted as equivalent proportions of good samples.\\nExtensive experiments on synthetic and real-world datasets demonstrate that\\nClipped Density and Clipped Coverage outperform existing methods in terms of\\nrobustness, sensitivity, and interpretability for evaluating generative models.', 'Gradient-based optimization is the workhorse of deep learning, offering\\nefficient and scalable training via backpropagation. However, its reliance on\\nlarge volumes of labeled data raises privacy and security concerns such as\\nsusceptibility to data poisoning attacks and the risk of overfitting. In\\ncontrast, black box optimization methods, which treat the model as an opaque\\nfunction, relying solely on function evaluations to guide optimization, offer a\\npromising alternative in scenarios where data access is restricted, adversarial\\nrisks are high, or overfitting is a concern. However, black box methods also\\npose significant challenges, including poor scalability to high-dimensional\\nparameter spaces, as prevalent in large language models (LLMs), and high\\ncomputational costs due to reliance on numerous model evaluations. This paper\\nintroduces BBoxER, an evolutionary black-box method for LLM post-training that\\ninduces an information bottleneck via implicit compression of the training\\ndata. Leveraging the tractability of information flow, we provide strong\\ntheoretical bounds on generalization, differential privacy, susceptibility to\\ndata poisoning attacks, and robustness to extraction attacks. BBoxER operates\\non top of pre-trained LLMs, offering a lightweight and modular enhancement\\nsuitable for deployment in restricted or privacy-sensitive environments, in\\naddition to non-vacuous generalization guarantees. In experiments with LLMs, we\\ndemonstrate empirically that Retrofitting methods are able to learn, showing\\nhow a few iterations of BBoxER improve performance and generalize well on a\\nbenchmark of reasoning datasets. This positions BBoxER as an attractive add-on\\non top of gradient-based optimization.', 'This paper examines the use of in-store customers as delivery couriers in a\\ncentralized crowd-shipping system, targeting the growing need for efficient\\nlast-mile delivery in urban areas. We consider a brick-and-mortar retail\\nsetting where shoppers are offered compensation to deliver time-sensitive\\nonline orders. To manage this process, we propose a Markov Decision Process\\n(MDP) model that captures key uncertainties, including the stochastic arrival\\nof orders and crowd-shippers, and the probabilistic acceptance of delivery\\noffers. Our solution approach integrates Neural Approximate Dynamic Programming\\n(NeurADP) for adaptive order-to-shopper assignment with a Deep Double Q-Network\\n(DDQN) for dynamic pricing. This joint optimization strategy enables multi-drop\\nrouting and accounts for offer acceptance uncertainty, aligning more closely\\nwith real-world operations. Experimental results demonstrate that the\\nintegrated NeurADP + DDQN policy achieves notable improvements in delivery cost\\nefficiency, with up to 6.7\\\\% savings over NeurADP with fixed pricing and\\napproximately 18\\\\% over myopic baselines. We also show that allowing flexible\\ndelivery delays and enabling multi-destination routing further reduces\\noperational costs by 8\\\\% and 17\\\\%, respectively. These findings underscore the\\nadvantages of dynamic, forward-looking policies in crowd-shipping systems and\\noffer practical guidance for urban logistics operators.', 'In this paper, we present details of the 1st W-CODA workshop, held in\\nconjunction with the ECCV 2024. W-CODA aims to explore next-generation\\nsolutions for autonomous driving corner cases, empowered by state-of-the-art\\nmultimodal perception and comprehension techniques. 5 Speakers from both\\nacademia and industry are invited to share their latest progress and opinions.\\nWe collect research papers and hold a dual-track challenge, including both\\ncorner case scene understanding and generation. As the pioneering effort, we\\nwill continuously bridge the gap between frontier autonomous driving techniques\\nand fully intelligent, reliable self-driving agents robust towards corner\\ncases.', 'Graph representation learning methods have been widely adopted in financial\\napplications to enhance company representations by leveraging inter-firm\\nrelationships. However, current approaches face three key challenges: (1) The\\nadvantages of relational information are obscured by limitations in downstream\\ntask designs; (2) Existing graph models specifically designed for stock\\nprediction often suffer from excessive complexity and poor generalization; (3)\\nExperience-based construction of corporate relationship graphs lacks effective\\ncomparison of different graph structures. To address these limitations, we\\npropose a long-term stock prediction task and develop a Node-level Graph\\nAttention Network (NGAT) specifically tailored for corporate relationship\\ngraphs. Furthermore, we experimentally demonstrate the limitations of existing\\ngraph comparison methods based on model downstream task performance.\\nExperimental results across two datasets consistently demonstrate the\\neffectiveness of our proposed task and model. The project is publicly available\\non GitHub to encourage reproducibility and future research.', \"There is justifiable interest in leveraging conversational AI (CAI) for\\nhealth across the majority world, but to be effective, CAI must respond\\nappropriately within culturally and linguistically diverse contexts. Therefore,\\nwe need ways to address the fact that current LLMs exclude many lived\\nexperiences globally. Various advances are underway which focus on top-down\\napproaches and increasing training data. In this paper, we aim to complement\\nthese with a bottom-up locally-grounded approach based on qualitative data\\ncollected during participatory workshops in Latin America. Our goal is to\\nconstruct a rich and human-centred understanding of: a) potential areas of\\ncultural misalignment in digital health; b) regional perspectives on chatbots\\nfor health and c)strategies for creating culturally-appropriate CAI; with a\\nfocus on the understudied Latin American context. Our findings show that\\nacademic boundaries on notions of culture lose meaning at the ground level and\\ntechnologies will need to engage with a broader framework; one that\\nencapsulates the way economics, politics, geography and local logistics are\\nentangled in cultural experience. To this end, we introduce a framework for\\n'Pluriversal Conversational AI for Health' which allows for the possibility\\nthat more relationality and tolerance, rather than just more data, may be\\ncalled for.\", 'Patents contain rich technical knowledge that can inspire innovative product\\nideas, yet accessing and interpreting this information remains a challenge.\\nThis work explores the use of Large Language Models (LLMs) and autonomous\\nagents to mine and generate product concepts from a given patent. In this work,\\nwe design Agent Ideate, a framework for automatically generating product-based\\nbusiness ideas from patents. We experimented with open-source LLMs and\\nagent-based architectures across three domains: Computer Science, Natural\\nLanguage Processing, and Material Chemistry. Evaluation results show that the\\nagentic approach consistently outperformed standalone LLMs in terms of idea\\nquality, relevance, and novelty. These findings suggest that combining LLMs\\nwith agentic workflows can significantly enhance the innovation pipeline by\\nunlocking the untapped potential of business idea generation from patent data.', 'The proliferation of multimodal memes in the social media era demands that\\nmultimodal Large Language Models (mLLMs) effectively understand meme\\nharmfulness. Existing benchmarks for assessing mLLMs on harmful meme\\nunderstanding rely on accuracy-based, model-agnostic evaluations using static\\ndatasets. These benchmarks are limited in their ability to provide up-to-date\\nand thorough assessments, as online memes evolve dynamically. To address this,\\nwe propose AdamMeme, a flexible, agent-based evaluation framework that\\nadaptively probes the reasoning capabilities of mLLMs in deciphering meme\\nharmfulness. Through multi-agent collaboration, AdamMeme provides comprehensive\\nevaluations by iteratively updating the meme data with challenging samples,\\nthereby exposing specific limitations in how mLLMs interpret harmfulness.\\nExtensive experiments show that our framework systematically reveals the\\nvarying performance of different target mLLMs, offering in-depth, fine-grained\\nanalyses of model-specific weaknesses. Our code is available at\\nhttps://github.com/Lbotirx/AdamMeme.', \"In this paper, we propose to incorporate the blackboard architecture into LLM\\nmulti-agent systems (MASs) so that (1) agents with various roles can share all\\nthe information and others' messages during the whole problem-solving process,\\n(2) agents that will take actions are selected based on the current content of\\nthe blackboard, and (3) the selection and execution round is repeated until a\\nconsensus is reached on the blackboard. We develop the first implementation of\\nthis proposal and conduct experiments on commonsense knowledge, reasoning and\\nmathematical datasets. The results show that our system can be competitive with\\nthe SOTA static and dynamic MASs by achieving the best average performance, and\\nat the same time manage to spend less tokens. Our proposal has the potential to\\nenable complex and dynamic problem-solving where well-defined structures or\\nworkflows are unavailable.\", 'Estimating causal effects from real-world relational data can be challenging\\nwhen the underlying causal model and potential confounders are unknown. While\\nseveral causal discovery algorithms exist for learning causal models with\\nlatent confounders from data, they assume that the data is independent and\\nidentically distributed (i.i.d.) and are not well-suited for learning from\\nrelational data. Similarly, existing relational causal discovery algorithms\\nassume causal sufficiency, which is unrealistic for many real-world datasets.\\nTo address this gap, we propose RelFCI, a sound and complete causal discovery\\nalgorithm for relational data with latent confounders. Our work builds upon the\\nFast Causal Inference (FCI) and Relational Causal Discovery (RCD) algorithms\\nand it defines new graphical models, necessary to support causal discovery in\\nrelational domains. We also establish soundness and completeness guarantees for\\nrelational d-separation with latent confounders. We present experimental\\nresults demonstrating the effectiveness of RelFCI in identifying the correct\\ncausal structure in relational causal models with latent confounders.', 'While existing auditing techniques attempt to identify potential unwanted\\nbehaviours in large language models (LLMs), we address the complementary\\nforensic problem of reconstructing the exact input that led to an existing LLM\\noutput - enabling post-incident analysis and potentially the detection of fake\\noutput reports. We formalize exact input reconstruction as a discrete\\noptimisation problem with a unique global minimum and introduce SODA, an\\nefficient gradient-based algorithm that operates on a continuous relaxation of\\nthe input search space with periodic restarts and parameter decay. Through\\ncomprehensive experiments on LLMs ranging in size from 33M to 3B parameters, we\\ndemonstrate that SODA significantly outperforms existing approaches. We succeed\\nin fully recovering 79.5% of shorter out-of-distribution inputs from next-token\\nlogits, without a single false positive, but struggle to extract private\\ninformation from the outputs of longer (15+ token) input sequences. This\\nsuggests that standard deployment practices may currently provide adequate\\nprotection against malicious use of our method. Our code is available at\\nhttps://doi.org/10.5281/zenodo.15539879.', \"Existing post-training techniques for large language models are broadly\\ncategorized into Supervised Fine-Tuning (SFT) and Reinforcement Fine-Tuning\\n(RFT). Each paradigm presents a distinct trade-off: SFT excels at mimicking\\ndemonstration data but can lead to problematic generalization as a form of\\nbehavior cloning. Conversely, RFT can significantly enhance a model's\\nperformance but is prone to learn unexpected behaviors, and its performance is\\nhighly sensitive to the initial policy. In this paper, we propose a unified\\nview of these methods and introduce Prefix-RFT, a hybrid approach that\\nsynergizes learning from both demonstration and exploration. Using mathematical\\nreasoning problems as a testbed, we empirically demonstrate that Prefix-RFT is\\nboth simple and effective. It not only surpasses the performance of standalone\\nSFT and RFT but also outperforms parallel mixed-policy RFT methods. A key\\nadvantage is its seamless integration into existing open-source frameworks,\\nrequiring only minimal modifications to the standard RFT pipeline. Our analysis\\nhighlights the complementary nature of SFT and RFT, and validates that\\nPrefix-RFT effectively harmonizes these two learning paradigms. Furthermore,\\nablation studies confirm the method's robustness to variations in the quality\\nand quantity of demonstration data. We hope this work offers a new perspective\\non LLM post-training, suggesting that a unified paradigm that judiciously\\nintegrates demonstration and exploration could be a promising direction for\\nfuture research.\", \"Deep Recommender Models (DLRMs) inference is a fundamental AI workload\\naccounting for more than 79% of the total AI workload in Meta's data centers.\\nDLRMs' performance bottleneck is found in the embedding layers, which perform\\nmany random memory accesses to retrieve small embedding vectors from tables of\\nvarious sizes. We propose the design of tailored data flows to speedup\\nembedding look-ups. Namely, we propose four strategies to look up an embedding\\ntable effectively on one core, and a framework to automatically map the tables\\nasymmetrically to the multiple cores of a SoC. We assess the effectiveness of\\nour method using the Huawei Ascend AI accelerators, comparing it with the\\ndefault Ascend compiler, and we perform high-level comparisons with Nvidia\\nA100. Results show a speed-up varying from 1.5x up to 6.5x for real workload\\ndistributions, and more than 20x for extremely unbalanced distributions.\\nFurthermore, the method proves to be much more independent of the query\\ndistribution than the baseline.\", 'The field of numerical optimization has recently seen a surge in the\\ndevelopment of \"novel\" metaheuristic algorithms, inspired by metaphors derived\\nfrom natural or human-made processes, which have been widely criticized for\\nobscuring meaningful innovations and failing to distinguish themselves from\\nexisting approaches. Aiming to address these concerns, we investigate the\\napplicability of statistical tests for comparing algorithms based on their\\nsearch behavior. We utilize the cross-match statistical test to compare\\nmultivariate distributions and assess the solutions produced by 114 algorithms\\nfrom the MEALPY library. These findings are incorporated into an empirical\\nanalysis aiming to identify algorithms with similar search behaviors.', 'Reinforcement learning (RL) has become a pivotal technology in the\\npost-training phase of large language models (LLMs). Traditional task-colocated\\nRL frameworks suffer from significant scalability bottlenecks, while\\ntask-separated RL frameworks face challenges in complex dataflows and the\\ncorresponding resource idling and workload imbalance. Moreover, most existing\\nframeworks are tightly coupled with LLM training or inference engines, making\\nit difficult to support custom-designed engines. To address these challenges,\\nwe propose AsyncFlow, an asynchronous streaming RL framework for efficient\\npost-training. Specifically, we introduce a distributed data storage and\\ntransfer module that provides a unified data management and fine-grained\\nscheduling capability in a fully streamed manner. This architecture inherently\\nfacilitates automated pipeline overlapping among RL tasks and dynamic load\\nbalancing. Moreover, we propose a producer-consumer-based asynchronous workflow\\nengineered to minimize computational idleness by strategically deferring\\nparameter update process within staleness thresholds. Finally, the core\\ncapability of AsynFlow is architecturally decoupled from underlying training\\nand inference engines and encapsulated by service-oriented user interfaces,\\noffering a modular and customizable user experience. Extensive experiments\\ndemonstrate an average of 1.59 throughput improvement compared with\\nstate-of-the-art baseline. The presented architecture in this work provides\\nactionable insights for next-generation RL training system designs.', \"Autoregressive (AR) models have garnered significant attention in image\\ngeneration for their ability to effectively capture both local and global\\nstructures within visual data. However, prevalent AR models predominantly rely\\non the transformer architectures, which are beset by quadratic computational\\ncomplexity concerning input sequence length and substantial memory overhead due\\nto the necessity of maintaining key-value caches. Although linear attention\\nmechanisms have successfully reduced this burden in language models, our\\ninitial experiments reveal that they significantly degrade image generation\\nquality because of their inability to capture critical long-range dependencies\\nin visual data. We propose Linear Attention with Spatial-Aware Decay (LASAD), a\\nnovel attention mechanism that explicitly preserves genuine 2D spatial\\nrelationships within the flattened image sequences by computing\\nposition-dependent decay factors based on true 2D spatial location rather than\\n1D sequence positions. Based on this mechanism, we present LASADGen, an\\nautoregressive image generator that enables selective attention to relevant\\nspatial contexts with linear complexity. Experiments on ImageNet show LASADGen\\nachieves state-of-the-art image generation performance and computational\\nefficiency, bridging the gap between linear attention's efficiency and spatial\\nunderstanding needed for high-quality generation.\", \"Gradients of neural networks encode valuable information for optimization,\\nediting, and analysis of models. Therefore, practitioners often treat gradients\\nas inputs to task-specific algorithms, e.g. for pruning or optimization. Recent\\nworks explore learning algorithms that operate directly on gradients but use\\narchitectures that are not specifically designed for gradient processing,\\nlimiting their applicability. In this paper, we present a principled approach\\nfor designing architectures that process gradients. Our approach is guided by\\nthree principles: (1) equivariant design that preserves neuron permutation\\nsymmetries, (2) processing sets of gradients across multiple data points to\\ncapture curvature information, and (3) efficient gradient representation\\nthrough rank-1 decomposition. Based on these principles, we introduce\\nGradMetaNet, a novel architecture for learning on gradients, constructed from\\nsimple equivariant blocks. We prove universality results for GradMetaNet, and\\nshow that previous approaches cannot approximate natural gradient-based\\nfunctions that GradMetaNet can. We then demonstrate GradMetaNet's effectiveness\\non a diverse set of gradient-based tasks on MLPs and transformers, such as\\nlearned optimization, INR editing, and estimating loss landscape curvature.\", 'We present an analysis of landscape features for predicting the performance\\nof multi-objective combinatorial optimization algorithms. We consider features\\nfrom the recently proposed compressed Pareto Local Optimal Solutions Networks\\n(C-PLOS-net) model of combinatorial landscapes. The benchmark instances are a\\nset of rmnk-landscapes with 2 and 3 objectives and various levels of ruggedness\\nand objective correlation. We consider the performance of three algorithms --\\nPareto Local Search (PLS), Global Simple EMO Optimizer (GSEMO), and\\nNon-dominated Sorting Genetic Algorithm (NSGA-II) - using the resolution and\\nhypervolume metrics. Our tailored analysis reveals feature combinations that\\ninfluence algorithm performance specific to certain landscapes. This study\\nprovides deeper insights into feature importance, tailored to specific\\nrmnk-landscapes and algorithms.', 'Generative AI systems have rapidly advanced, with multimodal input\\ncapabilities enabling reasoning beyond text-based tasks. In education, these\\nadvancements could influence assessment design and question answering,\\npresenting both opportunities and challenges. To investigate these effects, we\\nintroduce a high-quality dataset of 201 university-level STEM questions,\\nmanually annotated with features such as image type, role, problem complexity,\\nand question format. Our study analyzes how these features affect generative AI\\nperformance compared to students. We evaluate four model families with five\\nprompting strategies, comparing results to the average of 546 student responses\\nper question. Although the best model correctly answers on average 58.5 % of\\nthe questions using majority vote aggregation, human participants consistently\\noutperform AI on questions involving visual components. Interestingly, human\\nperformance remains stable across question features but varies by subject,\\nwhereas AI performance is susceptible to both subject matter and question\\nfeatures. Finally, we provide actionable insights for educators, demonstrating\\nhow question design can enhance academic integrity by leveraging features that\\nchallenge current AI systems without increasing the cognitive burden for\\nstudents.', 'We present Depth Anything at Any Condition (DepthAnything-AC), a foundation\\nmonocular depth estimation (MDE) model capable of handling diverse\\nenvironmental conditions. Previous foundation MDE models achieve impressive\\nperformance across general scenes but not perform well in complex open-world\\nenvironments that involve challenging conditions, such as illumination\\nvariations, adverse weather, and sensor-induced distortions. To overcome the\\nchallenges of data scarcity and the inability of generating high-quality\\npseudo-labels from corrupted images, we propose an unsupervised consistency\\nregularization finetuning paradigm that requires only a relatively small amount\\nof unlabeled data. Furthermore, we propose the Spatial Distance Constraint to\\nexplicitly enforce the model to learn patch-level relative relationships,\\nresulting in clearer semantic boundaries and more accurate details.\\nExperimental results demonstrate the zero-shot capabilities of DepthAnything-AC\\nacross diverse benchmarks, including real-world adverse weather benchmarks,\\nsynthetic corruption benchmarks, and general benchmarks.\\n  Project Page: https://ghost233lism.github.io/depthanything-AC-page\\n  Code: https://github.com/HVision-NKU/DepthAnythingAC', \"When robots perform complex and context-dependent tasks in our daily lives,\\ndeviations from expectations can confuse users. Explanations of the robot's\\nreasoning process can help users to understand the robot intentions. However,\\nwhen to provide explanations and what they contain are important to avoid user\\nannoyance. We have investigated user preferences for explanation demand and\\ncontent for a robot that helps with daily cleaning tasks in a kitchen. Our\\nresults show that users want explanations in surprising situations and prefer\\nconcise explanations that clearly state the intention behind the confusing\\naction and the contextual factors that were relevant to this decision. Based on\\nthese findings, we propose two algorithms to identify surprising actions and to\\nconstruct effective explanations for Belief-Desire-Intention (BDI) robots. Our\\nalgorithms can be easily integrated in the BDI reasoning process and pave the\\nway for better human-robot interaction with context- and user-specific\\nexplanations.\", 'Neural Radiance Fields (NeRF) have recently emerged as a paradigm for 3D\\nreconstruction from multiview satellite imagery. However, state-of-the-art NeRF\\nmethods are typically constrained to small scenes due to the memory footprint\\nduring training, which we study in this paper. Previous work on large-scale\\nNeRFs palliate this by dividing the scene into NeRFs. This paper introduces\\nSnake-NeRF, a framework that scales to large scenes. Our out-of-core method\\neliminates the need to load all images and networks simultaneously, and\\noperates on a single device. We achieve this by dividing the region of interest\\ninto NeRFs that 3D tile without overlap. Importantly, we crop the images with\\noverlap to ensure each NeRFs is trained with all the necessary pixels. We\\nintroduce a novel $2\\\\times 2$ 3D tile progression strategy and segmented\\nsampler, which together prevent 3D reconstruction errors along the tile edges.\\nOur experiments conclude that large satellite images can effectively be\\nprocessed with linear time complexity, on a single GPU, and without compromise\\nin quality.', \"The task of Human-Object conTact (HOT) detection involves identifying the\\nspecific areas of the human body that are touching objects. Nevertheless,\\ncurrent models are restricted to just one type of image, often leading to too\\nmuch segmentation in areas with little interaction, and struggling to maintain\\ncategory consistency within specific regions. To tackle this issue, a HOT\\nframework, termed \\\\textbf{P3HOT}, is proposed, which blends \\\\textbf{P}rompt\\nguidance and human \\\\textbf{P}roximal \\\\textbf{P}erception. To begin with, we\\nutilize a semantic-driven prompt mechanism to direct the network's attention\\ntowards the relevant regions based on the correlation between image and text.\\nThen a human proximal perception mechanism is employed to dynamically perceive\\nkey depth range around the human, using learnable parameters to effectively\\neliminate regions where interactions are not expected. Calculating depth\\nresolves the uncertainty of the overlap between humans and objects in a 2D\\nperspective, providing a quasi-3D viewpoint. Moreover, a Regional Joint Loss\\n(RJLoss) has been created as a new loss to inhibit abnormal categories in the\\nsame area. A new evaluation metric called ``AD-Acc.'' is introduced to address\\nthe shortcomings of existing methods in addressing negative samples.\\nComprehensive experimental results demonstrate that our approach achieves\\nstate-of-the-art performance in four metrics across two benchmark datasets.\\nSpecifically, our model achieves an improvement of \\\\textbf{0.7}$\\\\uparrow$,\\n\\\\textbf{2.0}$\\\\uparrow$, \\\\textbf{1.6}$\\\\uparrow$, and \\\\textbf{11.0}$\\\\uparrow$ in\\nSC-Acc., mIoU, wIoU, and AD-Acc. metrics, respectively, on the HOT-Annotated\\ndataset. Code is available at https://github.com/YuxiaoWang-AI/P3HOT.\", 'Group recommendation over social media streams has attracted significant\\nattention due to its wide applications in domains such as e-commerce,\\nentertainment, and online news broadcasting. By leveraging social connections\\nand group behaviours, group recommendation (GR) aims to provide more accurate\\nand engaging content to a set of users rather than individuals. Recently,\\ninfluence-aware GR has emerged as a promising direction, as it considers the\\nimpact of social influence on group decision-making. In earlier work, we\\nproposed Influence-aware Group Recommendation (IGR) to solve this task.\\nHowever, this task remains challenging due to three key factors: the large and\\never-growing scale of social graphs, the inherently dynamic nature of influence\\npropagation within user groups, and the high computational overhead of\\nreal-time group-item matching.\\n  To tackle these issues, we propose an Enhanced Influence-aware Group\\nRecommendation (EIGR) framework. First, we introduce a Graph Extraction-based\\nSampling (GES) strategy to minimise redundancy across multiple temporal social\\ngraphs and effectively capture the evolving dynamics of both groups and items.\\nSecond, we design a novel DYnamic Independent Cascade (DYIC) model to predict\\nhow influence propagates over time across social items and user groups.\\nFinally, we develop a two-level hash-based User Group Index (UG-Index) to\\nefficiently organise user groups and enable real-time recommendation\\ngeneration. Extensive experiments on real-world datasets demonstrate that our\\nproposed framework, EIGR, consistently outperforms state-of-the-art baselines\\nin both effectiveness and efficiency.', 'The widespread use of deep learning face recognition raises several security\\nconcerns. Although prior works point at existing vulnerabilities, DNN backdoor\\nattacks against real-life, unconstrained systems dealing with images captured\\nin the wild remain a blind spot of the literature. This paper conducts the\\nfirst system-level study of backdoors in deep learning-based face recognition\\nsystems. This paper yields four contributions by exploring the feasibility of\\nDNN backdoors on these pipelines in a holistic fashion. We demonstrate for the\\nfirst time two backdoor attacks on the face detection task: face generation and\\nface landmark shift attacks. We then show that face feature extractors trained\\nwith large margin losses also fall victim to backdoor attacks. Combining our\\nmodels, we then show using 20 possible pipeline configurations and 15 attack\\ncases that a single backdoor enables an attacker to bypass the entire function\\nof a system. Finally, we provide stakeholders with several best practices and\\ncountermeasures.', \"The rapid integration of artificial intelligence (AI) in education requires\\nteachers to develop AI competencies while preparing students for a society\\ninfluenced by AI. This study evaluates the impact of an online teacher training\\nprogram on German in-service teachers' AI literacy, usage behaviors, and\\nattitudes toward AI. A pre-post design study was conducted with teachers (N1 =\\n291 for AI literacy, N2 = 436 for attitude assessment) participating in the\\ncourse. The program combined synchronous and asynchronous learning formats,\\nincluding webinars, self-paced modules, and practical projects. The\\nparticipants exhibited notable improvements across all domains: AI literacy\\nscores increased significantly, and all attitude items regarding AI usage and\\nintegration demonstrated significant positive changes. Teachers reported\\nincreased confidence in AI integration. Structured teacher training programs\\neffectively enhance AI literacy and foster positive attitudes toward AI in\\neducation.\", \"Traditional Data+AI systems utilize data-driven techniques to optimize\\nperformance, but they rely heavily on human experts to orchestrate system\\npipelines, enabling them to adapt to changes in data, queries, tasks, and\\nenvironments. For instance, while there are numerous data science tools\\navailable, developing a pipeline planning system to coordinate these tools\\nremains challenging. This difficulty arises because existing Data+AI systems\\nhave limited capabilities in semantic understanding, reasoning, and planning.\\nFortunately, we have witnessed the success of large language models (LLMs) in\\nenhancing semantic understanding, reasoning, and planning abilities. It is\\ncrucial to incorporate LLM techniques to revolutionize data systems for\\norchestrating Data+AI applications effectively.\\n  To achieve this, we propose the concept of a 'Data Agent' - a comprehensive\\narchitecture designed to orchestrate Data+AI ecosystems, which focuses on\\ntackling data-related tasks by integrating knowledge comprehension, reasoning,\\nand planning capabilities. We delve into the challenges involved in designing\\ndata agents, such as understanding data/queries/environments/tools,\\norchestrating pipelines/workflows, optimizing and executing pipelines, and\\nfostering pipeline self-reflection. Furthermore, we present examples of data\\nagent systems, including a data science agent, data analytics agents (such as\\nunstructured data analytics agent, semantic structured data analytics agent,\\ndata lake analytics agent, and multi-modal data analytics agent), and a\\ndatabase administrator (DBA) agent. We also outline several open challenges\\nassociated with designing data agent systems.\", 'Temporal Knowledge Graph (TKG) is an efficient method for describing the\\ndynamic development of facts along a timeline. Most research on TKG reasoning\\n(TKGR) focuses on modelling the repetition of global facts and designing\\npatterns of local historical facts. However, they face two significant\\nchallenges: inadequate modeling of the event distribution shift between\\ntraining and test samples, and reliance on random entity substitution for\\ngenerating negative samples, which often results in low-quality sampling. To\\nthis end, we propose a novel distributional feature modeling approach for\\ntraining TKGR models, Test-Time Training-guided Distribution shift Modelling\\n(T3DM), to adjust the model based on distribution shift and ensure the global\\nconsistency of model reasoning. In addition, we design a negative-sampling\\nstrategy to generate higher-quality negative quadruples based on adversarial\\ntraining. Extensive experiments show that T3DM provides better and more robust\\nresults than the state-of-the-art baselines in most cases.', 'This study presents a novel classroom surveillance system that integrates\\nmultiple modalities, including drowsiness, tracking of mobile phone usage, and\\nface recognition,to assess student attentiveness with enhanced precision.The\\nsystem leverages the YOLOv8 model to detect both mobile phone and sleep\\nusage,(Ghatge et al., 2024) while facial recognition is achieved through\\nLResNet Occ FC body tracking using YOLO and MTCNN.(Durai et al., 2024) These\\nmodels work in synergy to provide comprehensive, real-time monitoring, offering\\ninsights into student engagement and behavior.(S et al., 2023) The framework is\\ntrained on specialized datasets, such as the RMFD dataset for face recognition\\nand a Roboflow dataset for mobile phone detection. The extensive evaluation of\\nthe system shows promising results. Sleep detection achieves 97. 42% mAP@50,\\nface recognition achieves 86. 45% validation accuracy and mobile phone\\ndetection reach 85. 89% mAP@50. The system is implemented within a core PHP web\\napplication and utilizes ESP32-CAM hardware for seamless data capture.(Neto et\\nal., 2024) This integrated approach not only enhances classroom monitoring, but\\nalso ensures automatic attendance recording via face recognition as students\\nremain seated in the classroom, offering scalability for diverse educational\\nenvironments.(Banada,2025)', 'The creativity of classical music arises not only from composers who craft\\nthe musical sheets but also from performers who interpret the static notations\\nwith expressive nuances. This paper addresses the challenge of generating\\nclassical piano performances from scratch, aiming to emulate the dual roles of\\ncomposer and pianist in the creative process. We introduce the Expressive\\nCompound Word (ECP) representation, which effectively captures both the\\nmetrical structure and expressive nuances of classical performances. Building\\non this, we propose the Expressive Music Variational AutoEncoder (XMVAE), a\\nmodel featuring two branches: a Vector Quantized Variational AutoEncoder\\n(VQ-VAE) branch that generates score-related content, representing the\\nComposer, and a vanilla VAE branch that produces expressive details, fulfilling\\nthe role of Pianist. These branches are jointly trained with similar Seq2Seq\\narchitectures, leveraging a multiscale encoder to capture beat-level contextual\\ninformation and an orthogonal Transformer decoder for efficient compound tokens\\ndecoding. Both objective and subjective evaluations demonstrate that XMVAE\\ngenerates classical performances with superior musical quality compared to\\nstate-of-the-art models. Furthermore, pretraining the Composer branch on extra\\nmusical score datasets contribute to a significant performance gain.', 'We present a full-stack emergency vehicle (EV) siren detection system\\ndesigned for real-time deployment on embedded hardware. The proposed approach\\nis based on E2PANNs, a fine-tuned convolutional neural network derived from\\nEPANNs, and optimized for binary sound event detection under urban acoustic\\nconditions. A key contribution is the creation of curated and semantically\\nstructured datasets - AudioSet-EV, AudioSet-EV Augmented, and Unified-EV -\\ndeveloped using a custom AudioSet-Tools framework to overcome the low\\nreliability of standard AudioSet annotations. The system is deployed on a\\nRaspberry Pi 5 equipped with a high-fidelity DAC+microphone board, implementing\\na multithreaded inference engine with adaptive frame sizing, probability\\nsmoothing, and a decision-state machine to control false positive activations.\\nA remote WebSocket interface provides real-time monitoring and facilitates live\\ndemonstration capabilities. Performance is evaluated using both framewise and\\nevent-based metrics across multiple configurations. Results show the system\\nachieves low-latency detection with improved robustness under realistic audio\\nconditions. This work demonstrates the feasibility of deploying IoS-compatible\\nSED solutions that can form distributed acoustic monitoring networks, enabling\\ncollaborative emergency vehicle tracking across smart city infrastructures\\nthrough WebSocket connectivity on low-cost edge devices.', 'Process Reinforcement Learning~(PRL) has demonstrated considerable potential\\nin enhancing the reasoning capabilities of Large Language Models~(LLMs).\\nHowever, introducing additional process reward models incurs substantial\\ncomputational overhead, and there is no unified theoretical framework for\\nprocess-level advantage estimation. To bridge this gap, we propose\\n\\\\textbf{S}elf-Guided \\\\textbf{P}rocess \\\\textbf{R}eward\\n\\\\textbf{O}ptimization~(\\\\textbf{SPRO}), a novel framework that enables\\nprocess-aware RL through two key innovations: (1) we first theoretically\\ndemonstrate that process rewards can be derived intrinsically from the policy\\nmodel itself, and (2) we introduce well-defined cumulative process rewards and\\n\\\\textbf{M}asked \\\\textbf{S}tep \\\\textbf{A}dvantage (\\\\textbf{MSA}), which\\nfacilitates rigorous step-wise action advantage estimation within shared-prompt\\nsampling groups. Our experimental results demonstrate that SPRO outperforms\\nvaniila GRPO with 3.4x higher training efficiency and a 17.5\\\\% test accuracy\\nimprovement. Furthermore, SPRO maintains a stable and elevated policy entropy\\nthroughout training while reducing the average response length by approximately\\n$1/3$, evidencing sufficient exploration and prevention of reward hacking.\\nNotably, SPRO incurs no additional computational overhead compared to\\noutcome-supervised RL methods such as GRPO, which benefit industrial\\nimplementation.', 'This paper explores how older adults, particularly aging migrants in urban\\nChina, can engage AI-assisted co-creation to express personal narratives that\\nare often fragmented, underrepresented, or difficult to verbalize. Through a\\npilot workshop combining oral storytelling and the symbolic reconstruction of\\nHanzi, participants shared memories of migration and recreated new character\\nforms using Xiaozhuan glyphs, suggested by the Large Language Model (LLM),\\ntogether with physical materials. Supported by human facilitation and a soft AI\\npresence, participants transformed lived experience into visual and tactile\\nexpressions without requiring digital literacy. This approach offers new\\nperspectives on human-AI collaboration and aging by repositioning AI not as a\\ncontent producer but as a supportive mechanism, and by supporting narrative\\nagency within sociotechnical systems.'], 'uris': None, 'included': ['metadatas', 'documents'], 'data': None, 'metadatas': [{'published': '2025-07-07', 'title': 'Evaluating Memory in LLM Agents via Incremental Multi-Turn Interactions', 'authors': 'Yuanzhe Hu, Yu Wang, Julian McAuley', 'url': 'http://arxiv.org/abs/2507.05257v1'}, {'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.05254v1', 'authors': 'Fabian Konstantinidis, Ariel Dallari Guerreiro, Raphael Trumpp, Moritz Sackmann, Ulrich Hofmann, Marco Caccamo, Christoph Stiller', 'title': 'From Marginal to Joint Predictions: Evaluating Scene-Consistent Trajectory Prediction Approaches for Automated Driving'}, {'published': '2025-07-07', 'authors': 'Elahe Delavari, Feeza Khan Khanzada, Jaerock Kwon', 'title': 'Action Space Reduction Strategies for Reinforcement Learning in Autonomous Driving', 'url': 'http://arxiv.org/abs/2507.05251v1'}, {'url': 'http://arxiv.org/abs/2507.05246v1', 'title': 'When Chain of Thought is Necessary, Language Models Struggle to Evade Monitors', 'published': '2025-07-07', 'authors': 'Scott Emmons, Erik Jenner, David K. Elson, Rif A. Saurous, Senthooran Rajamanoharan, Heng Chen, Irhum Shafkat, Rohin Shah'}, {'published': '2025-07-07', 'authors': 'Benjamin Li, Shuyang Shi, Lucia Romero, Huao Li, Yaqi Xie, Woojun Kim, Stefanos Nikolaidis, Michael Lewis, Katia Sycara, Simon Stepputtis', 'url': 'http://arxiv.org/abs/2507.05244v1', 'title': 'Modeling Latent Partner Strategies for Adaptive Zero-Shot Human-Agent Collaboration'}, {'url': 'http://arxiv.org/abs/2507.05241v1', 'published': '2025-07-07', 'authors': 'Jingyi Chai, Shuo Tang, Rui Ye, Yuwen Du, Xinyu Zhu, Mengcheng Zhou, Yanfeng Wang, Weinan E, Siheng Chen', 'title': \"SciMaster: Towards General-Purpose Scientific AI Agents, Part I. X-Master as Foundation: Can We Lead on Humanity's Last Exam?\"}, {'url': 'http://arxiv.org/abs/2507.05221v1', 'authors': 'Samuel Barbeau, Pedram Fekri, David Osowiechi, Ali Bahri, Moslem YazdanpanahMasih Aminbeidokhti, Christian Desrosiers', 'title': 'CTA: Cross-Task Alignment for Better Test Time Training', 'published': '2025-07-07'}, {'published': '2025-07-07', 'title': 'All in One: Visual-Description-Guided Unified Point Cloud Segmentation', 'url': 'http://arxiv.org/abs/2507.05211v1', 'authors': 'Zongyan Han, Mohamed El Amine Boudjoghra, Jiahua Dong, Jinhong Wang, Rao Muhammad Anwer'}, {'title': 'MedGemma Technical Report', 'url': 'http://arxiv.org/abs/2507.05201v1', 'published': '2025-07-07', 'authors': 'Andrew Sellergren, Sahar Kazemzadeh, Tiam Jaroensri, Atilla Kiraly, Madeleine Traverse, Timo Kohlberger, Shawn Xu, Fayaz Jamil, Cían Hughes, Charles Lau, Justin Chen, Fereshteh Mahvar, Liron Yatziv, Tiffany Chen, Bram Sterling, Stefanie Anna Baby, Susanna Maria Baby, Jeremy Lai, Samuel Schmidgall, Lu Yang, Kejia Chen, Per Bjornsson, Shashir Reddy, Ryan Brush, Kenneth Philbrick, Howard Hu, Howard Yang, Richa Tiwari, Sunny Jansen, Preeti Singh, Yun Liu, Shekoofeh Azizi, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Riviere, Louis Rouillard, Thomas Mesnard, Geoffrey Cideron, Jean-bastien Grill, Sabela Ramos, Edouard Yvinec, Michelle Casbon, Elena Buchatskaya, Jean-Baptiste Alayrac, Dmitry, Lepikhin, Vlad Feinberg, Sebastian Borgeaud, Alek Andreev, Cassidy Hardin, Robert Dadashi, Léonard Hussenot, Armand Joulin, Olivier Bachem, Yossi Matias, Katherine Chou, Avinatan Hassidim, Kavi Goel, Clement Farabet, Joelle Barral, Tris Warkentin, Jonathon Shlens, David Fleet, Victor Cotruta, Omar Sanseviero, Gus Martins, Phoebe Kirk, Anand Rao, Shravya Shetty, David F. Steiner, Can Kirmizibayrak, Rory Pilgrim, Daniel Golden, Lin Yang'}, {'url': 'http://arxiv.org/abs/2507.05198v1', 'title': 'EmbodieDreamer: Advancing Real2Sim2Real Transfer for Policy Training via Embodied World Modeling', 'authors': 'Boyuan Wang, Xinpan Meng, Xiaofeng Wang, Zheng Zhu, Angen Ye, Yang Wang, Zhiqin Yang, Chaojun Ni, Guan Huang, Xingang Wang', 'published': '2025-07-07'}, {'url': 'http://arxiv.org/abs/2507.05195v1', 'authors': 'Guanhua Zhang, Ricardo Dominguez-Olmedo, Moritz Hardt', 'title': 'Train-before-Test Harmonizes Language Model Rankings', 'published': '2025-07-07'}, {'title': 'Infrastructuring Contestability: A Framework for Community-Defined AI Value Pluralism', 'authors': 'Andreas Mayer', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.05187v1'}, {'authors': 'Jonathan Hyun, Nicholas R Waytowich, Boyuan Chen', 'title': 'CREW-WILDFIRE: Benchmarking Agentic Multi-Agent Collaborations at Scale', 'url': 'http://arxiv.org/abs/2507.05178v1', 'published': '2025-07-07'}, {'published': '2025-07-07', 'title': 'OpenS2S: Advancing Open-Source End-to-End Empathetic Large Speech Language Model', 'authors': 'Chen Wang, Tianyu Peng, Wen Yang, Yinan Bai, Guangfu Wang, Jun Lin, Lanpeng Jia, Lingxiang Wu, Jinqiao Wang, Chengqing Zong, Jiajun Zhang', 'url': 'http://arxiv.org/abs/2507.05177v1'}, {'url': 'http://arxiv.org/abs/2507.05169v1', 'authors': 'Eric Xing, Mingkai Deng, Jinyu Hou, Zhiting Hu', 'published': '2025-07-07', 'title': 'Critiques of World Models'}, {'url': 'http://arxiv.org/abs/2507.05162v1', 'title': 'LAID: Lightweight AI-Generated Image Detection in Spatial and Spectral Domains', 'published': '2025-07-07', 'authors': 'Nicholas Chivaran, Jianbing Ni'}, {'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.05157v1', 'title': 'AI Generated Text Detection Using Instruction Fine-tuned Large Language and Transformer-Based Models', 'authors': 'Chinnappa Guggilla, Budhaditya Roy, Trupti Ramdas Chavan, Abdul Rahman, Edward Bowen'}, {'published': '2025-07-07', 'title': 'Effects of Unplanned Incoming Flights on Airport Relief Processes after a Major Natural Disaster', 'authors': 'Luka Van de Sype, Matthieu Vert, Alexei Sharpanskykh, Seyed Sahand Mohammadi Ziabari', 'url': 'http://arxiv.org/abs/2507.05150v1'}, {'title': 'OGF: An Online Gradient Flow Method for Optimizing the Statistical Steady-State Time Averages of Unsteady Turbulent Flows', 'authors': 'Tom Hickling, Jonathan F. MacArt, Justin Sirignano, Den Waidmann', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.05149v1'}, {'authors': 'Wei Xu, Haoran Li, Baoyuan Ou, Lai Xu, Yingjie Qin, Ruilong Su, Ruiwen Xu', 'title': 'GIST: Cross-Domain Click-Through Rate Prediction via Guided Content-Behavior Distillation', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.05142v1'}, {'url': 'http://arxiv.org/abs/2507.05137v1', 'published': '2025-07-07', 'authors': 'Jaewook Lee, Alexander Scarlatos, Andrew Lan', 'title': 'Interpretable Mnemonic Generation for Kanji Learning via Expectation-Maximization'}, {'title': 'An Evaluation of Large Language Models on Text Summarization Tasks Using Prompt Engineering Techniques', 'authors': 'Walid Mohamed Aly, Taysir Hassan A. Soliman, Amr Mohamed AbdelAziz', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.05123v1'}, {'title': 'LVM4CSI: Enabling Direct Application of Pre-Trained Large Vision Models for Wireless Channel Tasks', 'authors': 'Jiajia Guo, Peiwen Jiang, Chao-Kai Wen, Shi Jin, Jun Zhang', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.05121v1'}, {'url': 'http://arxiv.org/abs/2507.05118v1', 'published': '2025-07-07', 'authors': 'Danil S. Grigorev, Alexey K. Kovalev, Aleksandr I. Panov', 'title': 'VerifyLLM: LLM-Based Pre-Execution Task Plan Verification for Robots'}, {'published': '2025-07-07', 'title': 'Rule Learning for Knowledge Graph Reasoning under Agnostic Distribution Shift', 'authors': 'Shixuan Liu, Yue He, Yunfei Wang, Hao Zou, Haoxiang Cheng, Wenjing Yang, Peng Cui, Zhong Liu', 'url': 'http://arxiv.org/abs/2507.05110v1'}, {'authors': 'Yuyi Zhang, Peirong Zhang, Zhenhua Yang, Pengyu Yan, Yongxin Shi, Pengwei Liu, Fengjun Guo, Lianwen Jin', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.05108v1', 'title': 'Reviving Cultural Heritage: A Novel Approach for Comprehensive Historical Document Restoration'}, {'authors': 'Xinzhe Zheng, Hao Du, Fanding Xu, Jinzhe Li, Zhiyuan Liu, Wenkang Wang, Tao Chen, Wanli Ouyang, Stan Z. Li, Yan Lu, Nanqing Dong, Yang Zhang', 'published': '2025-07-07', 'title': 'PRING: Rethinking Protein-Protein Interaction Prediction from Pairs to Graphs', 'url': 'http://arxiv.org/abs/2507.05101v1'}, {'authors': 'Tobias Demmler, Jakob Häringer, Andreas Tamke, Thao Dang, Alexander Hegai, Lars Mikelsons', 'title': 'Beyond Features: How Dataset Design Influences Multi-Agent Trajectory Prediction Performance', 'url': 'http://arxiv.org/abs/2507.05098v1', 'published': '2025-07-07'}, {'url': 'http://arxiv.org/abs/2507.05093v1', 'published': '2025-07-07', 'title': 'The Hidden Threat in Plain Text: Attacking RAG Data Loaders', 'authors': 'Alberto Castagnaro, Umberto Salviati, Mauro Conti, Luca Pajola, Simeone Pizzi'}, {'published': '2025-07-07', 'authors': 'Kilian Rückschloß, Felix Weitkämper', 'url': 'http://arxiv.org/abs/2507.05088v1', 'title': 'How Rules Represent Causal Knowledge: Causal Modeling with Abductive Logic Programs'}, {'url': 'http://arxiv.org/abs/2507.05077v1', 'title': 'Sequential Attention-based Sampling for Histopathological Analysis', 'published': '2025-07-07', 'authors': 'Tarun G, Naman Malpani, Gugan Thoppe, Sridharan Devarajan'}, {'authors': 'Hongyao Yu, Yixiang Qiu, Yiheng Yang, Hao Fang, Tianqu Zhuang, Jiaxin Hong, Bin Chen, Hao Wu, Shu-Tao Xia', 'url': 'http://arxiv.org/abs/2507.05068v1', 'title': 'ICAS: Detecting Training Data from Autoregressive Image Generative Models', 'published': '2025-07-07'}, {'title': 'Replacing thinking with tool usage enables reasoning in small language models', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.05065v1', 'authors': 'Corrado Rainone, Tim Bakker, Roland Memisevic'}, {'authors': 'Xin Dong, Shichao Dong, Jin Wang, Jing Huang, Li Zhou, Zenghui Sun, Lihua Jing, Jingsong Lan, Xiaoyong Zhu, Bo Zheng', 'url': 'http://arxiv.org/abs/2507.05056v1', 'published': '2025-07-07', 'title': 'INTER: Mitigating Hallucination in Large Vision-Language Models by Interaction Guidance Sampling'}, {'title': 'Perspectives on How Sociology Can Advance Theorizing about Human-Chatbot Interaction and Developing Chatbots for Social Good', 'url': 'http://arxiv.org/abs/2507.05030v1', 'published': '2025-07-07', 'authors': 'Celeste Campos-Castillo, Xuan Kang, Linnea I. Laestadius'}, {'title': 'Adaptation of Multi-modal Representation Models for Multi-task Surgical Computer Vision', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.05020v1', 'authors': 'Soham Walimbe, Britty Baby, Vinkle Srivastav, Nicolas Padoy'}, {'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.05019v1', 'authors': 'Lorenzo Braccaioli, Anna Vettoruzzo, Prabhant Singh, Joaquin Vanschoren, Mohamed-Rafik Bouguelia, Nicola Conci', 'title': 'Meta-Learning Transformers to Improve In-Context Generalization'}, {'title': 'When Imitation Learning Outperforms Reinforcement Learning in Surgical Action Planning', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.05011v1', 'authors': 'Maxence Boels, Harry Robertshaw, Alejandro Granados, Prokar Dasgupta, Sebastien Ourselin'}, {'published': '2025-07-07', 'title': 'Multi-modal Representations for Fine-grained Multi-label Critical View of Safety Recognition', 'authors': 'Britty Baby, Vinkle Srivastav, Pooja P. Jain, Kun Yuan, Pietro Mascagni, Nicolas Padoy', 'url': 'http://arxiv.org/abs/2507.05007v1'}, {'url': 'http://arxiv.org/abs/2507.04994v1', 'published': '2025-07-07', 'authors': 'Adam Gould, Gabriel de Olim Gaul, Francesca Toni', 'title': 'Supported Abstract Argumentation for Case-Based Reasoning'}, {'authors': 'Ruihao Zhang, Fei Ye, Dandan Meng, Yixuan Huang, Maochen, Xiao Liu', 'title': 'Classification of autoimmune diseases from Peripheral blood TCR repertoires by multimodal multi-instance learning', 'url': 'http://arxiv.org/abs/2507.04981v1', 'published': '2025-07-07'}, {'url': 'http://arxiv.org/abs/2507.04966v1', 'title': 'LAPS-Diff: A Diffusion-Based Framework for Singing Voice Synthesis With Language Aware Prosody-Style Guided Learning', 'authors': 'Sandipan Dhar, Mayank Gupta, Preeti Rao', 'published': '2025-07-07'}, {'title': 'Hear-Your-Click: Interactive Video-to-Audio Generation via Object-aware Contrastive Audio-Visual Fine-tuning', 'url': 'http://arxiv.org/abs/2507.04959v1', 'authors': 'Yingshan Liang, Keyu Fan, Zhicheng Du, Yiran Wang, Qingyang Shi, Xinyu Zhang, Jiasheng Lu, Peiwu Qin', 'published': '2025-07-07'}, {'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04955v1', 'title': 'EXPOTION: Facial Expression and Motion Control for Multimodal Music Generation', 'authors': 'Fathinah Izzati, Xinyue Li, Gus Xia'}, {'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04947v1', 'title': 'DC-AR: Efficient Masked Autoregressive Image Generation with Deep Compression Hybrid Tokenizer', 'authors': 'Yecheng Wu, Junyu Chen, Zhuoyang Zhang, Enze Xie, Jincheng Yu, Junsong Chen, Jinyi Hu, Yao Lu, Song Han, Han Cai'}, {'published': '2025-07-07', 'title': 'Object-centric Denoising Diffusion Models for Physical Reasoning', 'authors': 'Moritz Lange, Raphael C. Engelhardt, Wolfgang Konen, Andrew Melnik, Laurenz Wiskott', 'url': 'http://arxiv.org/abs/2507.04920v1'}, {'authors': 'Thayanne França da Silva, José Everardo Bessa Maia', 'title': 'Leadership Detection via Time-Lagged Correlation-Based Network Inference', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04917v1'}, {'url': 'http://arxiv.org/abs/2507.04909v1', 'title': 'HV-MMBench: Benchmarking MLLMs for Human-Centric Video Understanding', 'published': '2025-07-07', 'authors': 'Yuxuan Cai, Jiangning Zhang, Zhenye Gan, Qingdong He, Xiaobin Hu, Junwei Zhu, Yabiao Wang, Chengjie Wang, Zhucun Xue, Xinwei He, Xiang Bai'}, {'url': 'http://arxiv.org/abs/2507.04903v1', 'authors': 'Thinh Dao, Dung Thuy Nguyen, Khoa D Doan, Kok-Seng Wong', 'published': '2025-07-07', 'title': 'BackFed: An Efficient & Standardized Benchmark Suite for Backdoor Attacks in Federated Learning'}, {'url': 'http://arxiv.org/abs/2507.04893v1', 'title': 'MARBLE: A Multi-Agent Rule-Based LLM Reasoning Engine for Accident Severity Prediction', 'published': '2025-07-07', 'authors': 'Kaleem Ullah Qasim, Jiashu Zhang'}, {'url': 'http://arxiv.org/abs/2507.04886v1', 'authors': 'A. Bochkov', 'published': '2025-07-07', 'title': 'Emergent Semantics Beyond Token Embeddings: Transformer LMs with Frozen Visual Unicode Representations'}, {'url': 'http://arxiv.org/abs/2507.04883v1', 'title': 'Beyond Training-time Poisoning: Component-level and Post-training Backdoors in Deep Reinforcement Learning', 'authors': 'Sanyam Vyas, Alberto Caron, Chris Hicks, Pete Burnap, Vasilios Mavroudis', 'published': '2025-07-07'}, {'authors': 'Xiaofang Liu, Lingling Sun, Xuqing Zhang, Yuannong Ye, Bin zhao', 'title': 'HGNet: High-Order Spatial Awareness Hypergraph and Multi-Scale Context Attention Network for Colorectal Polyp Detection', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04880v1'}, {'url': 'http://arxiv.org/abs/2507.04877v1', 'authors': 'Zewen Sun, Ruoxiang Huang, Jiahe Feng, Rundong Kong, Yuqian Wang, Hengyu Liu, Ziqi Gong, Yuyuan Qin, Yingxue Wang, Yu Wang', 'published': '2025-07-07', 'title': 'DoPI: Doctor-like Proactive Interrogation LLM for Traditional Chinese Medicine'}, {'authors': 'A. Velichko, M. Belyaev, P. Boriskov', 'url': 'http://arxiv.org/abs/2507.04868v1', 'title': 'A Novel Approach for Estimating Positive Lyapunov Exponents in One-Dimensional Chaotic Time Series Using Machine Learning', 'published': '2025-07-07'}, {'title': 'Towards Human-in-the-Loop Onset Detection: A Transfer Learning Approach for Maracatu', 'url': 'http://arxiv.org/abs/2507.04858v1', 'authors': 'António Sá Pinto', 'published': '2025-07-07'}, {'authors': 'Mathilde Abrassart, Nicolas Obin, Axel Roebel', 'title': 'Fast-VGAN: Lightweight Voice Conversion with Explicit Control of F0 and Duration Parameters', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04817v1'}, {'authors': 'Mihai Masala, Marius Leordeanu', 'title': 'From Vision To Language through Graph of Events in Space and Time: An Explainable Self-supervised Approach', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04815v1'}, {'authors': 'George Jagadeesh, Srikrishna Iyer, Michal Polanowski, Kai Xin Thia', 'url': 'http://arxiv.org/abs/2507.04803v1', 'title': 'Application and Evaluation of Large Language Models for Forecasting the Impact of Traffic Incidents', 'published': '2025-07-07'}, {'published': '2025-07-07', 'title': 'A Survey of Pun Generation: Datasets, Evaluations and Methodologies', 'authors': 'Yuchen Su, Yonghua Zhu, Ruofan Wang, Zijian Huang, Diana Benavides-Prado, Michael Witbrock', 'url': 'http://arxiv.org/abs/2507.04793v1'}, {'title': 'Model Compression using Progressive Channel Pruning', 'url': 'http://arxiv.org/abs/2507.04792v1', 'published': '2025-07-07', 'authors': 'Jinyang Guo, Weichen Zhang, Wanli Ouyang, Dong Xu'}, {'title': 'Interaction-Merged Motion Planning: Effectively Leveraging Diverse Motion Datasets for Robust Planning', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04790v1', 'authors': 'Giwon Lee, Wooseong Jeong, Daehee Park, Jaewoo Jeong, Kuk-Jin Yoon'}, {'authors': 'Zexi Jia, Chuanwei Huang, Yeshuang Zhu, Hongyan Fei, Ying Deng, Zhiqiang Yuan, Jiapei Zhang, Jinchao Zhang, Jie Zhou', 'url': 'http://arxiv.org/abs/2507.04769v1', 'title': 'From Imitation to Innovation: The Emergence of AI Unique Artistic Styles and the Challenge of Copyright Protection', 'published': '2025-07-07'}, {'url': 'http://arxiv.org/abs/2507.04770v1', 'title': 'FurniMAS: Language-Guided Furniture Decoration using Multi-Agent System', 'authors': 'Toan Nguyen, Tri Le, Quang Nguyen, Anh Nguyen', 'published': '2025-07-07'}, {'title': 'CoSteer: Collaborative Decoding-Time Personalization via Local Delta Steering', 'url': 'http://arxiv.org/abs/2507.04756v1', 'authors': 'Hang Lv, Sheng Liang, Hao Wang, Hongchao Gu, Yaxiong Wu, Wei Guo, Defu Lian, Yong Liu, Enhong Chen', 'published': '2025-07-07'}, {'url': 'http://arxiv.org/abs/2507.04752v1', 'published': '2025-07-07', 'authors': 'Shuo Yang, Xinran Zheng, Xinchen Zhang, Jinfeng Xu, Jinze Li, Donglin Xie, Weicai Long, Edith C. H. Ngai', 'title': 'Large Language Models for Network Intrusion Detection Systems: Foundations, Implementations, and Future Directions'}, {'title': 'MCFormer: A Multi-Cost-Volume Network and Comprehensive Benchmark for Particle Image Velocimetry', 'url': 'http://arxiv.org/abs/2507.04750v1', 'authors': 'Zicheng Lin, Xiaoqiang Li, Yichao Wang, Chuan Zhu', 'published': '2025-07-07'}, {'authors': 'Sungmin Lee, Minju Kang, Joonhee Lee, Seungyong Lee, Dongju Kim, Jingi Hong, Jun Shin, Pei Zhang, JeongGil Ko', 'url': 'http://arxiv.org/abs/2507.04748v1', 'published': '2025-07-07', 'title': 'LLM-based Question-Answer Framework for Sensor-driven HVAC System Interaction'}, {'url': 'http://arxiv.org/abs/2507.04742v1', 'authors': 'Seyedarmin Azizi, Erfan Baghaei Potraghloo, Massoud Pedram', 'title': 'Activation Steering for Chain-of-Thought Compression', 'published': '2025-07-07'}, {'url': 'http://arxiv.org/abs/2507.04738v1', 'authors': 'Martijn Bentum, Louis ten Bosch, Tomas O. Lentz', 'title': 'Word stress in self-supervised speech models: A cross-linguistic comparison', 'published': '2025-07-07'}, {'title': 'ChipSeek-R1: Generating Human-Surpassing RTL with LLM via Hierarchical Reward-Driven Reinforcement Learning', 'authors': 'Zhirong Chen, Kaiyan Chang, Zhuolin Li, Xinyang He, Chujie Chen, Cangyuan Li, Mengdi Wang, Haobo Xu, Yinhe Han, Ying Wang', 'url': 'http://arxiv.org/abs/2507.04736v1', 'published': '2025-07-07'}, {'url': 'http://arxiv.org/abs/2507.04726v1', 'title': 'Losing Control: Data Poisoning Attack on Guided Diffusion via ControlNet', 'published': '2025-07-07', 'authors': 'Raz Lapid, Almog Dubin'}, {'published': '2025-07-07', 'title': \"Who's the Mole? Modeling and Detecting Intention-Hiding Malicious Agents in LLM-Based Multi-Agent Systems\", 'url': 'http://arxiv.org/abs/2507.04724v1', 'authors': 'Yizhe Xie, Congcong Zhu, Xinyue Zhang, Minghao Wang, Chi Liu, Minglu Zhu, Tianqing Zhu'}, {'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04722v1', 'authors': 'Jinzhi Wang, Bin Li, Qingke Peng, Haozhou Li, Zeyuan Zeng, Ruimeng Li, Biyi Zhou', 'title': 'LumiCRS: Asymmetric Contrastive Prototype Learning for Long-Tail Conversational Movie Recommendation'}, {'url': 'http://arxiv.org/abs/2507.04719v1', 'title': 'Advocate for Complete Benchmarks for Formal Reasoning with Formal/Informal Statements and Formal/Informal Proofs', 'authors': 'Roozbeh Yousefzadeh, Xuenan Cao', 'published': '2025-07-07'}, {'published': '2025-07-07', 'title': 'Geometric-Guided Few-Shot Dental Landmark Detection with Human-Centric Foundation Model', 'authors': 'Anbang Wang, Marawan Elbatel, Keyuan Liu, Lizhuo Lin, Meng Lan, Yanqi Yang, Xiaomeng Li', 'url': 'http://arxiv.org/abs/2507.04710v1'}, {'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04706v1', 'title': 'UrbanMind: Towards Urban General Intelligence via Tool-Enhanced Retrieval-Augmented Generation and Multilevel Optimization', 'authors': 'Kai Yang, Zelin Zhu, Chengtao Jian, Hui Ma, Shengjie Zhao, Xiaozhou Ye, Ye Ouyang'}, {'authors': 'Zhenglun Kong, Mufan Qiu, John Boesen, Xiang Lin, Sukwon Yun, Tianlong Chen, Manolis Kellis, Marinka Zitnik', 'url': 'http://arxiv.org/abs/2507.04704v1', 'title': 'SPATIA: Multimodal Model for Prediction and Generation of Spatial Cell Phenotypes', 'published': '2025-07-07'}, {'title': 'Tempo-R0: A Video-MLLM for Temporal Video Grounding through Efficient Temporal Sensing Reinforcement Learning', 'published': '2025-07-07', 'authors': 'Feng Yue, Zhaoxing Zhang, Junming Jiao, Zhengyu Liang, Shiwen Cao, Feifei Zhang, Rong Shen', 'url': 'http://arxiv.org/abs/2507.04702v1'}, {'published': '2025-07-07', 'authors': 'Hanseon Joo, Hayoung Choi, Ook Lee, Minjong Cheon', 'title': 'Bridging KAN and MLP: MJKAN, a Hybrid Architecture with Both Efficiency and Expressiveness', 'url': 'http://arxiv.org/abs/2507.04690v1'}, {'authors': 'Wenhao Li, Xiu Su, Jingyi Wu, Feng Yang, Yang Liu, Yi Chen, Shan You, Chang Xu', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04680v1', 'title': 'Identify, Isolate, and Purge: Mitigating Hallucinations in LVLMs via Self-Evolving Distillation'}, {'authors': 'Wei Duan, Li Qian', 'published': '2025-07-07', 'title': 'Trojan Horse Prompting: Jailbreaking Conversational Multimodal Models by Forging Assistant Message', 'url': 'http://arxiv.org/abs/2507.04673v1'}, {'url': 'http://arxiv.org/abs/2507.04667v1', 'published': '2025-07-07', 'authors': 'Hahyeon Choi, Junhoo Lee, Nojun Kwak', 'title': \"What's Making That Sound Right Now? Video-centric Audio-Visual Localization\"}, {'published': '2025-07-07', 'title': 'LTMSformer: A Local Trend-Aware Attention and Motion State Encoding Transformer for Multi-Agent Trajectory Prediction', 'authors': 'Yixin Yan, Yang Li, Yuanfan Wang, Xiaozhou Zhou, Beihao Xia, Manjiang Hu, Hongmao Qin', 'url': 'http://arxiv.org/abs/2507.04634v1'}, {'title': 'Can Prompt Difficulty be Online Predicted for Accelerating RL Finetuning of Reasoning Models?', 'authors': 'Yun Qu, Qi Cheems Wang, Yixiu Mao, Vincent Tao Hu, Xiangyang Ji', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04632v1'}, {'url': 'http://arxiv.org/abs/2507.04631v1', 'authors': 'Yun Wang, Longguang Wang, Chenghao Zhang, Yongjian Zhang, Zhanjie Zhang, Ao Ma, Chenyou Fan, Tin Lun Lam, Junjie Hu', 'title': 'Learning Robust Stereo Matching in the Wild with Selective Mixture-of-Experts', 'published': '2025-07-07'}, {'url': 'http://arxiv.org/abs/2507.04625v1', 'authors': 'Swayamjit Saha', 'title': 'Knowledge-Aware Self-Correction in Language Models via Structured Memory Graphs', 'published': '2025-07-07'}, {'published': '2025-07-07', 'authors': 'Jinpeng Chen, Jianxiang He, Huan Li, Senzhang Wang, Yuan Cao, Kaimin Wei, Zhenye Yang, Ye Ji', 'title': 'Hierarchical Intent-guided Optimization with Pluggable LLM-Driven Semantics for Session-based Recommendation', 'url': 'http://arxiv.org/abs/2507.04623v1'}, {'authors': 'Yusong Zhang, Yuxuan Sun, Lei Guo, Wei Chen, Bo Ai, Deniz Gunduz', 'title': 'Multimodal LLM Integrated Semantic Communications for 6G Immersive Experiences', 'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04621v1'}, {'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04619v1', 'title': 'Information-Guided Diffusion Sampling for Dataset Distillation', 'authors': 'Linfeng Ye, Shayan Mohajer Hamidi, Guang Li, Takahiro Ogawa, Miki Haseyama, Konstantinos N. Plataniotis'}, {'title': 'HiLa: Hierarchical Vision-Language Collaboration for Cancer Survival Prediction', 'authors': 'Jiaqi Cui, Lu Wen, Yuchen Fei, Bo Liu, Luping Zhou, Dinggang Shen, Yan Wang', 'url': 'http://arxiv.org/abs/2507.04613v1', 'published': '2025-07-07'}, {'url': 'http://arxiv.org/abs/2507.04610v1', 'title': 'any4: Learned 4-bit Numeric Representation for LLMs', 'published': '2025-07-07', 'authors': 'Mostafa Elhoushi, Jeff Johnson'}, {'authors': 'Xinliang Frederick Zhang, Nick Beauchamp, Lu Wang', 'url': 'http://arxiv.org/abs/2507.04607v1', 'title': 'PRIME: Large Language Model Personalization with Cognitive Memory and Thought Processes', 'published': '2025-07-07'}, {'published': '2025-07-07', 'url': 'http://arxiv.org/abs/2507.04606v1', 'title': 'Accelerated Online Reinforcement Learning using Auxiliary Start State Distributions', 'authors': 'Aman Mehra, Alexandre Capone, Jeff Schneider'}, {'published': '2025-07-07', 'title': 'DisMS-TS: Eliminating Redundant Multi-Scale Features for Time Series Classification', 'url': 'http://arxiv.org/abs/2507.04600v1', 'authors': 'Zhipeng Liu, Peibo Duan, Binwu Wang, Xuan Tang, Qi Chu, Changsheng Zhang, Yongsheng Huang, Bin Zhang'}, {'published': '2025-07-07', 'authors': 'Niloofar Shadab, Tyler Cody, Alejandro Salado, Taylan G. Topcu, Mohammad Shadab, Peter Beling', 'title': 'Exploring Core and Periphery Precepts in Biological and Artificial Intelligence: An Outcome-Based Perspective', 'url': 'http://arxiv.org/abs/2507.04594v1'}, {'url': 'http://arxiv.org/abs/2507.04575v1', 'authors': 'Mohid Farooqi, Alejandro Comas-Leon', 'title': 'Lilith: Developmental Modular LLMs with Chemical Signaling', 'published': '2025-07-06'}, {'title': 'Nile-Chat: Egyptian Language Models for Arabic and Latin Scripts', 'authors': 'Guokan Shang, Hadi Abdine, Ahmad Chamma, Amr Mohamed, Mohamed Anwar, Abdelaziz Bounhar, Omar El Herraoui, Preslav Nakov, Michalis Vazirgiannis, Eric Xing', 'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04569v1'}, {'url': 'http://arxiv.org/abs/2507.04562v1', 'authors': 'Janna Lu', 'title': 'Evaluating LLMs on Real-World Forecasting Against Human Superforecasters', 'published': '2025-07-06'}, {'published': '2025-07-06', 'title': 'SPIRA: Building an Intelligent System for Respiratory Insufficiency Detection', 'url': 'http://arxiv.org/abs/2507.04548v1', 'authors': 'Renato Cordeiro Ferreira, Dayanne Gomes, Vitor Tamae, Francisco Wernke, Alfredo Goldman'}, {'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04531v1', 'authors': 'Rushil Thareja, Preslav Nakov, Praneeth Vepakomma, Nils Lukas', 'title': 'DP-Fusion: Token-Level Differentially Private Inference for Large Language Models'}, {'authors': 'Sonal Allana, Rozita Dara, Xiaodong Lin, Pulei Xiong', 'url': 'http://arxiv.org/abs/2507.04528v1', 'published': '2025-07-06', 'title': 'Towards integration of Privacy Enhancing Technologies in Explainable Artificial Intelligence'}, {'authors': \"Anna Deichler, Jim O'Regan, Teo Guichoux, David Johansson, Jonas Beskow\", 'title': 'Grounded Gesture Generation: Language, Motion, and Space', 'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04522v1'}, {'url': 'http://arxiv.org/abs/2507.04513v1', 'published': '2025-07-06', 'authors': 'Gur Keinan, Omer Ben-Porat', 'title': 'Churn-Aware Recommendation Planning under Aggregated Preference Feedback'}, {'title': 'MVL-Loc: Leveraging Vision-Language Model for Generalizable Multi-Scene Camera Relocalization', 'authors': 'Zhendong Xiao, Wu Wei, Shujie Ji, Shan Yang, Changhao Chen', 'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04509v1'}, {'authors': 'Niels Leadholm, Viviane Clay, Scott Knudstrup, Hojae Lee, Jeff Hawkins', 'published': '2025-07-06', 'title': 'Thousand-Brains Systems: Sensorimotor Intelligence for Rapid, Robust Learning and Inference', 'url': 'http://arxiv.org/abs/2507.04494v1'}, {'published': '2025-07-06', 'title': 'A validity-guided workflow for robust large language model research in psychology', 'url': 'http://arxiv.org/abs/2507.04491v1', 'authors': 'Zhicheng Lin'}, {'authors': 'Luca Bindini, Lorenzo Perini, Stefano Nistri, Jesse Davis, Paolo Frasconi', 'published': '2025-07-06', 'title': 'Dealing with Uncertainty in Contextual Anomaly Detection', 'url': 'http://arxiv.org/abs/2507.04490v1'}, {'url': 'http://arxiv.org/abs/2507.04487v1', 'published': '2025-07-06', 'authors': 'Xujia Wang. Yunjia Qi, Bin Xu', 'title': 'LoSiA: Efficient High-Rank Fine-Tuning via Subnet Localization and Optimization'}, {'url': 'http://arxiv.org/abs/2507.04480v1', 'authors': 'Ikhtiyor Nematov, Tarik Kalai, Elizaveta Kuzmenko, Gabriele Fugagnoli, Dimitris Sacharidis, Katja Hose, Tomer Sagi', 'published': '2025-07-06', 'title': 'Source Attribution in Retrieval-Augmented Generation'}, {'title': 'Model Inversion Attacks on Llama 3: Extracting PII from Large Language Models', 'published': '2025-07-06', 'authors': 'Sathesh P. Sivashanmugam', 'url': 'http://arxiv.org/abs/2507.04478v1'}, {'title': 'The role of large language models in UI/UX design: A systematic literature review', 'url': 'http://arxiv.org/abs/2507.04469v1', 'authors': 'Ammar Ahmed, Ali Shariq Imran', 'published': '2025-07-06'}, {'title': 'Anomalous Decision Discovery using Inverse Reinforcement Learning', 'url': 'http://arxiv.org/abs/2507.04464v1', 'published': '2025-07-06', 'authors': 'Ashish Bastola, Mert D. Pesé, Long Cheng, Jonathon Smereka, Abolfazl Razi'}, {'authors': 'Michele Caprio', 'title': 'The Joys of Categorical Conformal Prediction', 'url': 'http://arxiv.org/abs/2507.04441v1', 'published': '2025-07-06'}, {'title': 'A Linguistic Analysis of Spontaneous Thoughts: Investigating Experiences of Déjà Vu, Unexpected Thoughts, and Involuntary Autobiographical Memories', 'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04439v1', 'authors': 'Videep Venkatesha, Mary Cati Poulos, Christopher Steadman, Caitlin Mills, Anne M. Cleary, Nathaniel Blanchard'}, {'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04431v1', 'authors': 'Debodeep Banerjee, Burcu Sayin, Stefano Teso, Andrea Passerini', 'title': 'MedGellan: LLM-Generated Medical Guidance to Support Physicians'}, {'published': '2025-07-06', 'authors': 'Feiyue Wu, Tianxing Wu, Shenqi Jing', 'title': 'ARMR: Adaptively Responsive Network for Medication Recommendation', 'url': 'http://arxiv.org/abs/2507.04428v1'}, {'title': 'Learning Software Bug Reports: A Systematic Literature Review', 'authors': 'Guoming Long, Jingzhi Gong, Hui Fang, Tao Chen', 'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04422v1'}, {'authors': 'Huy Hoan Le, Van Sy Thinh Nguyen, Thi Le Chi Dang, Vo Thanh Khang Nguyen, Truong Thanh Hung Nguyen, Hung Cao', 'url': 'http://arxiv.org/abs/2507.04410v1', 'title': 'Multimedia Verification Through Multi-Agent Deep Research Multimodal Large Language Models', 'published': '2025-07-06'}, {'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04404v1', 'authors': 'Jingze Zhu, Yongliang Wu, Wenbo Zhu, Jiawang Cao, Yanqiang Zheng, Jiawei Chen, Xu Yang, Bernt Schiele, Jonas Fischer, Xinting Hu', 'title': 'LayerCake: Token-Aware Contrastive Decoding within Large Language Model Layers'}, {'url': 'http://arxiv.org/abs/2507.04395v1', 'title': 'SpiritRAG: A Q&A System for Religion and Spirituality in the United Nations Archive', 'authors': 'Yingqiang Gao, Fabian Winiger, Patrick Montjourides, Anastassia Shaitarova, Nianlong Gu, Simon Peng-Keller, Gerold Schneider', 'published': '2025-07-06'}, {'title': 'Tractable Representation Learning with Probabilistic Circuits', 'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04385v1', 'authors': 'Steven Braun, Sahil Sidheekh, Antonio Vergari, Martin Mundt, Sriraam Natarajan, Kristian Kersting'}, {'authors': 'Bing Fan, Shusen Ma, Yun-Bo Zhao, Yu Kang', 'published': '2025-07-06', 'title': 'DC-Mamber: A Dual Channel Prediction Model based on Mamba and Linear Transformer for Multivariate Time Series Forecasting', 'url': 'http://arxiv.org/abs/2507.04381v1'}, {'authors': 'Yuya Yoshikawa, Ryotaro Shimizu, Takahiro Kawashima, Yuki Saito', 'title': 'Transferring Visual Explainability of Self-Explaining Models through Task Arithmetic', 'url': 'http://arxiv.org/abs/2507.04380v1', 'published': '2025-07-06'}, {'authors': 'Georgios Ioannides, Christos Constantinou, Vinija Jain, Aman Chadha, Aaron Elkins', 'url': 'http://arxiv.org/abs/2507.04376v1', 'published': '2025-07-06', 'title': 'MOD-X: A Modular Open Decentralized eXchange Framework proposal for Heterogeneous Interoperable Artificial Agents'}, {'url': 'http://arxiv.org/abs/2507.04370v1', 'title': 'WebSynthesis: World-Model-Guided MCTS for Efficient WebUI-Trajectory Synthesis', 'authors': 'Yifei Gao, Junhong Ye, Jiaqi Wang, Jitao Sang', 'published': '2025-07-06'}, {'authors': 'Xiaomeng Hu, Pin-Yu Chen, Tsung-Yi Ho', 'url': 'http://arxiv.org/abs/2507.04365v1', 'published': '2025-07-06', 'title': 'Attention Slipping: A Mechanistic Understanding of Jailbreak Attacks and Defenses in LLMs'}, {'url': 'http://arxiv.org/abs/2507.04356v1', 'title': 'Mission-Aligned Learning-Informed Control of Autonomous Systems: Formulation and Foundations', 'authors': 'Vyacheslav Kungurtsev, Gustav Sir, Akhil Anand, Sebastien Gros, Haozhe Tian, Homayoun Hamedmoghadam', 'published': '2025-07-06'}, {'authors': 'Greg Nyilasy, Harsha Gangadharbatla', 'url': 'http://arxiv.org/abs/2507.04352v1', 'title': 'AI-washing: The Asymmetric Effects of Its Two Types on Consumer Moral Judgments', 'published': '2025-07-06'}, {'title': 'MLLM-Fabric: Multimodal Large Language Model-Driven Robotic Framework for Fabric Sorting and Selection', 'published': '2025-07-06', 'authors': 'Liman Wang, Hanyang Zhong, Tianyuan Wang, Shan Luo, Jihong Zhu', 'url': 'http://arxiv.org/abs/2507.04351v1'}, {'url': 'http://arxiv.org/abs/2507.04348v1', 'title': 'SmartThinker: Learning to Compress and Preserve Reasoning by Step-Level Length Control', 'authors': 'Xingyang He, Xiao Ling, Jie Liu', 'published': '2025-07-06'}, {'authors': 'Yifei Li, Erik-jan van Kampen', 'url': 'http://arxiv.org/abs/2507.04346v1', 'published': '2025-07-06', 'title': 'Improving Action Smoothness for a Cascaded Online Learning Flight Control System'}, {'published': '2025-07-06', 'title': 'Efficient Perplexity Bound and Ratio Matching in Discrete Diffusion Language Models', 'authors': 'Etrit Haxholli, Yeti Z. Gürbüz, Oğul Can, Eli Waxman', 'url': 'http://arxiv.org/abs/2507.04341v1'}, {'published': '2025-07-06', 'title': 'Voltage Mode Winner-Take-All Circuit for Neuromorphic Systems', 'authors': 'Abdullah M. Zyarah, Dhireesha Kudithipudi', 'url': 'http://arxiv.org/abs/2507.04338v1'}, {'url': 'http://arxiv.org/abs/2507.04317v1', 'authors': 'Fatmaelzahraa Ali Ahmed, Muhammad Arsalan, Abdulaziz Al-Ali, Khalid Al-Jalham, Shidin Balakrishnan', 'published': '2025-07-06', 'title': 'CLIP-RL: Surgical Scene Segmentation Using Contrastive Language-Vision Pretraining & Reinforcement Learning'}, {'url': 'http://arxiv.org/abs/2507.04304v1', 'published': '2025-07-06', 'authors': 'Fatimaelzahraa Ahmed, Muraam Abdel-Ghani, Muhammad Arsalan, Mahmoud Ali, Abdulaziz Al-Ali, Shidin Balakrishnan', 'title': 'Surg-SegFormer: A Dual Transformer-Based Model for Holistic Surgical Scene Segmentation'}, {'authors': 'Feng Qi', 'published': '2025-07-06', 'title': 'QF: Quick Feedforward AI Model Training without Gradient Back Propagation', 'url': 'http://arxiv.org/abs/2507.04300v1'}, {'url': 'http://arxiv.org/abs/2507.04299v1', 'authors': 'Joohyung Lee, Yunsong Meng', 'published': '2025-07-06', 'title': 'Answer Set Programming Modulo Theories and Reasoning about Continuous Changes'}, {'authors': 'Runcong Zhao, Artem Borov, Jiazheng Li, Yulan He', 'url': 'http://arxiv.org/abs/2507.04295v1', 'published': '2025-07-06', 'title': 'LearnLens: LLM-Enabled Personalised, Curriculum-Grounded Feedback with Educators in the Loop'}, {'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04289v1', 'authors': 'Shenxi Liu, Kan Li, Mingyang Zhao, Yuhang Tian, Bin Li, Shoujun Zhou, Hongliang Li, Fuxia Yang', 'title': 'M$^3$-Med: A Benchmark for Multi-lingual, Multi-modal, and Multi-hop Reasoning in Medical Instructional Video Understanding'}, {'url': 'http://arxiv.org/abs/2507.04285v1', 'title': 'SeqTex: Generate Mesh Textures in Video Sequence', 'published': '2025-07-06', 'authors': 'Ze Yuan, Xin Yu, Yangtian Sun, Yuan-Chen Guo, Yan-Pei Cao, Ding Liang, Xiaojuan Qi'}, {'authors': 'Roy Uziel, Irit Chelly, Oren Freifeld, Ari Pakman', 'title': 'Clustering via Self-Supervised Diffusion', 'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04283v1'}, {'published': '2025-07-06', 'title': 'VOLTRON: Detecting Unknown Malware Using Graph-Based Zero-Shot Learning', 'authors': 'M. Tahir Akdeniz, Zeynep Yeşilkaya, İ. Enes Köse, İ. Ulaş Ünal, Sevil Şen', 'url': 'http://arxiv.org/abs/2507.04275v1'}, {'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04270v1', 'title': 'ZERO: Multi-modal Prompt-based Visual Grounding', 'authors': 'Sangbum Choi, Kyeongryeol Go'}, {'url': 'http://arxiv.org/abs/2507.04252v1', 'title': 'Deep-Learning-Assisted Highly-Accurate COVID-19 Diagnosis on Lung Computed Tomography Images', 'published': '2025-07-06', 'authors': 'Yinuo Wang, Juhyun Bae, Ka Ho Chow, Shenyang Chen, Shreyash Gupta'}, {'authors': 'Mahavir Dabas, Si Chen, Charles Fleming, Ming Jin, Ruoxi Jia', 'published': '2025-07-06', 'title': 'Just Enough Shifts: Mitigating Over-Refusal in Aligned Language Models with Targeted Representation Fine-Tuning', 'url': 'http://arxiv.org/abs/2507.04250v1'}, {'url': 'http://arxiv.org/abs/2507.04243v1', 'title': 'Domain Generalizable Portrait Style Transfer', 'published': '2025-07-06', 'authors': 'Xinbo Wang, Wenju Xu, Qing Zhang, Wei-Shi Zheng'}, {'url': 'http://arxiv.org/abs/2507.04239v1', 'title': 'Scaling Context Requires Rethinking Attention', 'authors': 'Carles Gelada, Jacob Buckman, Sean Zhang, Txus Bach', 'published': '2025-07-06'}, {'published': '2025-07-06', 'authors': 'Kento Kawaharazuka, Shintaro Inoue, Yuta Sahara, Keita Yoneda, Temma Suzuki, Kei Okada', 'title': 'Design Optimization of Three-Dimensional Wire Arrangement Considering Wire Crossings for Tendon-driven Robots', 'url': 'http://arxiv.org/abs/2507.04235v1'}, {'url': 'http://arxiv.org/abs/2507.04230v1', 'title': 'High-Resolution Sustain Pedal Depth Estimation from Piano Audio Across Room Acoustics', 'published': '2025-07-06', 'authors': 'Kun Fang, Hanwen Zhang, Ziyu Wang, Ichiro Fujinaga'}, {'title': 'Hijacking JARVIS: Benchmarking Mobile GUI Agents against Unprivileged Third Parties', 'authors': 'Guohong Liu, Jialei Ye, Jiacheng Liu, Yuanchun Li, Wei Liu, Pengzhi Gao, Jian Luan, Yunxin Liu', 'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04227v1'}, {'authors': 'Dapeng Jiang, Xiangzhe Kong, Jiaqi Han, Mingyu Li, Rui Jiao, Wenbing Huang, Stefano Ermon, Jianzhu Ma, Yang Liu', 'url': 'http://arxiv.org/abs/2507.04225v1', 'title': 'Zero-Shot Cyclic Peptide Design with Composable Geometric Conditions', 'published': '2025-07-06'}, {'authors': 'Haining Wang, Jason Clark, Yueru Yan, Star Bradley, Ruiyang Chen, Yiqiong Zhang, Hengyi Fu, Zuoyu Tian', 'url': 'http://arxiv.org/abs/2507.04224v1', 'published': '2025-07-06', 'title': 'Fairness Evaluation of Large Language Models in Academic Library Reference Services'}, {'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04221v1', 'authors': 'Jack Lu, Ryan Teehan, Zhenbang Yang, Mengye Ren', 'title': 'Context Tuning for In-Context Optimization'}, {'authors': 'Yan Scholten, Sophie Xhonneux, Stephan Günnemann, Leo Schwinn', 'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04219v1', 'title': 'Model Collapse Is Not a Bug but a Feature in Machine Unlearning for LLMs'}, {'published': '2025-07-06', 'url': 'http://arxiv.org/abs/2507.04206v1', 'title': 'Mpemba Effect in Large-Language Model Training Dynamics: A Minimal Analysis of the Valley-River model', 'authors': 'Sibei Liu, Zhijian Hu'}, {'authors': 'Yuyang Deng, Samory Kpotufe', 'title': 'Mixed-Sample SGD: an End-to-end Analysis of Supervised Transfer Learning', 'url': 'http://arxiv.org/abs/2507.04194v1', 'published': '2025-07-06'}, {'published': '2025-07-05', 'authors': 'Runcong Zhao, Qinglin Zhu, Hainiu Xu, Bin Liang, Yulan He, Lin Gui', 'url': 'http://arxiv.org/abs/2507.04189v1', 'title': 'SymbolicThought: Integrating Language Models and Symbolic Reasoning for Consistent and Interpretable Human Relationship Understanding'}, {'authors': 'Runar Helin, Ole-Christoffer Granmo, Mayur Kishor Shende, Lei Jiao, Vladimir I. Zadorozhny, Kunal Ganesh Dumbre, Rishad Shafik, Alex Yakovlev', 'title': 'Uncertainty Quantification in the Tsetlin Machine', 'url': 'http://arxiv.org/abs/2507.04175v1', 'published': '2025-07-05'}, {'title': 'Structure As Search: Unsupervised Permutation Learning for Combinatorial Optimization', 'authors': 'Yimeng Min, Carla P. Gomes', 'url': 'http://arxiv.org/abs/2507.04164v1', 'published': '2025-07-05'}, {'url': 'http://arxiv.org/abs/2507.04153v1', 'title': 'Physics-informed neural networks and neural operators for a study of EUV electromagnetic wave diffraction from a lithography mask', 'authors': \"Vasiliy A. Es'kin, Egor V. Ivanov\", 'published': '2025-07-05'}, {'published': '2025-07-05', 'title': 'Dissecting Clinical Reasoning in Language Models: A Comparative Study of Prompts and Model Adaptation Strategies', 'url': 'http://arxiv.org/abs/2507.04142v1', 'authors': 'Mael Jullien, Marco Valentino, Leonardo Ranaldi, Andre Freitas'}, {'authors': 'Mohsen Azarmi, Mahdi Rezaei, He Wang', 'url': 'http://arxiv.org/abs/2507.04141v1', 'published': '2025-07-05', 'title': 'Pedestrian Intention Prediction via Vision-Language Foundation Models'}, {'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.04139v1', 'title': 'Driver-Net: Multi-Camera Fusion for Assessing Driver Take-Over Readiness in Automated Vehicles', 'authors': 'Mahdi Rezaei, Mohsen Azarmi'}, {'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.04136v1', 'title': 'A Technical Survey of Reinforcement Learning Techniques for Large Language Models', 'authors': 'Saksham Sahai Srivastava, Vaneet Aggarwal'}, {'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.04123v1', 'authors': 'Linshen Liu, Boyan Su, Junyue Jiang, Guanlin Wu, Cong Guo, Ceyu Xu, Hao Frank Yang', 'title': 'Towards Accurate and Efficient 3D Object Detection for Autonomous Driving: A Mixture of Experts Computing System on Edge'}, {'title': 'When Data-Free Knowledge Distillation Meets Non-Transferable Teacher: Escaping Out-of-Distribution Trap is All You Need', 'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.04119v1', 'authors': 'Ziming Hong, Runnan Chen, Zengmao Wang, Bo Han, Bo Du, Tongliang Liu'}, {'title': 'Addressing The Devastating Effects Of Single-Task Data Poisoning In Exemplar-Free Continual Learning', 'authors': 'Stanisław Pawlak, Bartłomiej Twardowski, Tomasz Trzciński, Joost van de Weijer', 'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.04106v1'}, {'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.04105v1', 'authors': 'Jinwei Hu, Yi Dong, Zhengtao Ding, Xiaowei Huang', 'title': 'Enhancing Robustness of LLM-Driven Multi-Agent Systems through Randomized Smoothing'}, {'title': 'How to Train Your LLM Web Agent: A Statistical Diagnosis', 'url': 'http://arxiv.org/abs/2507.04103v1', 'authors': 'Dheeraj Vattikonda, Santhoshi Ravichandran, Emiliano Penaloza, Hadi Nekoei, Megh Thakkar, Thibault Le Sellier de Chezelles, Nicolas Gontier, Miguel Muñoz-Mármol, Sahar Omidi Shayegan, Stefania Raimondo, Xue Liu, Alexandre Drouin, Laurent Charlin, Alexandre Piché, Alexandre Lacoste, Massimo Caccia', 'published': '2025-07-05'}, {'title': 'Hierarchical Testing with Rabbit Optimization for Industrial Cyber-Physical Systems', 'authors': 'Jinwei Hu, Zezhi Tang, Xin Jin, Benyuan Zhang, Yi Dong, Xiaowei Huang', 'url': 'http://arxiv.org/abs/2507.04100v1', 'published': '2025-07-05'}, {'published': '2025-07-05', 'authors': 'Thomas Savage', 'url': 'http://arxiv.org/abs/2507.04099v1', 'title': 'Conversation Forests: The Key to Fine Tuning Large Language Models for Multi-Turn Medical Conversations is Branching'}, {'url': 'http://arxiv.org/abs/2507.04095v1', 'published': '2025-07-05', 'authors': 'Alireza Mortezapour, Giuliana Vitiello', 'title': 'Human-centered AI with focus on Human-robot interaction (Book chapter)'}, {'title': 'MMMOS: Multi-domain Multi-axis Audio Quality Assessment', 'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.04094v1', 'authors': 'Yi-Cheng Lin, Jia-Hung Chen, Hung-yi Lee'}, {'title': 'Accurate and Efficient World Modeling with Masked Latent Transformers', 'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.04075v1', 'authors': 'Maxime Burchi, Radu Timofte'}, {'title': 'Beyond Independent Passages: Adaptive Passage Combination Retrieval for Retrieval Augmented Open-Domain Question Answering', 'authors': 'Ting-Wen Ko, Jyun-Yu Jiang, Pu-Jen Cheng', 'url': 'http://arxiv.org/abs/2507.04069v1', 'published': '2025-07-05'}, {'url': 'http://arxiv.org/abs/2507.04067v1', 'published': '2025-07-05', 'authors': 'Yuyang Cheng, Yumiao Xu, Chaojia Yu, Yong Zhao', 'title': 'HAWK: A Hierarchical Workflow Framework for Multi-Agent Collaboration'}, {'title': 'Stochastic Human Motion Prediction with Memory of Action Transition and Action Characteristic', 'authors': 'Jianwei Tang, Hong Yang, Tengyue Chen, Jian-Fang Hu', 'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.04062v1'}, {'title': 'Temporal Continual Learning with Prior Compensation for Human Motion Prediction', 'url': 'http://arxiv.org/abs/2507.04060v1', 'published': '2025-07-05', 'authors': 'Jianwei Tang, Jiangxin Sun, Xiaotong Lin, Lifang Zhang, Wei-Shi Zheng, Jian-Fang Hu'}, {'url': 'http://arxiv.org/abs/2507.04059v1', 'title': 'Attributing Data for Sharpness-Aware Minimization', 'published': '2025-07-05', 'authors': 'Chenyang Ren, Yifan Jia, Huanyi Xie, Zhaobin Xu, Tianxing Wei, Liangyu Wang, Lijie Hu, Di Wang'}, {'url': 'http://arxiv.org/abs/2507.04055v1', 'authors': 'Yufan Chen, Daoyuan Wu, Juantao Zhong, Zicheng Zhang, Debin Gao, Shuai Wang, Yingjiu Li, Ning Liu', 'published': '2025-07-05', 'title': 'Rethinking and Exploring String-Based Malware Family Classification in the Era of LLMs and RAG'}, {'authors': 'Baohua Zhang, Xin Li, Huangchao Xu, Zhong Jin, Quansheng Wu, Ce Li', 'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.04053v1', 'title': 'TopoMAS: Large Language Model Driven Topological Materials Multiagent System'}, {'authors': 'Roy Elkayam', 'published': '2025-07-05', 'title': 'Predictive Modeling of Effluent Temperature in SAT Systems Using Ambient Meteorological Data: Implications for Infiltration Management', 'url': 'http://arxiv.org/abs/2507.04050v1'}, {'authors': 'Kai Deng', 'url': 'http://arxiv.org/abs/2507.04043v1', 'title': 'Evaluating the Effectiveness of Large Language Models in Solving Simple Programming Tasks: A User-Centered Study', 'published': '2025-07-05'}, {'published': '2025-07-05', 'title': 'T-SYNTH: A Knowledge-Based Dataset of Synthetic Breast Images', 'authors': 'Christopher Wiedeman, Anastasiia Sarmakeeva, Elena Sizikova, Daniil Filienko, Miguel Lago, Jana G. Delfino, Aldo Badano', 'url': 'http://arxiv.org/abs/2507.04038v1'}, {'published': '2025-07-05', 'title': 'Ready Jurist One: Benchmarking Language Agents for Legal Intelligence in Dynamic Environments', 'url': 'http://arxiv.org/abs/2507.04037v1', 'authors': 'Zheng Jia, Shengbin Yue, Wei Chen, Siyuan Wang, Yidong Liu, Yun Song, Zhongyu Wei'}, {'published': '2025-07-05', 'title': 'Lyria: A General LLM-Driven Genetic Algorithm Framework for Problem Solving', 'url': 'http://arxiv.org/abs/2507.04034v1', 'authors': 'Weizhi Tang, Kwabena Nuamah, Vaishak Belle'}, {'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.04014v1', 'authors': 'Kyuhee Kim, Sangah Lee', 'title': 'Nunchi-Bench: Benchmarking Language Models on Cultural Reasoning with a Focus on Korean Superstition'}, {'published': '2025-07-05', 'authors': 'Fan Zhang, Jinpeng Chen, Huan Li, Senzhang Wang, Yuan Cao, Kaimin Wei, JianXiang He, Feifei Kou, Jinqing Wang', 'url': 'http://arxiv.org/abs/2507.04000v1', 'title': 'Leveraging Multimodal Data and Side Users for Diffusion Cross-Domain Recommendation'}, {'url': 'http://arxiv.org/abs/2507.03998v1', 'published': '2025-07-05', 'title': 'Toward Better Generalisation in Uncertainty Estimators: Leveraging Data-Agnostic Features', 'authors': 'Thuy An Ha, Bao Quoc Vo'}, {'url': 'http://arxiv.org/abs/2507.03971v1', 'title': 'Real-TabPFN: Improving Tabular Foundation Models via Continued Pre-training With Real-World Data', 'published': '2025-07-05', 'authors': 'Anurag Garg, Muhammad Ali, Noah Hollmann, Lennart Purucker, Samuel Müller, Frank Hutter'}, {'published': '2025-07-05', 'authors': 'Hengran Zhang, Keping Bi, Jiafeng Guo', 'url': 'http://arxiv.org/abs/2507.03958v1', 'title': 'A Comparative Study of Specialized LLMs as Dense Retrievers'}, {'url': 'http://arxiv.org/abs/2507.03953v1', 'title': 'Evaluating Adversarial Protections for Diffusion Personalization: A Comprehensive Study', 'published': '2025-07-05', 'authors': 'Kai Ye, Tianyi Chen, Zhen Wang'}, {'authors': 'Yizhou Luo, Kwan-Wu Chin, Ruyi Guan, Xi Xiao, Caimeng Wang, Jingyin Feng, Tengjiao He', 'title': 'Optimizing Age of Trust and Throughput in Multi-Hop UAV-Aided IoT Networks', 'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.03950v1'}, {'title': 'EdgeSRIE: A hybrid deep learning framework for real-time speckle reduction and image enhancement on portable ultrasound systems', 'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.03937v1', 'authors': 'Hyunwoo Cho, Jongsoo Lee, Jinbum Kang, Yangmo Yoo'}, {'authors': 'Mohimenul Kabir, Kuldeep S Meel', 'title': 'An ASP-Based Framework for MUSes', 'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.03929v1'}, {'title': 'CortexDebate: Debating Sparsely and Equally for Multi-Agent Debate', 'authors': 'Yiliu Sun, Zicheng Zhao, Sheng Wan, Chen Gong', 'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.03928v1'}, {'title': 'Learning Disentangled Stain and Structural Representations for Semi-Supervised Histopathology Segmentation', 'published': '2025-07-05', 'authors': 'Ha-Hieu Pham, Nguyen Lan Vi Vu, Thanh-Huy Nguyen, Ulas Bagci, Min Xu, Trung-Nghia Le, Huy-Hieu Pham', 'url': 'http://arxiv.org/abs/2507.03923v1'}, {'title': 'Animation Needs Attention: A Holistic Approach to Slides Animation Comprehension with Visual-Language Models', 'published': '2025-07-05', 'authors': 'Yifan Jiang, Yibo Xue, Yukun Kang, Pin Zheng, Jian Peng, Feiran Wu, Changliang Xu', 'url': 'http://arxiv.org/abs/2507.03916v1'}, {'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.03904v1', 'title': 'Agent Exchange: Shaping the Future of AI Agent Economics', 'authors': 'Yingxuan Yang, Ying Wen, Jun Wang, Weinan Zhang'}, {'title': \"Transformer Model for Alzheimer's Disease Progression Prediction Using Longitudinal Visit Sequences\", 'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.03899v1', 'authors': 'Mahdi Moghaddami, Clayton Schubring, Mohammad-Reza Siadat'}, {'title': 'TayFCS: Towards Light Feature Combination Selection for Deep Recommender Systems', 'authors': 'Xianquan Wang, Zhaocheng Du, Jieming Zhu, Chuhan Wu, Qinglin Jia, Zhenhua Dong', 'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.03895v1'}, {'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.03893v1', 'title': 'Hierarchical Semantic-Visual Fusion of Visible and Near-infrared Images for Long-range Haze Removal', 'authors': 'Yi Li, Xiaoxiong Wang, Jiawei Wang, Yi Chang, Kai Cao, Luxin Yan'}, {'published': '2025-07-05', 'title': 'LLMs model how humans induce logically structured rules', 'authors': 'Alyssa Loo, Ellie Pavlick, Roman Feiman', 'url': 'http://arxiv.org/abs/2507.03876v1'}, {'url': 'http://arxiv.org/abs/2507.03875v1', 'published': '2025-07-05', 'title': 'Demystifying ChatGPT: How It Masters Genre Recognition', 'authors': 'Subham Raj, Sriparna Saha, Brijraj Singh, Niranjan Pedanekar'}, {'published': '2025-07-05', 'authors': 'Karine Karine, Benjamin M. Marlin', 'title': 'Enhancing Adaptive Behavioral Interventions with LLM Inference from Participant-Described States', 'url': 'http://arxiv.org/abs/2507.03871v1'}, {'published': '2025-07-05', 'title': 'Uncovering Systemic and Environment Errors in Autonomous Systems Using Differential Testing', 'url': 'http://arxiv.org/abs/2507.03870v1', 'authors': 'Rahil P Mehta, Yashwanthi Anand, Manish Motwani, Sandhya Saisubramanian'}, {'url': 'http://arxiv.org/abs/2507.03868v1', 'title': 'From Query to Explanation: Uni-RAG for Multi-Modal Retrieval-Augmented Learning in STEM', 'published': '2025-07-05', 'authors': 'Xinyi Wu, Yanhao Jia, Luwei Xiao, Shuai Zhao, Fengkuang Chiang, Erik Cambria'}, {'title': 'OrthoRank: Token Selection via Sink Token Orthogonality for Efficient LLM inference', 'url': 'http://arxiv.org/abs/2507.03865v1', 'authors': 'Seungjun Shin, Jaehoon Oh, Dokwan Oh', 'published': '2025-07-05'}, {'authors': 'Ishan Khurjekar, Indrashish Saha, Lori Graham-Brady, Somdatta Goswami', 'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.03863v1', 'title': 'Enhanced accuracy through ensembling of randomly initialized auto-regressive models for time-dependent PDEs'}, {'title': 'KEA Explain: Explanations of Hallucinations using Graph Kernel Analysis', 'published': '2025-07-05', 'url': 'http://arxiv.org/abs/2507.03847v1', 'authors': 'Reilly Haskins, Ben Adams'}, {'authors': 'Shuowen Li, Kexin Wang, Minglu Fang, Danqi Huang, Ali Asadipour, Haipeng Mi, Yitong Sun', 'url': 'http://arxiv.org/abs/2507.03839v1', 'published': '2025-07-04', 'title': 'Participatory Evolution of Artificial Life Systems via Semantic Feedback'}, {'published': '2025-07-04', 'title': 'Economic Evaluation of LLMs', 'authors': 'Michael J. Zellinger, Matt Thomson', 'url': 'http://arxiv.org/abs/2507.03834v1'}, {'published': '2025-07-04', 'title': 'RELRaE: LLM-Based Relationship Extraction, Labelling, Refinement, and Evaluation', 'url': 'http://arxiv.org/abs/2507.03829v1', 'authors': 'George Hannah, Jacopo de Berardinis, Terry R. Payne, Valentina Tamma, Andrew Mitchell, Ellen Piercy, Ewan Johnson, Andrew Ng, Harry Rostron, Boris Konev'}, {'published': '2025-07-04', 'title': 'Leveraging Large Language Models for Tacit Knowledge Discovery in Organizational Contexts', 'authors': 'Gianlucca Zuin, Saulo Mastelini, Túlio Loures, Adriano Veloso', 'url': 'http://arxiv.org/abs/2507.03811v1'}, {'title': 'Generating Novelty in Open-World Multi-Agent Strategic Board Games', 'authors': 'Mayank Kejriwal, Shilpa Thomas', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03802v1'}, {'authors': \"Jim O'Connor, Gary B. Parker, Mustafa Bugti\", 'published': '2025-07-04', 'title': 'Learning Dark Souls Combat Through Pixel Input With Neuroevolution', 'url': 'http://arxiv.org/abs/2507.03793v1'}, {'authors': 'Jiaqi Zhang, Juntuo Wang, Zhixin Sun, John Zou, Randall Balestriero', 'url': 'http://arxiv.org/abs/2507.03779v1', 'published': '2025-07-04', 'title': 'FastDINOv2: Frequency Based Curriculum Learning Improves Robustness and Training Speed'}, {'title': 'Optimizing UAV Trajectories via a Simplified Close Enough TSP Approach', 'published': '2025-07-04', 'authors': 'Hiba Bederina', 'url': 'http://arxiv.org/abs/2507.03775v1'}, {'authors': 'Bugra Kilictas, Faruk Alpay', 'url': 'http://arxiv.org/abs/2507.03774v1', 'published': '2025-07-04', 'title': 'Alpay Algebra IV: Symbiotic Semantics and the Fixed-Point Convergence of Observer Embeddings'}, {'published': '2025-07-04', 'authors': 'Akio Kodaira, Tingbo Hou, Ji Hou, Masayoshi Tomizuka, Yue Zhao', 'title': 'StreamDiT: Real-Time Streaming Text-to-Video Generation', 'url': 'http://arxiv.org/abs/2507.03745v1'}, {'authors': 'Gongwei Chen, Xurui Zhou, Rui Shao, Yibo Lyu, Kaiwen Zhou, Shuai Wang, Wentao Li, Yinchuan Li, Zhongang Qi, Liqiang Nie', 'url': 'http://arxiv.org/abs/2507.03730v1', 'published': '2025-07-04', 'title': 'Less is More: Empowering GUI Agent with Context-Aware Simplification'}, {'url': 'http://arxiv.org/abs/2507.03726v1', 'authors': 'Riya Naik, Ashwin Srinivasan, Swati Agarwal, Estrid He', 'published': '2025-07-04', 'title': 'Agent-Based Detection and Resolution of Incompleteness and Ambiguity in Interactions with Large Language Models'}, {'url': 'http://arxiv.org/abs/2507.03722v1', 'published': '2025-07-04', 'title': 'Roadmap for using large language models (LLMs) to accelerate cross-disciplinary research with an example from computational biology', 'authors': 'Ruian Ke, Ruy M. Ribeiro'}, {'url': 'http://arxiv.org/abs/2507.03721v1', 'authors': 'Yan Katcharovski, Andrew L. Maxwell', 'published': '2025-07-04', 'title': 'Predicting Business Angel Early-Stage Decision Making Using AI'}, {'url': 'http://arxiv.org/abs/2507.03704v1', 'title': 'Controlling Thinking Speed in Reasoning Models', 'authors': 'Zhengkai Lin, Zhihang Fu, Ze Chen, Chao Chen, Liang Xie, Wenxiao Wang, Deng Cai, Zheng Wang, Jieping Ye', 'published': '2025-07-04'}, {'authors': 'JianHe Low, Ozge Mercanoglu Sincan, Richard Bowden', 'title': 'Sign Spotting Disambiguation using Large Language Models', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03703v1'}, {'title': 'Towards Unified Neurosymbolic Reasoning on Knowledge Graphs', 'url': 'http://arxiv.org/abs/2507.03697v1', 'published': '2025-07-04', 'authors': 'Qika Lin, Fangzhi Xu, Hao Lu, Kai He, Rui Mao, Jun Liu, Erik Cambria, Mengling Feng'}, {'authors': 'Rebekah A. Gelpí, Eric Xue, William A. Cunningham', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03682v1', 'title': 'Towards Machine Theory of Mind with Large Language Model-Augmented Inverse Planning'}, {'url': 'http://arxiv.org/abs/2507.03674v1', 'published': '2025-07-04', 'title': 'STRUCTSENSE: A Task-Agnostic Agentic Framework for Structured Information Extraction with Human-In-The-Loop Evaluation and Benchmarking', 'authors': 'Tek Raj Chhetri, Yibei Chen, Puja Trivedi, Dorota Jarecka, Saif Haobsh, Patrick Ray, Lydia Ng, Satrajit S. Ghosh'}, {'url': 'http://arxiv.org/abs/2507.03673v1', 'title': 'TACOS: Open Tagging and Comparative Scoring for Instruction Fine-Tuning Data Selection', 'authors': 'Xixiang He, Hao Yu, Qiyao Sun, Ao Cheng, Tailai Zhang, Cong Liu, Shuxuan Guo', 'published': '2025-07-04'}, {'published': '2025-07-04', 'authors': 'Satyam Shukla, Himanshu Dutta, Pushpak Bhattacharyya', 'url': 'http://arxiv.org/abs/2507.03671v1', 'title': 'Recon, Answer, Verify: Agents in Search of Truth'}, {'published': '2025-07-04', 'authors': 'Nikhita Joshi, Daniel Vogel', 'url': 'http://arxiv.org/abs/2507.03670v1', 'title': 'Interaction Techniques that Encourage Longer Prompts Can Improve Psychological Ownership when Writing with AI'}, {'published': '2025-07-04', 'authors': 'Jeremiah Giordani', 'title': 'Re-Emergent Misalignment: How Narrow Fine-Tuning Erodes Safety Alignment in LLMs', 'url': 'http://arxiv.org/abs/2507.03662v1'}, {'url': 'http://arxiv.org/abs/2507.03641v1', 'authors': 'Lea Fischbach, Akbar Karimi, Caroline Kleen, Alfred Lameli, Lucie Flek', 'published': '2025-07-04', 'title': 'Improving Low-Resource Dialect Classification Using Retrieval-based Voice Conversion'}, {'url': 'http://arxiv.org/abs/2507.03637v1', 'published': '2025-07-04', 'title': 'Large Language Models for Combinatorial Optimization: A Systematic Review', 'authors': 'Francesca Da Ros, Michael Soprano, Luca Di Gaspero, Kevin Roitero'}, {'title': 'From Video to EEG: Adapting Joint Embedding Predictive Architecture to Uncover Visual Concepts in Brain Signal Analysis', 'authors': 'Amir Hojjati, Lu Li, Ibrahim Hameed, Anis Yazidi, Pedro G. Lind, Rabindra Khadka', 'url': 'http://arxiv.org/abs/2507.03633v1', 'published': '2025-07-04'}, {'url': 'http://arxiv.org/abs/2507.03622v1', 'authors': 'Cooper Doyle', 'title': 'Disentangling Doubt in Deep Causal AI', 'published': '2025-07-04'}, {'published': '2025-07-04', 'authors': 'Francisca Lemos, Victor Alves, Filipa Ferraz', 'url': 'http://arxiv.org/abs/2507.03620v1', 'title': 'Is It Time To Treat Prompts As Code? A Multi-Use Case Study For Prompt Optimization Using DSPy'}, {'title': 'EvoAgentX: An Automated Framework for Evolving Agentic Workflows', 'url': 'http://arxiv.org/abs/2507.03616v1', 'published': '2025-07-04', 'authors': 'Yingxu Wang, Siwei Liu, Jinyuan Fang, Zaiqiao Meng'}, {'authors': 'Simon Welz, Lucie Flek, Akbar Karimi', 'url': 'http://arxiv.org/abs/2507.03612v1', 'title': 'Multi-Hop Reasoning for Question Answering with Hyperbolic Representations', 'published': '2025-07-04'}, {'url': 'http://arxiv.org/abs/2507.03608v1', 'title': 'Benchmarking Vector, Graph and Hybrid Retrieval Augmented Generation (RAG) Pipelines for Open Radio Access Networks (ORAN)', 'authors': 'Sarat Ahmad, Zeinab Nezami, Maryam Hafeez, Syed Ali Raza Zaidi', 'published': '2025-07-04'}, {'authors': 'Niki van Stein, Haoran Yin, Anna V. Kononova, Thomas Bäck, Gabriela Ochoa', 'published': '2025-07-04', 'title': 'Behaviour Space Analysis of LLM-driven Meta-heuristic Discovery', 'url': 'http://arxiv.org/abs/2507.03605v1'}, {'url': 'http://arxiv.org/abs/2507.03599v1', 'authors': 'Roser Batlle-Roca, Laura Ibáñez-Martínez, Xavier Serra, Emilia Gómez, Martín Rocamora', 'title': 'MusGO: A Community-Driven Framework For Assessing Openness in Music-Generative AI', 'published': '2025-07-04'}, {'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03594v1', 'title': \"RECA-PD: A Robust Explainable Cross-Attention Method for Speech-based Parkinson's Disease Classification\", 'authors': 'Terry Yi Zhong, Cristian Tejedor-Garcia, Martha Larson, Bastiaan R. Bloem'}, {'published': '2025-07-04', 'authors': 'Tao Tang, Shijie Xu, Yiting Wu, Zhixiang Lu', 'url': 'http://arxiv.org/abs/2507.03585v1', 'title': 'Causal-SAM-LLM: Large Language Models as Causal Reasoners for Robust Medical Segmentation'}, {'authors': 'Riccardo Lo Bianco, Remco Dijkman, Wim Nuijten, Willem van Jaarsveld', 'title': 'A Universal Approach to Feature Representation in Dynamic Task Assignment Problems', 'url': 'http://arxiv.org/abs/2507.03579v1', 'published': '2025-07-04'}, {'url': 'http://arxiv.org/abs/2507.03578v1', 'authors': 'Yana Hasson, Pauline Luc, Liliane Momeni, Maks Ovsjanikov, Guillaume Le Moing, Alina Kuznetsova, Ira Ktena, Jennifer J. Sun, Skanda Koppula, Dilara Gokay, Joseph Heyward, Etienne Pot, Andrew Zisserman', 'published': '2025-07-04', 'title': 'SciVid: Cross-Domain Evaluation of Video Models in Scientific Applications'}, {'url': 'http://arxiv.org/abs/2507.03558v1', 'title': 'An Advanced Deep Learning Framework for Ischemic and Hemorrhagic Brain Stroke Diagnosis Using Computed Tomography (CT) Images', 'published': '2025-07-04', 'authors': 'Md. Sabbir Hossen, Eshat Ahmed Shuvo, Shibbir Ahmed Arif, Pabon Shaha, Md. Saiduzzaman, Mostofa Kamal Nasir'}, {'published': '2025-07-04', 'authors': 'Boyang Wang, Yalun Wu, Hongcheng Guo, Zhoujun Li', 'title': 'H2HTalk: Evaluating Large Language Models as Emotional Companion', 'url': 'http://arxiv.org/abs/2507.03543v1'}, {'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03541v1', 'authors': 'Redwan Sony, Parisa Farmanifard, Arun Ross, Anil K. Jain', 'title': 'Foundation versus Domain-specific Models: Performance Comparison, Fusion, and Explainability in Face Recognition'}, {'url': 'http://arxiv.org/abs/2507.03531v1', 'authors': 'Namho Kim, Junhwa Kim', 'published': '2025-07-04', 'title': 'Multimodal Alignment with Cross-Attentive GRUs for Fine-Grained Video Understanding'}, {'title': 'Generating Synthetic Relational Tabular Data via Structural Causal Models', 'authors': 'Frederik Hoppe, Astrid Franz, Lars Kleinemeier, Udo Göbel', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03528v1'}, {'title': 'Limits of Safe AI Deployment: Differentiating Oversight and Control', 'url': 'http://arxiv.org/abs/2507.03525v1', 'authors': 'David Manheim, Aidan Homewood', 'published': '2025-07-04'}, {'authors': 'Meng Xiao, Junfeng Zhou, Yuanchun Zhou', 'url': 'http://arxiv.org/abs/2507.03498v1', 'title': 'Reinforcement Learning-based Feature Generation Algorithm for Scientific Data', 'published': '2025-07-04'}, {'authors': 'Zhiheng Xi, Guanyu Li, Yutao Fan, Honglin Guo, Yufang Liu, Xiaoran Fan, Jiaqi Liu, Jingchao Ding, Wangmeng Zuo, Zhenfei Yin, Lei Bai, Tao Ji, Tao Gui, Qi Zhang, Xuanjing Huang', 'title': 'BMMR: A Large-Scale Bilingual Multimodal Multi-Discipline Reasoning Dataset', 'url': 'http://arxiv.org/abs/2507.03483v1', 'published': '2025-07-04'}, {'authors': 'Kexin Zhu, Yang Han', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03477v1', 'title': 'REAL: Benchmarking Abilities of Large Language Models for Housing Transactions and Services'}, {'url': 'http://arxiv.org/abs/2507.03473v1', 'published': '2025-07-04', 'title': 'Beyond Weaponization: NLP Security for Medium and Lower-Resourced Languages in Their Own Right', 'authors': 'Heather Lent'}, {'authors': 'Weitong Zhang, Mengyun Qiao, Chengqi Zang, Steven Niederer, Paul M Matthews, Wenjia Bai, Bernhard Kainz', 'title': 'Multi-Agent Reasoning for Cardiovascular Imaging Phenotype Analysis', 'url': 'http://arxiv.org/abs/2507.03460v1', 'published': '2025-07-04'}, {'authors': 'Leyan Xue, Zongbo Han, Guangyu Wang, Qinghua Hu, Mingyue Cheng, Changqing Zhang', 'url': 'http://arxiv.org/abs/2507.03458v1', 'published': '2025-07-04', 'title': 'Helping CLIP See Both the Forest and the Trees: A Decomposition and Description Approach'}, {'authors': 'Antonio Emanuele Cinà, Maura Pintor, Luca Demetrio, Ambra Demontis, Battista Biggio, Fabio Roli', 'url': 'http://arxiv.org/abs/2507.03450v1', 'title': 'Evaluating the Evaluators: Trust in Adversarial Robustness Tests', 'published': '2025-07-04'}, {'authors': 'Adrien Bazoge, Pacôme Constant dit Beaufils, Mohammed Hmitouch, Romain Bourcier, Emmanuel Morin, Richard Dufour, Béatrice Daille, Pierre-Antoine Gourraud, Matilde Karakachoff', 'url': 'http://arxiv.org/abs/2507.03433v1', 'published': '2025-07-04', 'title': 'Improving Social Determinants of Health Documentation in French EHRs Using Large Language Models'}, {'authors': 'XiaYu Liu, Hou-biao Li, Yang Liu, Chao Fan', 'published': '2025-07-04', 'title': 'Multi-Level Fusion Graph Neural Network for Molecule Property Prediction', 'url': 'http://arxiv.org/abs/2507.03430v1'}, {'url': 'http://arxiv.org/abs/2507.03409v1', 'title': 'Lessons from a Chimp: AI \"Scheming\" and the Quest for Ape Language', 'published': '2025-07-04', 'authors': 'Christopher Summerfield, Lennart Luettgau, Magda Dubois, Hannah Rose Kirk, Kobi Hackenburg, Catherine Fist, Katarina Slama, Nicola Ding, Rebecca Anselmetti, Andrew Strait, Mario Giulianelli, Cozmin Ududec'}, {'url': 'http://arxiv.org/abs/2507.03407v1', 'title': 'Artificial intelligence in drug discovery: A comprehensive review with a case study on hyperuricemia, gout arthritis, and hyperuricemic nephropathy', 'published': '2025-07-04', 'authors': 'Junwei Su, Cheng Xin, Ao Shang, Shan Wu, Zhenzhen Xie, Ruogu Xiong, Xiaoyu Xu, Cheng Zhang, Guang Chen, Yau-Tuen Chan, Guoyi Tang, Ning Wang, Yong Xu, Yibin Feng'}, {'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03402v1', 'authors': 'Yuran Dong, Mang Ye', 'title': 'Pose-Star: Anatomy-Aware Editing for Open-World Fashion Images'}, {'authors': 'Suchen Liu, Jun Gao, Yinjun Han, Yang Lin', 'title': 'LLM4Hint: Leveraging Large Language Models for Hint Recommendation in Offline Query Optimization', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03384v1'}, {'url': 'http://arxiv.org/abs/2507.03367v1', 'title': 'Be the Change You Want to See: Revisiting Remote Sensing Change Detection Practices', 'published': '2025-07-04', 'authors': 'Blaž Rolih, Matic Fučka, Filip Wolf, Luka Čehovin Zajc'}, {'url': 'http://arxiv.org/abs/2507.03350v1', 'published': '2025-07-04', 'authors': 'Elvys Linhares Pontes, Carlos-Emiliano González-Gallardo, Georgeta Bordea, José G. Moreno, Mohamed Ben Jannet, Yuxuan Zhao, Antoine Doucet', 'title': 'Backtesting Sentiment Signals for Trading: Evaluating the Viability of Alpha Generation from Sentiment Analysis'}, {'url': 'http://arxiv.org/abs/2507.03347v1', 'title': 'Effects of structure on reasoning in instance-level Self-Discover', 'authors': 'Sachith Gunasekara, Yasiru Ratnayake', 'published': '2025-07-04'}, {'published': '2025-07-04', 'authors': 'Sheng Liu, Yiheng Yu, Yuan Feng, Min Xu, Zhelun Jin, Yining Jiang, Tiantian Yuan', 'url': 'http://arxiv.org/abs/2507.03339v1', 'title': 'DESign: Dynamic Context-Aware Convolution and Efficient Subnet Regularization for Continuous Sign Language Recognition'}, {'url': 'http://arxiv.org/abs/2507.03336v1', 'title': 'Disambiguation-Centric Finetuning Makes Enterprise Tool-Calling LLMs More Realistic and Less Risky', 'published': '2025-07-04', 'authors': 'Ashutosh Hathidara, Julien Yu, Sebastian Schreiber'}, {'title': 'De-Fake: Style based Anomaly Deepfake Detection', 'url': 'http://arxiv.org/abs/2507.03334v1', 'authors': 'Sudev Kumar Padhi, Harshit Kumar, Umesh Kashyap, Sk. Subidh Ali', 'published': '2025-07-04'}, {'authors': 'Mingzhuo Li, Guang Li, Jiafeng Mao, Linfeng Ye, Takahiro Ogawa, Miki Haseyama', 'published': '2025-07-04', 'title': 'Task-Specific Generative Dataset Distillation with Difficulty-Guided Sampling', 'url': 'http://arxiv.org/abs/2507.03331v1'}, {'title': 'Exploring Object Status Recognition for Recipe Progress Tracking in Non-Visual Cooking', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03330v1', 'authors': 'Franklin Mingzhe Li, Kaitlyn Ng, Bin Zhu, Patrick Carrington'}, {'authors': 'Devendra Patel, Aaditya Jain, Jayant Verma, Divyansh Rajput, Sunil Mahala, Ketki Suresh Khapare, Jayateja Kalla', 'title': 'NDAI-NeuroMAP: A Neuroscience-Specific Embedding Model for Domain-Specific Retrieval', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03329v1'}, {'title': 'Read Quietly, Think Aloud: Decoupling Comprehension and Reasoning in LLMs', 'url': 'http://arxiv.org/abs/2507.03327v1', 'authors': 'Yuanxin Wang, Ganesh Venkatesh', 'published': '2025-07-04'}, {'url': 'http://arxiv.org/abs/2507.03321v1', 'published': '2025-07-04', 'authors': 'Amirfarhad Farhadi, Naser Mozayani, Azadeh Zamanifar', 'title': 'Source-Free Domain Adaptation via Multi-view Contrastive Learning'}, {'authors': 'Zanyu Shi, Yang Wang, Pathum Weerawarna, Jie Zhang, Timothy Richardson, Yijie Wang, Kun Huang', 'url': 'http://arxiv.org/abs/2507.03318v1', 'title': 'Structure-Aware Compound-Protein Affinity Prediction via Graph Neural Network with Group Lasso Regularization', 'published': '2025-07-04'}, {'url': 'http://arxiv.org/abs/2507.03314v1', 'title': 'Partial Label Learning for Automated Theorem Proving', 'authors': 'Zsolt Zombori, Balázs Indruck', 'published': '2025-07-04'}, {'authors': 'Sagar Gandhi, Vishal Gandhi', 'url': 'http://arxiv.org/abs/2507.03313v1', 'title': 'Personalized Image Generation from an Author Writing Style', 'published': '2025-07-04'}, {'title': 'GRAFT: A Graph-based Flow-aware Agentic Framework for Document-level Machine Translation', 'url': 'http://arxiv.org/abs/2507.03311v1', 'published': '2025-07-04', 'authors': 'Himanshu Dutta, Sunny Manchanda, Prakhar Bapat, Meva Ram Gurjar, Pushpak Bhattacharyya'}, {'published': '2025-07-04', 'authors': 'Weihong Li, Anpeng Wu, Kun Kuang, Keting Yin', 'url': 'http://arxiv.org/abs/2507.03310v1', 'title': 'ReTimeCausal: EM-Augmented Additive Noise Models for Interpretable Causal Discovery in Irregular Time Series'}, {'published': '2025-07-04', 'authors': 'Taewook Kim, Matthew Kay, Yuqian Sun, Melissa Roemmele, Max Kreminski, John Joon Young Chung', 'title': 'Scaffolding Recursive Divergence and Convergence in Story Ideation', 'url': 'http://arxiv.org/abs/2507.03307v1'}, {'title': 'Leveraging Out-of-Distribution Unlabeled Images: Semi-Supervised Semantic Segmentation with an Open-Vocabulary Model', 'authors': 'Wooseok Shin, Jisu Kang, Hyeonki Jeong, Jin Sob Kim, Sung Won Han', 'url': 'http://arxiv.org/abs/2507.03302v1', 'published': '2025-07-04'}, {'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03294v1', 'authors': 'Guangyan Li, Yongqiang Tang, Wensheng Zhang', 'title': 'MGAA: Multi-Granular Adaptive Allocation fof Low-Rank Compression of LLMs'}, {'authors': 'Anand Gokhale, Vaibhav Srivastava, Francesco Bullo', 'published': '2025-07-04', 'title': 'LTLCrit: A Temporal Logic-based LLM Critic for Safe and Efficient Embodied Agents', 'url': 'http://arxiv.org/abs/2507.03293v1'}, {'url': 'http://arxiv.org/abs/2507.03285v1', 'published': '2025-07-04', 'title': 'Memory Mosaics at scale', 'authors': 'Jianyu Zhang, Léon Bottou'}, {'title': 'Conformal Information Pursuit for Interactively Guiding Large Language Models', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03279v1', 'authors': 'Kwan Ho Ryan Chan, Yuyan Ge, Edgar Dobriban, Hamed Hassani, René Vidal'}, {'url': 'http://arxiv.org/abs/2507.03267v1', 'published': '2025-07-04', 'authors': 'Jie Peng, Jiarui Ji, Runlin Lei, Zhewei Wei, Yongchao Liu, Chuntao Hong', 'title': 'GDGB: A Benchmark for Generative Dynamic Text-Attributed Graph Learning'}, {'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03262v1', 'title': 'Investigating Redundancy in Multimodal Large Language Models with Multiple Vision Encoders', 'authors': 'Song Mao, Yang Chen, Pinglong Cai, Ding Wang, Guohang Yan, Zhi Yu, Botian Shi'}, {'title': 'ForgeHLS: A Large-Scale, Open-Source Dataset for High-Level Synthesis', 'url': 'http://arxiv.org/abs/2507.03255v1', 'authors': 'Zedong Peng, Zeju Li, Mingzhe Gao, Qiang Xu, Chen Zhang, Jieru Zhao', 'published': '2025-07-04'}, {'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03254v1', 'title': 'CodeAgents: A Token-Efficient Framework for Codified Multi-Agent Reasoning in LLMs', 'authors': 'Bruce Yang, Xinfeng He, Huan Gao, Yifan Cao, Xiaofan Li, David Hsu'}, {'authors': 'Baolong Bi, Shenghua Liu, Xingzhang Ren, Dayiheng Liu, Junyang Lin, Yiwei Wang, Lingrui Mei, Junfeng Fang, Jiafeng Guo, Xueqi Cheng', 'url': 'http://arxiv.org/abs/2507.03253v1', 'title': 'RefineX: Learning to Refine Pre-training Data at Scale from Expert-Guided Programs', 'published': '2025-07-04'}, {'url': 'http://arxiv.org/abs/2507.03251v1', 'published': '2025-07-04', 'authors': 'HyeYoung Lee, Muhammad Nadeem', 'title': 'Toward Efficient Speech Emotion Recognition via Spectral Learning and Attention'}, {'url': 'http://arxiv.org/abs/2507.03236v1', 'title': 'On Jailbreaking Quantized Language Models Through Fault Injection Attacks', 'authors': 'Noureldin Zahran, Ahmad Tahmasivand, Ihsen Alouani, Khaled Khasawneh, Mohammed E. Fouda', 'published': '2025-07-04'}, {'title': 'Efficient Knowledge Graph Construction and Retrieval from Unstructured Text for Large-Scale RAG Systems', 'published': '2025-07-04', 'url': 'http://arxiv.org/abs/2507.03226v1', 'authors': 'Congmin Min, Rhea Mathew, Joyce Pan, Sahil Bansal, Abbas Keshavarzi, Amar Viswanathan Kannan'}, {'published': '2025-07-03', 'authors': 'Jeshwanth Challagundla', 'url': 'http://arxiv.org/abs/2507.03223v1', 'title': 'SI-Agent: An Agentic Framework for Feedback-Driven Generation and Tuning of Human-Readable System Instructions for Large Language Models'}, {'authors': 'Alejandro Rodriguez-Garcia, Christopher J. Whyte, Brandon R. Munn, Jie Mei, James M. Shine, Srikanth Ramaswamy', 'url': 'http://arxiv.org/abs/2507.03222v1', 'published': '2025-07-03', 'title': 'The role of gain neuromodulation in layer-5 pyramidal neurons'}, {'title': 'Neural Inhibition Improves Dynamic Routing and Mixture of Experts', 'url': 'http://arxiv.org/abs/2507.03221v1', 'authors': 'Will Y. Zou, Jennifer Y. Zhang', 'published': '2025-07-03'}, {'url': 'http://arxiv.org/abs/2507.03220v1', 'title': 'Symbiosis: Multi-Adapter Inference and Fine-Tuning', 'published': '2025-07-03', 'authors': 'Saransh Gupta, Umesh Deshpande, Travis Janssen, Swami Sundararaman'}, {'published': '2025-07-03', 'title': 'Disclosing Generative AI Use in Digital Humanities Research', 'authors': 'Rongqian Ma, Xuhan Zhang, Adrian Wisnicki', 'url': 'http://arxiv.org/abs/2507.03216v1'}, {'published': '2025-07-03', 'authors': 'Pappu Kumar Yadav, Rishik Aggarwal, Supriya Paudel, Amee Parmar, Hasan Mirzakhaninafchi, Zain Ul Abideen Usmani, Dhe Yeong Tchalla, Shyam Solanki, Ravi Mural, Sachin Sharma, Thomas F. Burks, Jianwei Qin, Moon S. Kim', 'url': 'http://arxiv.org/abs/2507.03198v1', 'title': 'AI-driven Web Application for Early Detection of Sudden Death Syndrome (SDS) in Soybean Leaves Using Hyperspectral Images and Genetic Algorithm'}, {'url': 'http://arxiv.org/abs/2507.03194v1', 'published': '2025-07-03', 'title': 'How Much Content Do LLMs Generate That Induces Cognitive Bias in Users?', 'authors': 'Abeer Alessa, Akshaya Lakshminarasimhan, Param Somane, Julian Skirzynski, Julian McAuley, Jessica Echterhoff'}, {'url': 'http://arxiv.org/abs/2507.03190v1', 'authors': 'Theo Bourdais, Abeynaya Gnanasekaran, Houman Owhadi, Tuhin Sahai', 'published': '2025-07-03', 'title': 'Discovering Algorithms with Computational Language Processing'}, {'url': 'http://arxiv.org/abs/2507.03176v1', 'published': '2025-07-03', 'authors': 'Zilu Meng, Gregory J. Hakim, Wenchang Yang, Gabriel A. Vecchi', 'title': 'Deep Learning Atmospheric Models Reliably Simulate Out-of-Sample Land Heat and Cold Wave Frequencies'}, {'authors': 'Haohua Wang, Jingge Wang, Zijie Zhao, Yang Tan, Yanru Wu, Hanbing Liu, Jingyun Yang, Enming Zhang, Xiangyu Chen, Zhengze Rong, Shanxin Guo, Yang Li', 'title': 'Understanding Knowledge Transferability for Transfer Learning: A Survey', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.03175v1'}, {'authors': 'Kureha Yamaguchi, Benjamin Etheridge, Andy Arditi', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.03167v1', 'title': 'Adversarial Manipulation of Reasoning Models using Internal Representations'}, {'url': 'http://arxiv.org/abs/2507.03162v1', 'published': '2025-07-03', 'authors': 'Dumitran Adrian Marius, Theodor-Pierre Moroianu, Buca Mihnea-Vicentiu', 'title': 'MateInfoUB: A Real-World Benchmark for Testing LLMs in Competitive, Multilingual, and Multimodal Educational Tasks'}, {'title': 'The Impact of LLM-Assistants on Software Developer Productivity: A Systematic Literature Review', 'authors': 'Amr Mohamed, Maram Assi, Mariam Guizani', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.03156v1'}, {'url': 'http://arxiv.org/abs/2507.03152v1', 'published': '2025-07-03', 'authors': 'Asad Aali, Vasiliki Bikia, Maya Varma, Nicole Chiou, Sophie Ostmeier, Arnav Singhvi, Magdalini Paschali, Ashwin Kumar, Andrew Johnston, Karimar Amador-Martinez, Eduardo Juan Perez Guerrero, Paola Naovi Cruz Rivera, Sergios Gatidis, Christian Bluethgen, Eduardo Pontes Reis, Eddy D. Zandee van Rilland, Poonam Laxmappa Hosamani, Kevin R Keet, Minjoung Go, Evelyn Ling, David B. Larson, Curtis Langlotz, Roxana Daneshjou, Jason Hom, Sanmi Koyejo, Emily Alsentzer, Akshay S. Chaudhari', 'title': 'Expert-level validation of AI-generated medical text with scalable language models'}, {'url': 'http://arxiv.org/abs/2507.03149v1', 'title': 'On the Relationship between Accent Strength and Articulatory Features', 'authors': 'Kevin Huang, Sean Foley, Jihwan Lee, Yoonjeong Lee, Dani Byrd, Shrikanth Narayanan', 'published': '2025-07-03'}, {'authors': 'Dharshan Kumaran, Stephen M Fleming, Larisa Markeeva, Joe Heyward, Andrea Banino, Mrinal Mathur, Razvan Pascanu, Simon Osindero, Benedetto de Martino, Petar Velickovic, Viorica Patraucean', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.03120v1', 'title': 'How Overconfidence in Initial Choices and Underconfidence Under Criticism Modulate Change of Mind in Large Language Models'}, {'published': '2025-07-03', 'authors': 'Timo Thun, Andrea Merlo, Rory Conlin, Dario Panici, Daniel Böckenhoff', 'url': 'http://arxiv.org/abs/2507.03119v1', 'title': 'Neural-Network solver of ideal MHD equilibria'}, {'url': 'http://arxiv.org/abs/2507.03112v1', 'title': 'RLVER: Reinforcement Learning with Verifiable Emotion Rewards for Empathetic Agents', 'authors': 'Peisong Wang, Ruotian Ma, Bang Zhang, Xingyu Chen, Zhiwei He, Kang Luo, Qingsong Lv, Qingxuan Jiang, Zheng Xie, Shanyi Wang, Yuan Li, Fanghua Ye, Jian Li, Yifan Yang, Zhaopeng Tu, Xiaolong Li', 'published': '2025-07-03'}, {'url': 'http://arxiv.org/abs/2507.03095v1', 'title': 'Uncovering Synergistic Educational Injustices of COVID-19 and AI', 'authors': 'Ahmad Banyasady', 'published': '2025-07-03'}, {'authors': 'Yuqi Wu, Wenzhao Zheng, Jie Zhou, Jiwen Lu', 'url': 'http://arxiv.org/abs/2507.02863v1', 'published': '2025-07-03', 'title': 'Point3R: Streaming 3D Reconstruction with Explicit Spatial Pointer Memory'}, {'published': '2025-07-03', 'title': 'LiteReality: Graphics-Ready 3D Scene Reconstruction from RGB-D Scans', 'url': 'http://arxiv.org/abs/2507.02861v1', 'authors': 'Zhening Huang, Xiaoyang Wu, Fangcheng Zhong, Hengshuang Zhao, Matthias Nießner, Joan Lasenby'}, {'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.03069v1', 'title': 'ARF-RLHF: Adaptive Reward-Following for RLHF through Emotion-Driven Self-Supervision and Trace-Biased Dynamic Optimization', 'authors': 'YuXuan Zhang'}, {'title': 'Answer Matching Outperforms Multiple Choice for Language Model Evaluation', 'published': '2025-07-03', 'authors': 'Nikhil Chandak, Shashwat Goel, Ameya Prabhu, Moritz Hardt, Jonas Geiping', 'url': 'http://arxiv.org/abs/2507.02856v1'}, {'title': 'Subtyping in DHOL -- Extended preprint', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02855v1', 'authors': 'Colin Rothgang, Florian Rabe'}, {'authors': 'Purbesh Mitra, Sennur Ulukus', 'url': 'http://arxiv.org/abs/2507.02851v1', 'published': '2025-07-03', 'title': 'MOTIF: Modular Thinking via Reinforcement Fine-tuning in LLMs'}, {'published': '2025-07-03', 'authors': 'Kaiyi Zhang, Ang Lv, Jinpeng Li, Yongbo Wang, Feng Wang, Haoyuan Hu, Rui Yan', 'url': 'http://arxiv.org/abs/2507.02841v1', 'title': 'StepHint: Multi-level Stepwise Hints Enhance Reinforcement Learning to Reason'}, {'authors': 'Ying Yu, Hang Xiao, Siyao Li, Jiarui Li, Haotian Tang, Hanyu Liu, Chao Li', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02827v1', 'title': 'USAD: An Unsupervised Data Augmentation Spatio-Temporal Attention Diffusion Network'}, {'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02825v1', 'title': 'Establishing Best Practices for Building Rigorous Agentic Benchmarks', 'authors': 'Yuxuan Zhu, Tengjun Jin, Yada Pruksachatkun, Andy Zhang, Shu Liu, Sasha Cui, Sayash Kapoor, Shayne Longpre, Kevin Meng, Rebecca Weiss, Fazl Barez, Rahul Gupta, Jwala Dhamala, Jacob Merizian, Mario Giulianelli, Harry Coppock, Cozmin Ududec, Jasjeet Sekhon, Jacob Steinhardt, Antony Kellerman, Sarah Schwettmann, Matei Zaharia, Ion Stoica, Percy Liang, Daniel Kang'}, {'published': '2025-07-03', 'authors': 'Po-Heng Chou, Ching-Wen Chen, Wan-Jen Huang, Walid Saad, Yu Tsao, Ronald Y. Chang', 'url': 'http://arxiv.org/abs/2507.02824v2', 'title': 'DNN-Based Precoding in RIS-Aided mmWave MIMO Systems With Practical Phase Shift'}, {'url': 'http://arxiv.org/abs/2507.02822v1', 'authors': 'Wencheng Zhang, Shiqin Qiao, Lingjie Luo, Yinfeng Li, Chuanyang Zheng, Qian Xu, Meng Li, Yong Gui, Yijun He, Jianing Qiu, Jindong Hong, Jiankai Sun', 'title': 'SynapseRoute: An Auto-Route Switching Framework on Dual-State Large Language Model', 'published': '2025-07-03'}, {'url': 'http://arxiv.org/abs/2507.03067v1', 'published': '2025-07-03', 'authors': 'Alvaro Riquelme, Pedro Costa, Catalina Martinez', 'title': 'Large Language Models for Automating Clinical Data Standardization: HL7 FHIR Use Case'}, {'authors': 'Sudesh Bhagat, Ibne Farabi Shihab, Jonathan Wood', 'url': 'http://arxiv.org/abs/2507.03066v1', 'published': '2025-07-03', 'title': 'Identification of Potentially Misclassified Crash Narratives using Machine Learning (ML) and Deep Learning (DL)'}, {'url': 'http://arxiv.org/abs/2507.03064v1', 'published': '2025-07-03', 'authors': 'Hetvi Shastri, Walid A. Hanafy, Li Wu, David Irwin, Mani Srivastava, Prashant Shenoy', 'title': 'LLM-Driven Auto Configuration for Transient IoT Device Collaboration'}, {'authors': 'Joseph Boland', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02788v1', 'title': 'Moral Responsibility or Obedience: What Do We Want from AI?'}, {'url': 'http://arxiv.org/abs/2507.02778v1', 'title': 'Self-Correction Bench: Revealing and Addressing the Self-Correction Blind Spot in LLMs', 'authors': 'Ken Tsui', 'published': '2025-07-03'}, {'authors': 'Hao Yang, Angela Yao, Christopher Whalen, Gengchen Mai', 'url': 'http://arxiv.org/abs/2507.03062v1', 'published': '2025-07-03', 'title': 'BERT4Traj: Transformer Based Trajectory Reconstruction for Sparse Mobility Data'}, {'url': 'http://arxiv.org/abs/2507.02773v2', 'title': 'KERAP: A Knowledge-Enhanced Reasoning Approach for Accurate Zero-shot Diagnosis Prediction Using Multi-agent LLMs', 'published': '2025-07-03', 'authors': 'Yuzhang Xie, Hejie Cui, Ziyang Zhang, Jiaying Lu, Kai Shu, Fadi Nahab, Xiao Hu, Carl Yang'}, {'title': 'Grounding Intelligence in Movement', 'published': '2025-07-03', 'authors': 'Melanie Segado, Felipe Parodi, Jordan K. Matelsky, Michael L. Platt, Eva B. Dyer, Konrad P. Kording', 'url': 'http://arxiv.org/abs/2507.02771v1'}, {'authors': 'Guangwei Zhang', 'title': 'Knowledge Protocol Engineering: A New Paradigm for AI in Domain-Specific Knowledge Work', 'url': 'http://arxiv.org/abs/2507.02760v1', 'published': '2025-07-03'}, {'authors': 'Mark Zilberman', 'title': 'AI-Based Reconstruction from Inherited Personal Data: Analysis, Feasibility, and Prospects', 'url': 'http://arxiv.org/abs/2507.03059v1', 'published': '2025-07-03'}, {'title': 'Multi-agent Auditory Scene Analysis', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02755v1', 'authors': 'Caleb Rascon, Luis Gato-Diaz, Eduardo García-Alarcón'}, {'title': 'Fast and Simplex: 2-Simplicial Attention in Triton', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02754v1', 'authors': 'Aurko Roy, Timothy Chou, Sai Surya Duvvuri, Sijia Chen, Jiecao Yu, Xiaodong Wang, Manzil Zaheer, Rohan Anil'}, {'authors': 'Shuan Chen, Gunwook Nam, Yousung Jung', 'published': '2025-07-03', 'title': 'Synthesizable by Design: A Retrosynthesis-Guided Framework for Molecular Analog Generation', 'url': 'http://arxiv.org/abs/2507.02752v1'}, {'published': '2025-07-03', 'title': 'Linear Attention with Global Context: A Multipole Attention Mechanism for Vision and Physics', 'url': 'http://arxiv.org/abs/2507.02748v1', 'authors': 'Alex Colagrande, Paul Caillon, Eva Feillet, Alexandre Allauzen'}, {'authors': 'Artur Zolkowski, Kei Nishimura-Gasparian, Robert McCarthy, Roland S. Zimmermann, David Lindner', 'url': 'http://arxiv.org/abs/2507.02737v1', 'title': 'Early Signs of Steganographic Capabilities in Frontier LLMs', 'published': '2025-07-03'}, {'published': '2025-07-03', 'authors': 'Sizhe Chen, Arman Zharmagambetov, David Wagner, Chuan Guo', 'title': 'Meta SecAlign: A Secure Foundation LLM Against Prompt Injection Attacks', 'url': 'http://arxiv.org/abs/2507.02735v1'}, {'authors': 'Matthieu Zimmer, Xiaotong Ji, Rasul Tutunov, Anthony Bordg, Jun Wang, Haitham Bou Ammar', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02726v1', 'title': 'Bourbaki: Self-Generated and Goal-Conditioned MDPs for Theorem Proving'}, {'authors': 'Yuxuan Wang, Tianwei Cao, Huayu Zhang, Zhongjiang He, Kongming Liang, Zhanyu Ma', 'title': 'FairHuman: Boosting Hand and Face Quality in Human Image Generation with Minimum Potential Delay Fairness in Diffusion Models', 'url': 'http://arxiv.org/abs/2507.02714v1', 'published': '2025-07-03'}, {'url': 'http://arxiv.org/abs/2507.02703v1', 'published': '2025-07-03', 'authors': 'Robin Schmöcker, Lennart Kampmann, Alexander Dockhorn', 'title': 'Time-critical and confidence-based abstraction dropping methods'}, {'published': '2025-07-03', 'authors': 'JungWoo Chae, Jiyoon Kim, JaeWoong Choi, Kyungyul Kim, Sangheum Hwang', 'url': 'http://arxiv.org/abs/2507.02687v1', 'title': 'APT: Adaptive Personalized Training for Diffusion Models with Limited Data'}, {'authors': 'Behnam Parsaeifard, Christof Imhof, Tansu Pancar, Ioan-Sorin Comsa, Martin Hlosta, Nicole Bergamin, Per Bergamin', 'title': 'Detection of Disengagement from Voluntary Quizzes: An Explainable Machine Learning Approach in Higher Distance Education', 'url': 'http://arxiv.org/abs/2507.02681v2', 'published': '2025-07-03'}, {'published': '2025-07-03', 'authors': 'Junyu Wang, Tianrui Wang, Meng Ge, Longbiao Wang, Jianwu Dang', 'title': 'ASDA: Audio Spectrogram Differential Attention Mechanism for Self-Supervised Representation Learning', 'url': 'http://arxiv.org/abs/2507.02666v1'}, {'url': 'http://arxiv.org/abs/2507.02663v1', 'title': 'Think How to Think: Mitigating Overthinking with Autonomous Difficulty Cognition in Large Reasoning Models', 'authors': 'Yongjiang Liu, Haoxi Li, Xiaosong Ma, Jie Zhang, Song Guo', 'published': '2025-07-03'}, {'authors': 'Deepak Narayan Gadde, Keerthan Kopparam Radhakrishna, Vaisakh Naduvodi Viswambharan, Aman Kumar, Djones Lettnin, Wolfgang Kunz, Sebastian Simon', 'published': '2025-07-03', 'title': 'Hey AI, Generate Me a Hardware Code! Agentic AI-based Hardware Design & Verification', 'url': 'http://arxiv.org/abs/2507.02660v1'}, {'published': '2025-07-03', 'authors': 'Jiajie Jin, Xiaoxi Li, Guanting Dong, Yuyao Zhang, Yutao Zhu, Yang Zhao, Hongjin Qian, Zhicheng Dou', 'title': 'Decoupled Planning and Execution: A Hierarchical Reasoning Framework for Deep Search', 'url': 'http://arxiv.org/abs/2507.02652v1'}, {'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02644v1', 'authors': 'Yuntian Gu, Wenrui Li, Heng Lin, Bo Zhan, Ruichen Li, Yifei Huang, Di He, Yantao Wu, Tao Xiang, Mingpu Qin, Liwei Wang, Dingshun Lv', 'title': 'Solving the Hubbard model with Neural Quantum States'}, {'title': 'FlowSpec: Continuous Pipelined Speculative Decoding for Efficient Distributed LLM Inference', 'authors': 'Xing Liu, Lizhuo Luo, Ming Tang, Chao Huang', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02620v1'}, {'authors': 'Kenneth Payne, Baptiste Alloui-Cros', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02618v1', 'title': 'Strategic Intelligence in Large Language Models: Evidence from evolutionary Game Theory'}, {'url': 'http://arxiv.org/abs/2507.02616v1', 'authors': 'Tianqi Shang, Weiqing He, Charles Zheng, Lingyao Li, Li Shen, Bingxin Zhao', 'published': '2025-07-03', 'title': 'DynamiCare: A Dynamic Multi-Agent Framework for Interactive and Open-Ended Medical Decision-Making'}, {'url': 'http://arxiv.org/abs/2507.02606v1', 'authors': 'Wei Fan, Kejiang Chen, Chang Liu, Weiming Zhang, Nenghai Yu', 'published': '2025-07-03', 'title': 'De-AntiFake: Rethinking the Protective Perturbations Against Voice Cloning Attacks'}, {'title': \"Automated Grading of Students' Handwritten Graphs: A Comparison of Meta-Learning and Vision-Large Language Models\", 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.03056v1', 'authors': 'Behnam Parsaeifard, Martin Hlosta, Per Bergamin'}, {'authors': 'Riccardo Gallon, Fabian Schiemenz, Alessandra Menicucci, Eberhard Gill', 'url': 'http://arxiv.org/abs/2507.02602v1', 'published': '2025-07-03', 'title': 'Addressing Camera Sensors Faults in Vision-Based Navigation: Simulation and Dataset Development'}, {'authors': 'Chenhao Xue, Kezhi Li, Jiaxing Zhang, Yi Ren, Zhengyuan Shi, Chen Zhang, Yibo Lin, Lining Zhang, Qiang Xu, Guangyu Sun', 'title': 'AC-Refiner: Efficient Arithmetic Circuit Optimization Using Conditional Diffusion Models', 'url': 'http://arxiv.org/abs/2507.02598v1', 'published': '2025-07-03'}, {'authors': 'Xin Guan, PeiHsin Lin, Zekun Wu, Ze Wang, Ruibo Zhang, Emre Kazim, Adriano Koshiyama', 'url': 'http://arxiv.org/abs/2507.02595v1', 'published': '2025-07-03', 'title': 'MPF: Aligning and Debiasing Language Models post Deployment via Multi Perspective Fusion'}, {'title': 'WebSailor: Navigating Super-human Reasoning for Web Agent', 'published': '2025-07-03', 'authors': 'Kuan Li, Zhongwang Zhang, Huifeng Yin, Liwen Zhang, Litu Ou, Jialong Wu, Wenbiao Yin, Baixuan Li, Zhengwei Tao, Xinyu Wang, Weizhou Shen, Junkai Zhang, Dingchu Zhang, Xixi Wu, Yong Jiang, Ming Yan, Pengjun Xie, Fei Huang, Jingren Zhou', 'url': 'http://arxiv.org/abs/2507.02592v1'}, {'url': 'http://arxiv.org/abs/2507.03054v1', 'title': 'LATTE: Latent Trajectory Embedding for Diffusion-Generated Image Detection', 'published': '2025-07-03', 'authors': 'Ana Vasilcoiu, Ivona Najdenkoska, Zeno Geradts, Marcel Worring'}, {'title': 'Responsibility Gap and Diffusion in Sequential Decision-Making Mechanisms', 'authors': 'Junli Jiang, Pavel Naumov', 'url': 'http://arxiv.org/abs/2507.02582v1', 'published': '2025-07-03'}, {'published': '2025-07-03', 'authors': 'Egor Maximov, Yulia Kuzkina, Azamat Kanametov, Alexander Prutko, Aleksei Goncharov, Maxim Zhelnin, Egor Shvetsov', 'url': 'http://arxiv.org/abs/2507.03052v1', 'title': 'From 2:4 to 8:16 sparsity patterns in LLMs for Outliers and Weights with Variance Correction'}, {'title': 'AI Research Agents for Machine Learning: Search, Exploration, and Generalization in MLE-bench', 'url': 'http://arxiv.org/abs/2507.02554v1', 'published': '2025-07-03', 'authors': 'Edan Toledo, Karen Hambardzumyan, Martin Josifoski, Rishi Hazra, Nicolas Baldwin, Alexis Audran-Reiss, Michael Kuchnik, Despoina Magka, Minqi Jiang, Alisia Maria Lupidi, Andrei Lupu, Roberta Raileanu, Kelvin Niu, Tatiana Shavrina, Jean-Christophe Gagnon-Audet, Michael Shvartsman, Shagun Sodhani, Alexander H. Miller, Abhishek Charnalia, Derek Dunfield, Carole-Jean Wu, Pontus Stenetorp, Nicola Cancedda, Jakob Nicolaus Foerster, Yoram Bachrach'}, {'title': 'Improving LLM Reasoning for Vulnerability Detection via Group Relative Policy Optimization', 'url': 'http://arxiv.org/abs/2507.03051v1', 'published': '2025-07-03', 'authors': 'Marco Simoni, Aleksandar Fontana, Giulio Rossolini, Andrea Saracino'}, {'authors': \"David A. Danhofer, Davide D'Ascenzo, Rafael Dubach, Tomaso Poggio\", 'title': 'Position: A Theory of Deep Learning Must Include Compositional Sparsity', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02550v1'}, {'authors': 'Yanzhen Lu, Hanbin Yang, Xiaodie Wang, Ge Zhang, Biao Li, Chenxu Fu, Chao Li, Yang Yuan, Andrew Chi-Chih Yao', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02541v1', 'title': 'Clarifying Before Reasoning: A Coq Prover with Structural Context'}, {'authors': 'Paulo Ricardo Knob, Leonardo Scholler, Juliano Rigatti, Soraia Raupp Musse', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02537v1', 'title': 'Are You Listening to Me? Fine-Tuning Chatbots for Empathetic Dialogue'}, {'published': '2025-07-03', 'title': \"From Turing to Tomorrow: The UK's Approach to AI Regulation\", 'url': 'http://arxiv.org/abs/2507.03050v1', 'authors': 'Oliver Ritchie, Markus Anderljung, Tom Rachman'}, {'url': 'http://arxiv.org/abs/2507.03049v1', 'authors': 'Ferran Gebellí, Anaís Garrell, Jan-Gerrit Habekost, Séverin Lemaignan, Stefan Wermter, Raquel Ros', 'published': '2025-07-03', 'title': 'Personalised Explanations in Long-term Human-Robot Interactions'}, {'url': 'http://arxiv.org/abs/2507.02517v1', 'authors': 'Vivek Yadav, Anugrah Jain', 'title': 'Detecting Multiple Diseases in Multiple Crops Using Deep Learning', 'published': '2025-07-03'}, {'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.03048v1', 'title': 'Monitoring of Static Fairness', 'authors': 'Thomas A. Henzinger, Mahyar Karimi, Konstantin Kueffner, Kaushik Mallik'}, {'title': 'IndianBailJudgments-1200: A Multi-Attribute Dataset for Legal NLP on Indian Bail Orders', 'url': 'http://arxiv.org/abs/2507.02506v1', 'published': '2025-07-03', 'authors': 'Sneha Deshmukh, Prathmesh Kamble'}, {'url': 'http://arxiv.org/abs/2507.03047v1', 'published': '2025-07-03', 'authors': 'Yutian Liu, Zhengyi Yang, Jiancan Wu, Xiang Wang', 'title': 'Counterfactual Tuning for Temporal Sensitivity Enhancement in Large Language Model-based Recommendation'}, {'title': 'Continual Gradient Low-Rank Projection Fine-Tuning for LLMs', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02503v1', 'authors': 'Chenxu Wang, Yilin Lyu, Zicheng Sun, Liping Jing'}, {'url': 'http://arxiv.org/abs/2507.02493v1', 'authors': 'Luca Parolari, Andrea Cherubini, Lamberto Ballan, Carlo Biffi', 'published': '2025-07-03', 'title': 'Temporally-Aware Supervised Contrastive Learning for Polyp Counting in Colonoscopy'}, {'title': 'CrowdTrack: A Benchmark for Difficult Multiple Pedestrian Tracking in Real Scenarios', 'url': 'http://arxiv.org/abs/2507.02479v1', 'published': '2025-07-03', 'authors': 'Teng Fu, Yuwen Chen, Zhuofan Chen, Mengyang Zhao, Bin Li, Xiangyang Xue'}, {'title': \"Red grape detection with accelerated artificial neural networks in the FPGA's programmable logic\", 'url': 'http://arxiv.org/abs/2507.02443v1', 'published': '2025-07-03', 'authors': 'Sandro Costa Magalhães, Marco Almeida, Filipe Neves dos Santos, António Paulo Moreira, Jorge Dias'}, {'url': 'http://arxiv.org/abs/2507.02442v1', 'authors': 'Moto Kamiura', 'published': '2025-07-03', 'title': 'The Gauss-Markov Adjunction: Categorical Semantics of Residuals in Supervised Learning'}, {'url': 'http://arxiv.org/abs/2507.03045v1', 'title': 'Optimisation Is Not What You Need', 'published': '2025-07-03', 'authors': 'Alfredo Ibias'}, {'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02436v1', 'authors': 'Namjung Kim, Dongseok Lee, Jongbin Yu, Sung Woong Cho, Dosung Lee, Yesol Park, Youngjoon Hong', 'title': 'Toward a Robust and Generalizable Metamaterial Foundation Model'}, {'url': 'http://arxiv.org/abs/2507.02424v1', 'title': 'CyberRAG: An agentic RAG cyber attack classification and reporting tool', 'published': '2025-07-03', 'authors': 'Francesco Blefari, Cristian Cosentino, Francesco Aurelio Pironti, Angelo Furfaro, Fabrizio Marozzo'}, {'url': 'http://arxiv.org/abs/2507.03043v1', 'published': '2025-07-03', 'title': 'K-Function: Joint Pronunciation Transcription and Feedback for Evaluating Kids Language Function', 'authors': 'Shuhe Li, Chenxu Guo, Jiachen Lian, Cheol Jun Cho, Wenshuo Zhao, Xuanru Zhou, Dingkun Zhou, Sam Wang, Grace Wang, Jingze Yang, Jingyi Xu, Ruohan Bao, Elise Brenner, Brandon In, Francesca Pei, Maria Luisa Gorno-Tempini, Gopala Anumanchipalli'}, {'authors': 'Zihan Tan, Suyuan Huang, Guancheng Wan, Wenke Huang, He Li, Mang Ye', 'published': '2025-07-03', 'title': 'S2FGL: Spatial Spectral Federated Graph Learning', 'url': 'http://arxiv.org/abs/2507.02409v1'}, {'title': 'Wildlife Target Re-Identification Using Self-supervised Learning in Non-Urban Settings', 'published': '2025-07-03', 'authors': 'Mufhumudzi Muthivhi, Terence L. van Zyl', 'url': 'http://arxiv.org/abs/2507.02403v1'}, {'authors': 'Yuyang Lou, Charles Li', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.03042v1', 'title': 'Dynamic Long Short-Term Memory Based Memory Storage For Long Horizon LLM Interaction'}, {'title': 'Beyond Spatial Frequency: Pixel-wise Temporal Frequency-based Deepfake Video Detection', 'authors': 'Taehoon Kim, Jongwook Choi, Yonghyun Jeong, Haeun Noh, Jaejun Yoo, Seungryul Baek, Jongwon Choi', 'url': 'http://arxiv.org/abs/2507.02398v1', 'published': '2025-07-03'}, {'published': '2025-07-03', 'authors': 'Jorge J. Tejero-Fernández, Alfonso Sánchez-Macián', 'url': 'http://arxiv.org/abs/2507.02390v1', 'title': 'Evaluating Language Models For Threat Detection in IoT Security Logs'}, {'url': 'http://arxiv.org/abs/2507.02379v1', 'published': '2025-07-03', 'title': 'An AI-native experimental laboratory for autonomous biomolecular engineering', 'authors': 'Mingyu Wu, Zhaoguo Wang, Jiabin Wang, Zhiyuan Dong, Jingkai Yang, Qingting Li, Tianyu Huang, Lei Zhao, Mingqiang Li, Fei Wang, Chunhai Fan, Haibo Chen'}, {'authors': 'Chung-ju Huang, Ziqi Zhang, Yinggui Wang, Binghui Wang, Tao Wei, Leye Wang', 'url': 'http://arxiv.org/abs/2507.02376v1', 'published': '2025-07-03', 'title': 'VeFIA: An Efficient Inference Auditing Framework for Vertical Federated Collaborative Software'}, {'title': 'Optimas: Optimizing Compound AI Systems with Globally Aligned Local Rewards', 'authors': 'Shirley Wu, Parth Sarthi, Shiyu Zhao, Aaron Lee, Herumb Shandilya, Adrian Mladenic Grobelnik, Nurendra Choudhary, Eddie Huang, Karthik Subbian, Linjun Zhang, Diyi Yang, James Zou, Jure Leskovec', 'url': 'http://arxiv.org/abs/2507.03041v1', 'published': '2025-07-03'}, {'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02358v2', 'authors': 'Anlin Zheng, Haochen Wang, Yucheng Zhao, Weipeng Deng, Tiancai Wang, Xiangyu Zhang, Xiaojuan Qi', 'title': 'Holistic Tokenizer for Autoregressive Image Generation'}, {'url': 'http://arxiv.org/abs/2507.02356v1', 'published': '2025-07-03', 'authors': 'JunHyeok Oh, Byung-Jun Lee', 'title': 'Offline Reinforcement Learning with Penalized Action Noise Injection'}, {'url': 'http://arxiv.org/abs/2507.02353v1', 'published': '2025-07-03', 'authors': 'Bowen Chen, Zhao Wang, Shingo Takamatsu', 'title': 'OMS: On-the-fly, Multi-Objective, Self-Reflective Ad Keyword Generation via LLM Agent'}, {'title': 'Two-Steps Neural Networks for an Automated Cerebrovascular Landmark Detection', 'authors': \"Rafic Nader, Vincent L'Allinec, Romain Bourcier, Florent Autrusseau\", 'url': 'http://arxiv.org/abs/2507.02349v1', 'published': '2025-07-03'}, {'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02345v1', 'title': 'HelixDesign-Antibody: A Scalable Production-Grade Platform for Antibody Design Built on HelixFold3', 'authors': 'Jie Gao, Jing Hu, Shanzhuo Zhang, Kunrui Zhu, Sheng Qian, Yueyang Huang, Xiaonan Zhang, Xiaomin Fang'}, {'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02342v1', 'authors': 'Changhun Kim, Yechan Mun, Sangchul Hahn, Eunho Yang', 'title': 'DeltaSHAP: Explaining Prediction Evolutions in Online Patient Monitoring with Shapley Values'}, {'title': 'ClustOpt: A Clustering-based Approach for Representing and Visualizing the Search Dynamics of Numerical Metaheuristic Optimization Algorithms', 'published': '2025-07-03', 'authors': 'Gjorgjina Cenikj, Gašper Petelin, Tome Eftimov', 'url': 'http://arxiv.org/abs/2507.02337v1'}, {'title': 'Cautious Next Token Prediction', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.03038v1', 'authors': 'Yizhou Wang, Lingzhi Zhang, Yue Bai, Mang Tik Chiu, Zhengmian Hu, Mingyuan Zhang, Qihua Dong, Yu Yin, Sohrab Amirghodsi, Yun Fu'}, {'url': 'http://arxiv.org/abs/2507.02331v1', 'published': '2025-07-03', 'authors': 'Ana Nikolikj, Mario Andrés Muñoz, Eva Tuba, Tome Eftimov', 'title': 'Tracing the Interactions of Modular CMA-ES Configurations Across Problem Landscapes'}, {'url': 'http://arxiv.org/abs/2507.02322v1', 'published': '2025-07-03', 'authors': 'Farida Siddiqi Prity, Mirza Raquib, Saydul Akbar Murad, Md. Jubayar Alam Rafi, Md. Khairul Bashar Bhuiyan, Anupam Kumar Bairagi', 'title': 'Neural Network-based Study for Rice Leaf Disease Recognition and Classification: A Comparative Analysis Between Feature-based Model and Direct Imaging Model'}, {'url': 'http://arxiv.org/abs/2507.02319v1', 'authors': 'Paolo Liberatore', 'published': '2025-07-03', 'title': 'Iterated belief revision: from postulates to abilities'}, {'url': 'http://arxiv.org/abs/2507.02314v2', 'title': 'MAGIC: Mask-Guided Diffusion Inpainting with Multi-Level Perturbations and Context-Aware Alignment for Few-Shot Anomaly Generation', 'published': '2025-07-03', 'authors': 'JaeHyuck Choi, MinJun Kim, JeHyeong Hong'}, {'authors': 'Alif Ashrafee, Jedrzej Kozal, Michal Wozniak, Bartosz Krawczyk', 'published': '2025-07-03', 'title': 'Holistic Continual Learning under Concept Drift with Adaptive Memory Realignment', 'url': 'http://arxiv.org/abs/2507.02310v1'}, {'title': 'Synthetic Heuristic Evaluation: A Comparison between AI- and Human-Powered Usability Evaluation', 'url': 'http://arxiv.org/abs/2507.02306v1', 'authors': 'Ruican Zhong, David W. McDonald, Gary Hsieh', 'published': '2025-07-03'}, {'authors': 'Dohoon Kim, Donghun Kang, Taesup Moon', 'title': 'DoMIX: An Efficient Framework for Exploiting Domain Knowledge in Fine-Tuning', 'url': 'http://arxiv.org/abs/2507.02302v1', 'published': '2025-07-03'}, {'authors': 'Zhaoyu Zhang, Lingyi Wang, Wei Wu, Fuhui Zhou, Qihui Wu', 'title': 'Knowledge Graph-Based Explainable and Generalized Zero-Shot Semantic Communications', 'url': 'http://arxiv.org/abs/2507.02291v1', 'published': '2025-07-03'}, {'title': 'Content filtering methods for music recommendation: A review', 'authors': 'Terence Zeng, Abhishek K. Umrawal', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02282v1'}, {'title': 'Spotlighting Partially Visible Cinematic Language for Video-to-Audio Generation via Self-distillation', 'authors': 'Feizhen Huang, Yu Wu, Yutian Lin, Bo Du', 'url': 'http://arxiv.org/abs/2507.02271v1', 'published': '2025-07-03'}, {'published': '2025-07-03', 'authors': 'Jialiang Wang, Junzhou Wang, Xin Liao', 'title': 'Adaptive Cubic Regularized Second-Order Latent Factor Analysis Model', 'url': 'http://arxiv.org/abs/2507.03036v1'}, {'title': 'Multi-Label Classification Framework for Hurricane Damage Assessment', 'authors': 'Zhangding Liu, Neda Mohammadi, John E. Taylor', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02265v1'}, {'url': 'http://arxiv.org/abs/2507.02259v1', 'title': 'MemAgent: Reshaping Long-Context LLM with Multi-Conv RL-based Memory Agent', 'published': '2025-07-03', 'authors': 'Hongli Yu, Tinghong Chen, Jiangtao Feng, Jiangjie Chen, Weinan Dai, Qiying Yu, Ya-Qin Zhang, Wei-Ying Ma, Jingjing Liu, Mingxuan Wang, Hao Zhou'}, {'authors': 'Jungkoo Kang', 'url': 'http://arxiv.org/abs/2507.02253v1', 'published': '2025-07-03', 'title': 'Scaling LLM Planning: NL2FLOW for Parametric Problem Generation and Rigorous Evaluation'}, {'authors': 'Zeyu Lei, Hongyuan Yu, Jinlin Wu, Zhen Chen', 'title': 'SurgVisAgent: Multimodal Agentic Model for Versatile Surgical Visual Enhancement', 'url': 'http://arxiv.org/abs/2507.02252v1', 'published': '2025-07-03'}, {'published': '2025-07-03', 'authors': 'Yiming Li, Shuo Shao, Yu He, Junfeng Guo, Tianwei Zhang, Zhan Qin, Pin-Yu Chen, Michael Backes, Philip Torr, Dacheng Tao, Kui Ren', 'url': 'http://arxiv.org/abs/2507.03034v1', 'title': 'Rethinking Data Protection in the (Generative) Artificial Intelligence Era'}, {'authors': 'Fangzhou Shi, Xiaopeng Ke, Xinye Xiong, Kexin Meng, Chang Men, Zhengdan Zhu', 'title': 'Order Acquisition Under Competitive Pressure: A Rapidly Adaptive Reinforcement Learning Approach for Ride-Hailing Subsidy Strategies', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02244v2'}, {'url': 'http://arxiv.org/abs/2507.03033v1', 'authors': 'Johnson Thomas, Ayush Mudgal, Wendao Liu, Nisten Tahiraj, Zeeshaan Mohammed, Dhruv Diddi', 'title': 'Preserving Privacy, Increasing Accessibility, and Reducing Cost: An On-Device Artificial Intelligence Model for Medical Transcription and Note Generation', 'published': '2025-07-03'}, {'published': '2025-07-03', 'title': 'On the Mathematical Impossibility of Safe Universal Approximators', 'url': 'http://arxiv.org/abs/2507.03031v1', 'authors': 'Jasper Yao'}, {'title': 'Understanding Trade offs When Conditioning Synthetic Data', 'authors': 'Brandon Trabucco, Qasim Wani, Benjamin Pikus, Vasu Sharma', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02217v1'}, {'published': '2025-07-03', 'authors': 'Gustavo C. Mangold, Heitor C. M. Fernandes, Mendeli H. Vainstein', 'title': \"Dilution, Diffusion and Symbiosis in Spatial Prisoner's Dilemma with Reinforcement Learning\", 'url': 'http://arxiv.org/abs/2507.02211v2'}, {'authors': 'Ranyang Zhou, Abeer Matar A. Almalky, Gamana Aragonda, Sabbir Ahmed, Filip Roth Trønnes-Christensen, Adnan Siraj Rakin, Shaahin Angizi', 'title': 'EIM-TRNG: Obfuscating Deep Neural Network Weights with Encoding-in-Memory True Random Number Generator via RowHammer', 'published': '2025-07-03', 'url': 'http://arxiv.org/abs/2507.02206v1'}, {'url': 'http://arxiv.org/abs/2507.02200v1', 'authors': 'Xiao Wang, Jingtao Jiang, Qiang Chen, Lan Chen, Lin Zhu, Yaowei Wang, Yonghong Tian, Jin Tang', 'title': 'ESTR-CoT: Towards Explainable and Accurate Event Stream based Scene Text Recognition with Chain-of-Thought Reasoning', 'published': '2025-07-02'}, {'title': 'Latent Chain-of-Thought? Decoding the Depth-Recurrent Transformer', 'published': '2025-07-02', 'authors': 'Wenquan Lu, Yuechuan Yang, Kyle Lee, Yanshu Li, Enqi Liu', 'url': 'http://arxiv.org/abs/2507.02199v1'}, {'title': 'Do Role-Playing Agents Practice What They Preach? Belief-Behavior Consistency in LLM-Based Simulations of Human Trust', 'url': 'http://arxiv.org/abs/2507.02197v1', 'published': '2025-07-02', 'authors': 'Amogh Mannekote, Adam Davies, Guohao Li, Kristy Elizabeth Boyer, ChengXiang Zhai, Bonnie J Dorr, Francesco Pinto'}, {'published': '2025-07-02', 'authors': 'Berkan Dokmeci, Qingyang Wu, Ben Athiwaratkun, Ce Zhang, Shuaiwen Leon Song, James Zou', 'url': 'http://arxiv.org/abs/2507.02173v1', 'title': 'Data Diversification Methods In Alignment Enhance Math Performance In LLMs'}, {'url': 'http://arxiv.org/abs/2507.02171v1', 'authors': 'Miroslav Cibula, Kristína Malinovská, Matthias Kerzel', 'title': 'Towards Bio-Inspired Robotic Trajectory Planning via Self-Supervised RNN', 'published': '2025-07-02'}, {'published': '2025-07-02', 'title': 'Deep Learning-Based Forecasting of Hotel KPIs: A Cross-City Analysis of Global Urban Markets', 'authors': 'C. J. Atapattu, Xia Cui, N. R Abeynayake', 'url': 'http://arxiv.org/abs/2507.03028v1'}, {'authors': 'Rodrigo Tuna, Carlos Soares', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.02166v1', 'title': 'Generating Large Semi-Synthetic Graphs of Any Size'}, {'url': 'http://arxiv.org/abs/2507.03026v1', 'published': '2025-07-02', 'title': 'Generalized Adaptive Transfer Network: Enhancing Transfer Learning in Reinforcement Learning Across Domains', 'authors': 'Abhishek Verma, Nallarasan V, Balaraman Ravindran'}, {'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.02152v1', 'authors': 'Disa Sariola, Patrick Button, Aron Culotta, Nicholas Mattei', 'title': 'The Illusion of Fairness: Auditing Fairness Interventions with Audit Studies'}, {'url': 'http://arxiv.org/abs/2507.02145v1', 'title': 'Reasoning or Not? A Comprehensive Evaluation of Reasoning LLMs for Dialogue Summarization', 'published': '2025-07-02', 'authors': 'Keyan Jin, Yapeng Wang, Leonel Santos, Tao Fang, Xu Yang, Sio Kei Im, Hugo Gonçalo Oliveira'}, {'published': '2025-07-02', 'title': 'When LLMs Disagree: Diagnosing Relevance Filtering Bias and Retrieval Divergence in SDG Search', 'authors': 'William A. Ingram, Bipasha Banerjee, Edward A. Fox', 'url': 'http://arxiv.org/abs/2507.02139v1'}, {'url': 'http://arxiv.org/abs/2507.02125v1', 'title': 'Can Artificial Intelligence solve the blockchain oracle problem? Unpacking the Challenges and Possibilities', 'authors': 'Giulio Caldarelli', 'published': '2025-07-02'}, {'title': 'Resolving Turbulent Magnetohydrodynamics: A Hybrid Operator-Diffusion Framework', 'authors': 'Semih Kacmaz, E. A. Huerta, Roland Haas', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.02106v1'}, {'url': 'http://arxiv.org/abs/2507.02103v1', 'authors': 'Daniel Durstewitz, Bruno Averbeck, Georgia Koppe', 'title': 'What Neuroscience Can Teach AI About Learning in Continuously Changing Environments', 'published': '2025-07-02'}, {'title': 'Energy-Based Transformers are Scalable Learners and Thinkers', 'url': 'http://arxiv.org/abs/2507.02092v1', 'published': '2025-07-02', 'authors': 'Alexi Gladstone, Ganesh Nanduru, Md Mofijul Islam, Peixuan Han, Hyeonjeong Ha, Aman Chadha, Yilun Du, Heng Ji, Jundong Li, Tariq Iqbal'}, {'title': 'Completion of the DrugMatrix Toxicogenomics Database using 3-Dimensional Tensors', 'url': 'http://arxiv.org/abs/2507.03024v1', 'authors': 'Tan Nguyen, Guojing Cong', 'published': '2025-07-02'}, {'published': '2025-07-02', 'title': 'GeoAda: Efficiently Finetune Geometric Diffusion Models with Equivariant Adapters', 'authors': 'Wanjia Zhao, Jiaqi Han, Siyi Gu, Mingjian Jiang, James Zou, Stefano Ermon', 'url': 'http://arxiv.org/abs/2507.02085v1'}, {'url': 'http://arxiv.org/abs/2507.02083v1', 'authors': 'Haonan Duan, Stephen Zhewen Lu, Caitlin Fiona Harrigan, Nishkrit Desai, Jiarui Lu, Michał Koziarski, Leonardo Cotta, Chris J. Maddison', 'title': 'Measuring Scientific Capabilities of Language Models with a Systems Biology Dry Lab', 'published': '2025-07-02'}, {'authors': 'Mohammad Ali Alomrani, Yingxue Zhang, Derek Li, Qianyi Sun, Soumyasundar Pal, Zhanguang Zhang, Yaochen Hu, Rohan Deepak Ajwani, Antonios Valkanas, Raika Karimi, Peng Cheng, Yunzhou Wang, Pengyi Liao, Hanrui Huang, Bin Wang, Jianye Hao, Mark Coates', 'url': 'http://arxiv.org/abs/2507.02076v1', 'published': '2025-07-02', 'title': 'Reasoning on a Budget: A Survey of Adaptive and Controllable Test-Time Compute in LLMs'}, {'title': 'Large Language Models for Crash Detection in Video: A Survey of Methods, Datasets, and Challenges', 'url': 'http://arxiv.org/abs/2507.02074v1', 'published': '2025-07-02', 'authors': 'Sanjeda Akter, Ibne Farabi Shihab, Anuj Sharma'}, {'authors': 'Nikita Bhedasgaonkar, Rushikesh K. Joshi', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.02073v1', 'title': 'HCVR: A Hybrid Approach with Correlation-aware Voting Rules for Feature Selection'}, {'title': 'MGC: A Compiler Framework Exploiting Compositional Blindness in Aligned LLMs for Malware Generation', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.02057v1', 'authors': 'Lu Yan, Zhuo Zhang, Xiangzhe Xu, Shengwei An, Guangyu Shen, Zhou Xuan, Xuan Chen, Xiangyu Zhang'}, {'title': 'AC-DiT: Adaptive Coordination Diffusion Transformer for Mobile Manipulation', 'authors': 'Sixiang Chen, Jiaming Liu, Siyuan Qian, Han Jiang, Lily Li, Renrui Zhang, Zhuoyang Liu, Chenyang Gu, Chengkai Hou, Pengwei Wang, Zhongyuan Wang, Shanghang Zhang', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.01961v3'}, {'url': 'http://arxiv.org/abs/2507.01957v1', 'authors': 'Zhuoyang Zhang, Luke J. Huang, Chengyue Wu, Shang Yang, Kelly Peng, Yao Lu, Song Han', 'published': '2025-07-02', 'title': 'Locality-aware Parallel Decoding for Efficient Autoregressive Image Generation'}, {'published': '2025-07-02', 'title': 'How Well Does GPT-4o Understand Vision? Evaluating Multimodal Foundation Models on Standard Computer Vision Tasks', 'authors': 'Rahul Ramachandran, Ali Garjani, Roman Bachmann, Andrei Atanov, Oğuzhan Fatih Kar, Amir Zamir', 'url': 'http://arxiv.org/abs/2507.01955v1'}, {'url': 'http://arxiv.org/abs/2507.01939v1', 'authors': 'Xiaosheng Zhao, Yang Huang, Guirong Xue, Xiao Kong, Jifeng Liu, Xiaoyu Tang, Timothy C. Beers, Yuan-Sen Ting, A-Li Luo', 'title': 'SpecCLIP: Aligning and Translating Spectroscopic Measurements for Stars', 'published': '2025-07-02'}, {'url': 'http://arxiv.org/abs/2507.01931v1', 'authors': 'Md Sazzadul Islam Ridoy, Sumi Akter, Md. Aminur Rahman', 'published': '2025-07-02', 'title': 'Adaptability of ASR Models on Low-Resource Language: A Comparative Study of Whisper and Wav2Vec-BERT on Bangla'}, {'authors': 'Samirah Bakker, Yao Ma, Seyed Sahand Mohammadi Ziabari', 'title': 'Exploring a Hybrid Deep Learning Approach for Anomaly Detection in Mental Healthcare Provider Billing: Addressing Label Scarcity through Semi-Supervised Anomaly Detection', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.01924v1'}, {'authors': 'Christian Bongiorno, Efstratios Manolakis, Rosario Nunzio Mantegna', 'url': 'http://arxiv.org/abs/2507.01918v1', 'title': 'End-to-End Large Portfolio Optimization for Variance Minimization with Neural Networks through Covariance Cleaning', 'published': '2025-07-02'}, {'authors': 'Chengao Li, Hanyu Zhang, Yunkun Xu, Hongyan Xue, Xiang Ao, Qing He', 'published': '2025-07-02', 'title': 'Gradient-Adaptive Policy Optimization: Towards Multi-Objective Alignment of Large Language Models', 'url': 'http://arxiv.org/abs/2507.01915v1'}, {'authors': 'Qiguang Chen, Mingda Yang, Libo Qin, Jinhao Liu, Zheng Yan, Jiannan Guan, Dengyun Peng, Yiyan Ji, Hanjing Li, Mengkang Hu, Yimeng Zhang, Yihao Liang, Yuhang Zhou, Jiaqi Wang, Zhi Chen, Wanxiang Che', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.01903v1', 'title': 'AI4Research: A Survey of Artificial Intelligence for Scientific Research'}, {'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.01875v1', 'authors': 'Gastón García González, Pedro Casas, Emilio Martínez, Alicia Fernández', 'title': 'Towards Foundation Auto-Encoders for Time-Series Anomaly Detection'}, {'authors': 'Sanjay Krishna Anbalagan, Xinrui Nie, Umesh Mohan, Vijay Kumar Kanamarlapudi, Anughna Kommalapati, Xiaodan Zhao', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.01862v1', 'title': 'Bridging UI Design and chatbot Interactions: Applying Form-Based Principles to Conversational Agents'}, {'authors': 'Yi-Dong Shen, Thomas Eiter', 'published': '2025-07-02', 'title': 'Refining Gelfond Rationality Principle Towards More Comprehensive Foundational Principles for Answer Set Semantics', 'url': 'http://arxiv.org/abs/2507.01833v1'}, {'authors': 'Tristan Torchet, Christian Metzner, Laura Kriener, Melika Payvand', 'published': '2025-07-02', 'title': 'mGRADE: Minimal Recurrent Gating Meets Delay Convolutions for Lightweight Sequence Modeling', 'url': 'http://arxiv.org/abs/2507.01829v1'}, {'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.01825v1', 'title': 'MILP-SAT-GNN: Yet Another Neural SAT Solver', 'authors': 'Franco Alberto Cardillo, Hamza Khyari, Umberto Straccia'}, {'title': 'Empowering Manufacturers with Privacy-Preserving AI Tools: A Case Study in Privacy-Preserving Machine Learning to Solve Real-World Problems', 'published': '2025-07-02', 'authors': 'Xiaoyu Ji, Jessica Shorland, Joshua Shank, Pascal Delpe-Brice, Latanya Sweeney, Jan Allebach, Ali Shakouri', 'url': 'http://arxiv.org/abs/2507.01808v1'}, {'authors': 'Reza Arabpour, Haitz Sáez de Ocáriz Borde, Anastasis Kratsios', 'url': 'http://arxiv.org/abs/2507.01806v1', 'title': 'LoRA Fine-Tuning Without GPUs: A CPU-Efficient Meta-Generation Framework for LLMs', 'published': '2025-07-02'}, {'published': '2025-07-02', 'title': 'How Do Vision-Language Models Process Conflicting Information Across Modalities?', 'url': 'http://arxiv.org/abs/2507.01790v1', 'authors': 'Tianze Hua, Tian Yun, Ellie Pavlick'}, {'title': 'Are Vision Transformer Representations Semantically Meaningful? A Case Study in Medical Imaging', 'url': 'http://arxiv.org/abs/2507.01788v1', 'authors': 'Montasir Shams, Chashi Mahiul Islam, Shaeke Salman, Phat Tran, Xiuwen Liu', 'published': '2025-07-02'}, {'published': '2025-07-02', 'authors': 'Jord Nguyen, Khiem Hoang, Carlo Leonardo Attubato, Felix Hofstätter', 'title': 'Probing Evaluation Awareness of Language Models', 'url': 'http://arxiv.org/abs/2507.01786v1'}, {'authors': 'Zhixun Chen, Ping Guo, Wenhan Han, Yifan Zhang, Binbin Liu, Haobin Lin, Fengze Liu, Yan Zhao, Bingni Zhang, Taifeng Wang, Yin Zheng, Meng Fang', 'title': 'MuRating: A High Quality Data Selecting Approach to Multilingual Large Language Model Pretraining', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.01785v1'}, {'published': '2025-07-02', 'title': 'BranchNet: A Neuro-Symbolic Learning Framework for Structured Multi-Class Classification', 'url': 'http://arxiv.org/abs/2507.01781v1', 'authors': 'Dalia Rodríguez-Salas, Christian Riess'}, {'authors': 'Guanglu Zhang, Qihang Shan, Jonathan Cagan', 'title': 'GPU-based complete search for nonlinear minimization subject to bounds', 'url': 'http://arxiv.org/abs/2507.01770v2', 'published': '2025-07-02'}, {'authors': 'Nicolas Salvy, Hugues Talbot, Bertrand Thirion', 'url': 'http://arxiv.org/abs/2507.01761v1', 'published': '2025-07-02', 'title': 'Enhanced Generative Model Evaluation with Clipped Density and Coverage'}, {'authors': 'Ismail Labiad, Mathurin Videau, Matthieu Kowalski, Marc Schoenauer, Alessandro Leite, Julia Kempe, Olivier Teytaud', 'title': 'Tuning without Peeking: Provable Privacy and Generalization Bounds for LLM Post-Training', 'url': 'http://arxiv.org/abs/2507.01752v1', 'published': '2025-07-02'}, {'url': 'http://arxiv.org/abs/2507.01749v1', 'authors': 'Arash Dehghan, Mucahit Cevik, Merve Bodur, Bissan Ghaddar', 'title': 'Joint Matching and Pricing for Crowd-shipping with In-store Customers', 'published': '2025-07-02'}, {'title': 'ECCV 2024 W-CODA: 1st Workshop on Multimodal Perception and Comprehension of Corner Cases in Autonomous Driving', 'published': '2025-07-02', 'authors': 'Kai Chen, Ruiyuan Gao, Lanqing Hong, Hang Xu, Xu Jia, Holger Caesar, Dengxin Dai, Bingbing Liu, Dzmitry Tsishkou, Songcen Xu, Chunjing Xu, Qiang Xu, Huchuan Lu, Dit-Yan Yeung', 'url': 'http://arxiv.org/abs/2507.01735v1'}, {'published': '2025-07-02', 'authors': 'Yingjie Niu, Mingchuan Zhao, Valerio Poti, Ruihai Dong', 'title': 'NGAT: A Node-level Graph Attention Network for Long-term Stock Prediction', 'url': 'http://arxiv.org/abs/2507.02018v1'}, {'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.01719v1', 'title': 'Towards culturally-appropriate conversational AI for health in the majority world: An exploratory study with citizens and professionals in Latin America', 'authors': 'Dorian Peters, Fernanda Espinoza, Marco da Re, Guido Ivetta, Luciana Benotti, Rafael A. Calvo'}, {'published': '2025-07-02', 'title': 'Agent Ideate: A Framework for Product Idea Generation from Patents Using Agentic AI', 'authors': 'Gopichand Kanumolu, Ashok Urlana, Charaka Vinayak Kumar, Bala Mallikarjunarao Garlapati', 'url': 'http://arxiv.org/abs/2507.01717v1'}, {'url': 'http://arxiv.org/abs/2507.01702v1', 'published': '2025-07-02', 'authors': 'Zixin Chen, Hongzhan Lin, Kaixin Li, Ziyang Luo, Zhen Ye, Guang Chen, Zhiyong Huang, Jing Ma', 'title': 'AdamMeme: Adaptively Probe the Reasoning Capacity of Multimodal Large Language Models on Harmfulness'}, {'authors': 'Bochen Han, Songmao Zhang', 'published': '2025-07-02', 'title': 'Exploring Advanced LLM Multi-Agent Systems Based on Blackboard Architecture', 'url': 'http://arxiv.org/abs/2507.01701v1'}, {'title': 'Relational Causal Discovery with Latent Confounders', 'url': 'http://arxiv.org/abs/2507.01700v1', 'published': '2025-07-02', 'authors': 'Andrea Piras, Matteo Negro, Ragib Ahsan, David Arbour, Elena Zheleva'}, {'title': 'GPT, But Backwards: Exactly Inverting Language Model Outputs', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.01693v1', 'authors': 'Adrians Skapars, Edoardo Manino, Youcheng Sun, Lucas C. Cordeiro'}, {'authors': 'Zeyu Huang, Tianhao Cheng, Zihan Qiu, Zili Wang, Yinghui Xu, Edoardo M. Ponti, Ivan Titov', 'url': 'http://arxiv.org/abs/2507.01679v1', 'published': '2025-07-02', 'title': 'Blending Supervised and Reinforcement Fine-Tuning with Prefix Sampling'}, {'published': '2025-07-02', 'title': 'Deep Recommender Models Inference: Automatic Asymmetric Data Flow Optimization', 'url': 'http://arxiv.org/abs/2507.01676v1', 'authors': 'Giuseppe Ruggeri, Renzo Andri, Daniele Jahier Pagliari, Lukas Cavigelli'}, {'title': 'Comparing Optimization Algorithms Through the Lens of Search Behavior Analysis', 'authors': 'Gjorgjina Cenikj, Gašper Petelin, Tome Eftimov', 'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.01668v1'}, {'authors': 'Zhenyu Han, Ansheng You, Haibo Wang, Kui Luo, Guang Yang, Wenqi Shi, Menglong Chen, Sicheng Zhang, Zeshun Lan, Chunshi Deng, Huazhong Ji, Wenjie Liu, Yu Huang, Yixiang Zhang, Chenyi Pan, Jing Wang, Xin Huang, Chunsheng Li, Jianping Wu', 'published': '2025-07-02', 'title': 'AsyncFlow: An Asynchronous Streaming RL Framework for Efficient LLM Post-Training', 'url': 'http://arxiv.org/abs/2507.01663v1'}, {'url': 'http://arxiv.org/abs/2507.01652v1', 'authors': 'Yuxin Mao, Zhen Qin, Jinxing Zhou, Hui Deng, Xuyang Shen, Bin Fan, Jing Zhang, Yiran Zhong, Yuchao Dai', 'published': '2025-07-02', 'title': 'Autoregressive Image Generation with Linear Complexity: A Spatial-Aware Decay Perspective'}, {'url': 'http://arxiv.org/abs/2507.01649v1', 'title': 'GradMetaNet: An Equivariant Architecture for Learning on Gradients', 'published': '2025-07-02', 'authors': 'Yoav Gelberg, Yam Eitan, Aviv Navon, Aviv Shamsian, Theo, Putterman, Michael Bronstein, Haggai Maron'}, {'url': 'http://arxiv.org/abs/2507.01638v1', 'title': 'Customized Exploration of Landscape Features Driving Multi-Objective Combinatorial Optimization Performance', 'authors': 'Ana Nikolikj, Gabriela Ochoa, Tome Eftimov', 'published': '2025-07-02'}, {'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.03013v1', 'title': 'Challenges for AI in Multimodal STEM Assessments: a Human-AI Comparison', 'authors': 'Aymeric de Chillaz, Anna Sotnikova, Patrick Jermann, Antoine Bosselut'}, {'title': 'Depth Anything at Any Condition', 'published': '2025-07-02', 'authors': 'Boyuan Sun, Modi Jin, Bowen Yin, Qibin Hou', 'url': 'http://arxiv.org/abs/2507.01634v1'}, {'url': 'http://arxiv.org/abs/2507.02016v1', 'published': '2025-07-02', 'authors': 'Cong Wang, Roberto Calandra, Verena Klös', 'title': 'Effective Explanations for Belief-Desire-Intention Robots: When and What to Explain'}, {'authors': 'Camille Billouard, Dawa Derksen, Alexandre Constantin, Bruno Vallet', 'title': 'Tile and Slide : A New Framework for Scaling NeRF from Local to Global 3D Earth Observation', 'url': 'http://arxiv.org/abs/2507.01631v1', 'published': '2025-07-02'}, {'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.01630v1', 'title': 'Prompt Guidance and Human Proximal Perception for HOT Prediction with Regional Joint Loss', 'authors': 'Yuxiao Wang, Yu Lei, Zhenao Wei, Weiying Xue, Xinyu Jiang, Nan Zhuang, Qi Liu'}, {'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.01616v1', 'title': 'Enhanced Influence-aware Group Recommendation for Online Media Propagation', 'authors': 'Chengkun He, Xiangmin Zhou, Chen Wang, Longbing Cao, Jie Shao, Xiaodong Li, Guang Xu, Carrie Jinqiu Hu, Zahir Tari'}, {'url': 'http://arxiv.org/abs/2507.01607v1', 'title': 'Survivability of Backdoor Attacks on Unconstrained Face Recognition Systems', 'authors': 'Quentin Le Roux, Yannick Teglia, Teddy Furon, Philippe Loubet-Moundi, Eric Bourbao', 'published': '2025-07-02'}, {'authors': 'Julia Lademann, Jannik Henze, Nadine Honke, Caroline Wollny, Sebastian Becker-Genschow', 'title': \"Teacher training in the age of AI: Impact on AI Literacy and Teachers' Attitudes\", 'url': 'http://arxiv.org/abs/2507.03011v1', 'published': '2025-07-02'}, {'title': 'Data Agent: A Holistic Architecture for Orchestrating Data+AI Ecosystems', 'url': 'http://arxiv.org/abs/2507.01599v1', 'published': '2025-07-02', 'authors': 'Zhaoyan Sun, Jiayi Wang, Xinyang Zhao, Jiachi Wang, Guoliang Li'}, {'url': 'http://arxiv.org/abs/2507.01597v1', 'title': 'T3DM: Test-Time Training-Guided Distribution Shift Modelling for Temporal Knowledge Graph Reasoning', 'published': '2025-07-02', 'authors': 'Yuehang Si, Zefan Zeng, Jincai Huang, Qing Cheng'}, {'published': '2025-07-02', 'url': 'http://arxiv.org/abs/2507.01590v1', 'authors': 'Ameer Hamza, Zuhaib Hussain But, Umar Arif, Samiya, M. Abdullah Asad, Muhammad Naeem', 'title': 'Autonomous AI Surveillance: Multimodal Deep Learning for Cognitive and Behavioral Monitoring'}, {'url': 'http://arxiv.org/abs/2507.01582v1', 'authors': 'Jing Luo, Xinyu Yang, Jie Wei', 'title': 'Exploring Classical Piano Performance Generation with Expressive Music Variational AutoEncoder', 'published': '2025-07-02'}, {'url': 'http://arxiv.org/abs/2507.01563v1', 'authors': 'Marco Giordano, Stefano Giacomelli, Claudia Rinaldi, Fabio Graziosi', 'published': '2025-07-02', 'title': 'Real-Time Emergency Vehicle Siren Detection with Efficient CNNs on Embedded Hardware'}, {'title': 'Self-Guided Process Reward Optimization with Redefined Step-wise Advantage for Process Reinforcement Learning', 'url': 'http://arxiv.org/abs/2507.01551v2', 'authors': 'Wu Fei, Hao Kong, Shuxian Liang, Yang Lin, Yibo Yang, Jing Tang, Lei Chen, Xiansheng Hua', 'published': '2025-07-02'}, {'url': 'http://arxiv.org/abs/2507.01548v2', 'published': '2025-07-02', 'authors': 'Wen Zhan, Ziqun Hua, Peiyue Lin, Yunfei Chen', 'title': 'Crafting Hanzi as Narrative Bridges: An AI Co-Creation Workshop for Elderly Migrants'}]}\n"
     ]
    }
   ],
   "source": [
    "print(vectorstore._collection.get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9bf7e4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6c45e0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Learning_project\\.venv\\Lib\\site-packages\\langsmith\\client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub \n",
    "prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "be98507f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    "    google_api_key=\"XXXXX\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0d4d9b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dd20777d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1137197b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {'context':retriever | format_docs,\"question\":RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "47bdf9d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One novel architecture proposed is LILITH, which combines developmental training of modular language models with brain-inspired token-based communication protocols. Another is the 2-simplicial Transformer, which generalizes standard dot-product attention to trilinear functions. Additionally, research suggests exploring models with frozen embedding layers derived from visual structures, challenging the traditional view of trainable embeddings.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"What novel architectures are being proposed for language models?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a8795e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
